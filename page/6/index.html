<!DOCTYPE html>
<html lang=ja>
<head>
    <meta charset="utf-8">
    
    <title>memo-blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="This is just a memo">
<meta property="og:type" content="website">
<meta property="og:title" content="memo-blog">
<meta property="og:url" content="https://dobachi.github.io/memo-blog/page/6/index.html">
<meta property="og:site_name" content="memo-blog">
<meta property="og:description" content="This is just a memo">
<meta property="og:locale" content="ja">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="memo-blog">
<meta name="twitter:description" content="This is just a memo">
    

    
        <link rel="alternate" href="/" title="memo-blog" type="application/atom+xml" />
    

    

    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fontawesome.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fa-brands.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fa-solid.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/memo-blog/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/memo-blog/css/style.css">

    <script src="/memo-blog/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/memo-blog/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/memo-blog/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-155235180-1', 'auto');
ga('send', 'pageview');

</script>
    
    
    


</head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/memo-blog/" id="logo">
                <i class="logo"></i>
                <span class="site-title">memo-blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/memo-blog/.">Home</a>
                
                    <a class="main-nav-link" href="/memo-blog/archives">Archives</a>
                
                    <a class="main-nav-link" href="/memo-blog/categories">Categories</a>
                
                    <a class="main-nav-link" href="/memo-blog/tags">Tags</a>
                
                    <a class="main-nav-link" href="/memo-blog/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/memo-blog/css/images/avatar.png" />
                            <i class="fas fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="検索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fas fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '投稿',
            PAGES: 'Pages',
            CATEGORIES: 'カテゴリ',
            TAGS: 'タグ',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/memo-blog/',
        CONTENT_URL: '/memo-blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/memo-blog/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/memo-blog/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="検索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile" class="profile-fixed">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/memo-blog/css/images/avatar.png" />
            <h2 id="name">dobachi</h2>
            <h3 id="title">man of leisure</h3>
            <span id="location"><i class="fas fa-map-marker-alt" style="padding-right: 5px"></i>Tokyo, Japan</span>
            <a id="follow" target="_blank" href="https://github.com/dobachi">フォローする</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                210
                <span>投稿</span>
            </div>
            <div class="article-info-block">
                230
                <span>タグ</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="http://github.com/dobachi" target="_blank" title="github" class=tooltip>
                            <i class="fab fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/memo-blog/" target="_blank" title="rss" class=tooltip>
                            <i class="fab fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main">
    <article id="post-Storage-Layer-Software-for-Machine-Learning" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/">機械学習向けのFeature StoreないしStorage Layer Software</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/">
            <time datetime="2020-04-10T02:43:58.000Z" itemprop="datePublished">2020-04-10</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/Storage-Layer-Software/">Storage Layer Software</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a>
<ul>
<li><a href="#プロダクト" id="toc-プロダクト">プロダクト</a></li>
<li><a href="#企業アーキテクチャ" id="toc-企業アーキテクチャ">企業アーキテクチャ</a></li>
<li><a href="#まとめ" id="toc-まとめ">まとめ</a></li>
</ul></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#傾向" id="toc-傾向">傾向</a></li>
<li><a href="#feature-storeとして挙げられている特徴機能" id="toc-feature-storeとして挙げられている特徴機能">Feature
Storeとして挙げられている特徴・機能</a>
<ul>
<li><a href="#主にfeatuer-storeとしての特徴" id="toc-主にfeatuer-storeとしての特徴">主に、featuer
storeとしての特徴</a></li>
<li><a href="#rawデータストアを含めた特徴" id="toc-rawデータストアを含めた特徴">rawデータストアを含めた特徴</a></li>
<li><a href="#特徴量エンジニアリングの例" id="toc-特徴量エンジニアリングの例">特徴量エンジニアリングの例</a></li>
<li><a href="#feature-storeにおける画像の取扱は" id="toc-feature-storeにおける画像の取扱は">feature
storeにおける画像の取扱は？</a></li>
</ul></li>
<li><a href="#feastにおけるデータフロー概要" id="toc-feastにおけるデータフロー概要">Feastにおけるデータフロー概要</a></li>
<li><a href="#hopsworksにおけるfeature-store" id="toc-hopsworksにおけるfeature-store">hopsworksにおけるfeature
store</a></li>
<li><a href="#ストレージ製品の動向" id="toc-ストレージ製品の動向">ストレージ製品の動向</a>
<ul>
<li><a href="#netapp" id="toc-netapp">Netapp</a></li>
<li><a href="#dell-emc" id="toc-dell-emc">Dell EMC</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<h2><span id="プロダクト">プロダクト</span></h2>
<ul>
<li>Feast
<ul>
<li><a href="https://github.com/gojek/feast" target="_blank" rel="noopener">Feast</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li>メモ
<ul>
<li>Feature Store for Machine Learning https://feast.dev</li>
<li>GoJek/Google released Feast in early 2019 and it is built around
Google Cloud services: Big Query (offline) and Big Table (online) and
Redis (low-latency), using Beam for feature engineering.</li>
</ul></li>
</ul></li>
<li><a href="https://delta.io/" target="_blank" rel="noopener">Delta Lake</a>
<ul>
<li>メモ
<ul>
<li>Delta Lake is an open-source storage layer that brings ACID
transactions to Apache Spark™ and big data workloads.</li>
</ul></li>
</ul></li>
<li>Hopsworks
<ul>
<li>メモ
<ul>
<li>The Platform for Data-Intensive AI</li>
<li>Feature Storeに限らない</li>
</ul></li>
<li><a href="https://www.hopsworks.ai/" target="_blank" rel="noopener">Hopsworks</a></li>
<li><a href="https://github.com/logicalclocks/hopsworks" target="_blank" rel="noopener">HopsworksのGitHub</a></li>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li><a href="https://metaflow.org/" target="_blank" rel="noopener">Metaflow</a>
<ul>
<li>メモ
<ul>
<li>Netflixの機械学習パイプライン管理用のライブラリ</li>
<li>特徴量を保存するためのライブラリを内容（割と素朴に保存する仕組み）</li>
</ul></li>
</ul></li>
<li><a href="https://github.com/uber/petastorm" target="_blank" rel="noopener">Petastorm</a>
<ul>
<li>メモ
<ul>
<li>いろいろなフレームワークから利用できるデータ入出力のためのライブラリ
Parquetを利用する。</li>
</ul></li>
</ul></li>
<li><a href="https://www.zadara.com/" target="_blank" rel="noopener">Zadara</a>
<ul>
<li>どちらかということ純粋にストレージ</li>
</ul></li>
<li>Netapp
<ul>
<li>メモ
<ul>
<li>いわゆる「ストレージ」におけるアプローチの例</li>
</ul></li>
<li><a href="https://www.netapp.com/us/solutions/applications/ai-deep-learning.aspx" target="_blank" rel="noopener">Accelerated
AI and deep learning pipelines across edge, core, and cloud</a></li>
<li><a href="https://www.netapp.com/us/media/wp-7271.pdf" target="_blank" rel="noopener">Edge to Core
to Cloud Architecture for AI</a></li>
</ul></li>
<li>Dell EMC
<ul>
<li>メモ
<ul>
<li>単独の技術というより、コンピューティングと合わせてのソリューション、サーバ</li>
</ul></li>
<li><a href="https://www.dellemc.com/resources/en-us/asset/analyst-reports/products/storage/h17841_ar_enterprise_machine_and_deep_learning_with_intelligent_storage.pdf" target="_blank" rel="noopener">ENTERPRISE
MACHINE &amp; DEEP LEARNING WITH INTELLIGENT STORAGE</a></li>
</ul></li>
<li>IBM
<ul>
<li>メモ
<ul>
<li>Watsonの名のもとに様々なソリューションを集結</li>
</ul></li>
<li><a href="https://www.ibm.com/it-infrastructure/storage/ai-infrastructure" target="_blank" rel="noopener">IBM
Storage for AI and big data</a></li>
<li><a href="https://www.ibm.com/downloads/cas/JPKRD1R0" target="_blank" rel="noopener">IBM Spectrum
Storage for AI with Power Systems</a></li>
</ul></li>
<li>Kafka
<ul>
<li>メモ
<ul>
<li>推論用のイベントをやり取りするためのハブとして用いる</li>
<li>最近開発されているTiered
Storage機能を利用し、長期保存用のストレージと組み合わせた使い方が可能になる
つまり学習データ（ヒストリカルデータ）を含めて、Kafkaでデータをサーブすることが描かれている。</li>
</ul></li>
<li><a href="https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/" target="_blank" rel="noopener">Streaming
Machine Learning with Tiered Storage and Without a Data Lake</a>
<ul>
<li>Tiered Storageの紹介と機械学習への応用例</li>
</ul></li>
</ul></li>
<li><a href="https://www.usenix.org/conference/atc19/presentation/liang" target="_blank" rel="noopener">Cognitive
SSD</a>
<ul>
<li>メモ
<ul>
<li>USENIX ATC'19</li>
<li>非構造データを記憶装置の階層間で移動するのが無駄。そこでSSDのNAND
Flash横にDeep Learning用、グラフ検索用のエンジンを積む</li>
</ul></li>
</ul></li>
<li>Ignite
<ul>
<li><a href="https://ignite.apache.org/features/machinelearning.html" target="_blank" rel="noopener">Ignite
Machine Larning</a></li>
</ul></li>
<li>Bandana
<ul>
<li><a href="https://research.fb.com/publications/bandana-using-non-volatile-memory-for-storing-deep-learning-models/" target="_blank" rel="noopener">Bandana
Using Non-Volatile Memory for Storing Deep Learning Models</a></li>
<li>Facebook Research</li>
<li>深層学習モデルをストアするためのストレージの提案。
NVMを活用。一緒に読み込まれるベクトルを物理的近く配置し、プリフェッチの効果を向上。
キャッシュポリシーをシミュレーションに基づいて最適化。</li>
</ul></li>
</ul>
<h2><span id="企業アーキテクチャ">企業アーキテクチャ</span></h2>
<ul>
<li><a href="https://www.slideshare.net/Alluxio/pinterest-big-data-machine-learning-platform-at-pinterest" target="_blank" rel="noopener">Pinterest
- Big Data Machine Learning Platform at Pinterest</a></li>
<li>Michelangelo
<ul>
<li>メモ
<ul>
<li>UberのMLプラットフォーム。必ずしもFeature Storeに限らない。</li>
<li>Feature
Storeに関して特筆すると、「online」と「offline」のデータを統合して扱う、という発想。</li>
</ul></li>
<li><a href="https://eng.uber.com/michelangelo-machine-learning-platform/" target="_blank" rel="noopener">Michelangelo_0</a></li>
<li><a href="https://eng.uber.com/michelangelo-machine-learning-model-representation/" target="_blank" rel="noopener">Michelangelo_1</a></li>
</ul></li>
<li>Twiter</li>
<li>特徴量をライブラリとして保持、アプリケーションから使いやすくした？</li>
<li>Comcas
<ul>
<li>ReidsをFeature Storeに利用</li>
</ul></li>
<li>Pinterest
<ul>
<li><a href="https://www.slideshare.net/Alluxio/pinterest-big-data-machine-learning-platform-at-pinterest" target="_blank" rel="noopener">Pinterest
- Big Data Machine Learning Platform at Pinterest</a></li>
</ul></li>
<li>Zipline
<ul>
<li>メモ
<ul>
<li>特徴量エンジニアリングパイプラインを補助するライブラリ</li>
<li>バックフィルが特徴？</li>
<li>OSSではないように見える</li>
</ul></li>
<li><a href="https://www.slideshare.net/KarthikMurugesan2/airbnb-zipline-airbnbs-machine-learning-data-management-platform" target="_blank" rel="noopener">Zipline_0</a></li>
<li><a href="https://www.slideshare.net/databricks/ziplineairbnbs-declarative-feature-engineering-framework" target="_blank" rel="noopener">Zipline_1</a></li>
</ul></li>
</ul>
<h2><span id="まとめ">まとめ</span></h2>
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a>
<ul>
<li>割とよくまとまっている。観点が参考になる。</li>
</ul></li>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a>
<ul>
<li>プロダクトは、 <a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for
ML</a> と重なっているが、考察が載っている。</li>
</ul></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a>
<ul>
<li>Feature Storeの要件を整理しようとしている</li>
</ul></li>
<li><a href="https://www.cognizant.com/whitepapers/accelerating-machine-learning-as-a-service-with-automated-feature-engineering-codex4971.pdf" target="_blank" rel="noopener">Accelerating
Machine Learning as a Service with Automated Feature Engineering</a>
<ul>
<li>Feature Storeの定義、ビジネスメリットまで言及されている。</li>
</ul></li>
<li><a href="https://qumulo.com/wp-content/uploads/2019/11/data-storage-architectures-for-machine-learning-and-artificial-intelligence.pdf" target="_blank" rel="noopener">Data
Storage Architectures for Machine Learning and Artificial
Intelligence</a>
<ul>
<li>ベンダリストが載っていて便利そう。発行が2019/11なので比較的最近。</li>
<li>AI/ML向けのストレージアーキテクチャを「2層型」、「1層型」で分けている。
<ul>
<li>2層型は性能層と容量層に別れる。また性能層は1層型として用いられることもある。</li>
<li>各層に用いられる、ベンダ製品を例示している。</li>
</ul></li>
<li>ベンダリスト
<ul>
<li>Dell EMC（Isilon、ECS）</li>
<li>Qumulo</li>
<li>WekaIO</li>
<li>Scality RING</li>
<li>DataDirect Networks</li>
<li>IBM（Spectrum Scale、COS）</li>
<li>Minio</li>
<li>Netapp</li>
<li>OpenIO</li>
<li>Pavilion Data Systems</li>
<li>Pure Storage AIRI</li>
<li>Quobyte</li>
<li>VAST Data</li>
</ul></li>
</ul></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="傾向">傾向</span></h2>
<ul>
<li>Google Big Query、Big
Table、Redisあたりを特徴量置き場として使っている例が見られた。</li>
</ul>
<h2><span id="featurestoreとして挙げられている特徴機能">Feature
Storeとして挙げられている特徴・機能</span></h2>
<h3><span id="主に-featuerstoreとしての特徴">主に、featuer
storeとしての特徴</span></h3>
<h4><span id="機能分析補助">機能・分析補助</span></h4>
<ul>
<li>オンライン・オフライン統合（一貫性の実現、共通API）
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a>
<ul>
<li>Hopsworksではオンライン用にMySQL、オフライン用にHiveを利用</li>
<li>また一方でHudiにも対応</li>
</ul></li>
</ul></li>
<li>バージョンニング、point-in-time
correctness（特定のタイミングのレコードに対するラベルの更新）、タイムトラベル
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>ストレージレイヤ（実体の保存方法）
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
</ul></li>
<li>自動特徴量分析、ドキュメンテーション、特徴量のテスト
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a>
<ul>
<li>HopsworksではDeequを使った特徴量のユニットテストが可能</li>
</ul></li>
</ul></li>
<li>マルチテナンシ（ネームスペース、リソース)
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
</ul></li>
<li>読み書きAPI
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
</ul></li>
<li>アクセス管理
<ul>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>クエリプランナ（複数の特徴量グループの自動結合）
<ul>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>必要な特徴量だけ選んでデータセットを定義（DBMSでいうビュー）
<ul>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
</ul>
<h4><span id="計算">計算</span></h4>
<ul>
<li>遅延評価（必要なタイイングでの計算実行）
<ul>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a></li>
</ul></li>
<li>自動再計算（auto backfill）
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
</ul>
<h4><span id="性能">性能</span></h4>
<ul>
<li>オンラインFeature Storeとしての良いレスポンス、スケーラビリティ
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>オフラインデータストアの性能（スケーラビリティ）
<ul>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>特徴量サービングの分散化・非中央集権化（サービングのコピー）
<ul>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
</ul></li>
</ul>
<h4><span id="連係">連係</span></h4>
<ul>
<li>特徴量エンジニアリング手段との連係
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
</ul></li>
<li>共通フォーマットでのデータのマテリアライズ、複数のフレームワークから読み書き可能なフォーマット
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>メタデータ管理との統合、データカタログ、登録・探索、探索用のGUI
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
</ul>
<h3><span id="rawデータストアを含めた特徴">rawデータストアを含めた特徴</span></h3>
<ul>
<li>画像、動画、音声など非テキストデータとテキストデータの統合的な取り扱い</li>
</ul>
<h3><span id="特徴量エンジニアリングの例">特徴量エンジニアリングの例</span></h3>
<p><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a>
に一例が載っていたのでついでに転記。</p>
<ul>
<li>Converting categorical data into numeric data;</li>
<li>Normalizing data (to alleviate ill-conditioned optimization when
features originate from different distributions);</li>
<li>One-hot-encoding/binarization;</li>
<li>Feature binning (e.g., convert continuous features into
discrete);</li>
<li>Feature hashing (e.g., to reduce the memory footprint of
one-hot-encoded features);</li>
<li>Computing polynomial features;</li>
<li>Representation learning (e.g., extract features using clustering,
embeddings, or generative models);</li>
<li>Computing aggregate features (e.g., count, min, max, stdev).</li>
</ul>
<h3><span id="featurestoreにおける画像の取扱は">feature
storeにおける画像の取扱は？</span></h3>
<p>feature
storeのレベルになると行列化されているので、画像を特別なものとして扱わない？
rawデータストア上では画像は画像として扱う。</p>
<h2><span id="feastにおけるデータフロー概要">Feastにおけるデータフロー概要</span></h2>
<p>※Feastから幾つか図を引用。</p>
<p><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a> に載っていたイメージ。</p>
<figure>
<img src="/memo-blog/images/IhhINZxPsyQx25yZ-F676F.png" alt="Feastのデータフローから引用">
<figcaption aria-hidden="true">Feastのデータフローから引用</figcaption>
</figure>
<p>データオーナ側はストリームデータ（Kafka）、DWH（BigQuery）、File（BigQuery）が書かれている。
また真ん中にはApache
Beamが書かれており、ストリームETLを経ながらデータがサービングシステムに渡されている。
データは基本的にはストリームとして扱うようだ。</p>
<p>また特徴量を取得するときは以下のようにする。</p>
<figure>
<img src="/memo-blog/images/IhhINZxPsyQx25yZ-A94FA.png" alt="特徴量の取得">
<figcaption aria-hidden="true">特徴量の取得</figcaption>
</figure>
<h2><span id="hopsworksにおけるfeaturestore">hopsworksにおけるfeature
store</span></h2>
<p>※Hopsworksから幾つか図を引用。</p>
<p><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a> に掲載されていたイメージは以下の通り。
Rawデータストアとは異なる位置づけ。</p>
<figure>
<img src="/memo-blog/images/BSBzSj1E5pwx3uqN-C9B87.png" alt="hopsworksでのfeature storeの位置づけ">
<figcaption aria-hidden="true">hopsworksでのfeature
storeの位置づけ</figcaption>
</figure>
<p>Feastでも言われているが、データエンジニアとデータサイエンティストの間にあるもの、とされている。</p>
<p>データストアする部分の全体アーキテクチャ。</p>
<figure>
<img src="/memo-blog/images/BSBzSj1E5pwx3uqN-EA1F7.png" alt="feature storeのアーキテクチャ">
<figcaption aria-hidden="true">feature
storeのアーキテクチャ</figcaption>
</figure>
<figure>
<img src="/memo-blog/images/BSBzSj1E5pwx3uqN-86B00.png" alt="feature storeのレイヤ構成">
<figcaption aria-hidden="true">feature storeのレイヤ構成</figcaption>
</figure>
<p>複数のコンポーネントを組み合わせて、ひとつのfeature
storeを構成しているようである。</p>
<h2><span id="ストレージ製品の動向">ストレージ製品の動向</span></h2>
<h3><span id="netapp">Netapp</span></h3>
<p><a href="https://www.netapp.com/us/solutions/applications/ai-deep-learning.aspx" target="_blank" rel="noopener">Accelerated
AI and deep learning pipelines across edge, core, and cloud</a>
では、</p>
<ul>
<li>Create a smooth, secure flow of data for your AI workloads.</li>
<li>Unify AI compute and data silos across sites and regions.​</li>
<li>Your data, always available: right place, right time.</li>
</ul>
<p>が挙げられている。
また、クラウド・オンプレ、エッジ・センタを統合する、というのが重要なアピールポイントに見えた。
詳しくは、 <a href="https://www.netapp.com/us/media/wp-7271.pdf" target="_blank" rel="noopener">Edge to
Core to Cloud Architecture for AI</a> を読めばわかりそう。</p>
<h3><span id="dell-emc">Dell EMC</span></h3>
<p>単独の技術というより、コンピューティングの工夫を含めてのソリューションのようにみえる。
<a href="https://www.dellemc.com/resources/en-us/asset/analyst-reports/products/storage/h17841_ar_enterprise_machine_and_deep_learning_with_intelligent_storage.pdf" target="_blank" rel="noopener">ENTERPRISE
MACHINE &amp; DEEP LEARNING WITH INTELLIGENT STORAGE</a>
に思想が書いてありそう。まだ読んでいない。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/" data-id="clt1hv4nb01891vqrgqvcblof" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Hudi" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/03/25/Hudi/">Hudi</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/03/25/Hudi/">
            <time datetime="2020-03-25T14:40:12.000Z" itemprop="datePublished">2020-03-25</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Hudi/">Hudi</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Apache-Hudi/">Apache Hudi</a>, <a class="tag-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#公式ドキュメント" id="toc-公式ドキュメント">公式ドキュメント</a></li>
<li><a href="#クイックスタートから確認version-0.5.2前提" id="toc-クイックスタートから確認version-0.5.2前提">クイックスタートから確認（version
0.5.2前提）</a>
<ul>
<li><a href="#org.apache.hudi.defaultsourcecreaterelation書き込み" id="toc-org.apache.hudi.defaultsourcecreaterelation書き込み">org.apache.hudi.DefaultSource#createRelation（書き込み）</a></li>
<li><a href="#org.apache.hudi.defaultsourcecreaterelation読み込み" id="toc-org.apache.hudi.defaultsourcecreaterelation読み込み">org.apache.hudi.DefaultSource#createRelation（読み込み）</a></li>
<li><a href="#incrementalrelation" id="toc-incrementalrelation">IncrementalRelation</a></li>
</ul></li>
</ul></li>
<li><a href="#hudiへの書き込み" id="toc-hudiへの書き込み">Hudiへの書き込み</a>
<ul>
<li><a href="#オペレーション種類" id="toc-オペレーション種類">オペレーション種類</a></li>
<li><a href="#deltastreamer" id="toc-deltastreamer">DeltaStreamer</a>
<ul>
<li><a href="#動作確認" id="toc-動作確認">動作確認</a></li>
<li><a href="#実装確認" id="toc-実装確認">実装確認</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><p><a href="https://hudi.apache.org/" target="_blank" rel="noopener">公式ドキュメント</a></p></li>
<li><p><a href="https://hudi.apache.org/docs/quick-start-guide.html" target="_blank" rel="noopener">Quick Start
Guide</a></p></li>
<li><p><a href="https://hudi.apache.org/docs/writing_data.html" target="_blank" rel="noopener">Writing
Hudi Tables</a></p></li>
<li><p><a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a></p></li>
<li><p><a href="https://github.com/apurvam/streams-prototyping" target="_blank" rel="noopener">apurvam
streams-prototyping</a></p></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="公式ドキュメント">公式ドキュメント</span></h2>
<p>載っている特徴は、以下の通り。</p>
<ul>
<li>Upsert support with fast, pluggable indexing.</li>
<li>Atomically publish data with rollback support.</li>
<li>Snapshot isolation between writer &amp; queries.</li>
<li>Savepoints for data recovery.</li>
<li>Manages file sizes, layout using statistics.</li>
<li>Async compaction of row &amp; columnar data.</li>
<li>Timeline metadata to track lineage.</li>
</ul>
<h2><span id="クイックスタートから確認version052前提">クイックスタートから確認（version
0.5.2前提）</span></h2>
<p><a href="https://hudi.apache.org/docs/quick-start-guide.html" target="_blank" rel="noopener">Quick
Start Guide</a> を参考に進める。</p>
<p>公式ドキュメントではSpark2.4.4を利用しているが、ここでは2.4.5を利用する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> SPARK_HOME=/opt/spark/default</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-shell \</span></span><br><span class="line">  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.2-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \</span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'</span><br></pre></td></tr></table></figure>
<p>必要なライブラリをインポート</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">QuickstartUtils</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceReadOptions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceWriteOptions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.config.<span class="type">HoodieWriteConfig</span>._</span><br><span class="line">scala&gt; </span><br><span class="line">scala&gt; <span class="keyword">val</span> tableName = <span class="string">"hudi_trips_cow"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> basePath = <span class="string">"file:///tmp/hudi_trips_cow"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> dataGen = <span class="keyword">new</span> <span class="type">DataGenerator</span></span><br></pre></td></tr></table></figure>
<p>ダミーデータには
<code>org.apache.hudi.QuickstartUtils.DataGenerator</code>
クラスを利用する。 以下の例では、
<code>org.apache.hudi.QuickstartUtils.DataGenerator#generateInserts</code>
メソッドを利用しデータを生成するが、 どういうレコードが生成されるかは、
<code>org.apache.hudi.QuickstartUtils.DataGenerator#generateRandomValue</code>
メソッドあたりを見るとわかる。</p>
<p>ダミーデータを生成し、Spark DataFrameに変換。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> inserts = convertToStringList(dataGen.generateInserts(<span class="number">10</span>))</span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(inserts, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>中身は以下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show</span><br><span class="line">+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">|          begin_lat|          begin_lon|    driver|            end_lat|            end_lon|              fare|       partitionpath|    rider| ts|                uuid|</span><br><span class="line">+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">| <span class="number">0.4726905879569653</span>|<span class="number">0.46157858450465483</span>|driver<span class="number">-213</span>|  <span class="number">0.754803407008858</span>| <span class="number">0.9671159942018241</span>|<span class="number">34.158284716382845</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">28432</span>dec<span class="number">-53</span>eb<span class="number">-402.</span>..|</span><br><span class="line">| <span class="number">0.6100070562136587</span>| <span class="number">0.8779402295427752</span>|driver<span class="number">-213</span>| <span class="number">0.3407870505929602</span>| <span class="number">0.5030798142293655</span>|  <span class="number">43.4923811219014</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">1</span>bd3905e-a6c4<span class="number">-404.</span>..|</span><br><span class="line">| <span class="number">0.5731835407930634</span>| <span class="number">0.4923479652912024</span>|driver<span class="number">-213</span>|<span class="number">0.08988581780930216</span>|<span class="number">0.42520899698713666</span>| <span class="number">64.27696295884016</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|c9cc8f4b-acee<span class="number">-413.</span>..|</span><br><span class="line">|<span class="number">0.21624150367601136</span>|<span class="number">0.14285051259466197</span>|driver<span class="number">-213</span>| <span class="number">0.5890949624813784</span>| <span class="number">0.0966823831927115</span>| <span class="number">93.56018115236618</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">4</span>be1c199<span class="number">-86</span>dc<span class="number">-489.</span>..|</span><br><span class="line">|   <span class="number">0.40613510977307</span>| <span class="number">0.5644092139040959</span>|driver<span class="number">-213</span>|  <span class="number">0.798706304941517</span>|<span class="number">0.02698359227182834</span>|<span class="number">17.851135255091155</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">83</span>f4d3df<span class="number">-46</span>c1<span class="number">-48</span>a...|</span><br><span class="line">| <span class="number">0.8742041526408587</span>| <span class="number">0.7528268153249502</span>|driver<span class="number">-213</span>| <span class="number">0.9197827128888302</span>|  <span class="number">0.362464770874404</span>|<span class="number">19.179139106643607</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|cb8b392d-c9d0<span class="number">-445.</span>..|</span><br><span class="line">| <span class="number">0.1856488085068272</span>| <span class="number">0.9694586417848392</span>|driver<span class="number">-213</span>|<span class="number">0.38186367037201974</span>|<span class="number">0.25252652214479043</span>| <span class="number">33.92216483948643</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">66</span>aaf87d<span class="number">-4786</span><span class="number">-4</span>d0...|</span><br><span class="line">| <span class="number">0.0750588760043035</span>|<span class="number">0.03844104444445928</span>|driver<span class="number">-213</span>|<span class="number">0.04376353354538354</span>| <span class="number">0.6346040067610669</span>| <span class="number">66.62084366450246</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|c5a335f5-c57f<span class="number">-4</span>f5...|</span><br><span class="line">|  <span class="number">0.651058505660742</span>| <span class="number">0.8192868687714224</span>|driver<span class="number">-213</span>|<span class="number">0.20714896002914462</span>|<span class="number">0.06224031095826987</span>| <span class="number">41.06290929046368</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">53026</span>eda<span class="number">-28</span>c4<span class="number">-4</span>d8...|</span><br><span class="line">|<span class="number">0.11488393157088261</span>| <span class="number">0.6273212202489661</span>|driver<span class="number">-213</span>| <span class="number">0.7454678537511295</span>| <span class="number">0.3954939864908973</span>| <span class="number">27.79478688582596</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|cd42df54<span class="number">-5215</span><span class="number">-402.</span>..|</span><br><span class="line">+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">         options(getQuickstartWriteConfigs).</span><br><span class="line">         option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"ts"</span>).</span><br><span class="line">         option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"uuid"</span>).</span><br><span class="line">         option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">         option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">         mode(<span class="type">Overwrite</span>).</span><br><span class="line">         save(basePath)</span><br></pre></td></tr></table></figure>
<p>なお、生成されたファイルは以下の通り。
<code>PARTITIONPATH_FIELD_OPT_KEY</code>
で指定したカラムをパーティションキーとして用いていることがわかる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -R /tmp/hudi_trips_cow/</span></span><br><span class="line">/tmp/hudi_trips_cow/:</span><br><span class="line">americas  asia</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas:</span><br><span class="line">brazil  united_states</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/brazil:</span><br><span class="line">sao_paulo</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/brazil/sao_paulo:</span><br><span class="line">ae28c85a-38f0-487f-a42d-3a0babc9d321-0_0-21-25_20200329002247.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/united_states:</span><br><span class="line">san_francisco</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/united_states/san_francisco:</span><br><span class="line">849db286-1cbe-4a1f-b544-9939893e99f8-0_1-21-26_20200329002247.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/asia:</span><br><span class="line">india</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/asia/india:</span><br><span class="line">chennai</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/asia/india/chennai:</span><br><span class="line">2ebfbab0-4f8f-42db-b79e-1c0cbcc3cf39-0_2-21-27_20200329002247.parquet</span><br></pre></td></tr></table></figure>
<p>保存したデータを読み出してみる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsSnapshotDF = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*/*/*"</span>)</span><br><span class="line">scala&gt; tripsSnapshotDF.createOrReplaceTempView(<span class="string">"hudi_trips_snapshot"</span>)</span><br></pre></td></tr></table></figure>
<p>中身は以下の通り。
元データに対し、Hudiのカラムが追加されていることがわかる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; tripsSnapshotDF.show</span><br><span class="line">+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|          begin_lat|          begin_lon|    driver|            end_lat|            end_lon|              fare|       partitionpath|    rider| ts|                uuid|</span><br><span class="line">+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_1|<span class="number">7695</span>c291<span class="number">-8530</span><span class="number">-473.</span>..|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...|<span class="number">0.21624150367601136</span>|<span class="number">0.14285051259466197</span>|driver<span class="number">-213</span>| <span class="number">0.5890949624813784</span>| <span class="number">0.0966823831927115</span>| <span class="number">93.56018115236618</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">7695</span>c291<span class="number">-8530</span><span class="number">-473.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_3|<span class="number">2</span>f06fcd2<span class="number">-8296</span><span class="number">-423.</span>..|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...| <span class="number">0.5731835407930634</span>| <span class="number">0.4923479652912024</span>|driver<span class="number">-213</span>|<span class="number">0.08988581780930216</span>|<span class="number">0.42520899698713666</span>| <span class="number">64.27696295884016</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">2</span>f06fcd2<span class="number">-8296</span><span class="number">-423.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_5|<span class="number">6</span>ebc4028<span class="number">-9</span>aae<span class="number">-420.</span>..|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...| <span class="number">0.8742041526408587</span>| <span class="number">0.7528268153249502</span>|driver<span class="number">-213</span>| <span class="number">0.9197827128888302</span>|  <span class="number">0.362464770874404</span>|<span class="number">19.179139106643607</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">6</span>ebc4028<span class="number">-9</span>aae<span class="number">-420.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_6|<span class="number">8</span>bf60390-ad41<span class="number">-4</span>b0...|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...|<span class="number">0.11488393157088261</span>| <span class="number">0.6273212202489661</span>|driver<span class="number">-213</span>| <span class="number">0.7454678537511295</span>| <span class="number">0.3954939864908973</span>| <span class="number">27.79478688582596</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">8</span>bf60390-ad41<span class="number">-4</span>b0...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_7|<span class="number">762e8</span>cb2<span class="number">-8806</span><span class="number">-47</span>d...|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...| <span class="number">0.1856488085068272</span>| <span class="number">0.9694586417848392</span>|driver<span class="number">-213</span>|<span class="number">0.38186367037201974</span>|<span class="number">0.25252652214479043</span>| <span class="number">33.92216483948643</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">762e8</span>cb2<span class="number">-8806</span><span class="number">-47</span>d...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_0_8|<span class="number">28622337</span>-d76b<span class="number">-442.</span>..|  americas/brazil/s...|ae28c85a<span class="number">-38</span>f0<span class="number">-487.</span>..| <span class="number">0.6100070562136587</span>| <span class="number">0.8779402295427752</span>|driver<span class="number">-213</span>| <span class="number">0.3407870505929602</span>| <span class="number">0.5030798142293655</span>|  <span class="number">43.4923811219014</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">28622337</span>-d76b<span class="number">-442.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_0_9|<span class="number">33</span>aec15d<span class="number">-356</span>f<span class="number">-475.</span>..|  americas/brazil/s...|ae28c85a<span class="number">-38</span>f0<span class="number">-487.</span>..| <span class="number">0.0750588760043035</span>|<span class="number">0.03844104444445928</span>|driver<span class="number">-213</span>|<span class="number">0.04376353354538354</span>| <span class="number">0.6346040067610669</span>| <span class="number">66.62084366450246</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">33</span>aec15d<span class="number">-356</span>f<span class="number">-475.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>| <span class="number">20200329002247</span>_0_10|<span class="number">2</span>d71c9a3<span class="number">-26</span>a3<span class="number">-40</span>b...|  americas/brazil/s...|ae28c85a<span class="number">-38</span>f0<span class="number">-487.</span>..| <span class="number">0.4726905879569653</span>|<span class="number">0.46157858450465483</span>|driver<span class="number">-213</span>|  <span class="number">0.754803407008858</span>| <span class="number">0.9671159942018241</span>|<span class="number">34.158284716382845</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">2</span>d71c9a3<span class="number">-26</span>a3<span class="number">-40</span>b...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_2_2|a997a8f0<span class="number">-4</span>ab6<span class="number">-4</span>d5...|    asia/india/chennai|<span class="number">2</span>ebfbab0<span class="number">-4</span>f8f<span class="number">-42</span>d...|   <span class="number">0.40613510977307</span>| <span class="number">0.5644092139040959</span>|driver<span class="number">-213</span>|  <span class="number">0.798706304941517</span>|<span class="number">0.02698359227182834</span>|<span class="number">17.851135255091155</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|a997a8f0<span class="number">-4</span>ab6<span class="number">-4</span>d5...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_2_4|<span class="number">271</span>de424-a0f8<span class="number">-427.</span>..|    asia/india/chennai|<span class="number">2</span>ebfbab0<span class="number">-4</span>f8f<span class="number">-42</span>d...|  <span class="number">0.651058505660742</span>| <span class="number">0.8192868687714224</span>|driver<span class="number">-213</span>|<span class="number">0.20714896002914462</span>|<span class="number">0.06224031095826987</span>| <span class="number">41.06290929046368</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">271</span>de424-a0f8<span class="number">-427.</span>..|</span><br><span class="line">+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br></pre></td></tr></table></figure>
<p>上記の通り、SparkのData Source機能を利用している。
中では、<code>org.apache.hudi.DefaultSource#createRelation</code>
メソッドが用いられる。</p>
<p>つづいて、更新を試す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> updates = convertToStringList(dataGen.generateUpdates(<span class="number">10</span>))</span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(updates, <span class="number">2</span>))</span><br><span class="line">scala&gt; df.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">         options(getQuickstartWriteConfigs).</span><br><span class="line">         option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"ts"</span>).</span><br><span class="line">         option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"uuid"</span>).</span><br><span class="line">         option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">         option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">         mode(<span class="type">Append</span>).</span><br><span class="line">         save(basePath)</span><br></pre></td></tr></table></figure>
<p>もう一度、DataFrameとして読み出すと、レコードが追加されていることを確かめられる。（ここでは省略）
この後の、 <code>incremental</code>
クエリタイプの実験のため、上記の更新を幾度か実行しておく。</p>
<p>つづいて、 <code>incremental</code> クエリタイプで読み出す。</p>
<p>一度読み出し、最初のコミット時刻を取り出す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*/*/*"</span>).</span><br><span class="line">         createOrReplaceTempView(<span class="string">"hudi_trips_snapshot"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> commits = spark.sql(<span class="string">"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime"</span>).map(k =&gt; k.getString(<span class="number">0</span>)).take(<span class="number">50</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> beginTime = commits(commits.length - <span class="number">2</span>) <span class="comment">// commit time we are interested in</span></span><br></pre></td></tr></table></figure>
<p>今回は、初回書き込みに加えて2回更新したので、 <code>commits</code>
は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; commits</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">20200330002239</span>, <span class="number">20200330002354</span>, <span class="number">20200330003142</span>)</span><br></pre></td></tr></table></figure>
<p>また、今回「読み込みの最初」とするコミットは、以下の通り。
つまり、2回目の更新時。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; beginTime</span><br><span class="line">res13: <span class="type">String</span> = <span class="number">20200330002354</span></span><br></pre></td></tr></table></figure>
<p>では、 <code>incremental</code> クエリタイプで読み出す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsIncrementalDF = spark.read.format(<span class="string">"hudi"</span>).</span><br><span class="line">         option(<span class="type">QUERY_TYPE_OPT_KEY</span>, <span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>).</span><br><span class="line">         option(<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>, beginTime).</span><br><span class="line">         load(basePath)</span><br><span class="line">scala&gt; tripsIncrementalDF.createOrReplaceTempView(<span class="string">"hudi_trips_incremental"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br></pre></td></tr></table></figure>
<p>結果は以下のようなイメージ。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|_hoodie_commit_time|              fare|           begin_lon|          begin_lat| ts|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">87.68271062363665</span>|  <span class="number">0.9273857651526887</span>| <span class="number">0.1620033132033215</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">40.44073446276323</span>|<span class="number">9.842943407509797E-4</span>|<span class="number">0.47631824594751015</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">45.39370966816483</span>|    <span class="number">0.65888271115305</span>| <span class="number">0.8535610661589833</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">47.332186591003044</span>|  <span class="number">0.8006023508896579</span>| <span class="number">0.9025851737325563</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">93.34457064050349</span>|  <span class="number">0.6331319396951335</span>| <span class="number">0.5375953886834237</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">31.065524210209226</span>|  <span class="number">0.7608842984578864</span>| <span class="number">0.9514417909802292</span>|<span class="number">0.0</span>|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br></pre></td></tr></table></figure>
<p>なお、ここでbeginTimeを1遡ることにすると…。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> beginTime = commits(commits.length - <span class="number">3</span>) <span class="comment">// commit time we are interested in</span></span><br></pre></td></tr></table></figure>
<p>以下のように、2回目のコミットも含まれるようになる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|_hoodie_commit_time|              fare|           begin_lon|          begin_lat| ts|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">87.68271062363665</span>|  <span class="number">0.9273857651526887</span>| <span class="number">0.1620033132033215</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">40.44073446276323</span>|<span class="number">9.842943407509797E-4</span>|<span class="number">0.47631824594751015</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">45.39370966816483</span>|    <span class="number">0.65888271115305</span>| <span class="number">0.8535610661589833</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">47.332186591003044</span>|  <span class="number">0.8006023508896579</span>| <span class="number">0.9025851737325563</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">39.09858962414072</span>| <span class="number">0.08151154133724581</span>|<span class="number">0.21729959707372848</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">93.34457064050349</span>|  <span class="number">0.6331319396951335</span>| <span class="number">0.5375953886834237</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">80.87869643345753</span>|  <span class="number">0.0748253615757305</span>| <span class="number">0.9787639413761751</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">31.065524210209226</span>|  <span class="number">0.7608842984578864</span>| <span class="number">0.9514417909802292</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>|<span class="number">21.602186045036387</span>|   <span class="number">0.772134626462835</span>| <span class="number">0.3291184473506418</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">43.41497201940956</span>|  <span class="number">0.6226833057042072</span>| <span class="number">0.5501675314928346</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">35.71294622426758</span>|  <span class="number">0.6696123015022845</span>| <span class="number">0.7318572150654761</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">67.30906296028802</span>| <span class="number">0.16768228612130764</span>|<span class="number">0.29666655980198253</span>|<span class="number">0.0</span>|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br></pre></td></tr></table></figure>
<h3><span id="orgapachehudidefaultsourcecreaterelation書き込み">org.apache.hudi.DefaultSource#createRelation（書き込み）</span></h3>
<p>クイックスタートで、例えば更新などする際の動作を確認する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">     |   options(getQuickstartWriteConfigs).</span><br><span class="line">     |   option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"ts"</span>).</span><br><span class="line">     |   option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"uuid"</span>).</span><br><span class="line">     |   option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">     |   option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">     |   mode(<span class="type">Append</span>).</span><br><span class="line">     |   save(basePath)</span><br></pre></td></tr></table></figure>
<p>のような例を実行する際、内部的には
<code>org.apache.hudi.DefaultSource#createRelation</code>
が呼ばれる。</p>
<p>org/apache/hudi/DefaultSource.scala:85</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">                            mode: <span class="type">SaveMode</span>,</span><br><span class="line">                            optParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">                            df: <span class="type">DataFrame</span>): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> parameters = <span class="type">HoodieSparkSqlWriter</span>.parametersWithWriteDefaults(optParams)</span><br><span class="line">  <span class="type">HoodieSparkSqlWriter</span>.write(sqlContext, mode, parameters, df)</span><br><span class="line">  createRelation(sqlContext, parameters, df.schema)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記メソッド内では、
<code>org.apache.hudi.HoodieSparkSqlWriter$#write</code>
メソッドが呼ばれており、 これが書き込みの実態である。 なお、その下の
<code>org.apache.hudi.DefaultSource#createRelation</code>
は、読み込み時に呼ばれるものと同一。</p>
<p>ここでは <code>org.apache.hudi.HoodieSparkSqlWriter#write</code>
メソッドを確認する。
当該メソッドの冒頭では、オペレーションの判定などいくつか前処理が行われた後、
以下の箇所から実際に書き出す処理が定義されている。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:85</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (writeStatuses, writeClient: <span class="type">HoodieWriteClient</span>[<span class="type">HoodieRecordPayload</span>[<span class="type">Nothing</span>]]) =</span><br><span class="line">  <span class="keyword">if</span> (!operation.equalsIgnoreCase(<span class="type">DELETE_OPERATION_OPT_VAL</span>)) &#123;</span><br><span class="line">  <span class="comment">// register classes &amp; schemas</span></span><br><span class="line">  <span class="keyword">val</span> structName = <span class="string">s"<span class="subst">$&#123;tblName.get&#125;</span>_record"</span></span><br><span class="line">  <span class="keyword">val</span> nameSpace = <span class="string">s"hoodie.<span class="subst">$&#123;tblName.get&#125;</span>"</span></span><br><span class="line">  sparkContext.getConf.registerKryoClasses(</span><br><span class="line">    <span class="type">Array</span>(classOf[org.apache.avro.generic.<span class="type">GenericData</span>],</span><br><span class="line">      classOf[org.apache.avro.<span class="type">Schema</span>]))</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="type">AvroConversionUtils</span>.convertStructTypeToAvroSchema(df.schema, structName, nameSpace)</span><br><span class="line">  sparkContext.getConf.registerAvroSchemas(schema)</span><br><span class="line"></span><br><span class="line">  (snip)</span><br></pre></td></tr></table></figure>
<p>まず <code>delete</code>
オペレーションかどうかで処理が別れるが、上記の例では <code>upsert</code>
オペレーションなので一旦そのまま読み進める。
ネームスペース（データベースやテーブル？）を取得した後、SparkのStructTypeで保持されたスキーマ情報を、AvroのSchemaに変換する。
変換されたスキーマをSparkで登録する。</p>
<p>つづいて、DataFrameをRDDに変換する。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:97</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Convert to RDD[HoodieRecord]</span></span><br><span class="line"><span class="keyword">val</span> keyGenerator = <span class="type">DataSourceUtils</span>.createKeyGenerator(toProperties(parameters))</span><br><span class="line"><span class="keyword">val</span> genericRecords: <span class="type">RDD</span>[<span class="type">GenericRecord</span>] = <span class="type">AvroConversionUtils</span>.createRdd(df, structName, nameSpace)</span><br><span class="line"><span class="keyword">val</span> hoodieAllIncomingRecords = genericRecords.map(gr =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> orderingVal = <span class="type">DataSourceUtils</span>.getNestedFieldValAsString(</span><br><span class="line">    gr, parameters(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>), <span class="literal">false</span>).asInstanceOf[<span class="type">Comparable</span>[_]]</span><br><span class="line">  <span class="type">DataSourceUtils</span>.createHoodieRecord(gr,</span><br><span class="line">    orderingVal, keyGenerator.getKey(gr), parameters(<span class="type">PAYLOAD_CLASS_OPT_KEY</span>))</span><br><span class="line">&#125;).toJavaRDD()</span><br></pre></td></tr></table></figure>
<p>RDDに一度変換した後、mapメソッドで加工する。</p>
<p>まず、 <code>genericRecords</code>
の内容は以下のようなものが含まれる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;GenericRecord[1]@27822&#125; </span><br><span class="line"> 0 = &#123;GenericData$Record@27827&#125; &quot;&#123;&quot;begin_lat&quot;: 0.09632451474505643, &quot;begin_lon&quot;: 0.8989273848550128, &quot;driver&quot;: &quot;driver-164&quot;, &quot;end_lat&quot;: 0.6431885917325862, &quot;end_lon&quot;: 0.6664889106258252, &quot;fare&quot;: 86.865568091804, &quot;partitionpath&quot;: &quot;americas/brazil/sao_paulo&quot;, &quot;rider&quot;: &quot;rider-164&quot;, &quot;ts&quot;: 0.0, &quot;uuid&quot;: &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;&#125;&quot;</span><br><span class="line">  schema = &#123;Schema$RecordSchema@27835&#125; &quot;&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_trips_cow_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_trips_cow&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;begin_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;begin_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;driver&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;fare&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;rider&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;&quot;</span><br><span class="line">  values = &#123;Object[10]@27836&#125; </span><br><span class="line">   0 = &#123;Double@27838&#125; 0.09632451474505643</span><br><span class="line">   1 = &#123;Double@27839&#125; 0.8989273848550128</span><br><span class="line">   2 = &#123;Utf8@27840&#125; &quot;driver-164&quot;</span><br><span class="line">   3 = &#123;Double@27841&#125; 0.6431885917325862</span><br><span class="line">   4 = &#123;Double@27842&#125; 0.6664889106258252</span><br><span class="line">   5 = &#123;Double@27843&#125; 86.865568091804</span><br><span class="line">   6 = &#123;Utf8@27844&#125; &quot;americas/brazil/sao_paulo&quot;</span><br><span class="line">   7 = &#123;Utf8@27845&#125; &quot;rider-164&quot;</span><br><span class="line">   8 = &#123;Double@27846&#125; 0.0</span><br><span class="line">   9 = &#123;Utf8@27847&#125; &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;</span><br></pre></td></tr></table></figure>
<p>上記の通り、これは入ロクレコードそのものである。
その後、mapメソッドを使ってHudiで利用するキーを含む、Hudiのレコード形式に変換する。</p>
<p>変換された <code>hoodieAllIncomingRecords</code>
は以下のような内容になる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;Wrappers$SeqWrapper@27881&#125;  size = 1</span><br><span class="line"> 0 = &#123;HoodieRecord@27883&#125; &quot;HoodieRecord&#123;key=HoodieKey &#123; recordKey=5d49cfb5-0db4-4172-bff4-e581eb1f9783 partitionPath=americas/brazil/sao_paulo&#125;, currentLocation=&apos;null&apos;, newLocation=&apos;null&apos;&#125;&quot;</span><br><span class="line">  key = &#123;HoodieKey@27892&#125; &quot;HoodieKey &#123; recordKey=5d49cfb5-0db4-4172-bff4-e581eb1f9783 partitionPath=americas/brazil/sao_paulo&#125;&quot;</span><br><span class="line">   recordKey = &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;</span><br><span class="line">   partitionPath = &quot;americas/brazil/sao_paulo&quot;</span><br><span class="line">  data = &#123;OverwriteWithLatestAvroPayload@27893&#125; </span><br><span class="line">   recordBytes = &#123;byte[142]@27895&#125; </span><br><span class="line">   orderingVal = &quot;0.0&quot;</span><br><span class="line">  currentLocation = null</span><br><span class="line">  newLocation = null</span><br><span class="line">  sealed = false</span><br></pre></td></tr></table></figure>
<p>上記の例の通り、ペイロードは
<code>org.apache.hudi.common.model.OverwriteWithLatestAvroPayload</code>
で保持される。</p>
<p>その後、いくつかモードの確認が行われた後、もしテーブルがなければ
<code>org.apache.hudi.common.table.HoodieTableMetaClient#initTableType</code>
を用いて テーブルを初期化する。</p>
<p>その後、重複レコードを必要に応じて落とす。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:132</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> hoodieRecords =</span><br><span class="line">  <span class="keyword">if</span> (parameters(<span class="type">INSERT_DROP_DUPS_OPT_KEY</span>).toBoolean) &#123;</span><br><span class="line">    <span class="type">DataSourceUtils</span>.dropDuplicates(</span><br><span class="line">      jsc,</span><br><span class="line">      hoodieAllIncomingRecords,</span><br><span class="line">      mapAsJavaMap(parameters), client.getTimelineServer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    hoodieAllIncomingRecords</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>レコードが空かどうかを改めて確認しつつ、 最後に書き込み実施。
<code>org.apache.hudi.DataSourceUtils#doWriteOperation</code>
が実態である。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:147</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val writeStatuses = DataSourceUtils.doWriteOperation(client, hoodieRecords, commitTime, operation)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.DataSourceUtils#doWriteOperation</code>
メソッドは以下の通り。</p>
<p>org/apache/hudi/DataSourceUtils.java:162</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JavaRDD&lt;WriteStatus&gt; <span class="title">doWriteOperation</span><span class="params">(HoodieWriteClient client, JavaRDD&lt;HoodieRecord&gt; hoodieRecords,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                    String commitTime, String operation)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) &#123;</span><br><span class="line">    <span class="keyword">return</span> client.bulkInsert(hoodieRecords, commitTime);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation.equals(DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())) &#123;</span><br><span class="line">    <span class="keyword">return</span> client.insert(hoodieRecords, commitTime);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// default is upsert</span></span><br><span class="line">    <span class="keyword">return</span> client.upsert(hoodieRecords, commitTime);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>今回の例だと、 <code>upseart</code> オペレーションなので
<code>org.apache.hudi.client.HoodieWriteClient#upsert</code>
メソッドが呼ばれる。 このメソッドは以下のとおりだが、ポイントは、
<code>org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal</code>
メソッドである。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;WriteStatus&gt; <span class="title">upsert</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, <span class="keyword">final</span> String commitTime)</span> </span>&#123;</span><br><span class="line">  HoodieTable&lt;T&gt; table = getTableAndInitCtx(OperationType.UPSERT);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// De-dupe/merge if needed</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords =</span><br><span class="line">        combineOnCondition(config.shouldCombineBeforeUpsert(), records, config.getUpsertShuffleParallelism());</span><br><span class="line"></span><br><span class="line">    Timer.Context indexTimer = metrics.getIndexCtx();</span><br><span class="line">    <span class="comment">// perform index loop up to get existing location of records</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; taggedRecords = getIndex().tagLocation(dedupedRecords, jsc, table);</span><br><span class="line">    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == <span class="keyword">null</span> ? <span class="number">0L</span> : indexTimer.stop()));</span><br><span class="line">    <span class="keyword">return</span> upsertRecordsInternal(taggedRecords, commitTime, table, <span class="keyword">true</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> HoodieUpsertException) &#123;</span><br><span class="line">      <span class="keyword">throw</span> (HoodieUpsertException) e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(<span class="string">"Failed to upsert for commit time "</span> + commitTime, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal</code>
メソッド内のポイントは、 以下の箇所。
<code>org.apache.spark.api.java.AbstractJavaRDDLike#mapPartitionsWithIndex</code>
メソッドで、upsertやinsertの処理を定義している。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:470</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;WriteStatus&gt; writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -&gt; &#123;</span><br><span class="line">  <span class="keyword">if</span> (isUpsert) &#123;</span><br><span class="line">    <span class="keyword">return</span> hoodieTable.handleUpsertPartition(commitTime, partition, recordItr, partitioner);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> hoodieTable.handleInsertPartition(commitTime, partition, recordItr, partitioner);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;, <span class="keyword">true</span>).flatMap(List::iterator);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime);</span><br></pre></td></tr></table></figure>
<p>ここでは、
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleUpsertPartition</code>
メソッドを確認してみる。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:253</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String commitTime, Integer partition, Iterator recordItr,</span><br><span class="line">    Partitioner partitioner) &#123;</span><br><span class="line">  UpsertPartitioner upsertPartitioner = (UpsertPartitioner) partitioner;</span><br><span class="line">  BucketInfo binfo = upsertPartitioner.getBucketInfo(partition);</span><br><span class="line">  BucketType btype = binfo.bucketType;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (btype.equals(BucketType.INSERT)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleInsert(commitTime, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (btype.equals(BucketType.UPDATE)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleUpdate(commitTime, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(<span class="string">"Unknown bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    String msg = <span class="string">"Error upserting bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition;</span><br><span class="line">    LOG.error(msg, t);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(msg, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>真ん中あたりに、INSERTかUPDATEかで条件分岐しているが、ここでは例としてINSERT側を確認する。
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsert</code>
メソッドがポイントとなる。
なお、当該メッソッドには同期的な実装と、非同期的な実装があるよう。
ここでは上記呼び出しに基づき、非同期的な実装の方を確認する。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:233</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String commitTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records</span></span><br><span class="line">  <span class="keyword">if</span> (!recordItr.hasNext()) &#123;</span><br><span class="line">    LOG.info(<span class="string">"Empty partition"</span>);</span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, commitTime, <span class="keyword">this</span>, idPfx);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>戻り値が、
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable</code>
クラスのインスタンスになっていることがわかる。 このイテレータは、
<code>org.apache.hudi.client.utils.LazyIterableIterator</code>
アブストラクトクラスを継承している。
<code>org.apache.hudi.client.utils.LazyIterableIterator</code>
では、nextメソッドが</p>
<p>org/apache/hudi/client/utils/LazyIterableIterator.java:116</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> O <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> computeNext();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(ex);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>のように定義されており、実態が
<code>org.apache.hudi.client.utils.LazyIterableIterator#computeNext</code>
であることがわかる。 当該メソッドは、
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#CopyOnWriteLazyInsertIterable</code>
クラスではオーバーライドされており、 以下のように定義されている。</p>
<p>org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java:93</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> List&lt;WriteStatus&gt; <span class="title">computeNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Executor service used for launching writer thread.</span></span><br><span class="line">  BoundedInMemoryExecutor&lt;HoodieRecord&lt;T&gt;, HoodieInsertValueGenResult&lt;HoodieRecord&gt;, List&lt;WriteStatus&gt;&gt; bufferedIteratorExecutor =</span><br><span class="line">      <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> Schema schema = <span class="keyword">new</span> Schema.Parser().parse(hoodieConfig.getSchema());</span><br><span class="line">    bufferedIteratorExecutor =</span><br><span class="line">        <span class="keyword">new</span> SparkBoundedInMemoryExecutor&lt;&gt;(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema));</span><br><span class="line">    <span class="keyword">final</span> List&lt;WriteStatus&gt; result = bufferedIteratorExecutor.execute();</span><br><span class="line">    <span class="keyword">assert</span> result != <span class="keyword">null</span> &amp;&amp; !result.isEmpty() &amp;&amp; !bufferedIteratorExecutor.isRemaining();</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieException(e);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> != bufferedIteratorExecutor) &#123;</span><br><span class="line">      bufferedIteratorExecutor.shutdownNow();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>どうやら内部でFutureパターンを利用し、非同期化して書き込みを行っているようだ。（これが筋よしなのかどうかは要議論。update、つまりマージも同様になっている。）
処理内容を知る上でポイントとなるのは、</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bufferedIteratorExecutor =</span><br><span class="line">    <span class="keyword">new</span> SparkBoundedInMemoryExecutor&lt;&gt;(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema));</span><br></pre></td></tr></table></figure>
<p>の箇所。
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#getInsertHandler</code>
あたり。 中で用いられている、
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.CopyOnWriteInsertHandler</code>
クラスがポイントとなる。
このクラスは、書き込みデータのキュー（要確認）からレコードを受け取って、処理していると考えられる。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">consumeOneRecord</span><span class="params">(HoodieInsertValueGenResult&lt;HoodieRecord&gt; payload)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> HoodieRecord insertPayload = payload.record;</span><br><span class="line">  <span class="comment">// lazily initialize the handle, for the first time</span></span><br><span class="line">  <span class="keyword">if</span> (handle == <span class="keyword">null</span>) &#123;</span><br><span class="line">    handle = <span class="keyword">new</span> HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, insertPayload.getPartitionPath(),</span><br><span class="line">        getNextFileId(idPrefix));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (handle.canWrite(payload.record)) &#123;</span><br><span class="line">    <span class="comment">// write the payload, if the handle has capacity</span></span><br><span class="line">    handle.write(insertPayload, payload.insertValue, payload.exception);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// handle is full.</span></span><br><span class="line">    statuses.add(handle.close());</span><br><span class="line">    <span class="comment">// Need to handle the rejected payload &amp; open new handle</span></span><br><span class="line">    handle = <span class="keyword">new</span> HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, insertPayload.getPartitionPath(),</span><br><span class="line">        getNextFileId(idPrefix));</span><br><span class="line">    handle.write(insertPayload, payload.insertValue, payload.exception); <span class="comment">// we should be able to write 1 payload.</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下の方にある <code>org.apache.hudi.io.HoodieCreateHandle</code>
クラスを用いているあたりがポイント。 そのwriteメソッドは以下の通り。
<code>org.apache.hudi.io.storage.HoodieStorageWriter#writeAvroWithMetadata</code>
を用いて書き出しているように見える。 （実際には
<code>org.apache.hudi.io.storage.HoodieParquetWriter</code> ）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(HoodieRecord record, Option&lt;IndexedRecord&gt; avroRecord)</span> </span>&#123;</span><br><span class="line">  Option recordMetadata = record.getData().getMetadata();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (avroRecord.isPresent()) &#123;</span><br><span class="line">      <span class="comment">// Convert GenericRecord to GenericRecord with hoodie commit metadata in schema</span></span><br><span class="line">      IndexedRecord recordWithMetadataInSchema = rewriteRecord((GenericRecord) avroRecord.get());</span><br><span class="line">      storageWriter.writeAvroWithMetadata(recordWithMetadataInSchema, record);</span><br><span class="line">      <span class="comment">// update the new location of record, so we know where to find it next</span></span><br><span class="line">      record.unseal();</span><br><span class="line">      record.setNewLocation(<span class="keyword">new</span> HoodieRecordLocation(instantTime, writeStatus.getFileId()));</span><br><span class="line">      record.seal();</span><br><span class="line">      recordsWritten++;</span><br><span class="line">      insertRecordsWritten++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      recordsDeleted++;</span><br><span class="line">    &#125;</span><br><span class="line">    writeStatus.markSuccess(record, recordMetadata);</span><br><span class="line">    <span class="comment">// deflate record payload after recording success. This will help users access payload as a</span></span><br><span class="line">    <span class="comment">// part of marking</span></span><br><span class="line">    <span class="comment">// record successful.</span></span><br><span class="line">    record.deflate();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    <span class="comment">// Not throwing exception from here, since we don't want to fail the entire job</span></span><br><span class="line">    <span class="comment">// for a single record</span></span><br><span class="line">    writeStatus.markFailure(record, t, recordMetadata);</span><br><span class="line">    LOG.error(<span class="string">"Error writing record "</span> + record, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.io.storage.HoodieParquetWriter#writeAvroWithMetadata</code>
メソッドは以下の通りである。 つまり、
<code>org.apache.parquet.hadoop.ParquetWriter#write</code>
を用いてParquet内に、Avroレコードを書き出していることがわかる。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeAvroWithMetadata</span><span class="params">(R avroRecord, HoodieRecord record)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  String seqId =</span><br><span class="line">      HoodieRecord.generateSequenceId(commitTime, TaskContext.getPartitionId(), recordIndex.getAndIncrement());</span><br><span class="line">  HoodieAvroUtils.addHoodieKeyToRecord((GenericRecord) avroRecord, record.getRecordKey(), record.getPartitionPath(),</span><br><span class="line">      file.getName());</span><br><span class="line">  HoodieAvroUtils.addCommitMetadataToRecord((GenericRecord) avroRecord, commitTime, seqId);</span><br><span class="line">  <span class="keyword">super</span>.write(avroRecord);</span><br><span class="line">  writeSupport.add(record.getRecordKey());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>今回のクイックスタートの例では、 <code>avroRecord</code>
には以下のような内容が含まれていた。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;GenericData$Record@18566&#125; &quot;&#123;&quot;_hoodie_commit_time&quot;: &quot;20200331002133&quot;, &quot;_hoodie_commit_seqno&quot;: &quot;20200331002133_0_44&quot;, &quot;_hoodie_record_key&quot;: &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;, &quot;_hoodie_partition_path&quot;: &quot;asia/india/chennai&quot;, &quot;_hoodie_file_name&quot;: &quot;317a54b0-70b8-4bdc-bfde-12ba4fde982b-0_0-207-301_20200331002133.parquet&quot;, &quot;begin_lat&quot;: 0.4789745387904072, &quot;begin_lon&quot;: 0.14781856144057215, &quot;driver&quot;: &quot;driver-022&quot;, &quot;end_lat&quot;: 0.10509642405359532, &quot;end_lon&quot;: 0.07682825311613706, &quot;fare&quot;: 30.429177017810616, &quot;partitionpath&quot;: &quot;asia/india/chennai&quot;, &quot;rider&quot;: &quot;rider-022&quot;, &quot;ts&quot;: 0.0, &quot;uuid&quot;: &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;&#125;&quot;</span><br><span class="line"> schema = &#123;Schema$RecordSchema@18582&#125; &quot;&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_trips_cow_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_trips_cow&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;_hoodie_commit_time&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_commit_seqno&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_record_key&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_partition_path&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_file_name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;begin_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;begin_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;driver&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;fare&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;rider&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;&quot;</span><br><span class="line"> values = &#123;Object[15]@18583&#125; </span><br><span class="line">  0 = &quot;20200331002133&quot;</span><br><span class="line">  1 = &quot;20200331002133_0_44&quot;</span><br><span class="line">  2 = &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;</span><br><span class="line">  3 = &quot;asia/india/chennai&quot;</span><br><span class="line">  4 = &quot;317a54b0-70b8-4bdc-bfde-12ba4fde982b-0_0-207-301_20200331002133.parquet&quot;</span><br><span class="line">  5 = &#123;Double@18596&#125; 0.4789745387904072</span><br><span class="line">  6 = &#123;Double@18597&#125; 0.14781856144057215</span><br><span class="line">  7 = &#123;Utf8@18598&#125; &quot;driver-022&quot;</span><br><span class="line">  8 = &#123;Double@18599&#125; 0.10509642405359532</span><br><span class="line">  9 = &#123;Double@18600&#125; 0.07682825311613706</span><br><span class="line">  10 = &#123;Double@18601&#125; 30.429177017810616</span><br><span class="line">  11 = &#123;Utf8@18602&#125; &quot;asia/india/chennai&quot;</span><br><span class="line">  12 = &#123;Utf8@18603&#125; &quot;rider-022&quot;</span><br><span class="line">  13 = &#123;Double@18604&#125; 0.0</span><br><span class="line">  14 = &#123;Utf8@18605&#125; &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;</span><br></pre></td></tr></table></figure>
<h3><span id="orgapachehudidefaultsourcecreaterelation読み込み">org.apache.hudi.DefaultSource#createRelation（読み込み）</span></h3>
<p>当該メソッドのポイントを確認する。</p>
<p><code>hoodie.datasource.query.type</code>
の種類によって返すRelationが変わる。</p>
<p>org/apache/hudi/DefaultSource.scala:60</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (parameters(<span class="type">QUERY_TYPE_OPT_KEY</span>).equals(<span class="type">QUERY_TYPE_SNAPSHOT_OPT_VAL</span>)) &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (parameters(<span class="type">QUERY_TYPE_OPT_KEY</span>).equals(<span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>)) &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">HoodieException</span>(<span class="string">"Invalid query type :"</span> + parameters(<span class="type">QUERY_TYPE_OPT_KEY</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、 <code>snapshot</code> 、もしくは
<code>incremental</code> クエリタイプである。 なお、以下の通り、
<code>MERGE_ON_READ</code> テーブルに対する <code>snapshot</code>
クエリタイプは利用できない。 もし使いたければ、SparkのData
Source機能ではなく、Hiveテーブルとして読み込むこと。</p>
<p>org/apache/hudi/DefaultSource.scala:69</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log.warn(<span class="string">"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. "</span> +</span><br><span class="line">  <span class="string">"Please query the Hive table registered using Spark SQL."</span>)</span><br></pre></td></tr></table></figure>
<p>まずクエリタイプが <code>snapshot</code> である場合は、
以下の通り、Parquetとして読み込みが定義され、Relationが返される。</p>
<p>org/apache/hudi/DefaultSource.scala:72</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataSource</span>.apply(</span><br><span class="line">  sparkSession = sqlContext.sparkSession,</span><br><span class="line">  userSpecifiedSchema = <span class="type">Option</span>(schema),</span><br><span class="line">  className = <span class="string">"parquet"</span>,</span><br><span class="line">  options = parameters)</span><br><span class="line">  .resolveRelation()</span><br></pre></td></tr></table></figure>
<p>例えば、クイックスタートの例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsSnapshotDF = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*/*/*"</span>)</span><br><span class="line">scala&gt; tripsSnapshotDF.createOrReplaceTempView(<span class="string">"hudi_trips_snapshot"</span>)</span><br></pre></td></tr></table></figure>
<p>では、こちらのタイプ。
ParquetベースのRelation（実際には、HadoopFsRelation）が返される。
上記の例では、当該RelationのrootPathsに、以下のような値が含まれる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rootPaths = &#123;$colon$colon@14885&#125; &quot;::&quot; size = 6</span><br><span class="line"> 0 = &#123;Path@15421&#125; &quot;file:/tmp/hudi_trips_cow/americas/brazil/sao_paulo/ae28c85a-38f0-487f-a42d-3a0babc9d321-0_0-21-25_20200329002247.parquet&quot;</span><br><span class="line"> 1 = &#123;Path@15422&#125; &quot;file:/tmp/hudi_trips_cow/americas/brazil/sao_paulo/.hoodie_partition_metadata&quot;</span><br><span class="line"> 2 = &#123;Path@15423&#125; &quot;file:/tmp/hudi_trips_cow/americas/united_states/san_francisco/849db286-1cbe-4a1f-b544-9939893e99f8-0_1-21-26_20200329002247.parquet&quot;</span><br><span class="line"> 3 = &#123;Path@15424&#125; &quot;file:/tmp/hudi_trips_cow/americas/united_states/san_francisco/.hoodie_partition_metadata&quot;</span><br><span class="line"> 4 = &#123;Path@15425&#125; &quot;file:/tmp/hudi_trips_cow/asia/india/chennai/2ebfbab0-4f8f-42db-b79e-1c0cbcc3cf39-0_2-21-27_20200329002247.parquet&quot;</span><br><span class="line"> 5 = &#123;Path@15426&#125; &quot;file:/tmp/hudi_trips_cow/asia/india/chennai/.hoodie_partition_metadata&quot;</span><br></pre></td></tr></table></figure>
<p>次にクエリタイプが <code>incremental</code> である場合は、
以下の通り、
<code>org.apache.hudi.IncrementalRelation#IncrementalRelation</code>
が返される。</p>
<p>org/apache/hudi/DefaultSource.scala:79</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">IncrementalRelation</span>(sqlContext, path.get, optParams, schema)</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsIncrementalDF = spark.read.format(<span class="string">"hudi"</span>).</span><br><span class="line">         option(<span class="type">QUERY_TYPE_OPT_KEY</span>, <span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>).</span><br><span class="line">         option(<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>, beginTime).</span><br><span class="line">         load(basePath)</span><br><span class="line">scala&gt; tripsIncrementalDF.createOrReplaceTempView(<span class="string">"hudi_trips_incremental"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br></pre></td></tr></table></figure>
<p>では、
<code>org.apache.hudi.IncrementalRelation#IncrementalRelation</code>
が戻り値として返される。</p>
<h3><span id="incrementalrelation">IncrementalRelation</span></h3>
<h4><span id="コンストラクタ">コンストラクタ</span></h4>
<p>Parquetをファイルを単純に読めば良いのと比べて、格納された最新データを返すようにしないとならないので
それなりに複雑なRelationとなっている。</p>
<p>以下、簡単にコンストラクタのポイントを確認する。</p>
<p>最初にメタデータを取得するクライアント。
コミット、セーブポイント、コンパクションなどの情報を得られるようになる。</p>
<p>org/apache/hudi/IncrementalRelation.scala:51</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> metaClient = <span class="keyword">new</span> <span class="type">HoodieTableMetaClient</span>(sqlContext.sparkContext.hadoopConfiguration, basePath, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例では、 <code>metaPath</code> は、
<code>file:/tmp/hudi_trips_cow/.hoodie</code> だった。</p>
<p>続いてテーブル情報のインスタンスを取得する。
テーブル情報からタイムラインを取り出す。</p>
<p>org/apache/hudi/IncrementalRelation.scala:57</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> hoodieTable = <span class="type">HoodieTable</span>.getHoodieTable(metaClient, <span class="type">HoodieWriteConfig</span>.newBuilder().withPath(basePath).build(),</span><br><span class="line">  sqlContext.sparkContext)</span><br><span class="line"><span class="keyword">val</span> commitTimeline = hoodieTable.getMetaClient.getCommitTimeline.filterCompletedInstants()</span><br><span class="line"><span class="keyword">if</span> (commitTimeline.empty()) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">HoodieException</span>(<span class="string">"No instants to incrementally pull"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (!optParams.contains(<span class="type">DataSourceReadOptions</span>.<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>)) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">HoodieException</span>(<span class="string">s"Specify the begin instant time to pull from using "</span> +</span><br><span class="line">    <span class="string">s"option <span class="subst">$&#123;DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY&#125;</span>"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例で実際に生成されたタイムラインは以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instants = &#123;ArrayList@25586&#125;  size = 3</span><br><span class="line"> 0 = &#123;HoodieInstant@25589&#125; &quot;[20200330002239__commit__COMPLETED]&quot;</span><br><span class="line"> 1 = &#123;HoodieInstant@25590&#125; &quot;[20200330002354__commit__COMPLETED]&quot;</span><br><span class="line"> 2 = &#123;HoodieInstant@25591&#125; &quot;[20200330003142__commit__COMPLETED]&quot;</span><br></pre></td></tr></table></figure>
<p>オプションとして与えられた「はじめ」と「おわり」から、
対象となるタイムラインを構成する。
タイムライン上、最も新しいインスタンスを取得し、
Parquetファイルからスキーマを読んでいる。</p>
<p>org/apache/hudi/IncrementalRelation.scala:68</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lastInstant = commitTimeline.lastInstant().get()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> commitsToReturn = commitTimeline.findInstantsInRange(</span><br><span class="line">  optParams(<span class="type">DataSourceReadOptions</span>.<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>),</span><br><span class="line">  optParams.getOrElse(<span class="type">DataSourceReadOptions</span>.<span class="type">END_INSTANTTIME_OPT_KEY</span>, lastInstant.getTimestamp))</span><br><span class="line">  .getInstants.iterator().toList</span><br><span class="line"></span><br><span class="line"><span class="comment">// use schema from a file produced in the latest instant</span></span><br><span class="line"><span class="keyword">val</span> latestSchema = &#123;</span><br><span class="line">  <span class="comment">// use last instant if instant range is empty</span></span><br><span class="line">  <span class="keyword">val</span> instant = commitsToReturn.lastOption.getOrElse(lastInstant)</span><br><span class="line">  <span class="keyword">val</span> latestMeta = <span class="type">HoodieCommitMetadata</span></span><br><span class="line">        .fromBytes(commitTimeline.getInstantDetails(instant).get, classOf[<span class="type">HoodieCommitMetadata</span>])</span><br><span class="line">  <span class="keyword">val</span> metaFilePath = latestMeta.getFileIdAndFullPaths(basePath).values().iterator().next()</span><br><span class="line">  <span class="type">AvroConversionUtils</span>.convertAvroSchemaToStructType(<span class="type">ParquetUtils</span>.readAvroSchema(</span><br><span class="line">    sqlContext.sparkContext.hadoopConfiguration, <span class="keyword">new</span> <span class="type">Path</span>(metaFilePath)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例では、 <code>commitsToReturn</code>
は以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;$colon$colon@25626&#125; &quot;::&quot; size = 1</span><br><span class="line"> 0 = &#123;HoodieInstant@25591&#125; &quot;[20200330003142__commit__COMPLETED]&quot;</span><br><span class="line">  state = &#123;HoodieInstant$State@25602&#125; &quot;COMPLETED&quot;</span><br><span class="line">  action = &quot;commit&quot;</span><br><span class="line">  timestamp = &quot;20200330003142&quot;</span><br></pre></td></tr></table></figure>
<p>また、少々気になるのは、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">AvroConversionUtils</span>.convertAvroSchemaToStructType(<span class="type">ParquetUtils</span>.readAvroSchema(</span><br><span class="line">  sqlContext.sparkContext.hadoopConfiguration, <span class="keyword">new</span> <span class="type">Path</span>(metaFilePath)))</span><br></pre></td></tr></table></figure>
<p>という箇所で、もともとParquet形式のデータからAvro形式のスキーマを取り出し、それをさらにSparkのStructTypeに変換しているところ。
実際にParquetのfooterから取り出したスキーマ情報を、AvroのSchemaに変換しているのは以下の箇所。</p>
<p>org/apache/hudi/common/util/ParquetUtils.java:140</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public static <span class="type">Schema</span> readAvroSchema(<span class="type">Configuration</span> configuration, <span class="type">Path</span> parquetFilePath) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">AvroSchemaConverter</span>().convert(readSchema(configuration, parquetFilePath));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Parquet自身にAvroへの変換器
<code>org.apache.parquet.avro.AvroSchemaConverter</code>
が備わっているので便利？</li>
<li>SparkのData
Source機能でDataFrame化してからスキーマを取り出すと、一度読み込みが生じていしまうから非効率？</li>
</ul>
<p>という理由が想像されるが、やや回りくどいような印象を持った。
★要確認</p>
<p>本編に戻る。続いてフィルタを定義。</p>
<p>org/apache/hudi/IncrementalRelation.scala:86</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> filters = &#123;</span><br><span class="line">  <span class="keyword">if</span> (optParams.contains(<span class="type">DataSourceReadOptions</span>.<span class="type">PUSH_DOWN_INCR_FILTERS_OPT_KEY</span>)) &#123;</span><br><span class="line">    <span class="keyword">val</span> filterStr = optParams.getOrElse(</span><br><span class="line">      <span class="type">DataSourceReadOptions</span>.<span class="type">PUSH_DOWN_INCR_FILTERS_OPT_KEY</span>,</span><br><span class="line">      <span class="type">DataSourceReadOptions</span>.<span class="type">DEFAULT_PUSH_DOWN_FILTERS_OPT_VAL</span>)</span><br><span class="line">    filterStr.split(<span class="string">","</span>).filter(!_.isEmpty)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">Array</span>[<span class="type">String</span>]()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ここまでがコンストラクタ。</p>
<h4><span id="buildscan">buildScan</span></h4>
<p>実際にSparkのData
Sourceで読み込むときに用いられる読み込みの手段が定義されている。
以下にポイントを述べる。</p>
<p>org/apache/hudi/IncrementalRelation.scala:99</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>ファイルIDとフルPATHのマップを作る。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileIdToFullPath = mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line"><span class="keyword">for</span> (commit &lt;- commitsToReturn) &#123;</span><br><span class="line">  <span class="keyword">val</span> metadata: <span class="type">HoodieCommitMetadata</span> = <span class="type">HoodieCommitMetadata</span>.fromBytes(commitTimeline.getInstantDetails(commit)</span><br><span class="line">    .get, classOf[<span class="type">HoodieCommitMetadata</span>])</span><br><span class="line">  fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記マップに対し、必要に応じてフィルタを適用する。</p>
<p>org/apache/hudi/IncrementalRelation.scala:106</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pathGlobPattern = optParams.getOrElse(</span><br><span class="line">  <span class="type">DataSourceReadOptions</span>.<span class="type">INCR_PATH_GLOB_OPT_KEY</span>,</span><br><span class="line">  <span class="type">DataSourceReadOptions</span>.<span class="type">DEFAULT_INCR_PATH_GLOB_OPT_VAL</span>)</span><br><span class="line"><span class="keyword">val</span> filteredFullPath = <span class="keyword">if</span>(!pathGlobPattern.equals(<span class="type">DataSourceReadOptions</span>.<span class="type">DEFAULT_INCR_PATH_GLOB_OPT_VAL</span>)) &#123;</span><br><span class="line">  <span class="keyword">val</span> globMatcher = <span class="keyword">new</span> <span class="type">GlobPattern</span>(<span class="string">"*"</span> + pathGlobPattern)</span><br><span class="line">  fileIdToFullPath.filter(p =&gt; globMatcher.matches(p._2))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  fileIdToFullPath</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>コンストラクタで定義されたフィルタを適用しながら、
対象となるParquetファイルを読み込み、RDDを生成する。</p>
<p>org/apache/hudi/IncrementalRelation.scala:117</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sparkContext.hadoopConfiguration.unset(<span class="string">"mapreduce.input.pathFilter.class"</span>)</span><br><span class="line"><span class="keyword">val</span> sOpts = optParams.filter(p =&gt; !p._1.equalsIgnoreCase(<span class="string">"path"</span>))</span><br><span class="line"><span class="keyword">if</span> (filteredFullPath.isEmpty) &#123;</span><br><span class="line">  sqlContext.sparkContext.emptyRDD[<span class="type">Row</span>]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  log.info(<span class="string">"Additional Filters to be applied to incremental source are :"</span> + filters)</span><br><span class="line">  filters.foldLeft(sqlContext.read.options(sOpts)</span><br><span class="line">    .schema(latestSchema)</span><br><span class="line">    .parquet(filteredFullPath.values.toList: _*)</span><br><span class="line">    .filter(<span class="type">String</span>.format(<span class="string">"%s &gt;= '%s'"</span>, <span class="type">HoodieRecord</span>.<span class="type">COMMIT_TIME_METADATA_FIELD</span>, commitsToReturn.head.getTimestamp))</span><br><span class="line">    .filter(<span class="type">String</span>.format(<span class="string">"%s &lt;= '%s'"</span>,</span><br><span class="line">      <span class="type">HoodieRecord</span>.<span class="type">COMMIT_TIME_METADATA_FIELD</span>, commitsToReturn.last.getTimestamp)))((e, f) =&gt; e.filter(f))</span><br><span class="line">    .toDF().rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1><span id="hudiへの書き込み">Hudiへの書き込み</span></h1>
<p><a href="https://hudi.apache.org/docs/writing_data.html" target="_blank" rel="noopener">Writing Hudi
Tables</a> をベースに調べる。</p>
<h2><span id="オペレーション種類">オペレーション種類</span></h2>
<p>書き込みのオペレーション種類は、upsert、insert、bulk_insert。
クイックスタートにはbulk_insertはなかった。</p>
<h2><span id="deltastreamer">DeltaStreamer</span></h2>
<p>ユーティリティとして付属するDeltaStreamerを用いると、
Kafka等からデータを取り込める。
Avro等のスキーマのデータを読み取れる。</p>
<h3><span id="動作確認">動作確認</span></h3>
<h4><span id="パッケージ化">パッケージ化</span></h4>
<p><a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a> の手順に基づくと、
ビルドされたユーティリティを使うことになるので、
予めパッケージ化しておく。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p ~/Sources</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/Sources</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/apache/incubator-hudi.git incubator-hudi-052</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> incubator-hudi-052</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git checkout -b release-0.5.2-incubating refs/tags/release-0.5.2-incubating</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mvn clean package -DskipTests -DskipITs</span></span><br></pre></td></tr></table></figure>
<h4><span id="実行">実行</span></h4>
<p><a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a>に基づくと、Confluentメンバが作成した （ <a href="https://github.com/apurvam/streams-prototyping" target="_blank" rel="noopener">apurvam
streams-prototyping</a> ）サンプルデータ作成用のAvroスキーマと Confluent
PlatformのKSQLのユーティリティを 使ってサンプルデータを作成する。</p>
<p>ついては。予めConfluent Platformをインストールしておくこと。</p>
<p>まずはスキーマをダウンロードする。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl https://raw.githubusercontent.com/apurvam/streams-prototyping/master/src/main/resources/impressions.avro &gt; /tmp/impressions.avro</span></span><br></pre></td></tr></table></figure>
<p>テストデータを生成する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ksql-datagen schema=/tmp/impressions.avro format=avro topic=impressions key=impressionid</span></span><br></pre></td></tr></table></figure>
<p>別の端末を開き、ユーティリティを起動する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> SPARK_HOME=/opt/spark/default</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/Sources/incubator-hudi-052</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.5.2-incubating.jar \</span></span><br><span class="line">  --props file://$&#123;PWD&#125;/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \</span><br><span class="line">  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \</span><br><span class="line">  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \</span><br><span class="line">  --source-ordering-field impresssiontime \</span><br><span class="line">  --target-base-path file:\/\/\/tmp/hudi-deltastreamer-op \</span><br><span class="line">  --target-table uber.impressions \</span><br><span class="line">  --table-type COPY_ON_WRITE \</span><br><span class="line">  --op BULK_INSERT</span><br></pre></td></tr></table></figure>
<p>なお、 <a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a> から2箇所修正した。（JarファイルPATH、
<code>--table-type</code> オプション追加。</p>
<p>Kafkaから読み込んで書き出したデータ（
<code>/tmp/hudi-deltastreamer-op</code> ）を確認してみる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-shell \</span></span><br><span class="line">  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.2-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \</span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'</span><br></pre></td></tr></table></figure>
<p>シェルが起動したら、以下の通り読み込んで見る。 なお、ここでは
<code>userid</code>
がパーティションキーとなっているので、ロード時にそれを指定した。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> basePath = <span class="string">"file:///tmp/hudi-deltastreamer-op"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> impressionDF = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*"</span>)</span><br></pre></td></tr></table></figure>
<p>内容は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; impressionDF.show</span><br><span class="line">+-------------------+--------------------+------------------+----------------------+--------------------+---------------+--------------+-------+-----+</span><br><span class="line">|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|impresssiontime|  impressionid| userid| adid|</span><br><span class="line">+-------------------+--------------------+------------------+----------------------+--------------------+---------------+--------------+-------+-----+</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_106|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586096500438</span>|impression_106|user_83|ad_57|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_107|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586096464324</span>|impression_107|user_83|ad_11|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_111|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586096366450</span>|impression_111|user_83|ad_14|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_111|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586099019181</span>|impression_111|user_83|ad_38|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_116|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586099146437</span>|impression_116|user_83|ad_48|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_121|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586098316334</span>|impression_121|user_83|ad_26|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<h3><span id="実装確認">実装確認</span></h3>
<p><code>org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer</code>
クラスの実装を確認する。</p>
<p>まずmainは以下の通り。</p>
<p>org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java:298</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Config cfg = <span class="keyword">new</span> Config();</span><br><span class="line">  JCommander cmd = <span class="keyword">new</span> JCommander(cfg, <span class="keyword">null</span>, args);</span><br><span class="line">  <span class="keyword">if</span> (cfg.help || args.length == <span class="number">0</span>) &#123;</span><br><span class="line">    cmd.usage();</span><br><span class="line">    System.exit(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Map&lt;String, String&gt; additionalSparkConfigs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg);</span><br><span class="line">  JavaSparkContext jssc =</span><br><span class="line">      UtilHelpers.buildSparkContext(<span class="string">"delta-streamer-"</span> + cfg.targetTableName, cfg.sparkMaster, additionalSparkConfigs);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> HoodieDeltaStreamer(cfg, jssc).sync();</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    jssc.stop();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、
<code>org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer#sync</code>
メソッドがエントリポイント。</p>
<p>org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java:116</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sync</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (cfg.continuousMode) &#123;</span><br><span class="line">    deltaSyncService.start(<span class="keyword">this</span>::onDeltaSyncShutdown);</span><br><span class="line">    deltaSyncService.waitForShutdown();</span><br><span class="line">    LOG.info(<span class="string">"Delta Sync shutting down"</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    LOG.info(<span class="string">"Delta Streamer running only single round"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      deltaSyncService.getDeltaSync().syncOnce();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">      LOG.error(<span class="string">"Got error running delta sync once. Shutting down"</span>, ex);</span><br><span class="line">      <span class="keyword">throw</span> ex;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      deltaSyncService.close();</span><br><span class="line">      LOG.info(<span class="string">"Shut down delta streamer"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、 <code>continous</code>
モードかどうかで動作が変わる。</p>
<p>ここでは一旦、ワンショットの場合を確認する。</p>
<p>上記の通り、
<code>org.apache.hudi.utilities.deltastreamer.DeltaSync#syncOnce</code>
メソッドがエントリポイント。
当該メソッドは以下のようにシンプルな内容。</p>
<p>org/apache/hudi/utilities/deltastreamer/DeltaSync.java:218</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Option&lt;String&gt; <span class="title">syncOnce</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Option&lt;String&gt; scheduledCompaction = Option.empty();</span><br><span class="line">  HoodieDeltaStreamerMetrics metrics = <span class="keyword">new</span> HoodieDeltaStreamerMetrics(getHoodieClientConfig(schemaProvider));</span><br><span class="line">  Timer.Context overallTimerContext = metrics.getOverallTimerContext();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Refresh Timeline</span></span><br><span class="line">  refreshTimeline();</span><br><span class="line"></span><br><span class="line">  Pair&lt;SchemaProvider, Pair&lt;String, JavaRDD&lt;HoodieRecord&gt;&gt;&gt; srcRecordsWithCkpt = readFromSource(commitTimelineOpt);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">null</span> != srcRecordsWithCkpt) &#123;</span><br><span class="line">    <span class="comment">// this is the first input batch. If schemaProvider not set, use it and register Avro Schema and start</span></span><br><span class="line">    <span class="comment">// compactor</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == schemaProvider) &#123;</span><br><span class="line">      <span class="comment">// Set the schemaProvider if not user-provided</span></span><br><span class="line">      <span class="keyword">this</span>.schemaProvider = srcRecordsWithCkpt.getKey();</span><br><span class="line">      <span class="comment">// Setup HoodieWriteClient and compaction now that we decided on schema</span></span><br><span class="line">      setupWriteClient();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    scheduledCompaction = writeToSink(srcRecordsWithCkpt.getRight().getRight(),</span><br><span class="line">        srcRecordsWithCkpt.getRight().getLeft(), metrics, overallTimerContext);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Clear persistent RDDs</span></span><br><span class="line">  jssc.getPersistentRDDs().values().forEach(JavaRDD::unpersist);</span><br><span class="line">  <span class="keyword">return</span> scheduledCompaction;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最初にメトリクスの準備、データソースから読み出してRDD化する定義（
<code>org.apache.hudi.utilities.deltastreamer.DeltaSync#readFromSource</code>
メソッド） その後、
<code>org.apache.hudi.utilities.deltastreamer.DeltaSync#writeToSink</code>
メソッドにより、定義されたRDDの内容を実際に書き込む。</p>
<p>ここでは上記メソッドを確認する。</p>
<p>まず与えられたRDDから重複排除する。</p>
<p>org/apache/hudi/utilities/deltastreamer/DeltaSync.java:352</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> Option&lt;String&gt; <span class="title">writeToSink</span><span class="params">(JavaRDD&lt;HoodieRecord&gt; records, String checkpointStr,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     HoodieDeltaStreamerMetrics metrics, Timer.Context overallTimerContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    Option&lt;String&gt; scheduledCompactionInstant = Option.empty();</span><br><span class="line">    <span class="comment">// filter dupes if needed</span></span><br><span class="line">    <span class="keyword">if</span> (cfg.filterDupes) &#123;</span><br><span class="line">      <span class="comment">// turn upserts to insert</span></span><br><span class="line">      cfg.operation = cfg.operation == Operation.UPSERT ? Operation.INSERT : cfg.operation;</span><br><span class="line">      records = DataSourceUtils.dropDuplicates(jssc, records, writeClient.getConfig());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isEmpty = records.isEmpty();</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>その後実際の書き込みになるが、そのとき採用したオペレーション種類によって動作が異なる。</p>
<p>org/apache/hudi/utilities/deltastreamer/DeltaSync.java:369</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (cfg.operation == Operation.INSERT) &#123;</span><br><span class="line">  writeStatusRDD = writeClient.insert(records, instantTime);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (cfg.operation == Operation.UPSERT) &#123;</span><br><span class="line">  writeStatusRDD = writeClient.upsert(records, instantTime);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (cfg.operation == Operation.BULK_INSERT) &#123;</span><br><span class="line">  writeStatusRDD = writeClient.bulkInsert(records, instantTime);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> HoodieDeltaStreamerException(<span class="string">"Unknown operation :"</span> + cfg.operation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="bulkinsert">bulkInsert</span></h4>
<p>ここではためしに
<code>org.apache.hudi.client.HoodieWriteClient#bulkInsert</code>
メソッドを確認してみる。</p>
<p>当該メソッドでは、最初にCOPY_ON_WRITEかMERGE_ON_READかに応じて、それぞれの種類のテーブル情報を取得する。
その後、
<code>org.apache.hudi.client.HoodieWriteClient#bulkInsertInternal</code>
メソッドを使ってデータを書き込む。</p>
<p>なお、その間で重複排除されているが、上記の通り、もともと重複排除しているはずなので、要確認。（重複排除のロジックが異なるのかどうか、など）
パット見た感じ、
<code>org.apache.hudi.DataSourceUtils#dropDuplicates</code>
メソッドはロケーション情報（インデックス？）がない場合をドロップする。
<code>org.apache.hudi.client.HoodieWriteClient#combineOnCondition</code>
メソッドはキーに基づきreduce処理する。 という違いがあるようだ。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:300</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;WriteStatus&gt; <span class="title">bulkInsert</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, <span class="keyword">final</span> String instantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">    Option&lt;UserDefinedBulkInsertPartitioner&gt; bulkInsertPartitioner)</span> </span>&#123;</span><br><span class="line">  HoodieTable&lt;T&gt; table = getTableAndInitCtx(WriteOperationType.BULK_INSERT);</span><br><span class="line">  setOperationType(WriteOperationType.BULK_INSERT);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// De-dupe/merge if needed</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords =</span><br><span class="line">        combineOnCondition(config.shouldCombineBeforeInsert(), records, config.getInsertShuffleParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bulkInsertInternal(dedupedRecords, instantTime, table, bulkInsertPartitioner);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> HoodieInsertException) &#123;</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieInsertException(<span class="string">"Failed to bulk insert for commit time "</span> + instantTime, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、
<code>org.apache.hudi.client.HoodieWriteClient#bulkInsertInternal</code>
メソッドが中で用いられている。
当該メソッドでは、再パーティションないしソートが行われた後、書き込みが実行される。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:412</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;WriteStatus&gt; writeStatusRDD = repartitionedRecords</span><br><span class="line">    .mapPartitionsWithIndex(<span class="keyword">new</span> BulkInsertMapFunction&lt;T&gt;(instantTime, config, table, fileIDPrefixes), <span class="keyword">true</span>)</span><br><span class="line">    .flatMap(List::iterator);</span><br></pre></td></tr></table></figure>
<p>ポイントは、<code>org.apache.hudi.execution.BulkInsertMapFunction</code>
クラスである。 このクラスが関数として渡されている。
<code>org.apache.hudi.execution.BulkInsertMapFunction#call</code>
メソッドは以下の通り。</p>
<p>org/apache/hudi/execution/BulkInsertMapFunction.java:52</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; call(Integer partition, Iterator&lt;HoodieRecord&lt;T&gt;&gt; sortedRecordItr) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(sortedRecordItr, config, instantTime, hoodieTable,</span><br><span class="line">      fileIDPrefixes.get(partition), hoodieTable.getSparkTaskContextSupplier());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable</code>
クラスについては、別の節で書いたとおり。</p>
<h4><span id="insert">insert</span></h4>
<p><code>org.apache.hudi.client.HoodieWriteClient#insert</code>
メソッド。</p>
<p>大まかな構造は、 <code>bulkInsert</code> と同様。 ポイントは、
<code>org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal</code>
メソッド。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:229</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;WriteStatus&gt; <span class="title">insert</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, <span class="keyword">final</span> String instantTime)</span> </span>&#123;</span><br><span class="line">  HoodieTable&lt;T&gt; table = getTableAndInitCtx(WriteOperationType.INSERT);</span><br><span class="line">  setOperationType(WriteOperationType.INSERT);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// De-dupe/merge if needed</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords =</span><br><span class="line">        combineOnCondition(config.shouldCombineBeforeInsert(), records, config.getInsertShuffleParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> upsertRecordsInternal(dedupedRecords, instantTime, table, <span class="keyword">false</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> HoodieInsertException) &#123;</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieInsertException(<span class="string">"Failed to insert for commit time "</span> + instantTime, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、挿入対象のデータを表すRDDを引数に取り、データを書き込む。
これは、upsertのときと同じメソッドである。第4引数でinsertかupsertかを分ける。</p>
<p>当該メソッドは以下の通り。 <code>bulkInsert</code>
と同様にリパーティションなどを経て、
<code>org.apache.hudi.table.HoodieTable#handleUpsertPartition</code>
が呼び出される。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:457</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> JavaRDD&lt;WriteStatus&gt; <span class="title">upsertRecordsInternal</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; preppedRecords, String instantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">      HoodieTable&lt;T&gt; hoodieTable, <span class="keyword">final</span> <span class="keyword">boolean</span> isUpsert)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">    JavaRDD&lt;WriteStatus&gt; writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (isUpsert) &#123;</span><br><span class="line">        <span class="keyword">return</span> hoodieTable.handleUpsertPartition(instantTime, partition, recordItr, partitioner);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> hoodieTable.handleInsertPartition(instantTime, partition, recordItr, partitioner);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="keyword">true</span>).flatMap(List::iterator);</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.table.HoodieTable#handleUpsertPartition</code>
と <code>org.apache.hudi.table.HoodieTable#handleInsertPartition</code>
が用いられている。 今回は、insertなので後者。</p>
<p>なお、
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsertPartition</code>
は以下の通り、実態としては
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleUpsertPartition</code>
である。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsertPartition(String instantTime, Integer partition, Iterator recordItr,</span><br><span class="line">                                                         Partitioner partitioner) &#123;</span><br><span class="line">  <span class="keyword">return</span> handleUpsertPartition(instantTime, partition, recordItr, partitioner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当該メソッドは以下の通り。
insertやupsertでは、RDDひとつを1バケットと表現している。
バケットの情報から、insertやupdateの情報を取得して用いる。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String instantTime, Integer partition, Iterator recordItr,</span><br><span class="line">                                                         Partitioner partitioner) &#123;</span><br><span class="line">  UpsertPartitioner upsertPartitioner = (UpsertPartitioner) partitioner;</span><br><span class="line">  BucketInfo binfo = upsertPartitioner.getBucketInfo(partition);</span><br><span class="line">  BucketType btype = binfo.bucketType;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (btype.equals(BucketType.INSERT)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleInsert(instantTime, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (btype.equals(BucketType.UPDATE)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleUpdate(instantTime, binfo.partitionPath, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(<span class="string">"Unknown bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    String msg = <span class="string">"Error upserting bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition;</span><br><span class="line">    LOG.error(msg, t);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(msg, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例えば、insertの場合は、
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsert</code>
が呼び出される。 当該メソッドでは、戻り値として
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#CopyOnWriteLazyInsertIterable</code>
が返される。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:186</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records</span></span><br><span class="line">  <span class="keyword">if</span> (!recordItr.hasNext()) &#123;</span><br><span class="line">    LOG.info(<span class="string">"Empty partition"</span>);</span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, <span class="keyword">this</span>, idPfx, sparkTaskContextSupplier);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>このメソッドについては上記ですでに説明したとおり。</p>
<h4><span id="hoodiecopyonwritetableと-hoodiemergeonreadtable">HoodieCopyOnWriteTable
と HoodieMergeOnReadTable</span></h4>
<p>テーブルの種類によって、書き込みの実装上どういう違いがあるかを確認する。</p>
<p>例えば、<code>handleInsert</code>
メソッドを確認する。なお、当該メソッドには同期的、非同期的な処理方式がそれぞれ実装されている。</p>
<p>HoodieCopyOnWriteTableの場合は以下の通り。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:186</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records</span></span><br><span class="line">  <span class="keyword">if</span> (!recordItr.hasNext()) &#123;</span><br><span class="line">    LOG.info(<span class="string">"Empty partition"</span>);</span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, <span class="keyword">this</span>, idPfx, sparkTaskContextSupplier);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String partitionPath, String fileId,</span><br><span class="line">    Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) &#123;</span><br><span class="line">  HoodieCreateHandle createHandle =</span><br><span class="line">      <span class="keyword">new</span> HoodieCreateHandle(config, instantTime, <span class="keyword">this</span>, partitionPath, fileId, recordItr, sparkTaskContextSupplier);</span><br><span class="line">  createHandle.write();</span><br><span class="line">  <span class="keyword">return</span> Collections.singletonList(Collections.singletonList(createHandle.close())).iterator();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上が非同期的な方式、下が同期的な方式と見られる。
なお、実装上は同期的な処理方式は今は使われていないようにもみえるが、要確認。</p>
<p>HoodieMergeOnReadTableの場合は、非同期的な処理だけoverrideされている。</p>
<p>org/apache/hudi/table/HoodieMergeOnReadTable.java:120</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// If canIndexLogFiles, write inserts to log files else write inserts to parquet files</span></span><br><span class="line">  <span class="keyword">if</span> (index.canIndexLogFiles()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MergeOnReadLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, <span class="keyword">this</span>, idPfx, sparkTaskContextSupplier);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">super</span>.handleInsert(instantTime, idPfx, recordItr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/03/25/Hudi/" data-id="clt1hv4qy01g81vqrnvzg919q" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-pandoc-template-and-css" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/03/06/pandoc-template-and-css/">pandoc template and css</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/03/06/pandoc-template-and-css/">
            <time datetime="2020-03-06T13:46:59.000Z" itemprop="datePublished">2020-03-06</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Documentation/">Documentation</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/pandoc/">pandoc</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://qiita.com/cawpea/items/cea1243e106ababd15e7" target="_blank" rel="noopener">Pandocを使ってMarkdownを整形されたHTMLに変換する</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>Pandocのバージョンは、 2.9.2を使用。</p>
<p><a href="https://qiita.com/cawpea/items/cea1243e106ababd15e7" target="_blank" rel="noopener">Pandocを使ってMarkdownを整形されたHTMLに変換する</a>
を参考に、テンプレートを作成してCSSを用いた。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p ~/.pandoc/templates</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pandoc -D html5 &gt; ~/.pandoc/templates/mytemplate.html</span></span><br></pre></td></tr></table></figure>
<p>テンプレートを適当にいじる。</p>
<p>その後、HTMLを以下のように生成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pandoc --css ./pandoc-github.css --template=mytemplate -i ./README.md -o ./README.html</span></span><br></pre></td></tr></table></figure>
<p>なお、GitHub風になるCSSは、 <a href="https://gist.github.com/dashed/6714393" target="_blank" rel="noopener">dashed/github-pandoc.css</a>
に公開されていたものを利用。 <code>--css</code>
はcssのURLを表すだけなので、上記の例ではREADME.htmlと同じディレクトリに
<code>pandoc-github.css</code> があることを期待する。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/03/06/pandoc-template-and-css/" data-id="clt1hv4fi00et1vqrmi17j6sq" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Flow-Engine-for-ML" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/02/16/Flow-Engine-for-ML/">Flow Engine for ML</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/02/16/Flow-Engine-for-ML/">
            <time datetime="2020-02-16T13:31:14.000Z" itemprop="datePublished">2020-02-16</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Flow-Engine/">Flow Engine</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Flow-Engine/">Flow Engine</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a>
<ul>
<li><a href="#総合" id="toc-総合">総合</a></li>
<li><a href="#azkaban" id="toc-azkaban">Azkaban</a></li>
</ul></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#よく名前の挙がるもの" id="toc-よく名前の挙がるもの">よく名前の挙がるもの</a></li>
<li><a href="#機械的な検索" id="toc-機械的な検索">機械的な検索</a>
<ul>
<li><a href="#airflowの代替" id="toc-airflowの代替">Airflowの代替</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<h2><span id="総合">総合</span></h2>
<ul>
<li><a href="https://alternativeto.net/software/apache-airflow/" target="_blank" rel="noopener">alternativetoでAirflowを検索した結果</a></li>
</ul>
<h2><span id="azkaban">Azkaban</span></h2>
<ul>
<li><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html#flow-2-0-basics" target="_blank" rel="noopener">Azkabanのフロー書き方</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>機械学習で利用されるフロー管理ツールを軽くさらってみる。</p>
<h2><span id="よく名前の挙がるもの">よく名前の挙がるもの</span></h2>
<ul>
<li>Apache Airflow</li>
<li>DigDag</li>
<li>Oozie</li>
</ul>
<h2><span id="機械的な検索">機械的な検索</span></h2>
<h3><span id="airflowの代替">Airflowの代替</span></h3>
<p><a href="https://alternativeto.net/software/apache-airflow/" target="_blank" rel="noopener">alternativetoでAirflowを検索した結果</a>
では以下の通り。</p>
<ul>
<li>RunDeck
<ul>
<li>OSSだが商用版もある。自動化ツール。ワークフローも管理できるようだ</li>
</ul></li>
<li>StackStorm
<ul>
<li>どちらかというとIFTTTみたいなものか？</li>
</ul></li>
<li>Zenaton
<ul>
<li>ワークフローエンジン。JavaScriptで記述できるようだ</li>
</ul></li>
<li>Apache Oozie
<ul>
<li>Hadoopエコシステムのワークフローエンジン</li>
</ul></li>
<li>Azkaban
<ul>
<li>ワークフローエンジン</li>
<li><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html#flow-2-0-basics" target="_blank" rel="noopener">Azkabanのフロー書き方</a>
の通り、YAMLで書ける。</li>
<li>LinkedIn が主に開発</li>
</ul></li>
<li>Metaflow ★
<ul>
<li>ワークフローエンジン</li>
<li>機械学習にフォーカス</li>
<li>Netflix と AWS が主に開発</li>
</ul></li>
<li>luigi
<ul>
<li>ワークフローエンジン</li>
<li>Pythonモジュール</li>
<li>Spotify が主に開発</li>
</ul></li>
</ul>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/02/16/Flow-Engine-for-ML/" data-id="clt1hv4b3003t1vqrf1hkd064" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Starting-Kafka-Streams" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/02/14/Starting-Kafka-Streams/">Kafka Streamsの始め方</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/02/14/Starting-Kafka-Streams/">
            <time datetime="2020-02-14T05:56:58.000Z" itemprop="datePublished">2020-02-14</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Kafka/">Kafka</a>, <a class="tag-link" href="/memo-blog/tags/Kafka-Streams/">Kafka Streams</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#はじめに読む文献" id="toc-はじめに読む文献">はじめに読む文献</a></li>
<li><a href="#レファレンスとして使う文献" id="toc-レファレンスとして使う文献">レファレンスとして使う文献</a></li>
<li><a href="#環境準備" id="toc-環境準備">環境準備</a></li>
<li><a href="#プロジェクト作成" id="toc-プロジェクト作成">プロジェクト作成</a>
<ul>
<li><a href="#wordcountpipe.java" id="toc-wordcountpipe.java">wordcount/Pipe.java</a></li>
<li><a href="#wordcountlinesplit.java" id="toc-wordcountlinesplit.java">wordcount/LineSplit.java</a></li>
<li><a href="#wordcountwordcount.java" id="toc-wordcountwordcount.java">wordcount/WordCount.java</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">公式ドキュメント</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/tutorial" target="_blank" rel="noopener">公式チュートリアル</a></li>
<li><a href="https://docs.confluent.io/current/installation/index.html" target="_blank" rel="noopener">Confluent
Platformドキュメント</a></li>
<li><a href="https://docs.confluent.io/current/cli/index.html" target="_blank" rel="noopener">Confluent
CLI</a></li>
<li><a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KStream.html#groupBy-org.apache.kafka.streams.kstream.KeyValueMapper-" target="_blank" rel="noopener">公式API説明（groupBy）</a></li>
<li><a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html#count-org.apache.kafka.streams.kstream.Materialized-" target="_blank" rel="noopener">公式API説明（count）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/quickstart#quickstart_streams_process" target="_blank" rel="noopener">公式ドキュメント（Step
5: Process some data）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/" target="_blank" rel="noopener">公式ドキュメント（Developer
Guide）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams DSL）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/processor-api.html#streams-developer-guide-processor-api" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Processor API）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/testing.html" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Test Utils）</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>まとまった情報が無いような気がするので、初心者向けのメモをここに書いておくことにする。</p>
<h2><span id="はじめに読む文献">はじめに読む文献</span></h2>
<ul>
<li><a href="https://kafka.apache.org/24/documentation/streams/tutorial" target="_blank" rel="noopener">公式チュートリアル</a>
<ul>
<li>最初にこのあたりを読み、イメージをつかむのが良い</li>
</ul></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/" target="_blank" rel="noopener">公式ドキュメント（Developer
Guide）</a>
<ul>
<li>つづいて開発者ガイドを読むと良い</li>
</ul></li>
</ul>
<h2><span id="レファレンスとして使う文献">レファレンスとして使う文献</span></h2>
<ul>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams DSL）</a>
<ul>
<li>チュートリアルを終えたあとくらいに使用し始めると良い</li>
</ul></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/processor-api.html#streams-developer-guide-processor-api" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Processor API）</a>
<ul>
<li>Kafka Streams DSLでは対応しきれないときにProcessor
APIを用いるときに使う</li>
</ul></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/testing.html" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Test Utils）</a>
<ul>
<li>Kafka Streamsのテスト作るときに使用</li>
</ul></li>
</ul>
<h2><span id="環境準備">環境準備</span></h2>
<p>Apache Kafka、もしくはConfluent
Platformで環境構築しておくことを前提とする。 Apache Kafkaであれば、 <a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">公式ドキュメント</a>
のインストール手順。 Confluent Platformであれば、 <a href="https://docs.confluent.io/current/installation/index.html" target="_blank" rel="noopener">Confluent
Platformドキュメント</a>のインストール手順。</p>
<p>また、Confluent Platformを用いるときは、 <a href="https://docs.confluent.io/current/cli/index.html" target="_blank" rel="noopener">Confluent
CLI</a> をインストールしておくと便利である。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> confluent <span class="built_in">local</span> start</span></span><br></pre></td></tr></table></figure>
<p>だけでKafka関連のサービスを開発用にローカル環境に起動できる。
具体的には、以下のサービスを立ち上げられる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">control-center is [UP]</span><br><span class="line">ksql-server is [UP]</span><br><span class="line">connect is [UP]</span><br><span class="line">kafka-rest is [UP]</span><br><span class="line">schema-registry is [UP]</span><br><span class="line">kafka is [UP]</span><br><span class="line">zookeeper is [UP]</span><br></pre></td></tr></table></figure>
<p>ちなみに、
<code>org.apache.kafka.connect.cli.ConnectDistributed</code>
が意外とメモリを使用するので注意。</p>
<p>また、デフォルトでは <code>/tmp</code>
以下にワーキングディレクトリを作成する。 また実行時には
<code>/tmp/confluent.current</code>
を作成し、その時に使用しているワーキングディレクトリを識別できるようになっている。
tmpwatch等により、ワーキングディレクトリを乱してしまい、
<code>confluent local start</code>
によりKafkaクラスタを起動できなくなったときは、
<code>/tmp/confluent.current</code>
を削除してもう一度起動すると良い。</p>
<p>以降の説明では、Confluent
Platformをインストールしたものとして説明する。</p>
<h2><span id="プロジェクト作成">プロジェクト作成</span></h2>
<p><a href="https://kafka.apache.org/24/documentation/streams/tutorial" target="_blank" rel="noopener">公式チュートリアル</a>
が最初は参考になるはず。</p>
<p>MavenのArchetypeを使い、プロジェクトを生成する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn archetype:generate \</span></span><br><span class="line">    -DarchetypeGroupId=org.apache.kafka \</span><br><span class="line">    -DarchetypeArtifactId=streams-quickstart-java \</span><br><span class="line">    -DarchetypeVersion=2.4.0 \</span><br><span class="line">    -DgroupId=net.dobachi.kafka.streams.examples \</span><br><span class="line">    -DartifactId=firstapp \</span><br><span class="line">    -Dversion=0.1 \</span><br><span class="line">    -Dpackage=wordcount</span><br></pre></td></tr></table></figure>
<p>適宜パッケージ名などを変更して用いること。</p>
<p>雛形に基づいたプロジェクトには、簡単なアプリが含まれている。
最初はこれらを修正しながら、アプリの書き方に慣れるとよい。</p>
<h3><span id="wordcountpipejava">wordcount/Pipe.java</span></h3>
<p>Kafka
Streamsのアプリは通常のJavaアプリと同様に、1プロセスからスタンドアローンで起動する。
ここでは、Pipe.javaの内容を確認しよう。
以下、ポイントとなるソースコードとその説明を並べる。</p>
<p>wordcount/Pipe.java:36</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"streams-pipe"</span>);</span><br><span class="line">props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br></pre></td></tr></table></figure>
<p>メインの中では最初にストリームを作るための設定が定義される。
上記の例では、ストリーム処理アプリの名前、Kafkaクラスタのブートストラップサーバ（つまり、Broker）、
またキーやバリューのデフォルトのシリアライゼーションの仕組みを指定します。
今回はキー・バリューともにStringであることがわかります。</p>
<p>wordcount/Pipe.java:42</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamsBuilder builder = <span class="keyword">new</span> StreamsBuilder();</span><br><span class="line"></span><br><span class="line">builder.stream(<span class="string">"streams-plaintext-input"</span>).to(<span class="string">"streams-pipe-output"</span>);</span><br></pre></td></tr></table></figure>
<p>つづいて、ストリームのビルダをインスタンス化。
このとき、入力・出力トピックを指定する。</p>
<p>wordcount/Pipe.java:46</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> Topology topology = builder.build();</span><br><span class="line"><span class="keyword">final</span> KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(topology, props);</span><br><span class="line"><span class="keyword">final</span> CountDownLatch latch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>ビルダでストリームをビルドし、トポロジを定義する。</p>
<p>wordcount/Pipe.java:46</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// attach shutdown handler to catch control-c</span></span><br><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="string">"streams-shutdown-hook"</span>) &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        streams.close();</span><br><span class="line">        latch.countDown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>シャットダウンフックを定義。</p>
<p>wordcount/Pipe.java:59</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    streams.start();</span><br><span class="line">    latch.await();</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    System.exit(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">System.exit(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>ストリーム処理を開始。</p>
<p>上記アプリを実行するには、事前に</p>
<ul>
<li>streams-plaintext-input</li>
<li>streams-pipe-output</li>
</ul>
<p>の2種類のトピックを生成しておく。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic streams-plaintext-input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic streams-pipe-output</span></span><br></pre></td></tr></table></figure>
<p>トピックが作られたかどうかは、以下のように確認する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics --list --zookeeper localhost:2181</span></span><br></pre></td></tr></table></figure>
<p>なお、ユーザが明示的に作るトピックの他にも、Kafkaの動作等のために作られるトピックがあるので、
上記コマンドを実行するとずらーっと出力されるはず。</p>
<p>コンパイル、パッケージングする。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies</span></span><br></pre></td></tr></table></figure>
<p>入力ファイルを作成し、入ロトピックに書き込み。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; /tmp/file-input.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input</span></span><br></pre></td></tr></table></figure>
<p>アプリを実行する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.Pipe</span></span><br></pre></td></tr></table></figure>
<p>別のターミナルを改めて開き、コンソール上に出力トピックの内容を出力する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning  --property print.key=<span class="literal">true</span> --topic streams-pipe-output</span></span><br></pre></td></tr></table></figure>
<p>以下のような結果が見られるはずである。なお、今回はキーを使用しないアプリだから、左側（キーを表示する場所）には
<code>null</code> が並ぶ。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null    all streams lead to kafka</span><br><span class="line">null    hello kafka streams</span><br><span class="line">null    join kafka summit</span><br></pre></td></tr></table></figure>
<p>さて、ここでキーを使うようにしてみる。 今回使用したアプリをコピーし、
<code>wordcount/PipeWithKey.java</code> を作る。</p>
<p>ここで変更点は以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">--- src/main/java/wordcount/Pipe.java   2020-02-14 15:23:23.808282200 +0900</span><br><span class="line">+++ src/main/java/wordcount/PipeWithKey.java    2020-02-14 16:54:17.623090500 +0900</span><br><span class="line">@@ -17,10 +17,8 @@</span><br><span class="line"> package wordcount;</span><br><span class="line"></span><br><span class="line"> import org.apache.kafka.common.serialization.Serdes;</span><br><span class="line">-import org.apache.kafka.streams.KafkaStreams;</span><br><span class="line">-import org.apache.kafka.streams.StreamsBuilder;</span><br><span class="line">-import org.apache.kafka.streams.StreamsConfig;</span><br><span class="line">-import org.apache.kafka.streams.Topology;</span><br><span class="line">+import org.apache.kafka.streams.*;</span><br><span class="line">+import org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"></span><br><span class="line"> import java.util.Properties;</span><br><span class="line"> import java.util.concurrent.CountDownLatch;</span><br><span class="line">@@ -30,7 +28,7 @@</span><br><span class="line">  * that reads from a source topic &quot;streams-plaintext-input&quot;, where the values of messages represent lines of text,</span><br><span class="line">  * and writes the messages as-is into a sink topic &quot;streams-pipe-output&quot;.</span><br><span class="line">  */</span><br><span class="line">-public class Pipe &#123;</span><br><span class="line">+public class PipeWithKey &#123;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         Properties props = new Properties();</span><br><span class="line">@@ -41,7 +39,8 @@</span><br><span class="line"></span><br><span class="line">         final StreamsBuilder builder = new StreamsBuilder();</span><br><span class="line"></span><br><span class="line">-        builder.stream(&quot;streams-plaintext-input&quot;).to(&quot;streams-pipe-output&quot;);</span><br><span class="line">+        KStream&lt;String, String&gt; raw = builder.stream(&quot;streams-plaintext-input&quot;);</span><br><span class="line">+        raw.map((key, value ) -&gt; new KeyValue&lt;&gt;(value.split(&quot; &quot;)[0], value)).to(&quot;streams-pipe-output&quot;);</span><br><span class="line"></span><br><span class="line">         final Topology topology = builder.build();</span><br><span class="line">         final KafkaStreams streams = new KafkaStreams(topology, props);</span><br></pre></td></tr></table></figure>
<p>主な変更は、ストリームビルダから定義されたストリームをいったん、
<code>raw</code> にバインドし、
mapメソッドを使って変換している箇所である。
ここでは、バリューをスペースで区切り、先頭の単語をキーとすることにした。</p>
<p>このアプリをコンパイル、パッケージ化し実行すると、以下のような結果が得られる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.PipeWithKey</span></span><br></pre></td></tr></table></figure>
<p>実行結果の例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all     all streams lead to kafka</span><br><span class="line">hello   hello kafka streams</span><br><span class="line">join    join kafka summit</span><br></pre></td></tr></table></figure>
<h3><span id="wordcountlinesplitjava">wordcount/LineSplit.java</span></h3>
<p>先程作成したPipeWithKeyとほぼ同じ。 実行すると、
<code>streams-linesplit-output</code>
というトピックに結果が出力される。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.LineSplit</span></span><br></pre></td></tr></table></figure>
<p>結果の例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning  --property print.key=<span class="literal">true</span> --topic streams-linesplit-output</span></span><br><span class="line">null    all</span><br><span class="line">null    streams</span><br><span class="line">null    lead</span><br><span class="line">null    to</span><br><span class="line">null    kafka</span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<h3><span id="wordcountwordcountjava">wordcount/WordCount.java</span></h3>
<p>最後にWordCountを確認する。
ほぼ他のアプリと同じだが、ポイントはストリームを加工する定義の部分である。</p>
<p>wordcount/WordCount.java:53</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">builder.&lt;String, String&gt;stream(<span class="string">"streams-plaintext-input"</span>)</span><br><span class="line">       .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(<span class="string">"\\W+"</span>)))</span><br><span class="line">       .groupBy((key, value) -&gt; value)</span><br><span class="line">       .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, <span class="keyword">byte</span>[]&gt;&gt;as(<span class="string">"counts-store"</span>))</span><br><span class="line">       .toStream()</span><br><span class="line">       .to(<span class="string">"streams-wordcount-output"</span>, Produced.with(Serdes.String(), Serdes.Long()));</span><br></pre></td></tr></table></figure>
<p>以下、上記実装を説明する。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">builder.&lt;String, String&gt;stream(<span class="string">"streams-plaintext-input"</span>)</span><br></pre></td></tr></table></figure>
<p>ストリームビルダを利用し、入力トピックからストリームを定義</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(<span class="string">"\\W+"</span>)))</span><br></pre></td></tr></table></figure>
<p>バリューに入っている文字列をスペース等で分割し、配列にする。
合わせて配列をflattenする。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.groupBy((key, value) -&gt; value)</span><br></pre></td></tr></table></figure>
<p>キーバリューから新しいキーを生成し、新しいキーに基づいてグループ化する。
今回の例では、分割されて生成された単語（バリューに入っている）をキーとしてグループ化する。
詳しくは、 <a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KStream.html#groupBy-org.apache.kafka.streams.kstream.KeyValueMapper-" target="_blank" rel="noopener">公式API説明（groupBy）</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, <span class="keyword">byte</span>[]&gt;&gt;as(<span class="string">"counts-store"</span>))</span><br></pre></td></tr></table></figure>
<p>groupByにより生成された <code>KGroupedStream</code> の
<code>count</code> メソッドを呼び出し、 キーごとの合計値を求める。
今回はキーはString型であり、合計値はLong型。
また集計結果を保持するストアは <code>counts-store</code>
という名前とする。 詳しくは、 <a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html#count-org.apache.kafka.streams.kstream.Materialized-" target="_blank" rel="noopener">公式API説明（count）</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.toStream()</span><br><span class="line">.to(<span class="string">"streams-wordcount-output"</span>, Produced.with(Serdes.String(), Serdes.Long()));</span><br></pre></td></tr></table></figure>
<p><code>count</code> の結果は <code>KTable</code>
になるので、これをストリームに変換し、出力先トピックを指定する。</p>
<p>実行してみる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.WordCount</span></span><br></pre></td></tr></table></figure>
<p>別のターミナルを改めて立ち上げ、入力トピックに書き込む。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input</span></span><br></pre></td></tr></table></figure>
<p>出力は以下のようになる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning  --property print.key=<span class="literal">true</span> --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --topic streams-wordcount-output</span></span><br><span class="line">all     19</span><br><span class="line">lead    19</span><br><span class="line">to      19</span><br><span class="line">hello   19</span><br><span class="line">streams 38</span><br><span class="line">join    19</span><br><span class="line">kafka   57</span><br><span class="line">summit  19</span><br></pre></td></tr></table></figure>
<p>なお、ここでは <code>kafka-console-consumer</code> にプロパティ
<code>value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</code>
を渡した。 アプリケーションでは集計した値はLong型だったためである。
詳しくは、 <a href="https://kafka.apache.org/24/documentation/streams/quickstart#quickstart_streams_process" target="_blank" rel="noopener">公式ドキュメント（Step
5: Process some data）</a> 参照。</p>
<p>なお、指定しない場合は入力されたバイト列がそのまま標準出力に渡されるようになっている。
その結果、期待する出力が得られないことになるので注意。</p>
<p>kafka/tools/ConsoleConsumer.scala:512</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(deserializer: <span class="type">Option</span>[<span class="type">Deserializer</span>[_]], sourceBytes: <span class="type">Array</span>[<span class="type">Byte</span>], topic: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> nonNullBytes = <span class="type">Option</span>(sourceBytes).getOrElse(<span class="string">"null"</span>.getBytes(<span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>))</span><br><span class="line">  <span class="keyword">val</span> convertedBytes = deserializer.map(_.deserialize(topic, nonNullBytes).toString.</span><br><span class="line">    getBytes(<span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>)).getOrElse(nonNullBytes)</span><br><span class="line">  output.write(convertedBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、別の方法として <code>WordCount</code>
の実装を修正する方法がある。以下、参考までに修正方法を紹介する。</p>
<p>想定と異なる表示だが、これは今回バリューの方にLongを用いたため。
kafka-console-consumer
で表示させるために以下のように実装を修正する。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@@ -50,12 +44,15 @@ public class WordCount &#123;</span><br><span class="line"></span><br><span class="line">         final StreamsBuilder builder = new StreamsBuilder();</span><br><span class="line"></span><br><span class="line"><span class="deletion">-        builder.&lt;String, String&gt;stream("streams-plaintext-input")</span></span><br><span class="line"><span class="addition">+        KStream&lt;String, Long&gt; wordCount = builder.&lt;String, String&gt;stream("streams-plaintext-input")</span></span><br><span class="line">                .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+")))</span><br><span class="line">                .groupBy((key, value) -&gt; value)</span><br><span class="line"><span class="deletion">-               .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as("counts-store"))</span></span><br><span class="line"><span class="deletion">-               .toStream()</span></span><br><span class="line"><span class="deletion">-               .to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));</span></span><br><span class="line"><span class="addition">+               .count(Materialized.as("counts-store"))</span></span><br><span class="line"><span class="addition">+               .toStream();</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        wordCount.foreach((key, value) -&gt; System.out.println("key: " + key + ", value: " + value));</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        wordCount.map((key, value) -&gt; new KeyValue&lt;&gt;(key, String.valueOf(value))).to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.String()));</span></span><br></pre></td></tr></table></figure>
<p>つまり、もともと <code>to</code>
で終えていたところを、いったん変数にバインドし、 <code>foreach</code>
を使ってストリームの内容を標準出力に表示させるようにしている。 また、
<code>map</code>
メソッドを利用し、バリューの型をLongからStringに変換してから
<code>to</code> で書き出すようにしている。</p>
<p>上記修正を加えた上で、改めてパッケージ化して実行したところ、以下のような表示が得られる。</p>
<p>kafka-console-consumer での表示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">all     9</span><br><span class="line">lead    9</span><br><span class="line">to      9</span><br><span class="line">hello   9</span><br><span class="line">streams 18</span><br><span class="line">join    9</span><br><span class="line">kafka   27</span><br><span class="line">summit  9</span><br></pre></td></tr></table></figure>
<p>ストリーム処理アプリの標準出力例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">key: all, value: 9</span><br><span class="line">key: lead, value: 9</span><br><span class="line">key: to, value: 9</span><br><span class="line">key: hello, value: 9</span><br><span class="line">key: streams, value: 18</span><br><span class="line">key: join, value: 9</span><br><span class="line">key: kafka, value: 27</span><br><span class="line">key: summit, value: 9</span><br></pre></td></tr></table></figure>
<p>無事に表示できたことが確かめられただろうか。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/02/14/Starting-Kafka-Streams/" data-id="clt1hv4p301cl1vqr37n3lyz1" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-CDC-Kafka-and-master-table-cache" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/01/25/CDC-Kafka-and-master-table-cache/">CDC Kafka and master table cache</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/01/25/CDC-Kafka-and-master-table-cache/">
            <time datetime="2020-01-25T14:32:13.000Z" itemprop="datePublished">2020-01-25</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/CDC/">CDC</a>, <a class="tag-link" href="/memo-blog/tags/Kafka/">Kafka</a>, <a class="tag-link" href="/memo-blog/tags/Kafka-Connect/">Kafka Connect</a>, <a class="tag-link" href="/memo-blog/tags/RDBMS/">RDBMS</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#やりたいこと" id="toc-やりたいこと">やりたいこと</a></li>
<li><a href="#rdbmsからのデータ取り込み" id="toc-rdbmsからのデータ取り込み">RDBMSからのデータ取り込み</a>
<ul>
<li><a href="#jdbc-kafka-connector" id="toc-jdbc-kafka-connector">JDBC
Kafka Connector</a></li>
<li><a href="#更新時間とユニークidを利用したキャプチャ" id="toc-更新時間とユニークidを利用したキャプチャ">更新時間とユニークIDを利用したキャプチャ</a></li>
<li><a href="#kafka-connect" id="toc-kafka-connect">Kafka
Connect</a></li>
</ul></li>
<li><a href="#kafka-stramsでテーブルに変換" id="toc-kafka-stramsでテーブルに変換">Kafka Stramsでテーブルに変換</a>
<ul>
<li><a href="#globalktableへの読み込み" id="toc-globalktableへの読み込み">GlobalKTableへの読み込み</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<p>過去の古いメモを少し細くしてアップロード。</p>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://tutuz-tech.hatenablog.com/entry/2019/03/21/000835" target="_blank" rel="noopener">KafkaConnectを試す
その2</a></li>
<li><a href="https://www.confluent.io/blog/no-more-silos-how-to-integrate-your-databases-with-apache-kafka-and-cdc/" target="_blank" rel="noopener">No
More Silos: How to Integrate Your Databases with Apache Kafka and
CDC</a></li>
<li><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/index.html" target="_blank" rel="noopener">JDBC
Connector (Source and Sink) for Confluent Platform</a></li>
<li><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#prerequisites" target="_blank" rel="noopener">JDBC
Connector Prerequisites</a></li>
<li><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#incremental-query-modes" target="_blank" rel="noopener">JDBC
Connector Incremental Query Modes</a></li>
<li><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#message-keys" target="_blank" rel="noopener">JDBC
Connector Message Keys</a></li>
<li><a href="https://docs.confluent.io/current/installation/installing_cp/deb-ubuntu.html#systemd-ubuntu-debian-install" target="_blank" rel="noopener">Confluent
PlatformのUbuntuへのインストール手順</a></li>
<li><a href="https://docs.confluent.io/current/cli/installing.html" target="_blank" rel="noopener">Confluent
CLIのインストール手順</a></li>
<li><a href="https://open-groove.net/postgresql/update-timestamp-function/" target="_blank" rel="noopener">PostgreSQLで更新時のtimestampをアップデートするには</a></li>
<li><a href="http://www.abe-tatsuya.com/web_prog/postgresql/seaquence.php" target="_blank" rel="noopener">PostgreSQL
で連番の数字のフィールドを作る方法 (sequence について)</a></li>
<li><a href="http://developpp.blog.jp/archives/8224244.html" target="_blank" rel="noopener">postgres -
シーケンス　inser時に自動採番</a></li>
<li><a href="https://github.com/confluentinc/kafka-streams-examples#available-examples" target="_blank" rel="noopener">Kafka
Streams例</a></li>
<li><a href="https://github.com/confluentinc/kafka-streams-examples/blob/5.4.0-post/src/main/java/io/confluent/examples/streams/GlobalKTablesExample.java" target="_blank" rel="noopener">GlobalKTablesExample.java</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="やりたいこと">やりたいこと</span></h2>
<p>マスタデータを格納しているRDBMSからKafkaにデータを取り込み、
KTableとしてキャッシュしたうえで、ストリームデータとJoinする。</p>
<h2><span id="rdbmsからのデータ取り込み">RDBMSからのデータ取り込み</span></h2>
<p><a href="https://tutuz-tech.hatenablog.com/entry/2019/03/21/000835" target="_blank" rel="noopener">KafkaConnectを試す
その2</a> や <a href="https://www.confluent.io/blog/no-more-silos-how-to-integrate-your-databases-with-apache-kafka-and-cdc/" target="_blank" rel="noopener">No
More Silos: How to Integrate Your Databases with Apache Kafka and
CDC</a> を 参考に、Kafka
Connectのjdbcコネクタを使用してみようと思った。</p>
<p>しかしこの例で載っているのは、差分取り込みのためにシーケンシャルなIDを持つカラムが必要であることが要件を満たさないかも、と思った。
やりたいのは、Updateも含むChange Data Capture。</p>
<p>test_db.json 内の該当箇所は以下の通り。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(snip)</span><br><span class="line">    "mode" : "incrementing",</span><br><span class="line">    "incrementing.column.name" : "seq",</span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>ということで、 <a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/index.html" target="_blank" rel="noopener">JDBC
Connector (Source and Sink) for Confluent Platform</a> を確認してみた。
結論としては、更新時間とユニークIDを使うモードを利用すると良さそうだ。</p>
<h3><span id="jdbc-kafka-connector">JDBC Kafka Connector</span></h3>
<p><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#prerequisites" target="_blank" rel="noopener">JDBC
Connector Prerequisites</a> によると、 Kafka と Schema
Registryがあればよさそうだ。</p>
<p><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#incremental-query-modes" target="_blank" rel="noopener">JDBC
Connector Incremental Query Modes</a> によると、モードは以下の通り。</p>
<ul>
<li>Incrementing Column
<ul>
<li>ユニークで必ず値が大きくなるカラムを持つことを前提としたモード</li>
<li>更新はキャプチャできない。そのためDWHのfactテーブルなどをキャプチャすることを想定している。</li>
</ul></li>
<li>Timestamp Column
<ul>
<li>更新時間のカラムを持つことを前提としたモード</li>
<li>更新時間はユニークではないことを起因とした注意点がある。
同一時刻に2個のレコードが更新された場合、もし1個目のレコードが処理されたあとに障害が生じたとしたら、
復旧時にもう1個のレコードが処理されないことが起こりうる。</li>
<li>疑問点：処理後に「処理済みTimestamp」を更新できないのだろうか</li>
</ul></li>
<li>Timestamp and Incrementing Columns
<ul>
<li>更新時刻カラム、ユニークIDカラムの両方を前提としたモード</li>
<li>先のTimestamp
Columnモードで問題となった、同一時刻に複数レコードが生成された場合における、
部分的なキャプチャ失敗を防ぐことができる。</li>
<li>確認点：先に更新時刻を見て、ユニークIDで確認するのだろうか。だとしたら、更新もキャプチャできそう。</li>
</ul></li>
<li>Custom Query
<ul>
<li>クエリを用いてフィルタされた結果をキャプチャするモード</li>
<li>Incrementing Column、Timestamp
Columnなどと比べ、オフセットをトラックしないので、
クエリ自身がオフセットをトラックするのと同等の処理内容になっていないと行けない。</li>
</ul></li>
<li>Bulk
<ul>
<li>テーブルをまるごとキャプチャするモード</li>
<li>レコードが消える場合になどに対応</li>
<li>補足：他のモードで、レコードの消去に対応するには、実際に消去するのではなく、消去フラグを立てる、などの工夫が必要そう</li>
</ul></li>
</ul>
<p>当然だが、必要に応じて元のRDBMS側でインデックスが貼られていないとならない。</p>
<p>timestamp.delay.interval.ms
設定を使い、更新時刻に対し、実際に取り込むタイミングを遅延させられる。
これはトランザクションを伴うときに、一連のレコードが更新されるのを待つための機能。</p>
<p>なお、 <a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#message-keys" target="_blank" rel="noopener">JDBC
Connector Message Keys</a>
によると、レコードの値や特定のカラムから、Kafkaメッセージのキーを生成できる。</p>
<h3><span id="更新時間とユニークidを利用したキャプチャ">更新時間とユニークIDを利用したキャプチャ</span></h3>
<p><a href="https://www.confluent.io/blog/no-more-silos-how-to-integrate-your-databases-with-apache-kafka-and-cdc/" target="_blank" rel="noopener">No
More Silos: How to Integrate Your Databases with Apache Kafka and
CDC</a> のあたりを参考に、 別のモードを使って試してみる。</p>
<p>まずはKafka環境を構築するが、ここでは簡単化のためにConfluent
Platformを用いることとした。 <a href="https://docs.confluent.io/current/installation/installing_cp/deb-ubuntu.html#systemd-ubuntu-debian-install" target="_blank" rel="noopener">Confluent
PlatformのUbuntuへのインストール手順</a> あたりを参考にすすめると良い。
また、 <code>confluent</code> コマンドがあると楽なので、 <a href="https://docs.confluent.io/current/cli/installing.html" target="_blank" rel="noopener">Confluent
CLIのインストール手順</a> を参考にインストールしておく。</p>
<p>インストールしたら、シングルモードでKafka環境を起動しておく。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ confluent <span class="built_in">local</span> start</span><br></pre></td></tr></table></figure>
<p><a href="https://tutuz-tech.hatenablog.com/entry/2019/03/21/000835" target="_blank" rel="noopener">KafkaConnectを試す
その2</a>
あたりを参考に、Kafkaと同一マシンにPostgreSQLの環境を構築しておく。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install -y postgresql</span><br><span class="line">$ sudo vim /etc/postgresql/10/main/postgresql.conf</span><br><span class="line">$ sudo cp /usr/share/postgresql/10/pg_hba.conf&#123;.sample,&#125;</span><br><span class="line">$ sudo vim /usr/share/postgresql/10/pg_hba.conf</span><br><span class="line">$ sudo systemctl restart postgresql</span><br></pre></td></tr></table></figure>
<p>/etc/postgresql/10/main/postgresql.conf
に追加する内容は以下の通り。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">listen_addresses</span> = <span class="string">'*'</span></span><br></pre></td></tr></table></figure>
<p>/usr/share/postgresql/10/pg_hba.conf に追加する内容は以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># PostgreSQL Client Authentication Configuration File</span><br><span class="line"># ===================================================</span><br><span class="line">local all all                trust</span><br><span class="line">host  all all 127.0.0.1/32 trust</span><br></pre></td></tr></table></figure>
<p>Kakfa用のデータベースとテーブルを作る。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ psql -c <span class="string">"alter user postgres with password 'kafkatest'"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -u postgres psql -U postgres -W -c <span class="string">"CREATE DATABASE testdb"</span>;</span><br><span class="line">Password <span class="keyword">for</span> user postgres:</span><br></pre></td></tr></table></figure>
<p>テーブルを作る際、Timestampとインクリメンタルな値を使ったデータキャプチャを実現するためのカラムを含むようにする。
<a href="https://open-groove.net/postgresql/update-timestamp-function/" target="_blank" rel="noopener">PostgreSQLで更新時のtimestampをアップデートするには</a>
、 <a href="http://www.abe-tatsuya.com/web_prog/postgresql/seaquence.php" target="_blank" rel="noopener">PostgreSQL
で連番の数字のフィールドを作る方法 (sequence について)</a> 、 <a href="http://developpp.blog.jp/archives/8224244.html" target="_blank" rel="noopener">postgres -
シーケンス　inser時に自動採番</a> あたりを参考とする。</p>
<p>以下、テーブルを作り、ユニークID用のシーケンスを作り、タイムスタンプを作る流れ。
タイムスタンプはレコード更新時に合わせて更新されるようにトリガを設定する。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -u postgres psql -U postgres testdb</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table (</span><br><span class="line">    seq <span class="built_in">SERIAL</span> PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line">    ts <span class="keyword">timestamp</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">now</span>(),</span><br><span class="line">    item <span class="built_in">varchar</span>(<span class="number">256</span>),</span><br><span class="line">    price <span class="built_in">integer</span>,</span><br><span class="line">    <span class="keyword">category</span> <span class="built_in">varchar</span>(<span class="number">256</span>)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">FUNCTION</span> set_update_time() <span class="keyword">RETURNS</span> <span class="keyword">OPAQUE</span> <span class="keyword">AS</span> <span class="string">'</span></span><br><span class="line"><span class="string">  begin</span></span><br><span class="line"><span class="string">    new.ts := ''now'';</span></span><br><span class="line"><span class="string">    return new;</span></span><br><span class="line"><span class="string">  end;</span></span><br><span class="line"><span class="string">'</span> <span class="keyword">LANGUAGE</span> <span class="string">'plpgsql'</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TRIGGER</span> update_tri <span class="keyword">BEFORE</span> <span class="keyword">UPDATE</span> <span class="keyword">ON</span> test_table <span class="keyword">FOR</span> <span class="keyword">EACH</span> <span class="keyword">ROW</span></span><br><span class="line">  <span class="keyword">EXECUTE</span> <span class="keyword">PROCEDURE</span> set_update_time();</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> connectuser <span class="keyword">WITH</span> <span class="keyword">password</span> <span class="string">'connectuser'</span>;</span><br><span class="line"><span class="keyword">GRANT</span> ALL <span class="keyword">ON</span> test_table <span class="keyword">TO</span> connectuser;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table(item, price, <span class="keyword">category</span>) <span class="keyword">VALUES</span> (<span class="string">'apple'</span>, <span class="number">400</span>, <span class="string">'fruit'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table(item, price, <span class="keyword">category</span>) <span class="keyword">VALUES</span> (<span class="string">'banana'</span>, <span class="number">160</span>, <span class="string">'fruit'</span>);</span><br><span class="line"><span class="keyword">UPDATE</span> test_table <span class="keyword">SET</span> item=<span class="string">'orange'</span>, price=<span class="number">100</span> <span class="keyword">where</span> seq = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table(item, price, <span class="keyword">category</span>) <span class="keyword">VALUES</span> (<span class="string">'banana'</span>, <span class="number">200</span>, <span class="string">'fruit'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table(item, price, <span class="keyword">category</span>) <span class="keyword">VALUES</span> (<span class="string">'pork'</span>, <span class="number">400</span>, <span class="string">'meet'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table(item, price, <span class="keyword">category</span>) <span class="keyword">VALUES</span> (<span class="string">'beef'</span>, <span class="number">800</span>, <span class="string">'meet'</span>);</span><br></pre></td></tr></table></figure>
<p>以下のような結果が得られるはずである。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">testdb=# SELECT * FROM test_table;</span><br><span class="line"> seq |             ts             |  item  | price | category</span><br><span class="line">-----+----------------------------+--------+-------+----------</span><br><span class="line">   1 | 2020-02-02 13:31:12.065458 | apple  |   400 | fruit</span><br><span class="line">   2 | 2020-02-02 13:31:49.220178 | orange |   100 | fruit</span><br><span class="line">   3 | 2020-02-02 13:32:32.324241 | banana |   200 | fruit</span><br><span class="line">   4 | 2020-02-02 13:33:06.560747 | pork   |   400 | meet</span><br><span class="line">   5 | 2020-02-02 13:33:06.561966 | beef   |   800 | meet</span><br><span class="line">(5 rows)</span><br></pre></td></tr></table></figure>
<h3><span id="kafka-connect">Kafka Connect</span></h3>
<p><a href="https://docs.confluent.io/current/connect/kafka-connect-jdbc/source-connector/index.html#incremental-query-modes" target="_blank" rel="noopener">JDBC
Connector Incremental Query Modes</a>
を参考に、タイムスタンプ＋インクリメンティングモードを用いる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; test_db.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;load-test-table&quot;,</span><br><span class="line">  &quot;config&quot;: &#123;</span><br><span class="line">    &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSourceConnector&quot;, </span><br><span class="line">    &quot;connection.url&quot; : &quot;jdbc:postgresql://localhost:5432/testdb&quot;,</span><br><span class="line">    &quot;connection.user&quot; : &quot;connectuser&quot;,</span><br><span class="line">    &quot;connection.password&quot; : &quot;connectuser&quot;,</span><br><span class="line">    &quot;mode&quot; : &quot;timestamp+incrementing&quot;,</span><br><span class="line">    &quot;incrementing.column.name&quot; : &quot;seq&quot;,</span><br><span class="line">    &quot;timestamp.column.name&quot; : &quot;ts&quot;,</span><br><span class="line">    &quot;table.whitelist&quot; : &quot;test_table&quot;,</span><br><span class="line">    &quot;topic.prefix&quot; : &quot;db_&quot;,</span><br><span class="line">    &quot;tasks.max&quot; : &quot;1&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">$ curl -X DELETE http://localhost:8083/connectors/load-test-table</span><br><span class="line">$ curl -X POST -H &quot;Content-Type: application/json&quot; http://localhost:8083/connectors -d @test_db.json</span><br><span class="line">$ curl http://localhost:8083/connectors</span><br></pre></td></tr></table></figure>
<p>上記コネクタでは、KafkaにAvro形式で書き込むので、
<code>kafka-avro-console-consumer</code>で確認する。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic db_test_table --from-beginning</span><br></pre></td></tr></table></figure>
<p>上記を起動した後、PostgreSQL側で適当にレコードを挿入・更新すると、
以下のような内容がコンソールコンシューマの出力に表示される。</p>
<p>変化がキャプチャされて取り込まれることがわかる。
挿入だけではなく、更新したものも取り込まれる。メッセージには、シーケンスとタイムスタンプの療法が含まれている。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;seq&quot;:1,&quot;ts&quot;:1580650272065,&quot;item&quot;:&#123;&quot;string&quot;:&quot;apple&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:400&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;seq&quot;:2,&quot;ts&quot;:1580650296666,&quot;item&quot;:&#123;&quot;string&quot;:&quot;banana&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:160&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;seq&quot;:2,&quot;ts&quot;:1580650309220,&quot;item&quot;:&#123;&quot;string&quot;:&quot;orange&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:100&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;seq&quot;:3,&quot;ts&quot;:1580650352324,&quot;item&quot;:&#123;&quot;string&quot;:&quot;banana&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:200&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;seq&quot;:4,&quot;ts&quot;:1580650386560,&quot;item&quot;:&#123;&quot;string&quot;:&quot;pork&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:400&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;meet&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;seq&quot;:5,&quot;ts&quot;:1580650386561,&quot;item&quot;:&#123;&quot;string&quot;:&quot;beef&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:800&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;meet&quot;&#125;&#125;</span><br></pre></td></tr></table></figure>
<h2><span id="kafka-stramsでテーブルに変換">Kafka Stramsでテーブルに変換</span></h2>
<p>上記の通り、RDBMSからデータを取り込んだものに対し、
マスタテーブルとして使うため、KTableに変換してみる。</p>
<h3><span id="globalktableへの読み込み">GlobalKTableへの読み込み</span></h3>
<p><a href="https://github.com/confluentinc/kafka-streams-examples#available-examples" target="_blank" rel="noopener">Kafka
Streams例</a> あたりを参考にする。 特に、 <a href="https://github.com/confluentinc/kafka-streams-examples/blob/5.4.0-post/src/main/java/io/confluent/examples/streams/GlobalKTablesExample.java" target="_blank" rel="noopener">GlobalKTablesExample.java</a>
あたりが参考になるかと思う。
今回は、上記レポジトリをベースに少しいじって、本例向けのサンプルアプリを作る。</p>
<p>[WIP]</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/01/25/CDC-Kafka-and-master-table-cache/" data-id="clt1hv4lz01671vqr2pnjejup" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-ThirdEye-LinkedIns-business-wide-monitoring-platform" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/01/16/ThirdEye-LinkedIns-business-wide-monitoring-platform/">ThirdEye LinkedIn’s business-wide monitoring platform</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/01/16/ThirdEye-LinkedIns-business-wide-monitoring-platform/">
            <time datetime="2020-01-15T15:41:28.000Z" itemprop="datePublished">2020-01-16</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Monitering/">Monitering</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/LinkedIn/">LinkedIn</a>, <a class="tag-link" href="/memo-blog/tags/Monitering/">Monitering</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#セッションメモ" id="toc-セッションメモ">セッションメモ</a></li>
<li><a href="#ブログメモ" id="toc-ブログメモ">ブログメモ</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://conferences.oreilly.com/strata/strata-ny-2019/public/schedule/detail/77219" target="_blank" rel="noopener">ThirdEye
LinkedIn’s business-wide monitoring platform</a></li>
<li><a href="https://github.com/apache/incubator-pinot/tree/master/thirdeye" target="_blank" rel="noopener">thirdeyeの実装</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="セッションメモ">セッションメモ</span></h2>
<p>Strataの <a href="https://conferences.oreilly.com/strata/strata-ny-2019/public/schedule/detail/77219" target="_blank" rel="noopener">ThirdEye
LinkedIn’s business-wide monitoring platform</a> のメモ。</p>
<p>LinkedInで運用されている異常検知と原因分析のためのプラットフォーム。
オープンソースになっているようだ。 -&gt; <a href="https://github.com/apache/incubator-pinot/tree/master/thirdeye" target="_blank" rel="noopener">thirdeyeの実装</a></p>
<p>主に以下の内容が記載されていた。</p>
<ul>
<li>MTTD: Mean time to detect</li>
<li>MTTR: Mean time to repair</li>
</ul>
<p>50 チーム以上がThirdEyeを利用。
何千もの時系列データが監視されている？</p>
<p>攻撃を検知したり、AIモデル周りの監視。</p>
<p>アーキ図あり。</p>
<p>異常検知における課題：スケーラビリティ、性能</p>
<p>手動でのコンフィグ、監視は現実的でない。</p>
<p>ルールベースの単純な仕組みは不十分。 多すぎるアラートは邪魔。</p>
<h2><span id="ブログメモ">ブログメモ</span></h2>
<p><a href="https://engineering.linkedin.com/blog/2020/analyzing-anomalies-with-thirdeye" target="_blank" rel="noopener">Analyzing
anomalies with ThirdEye</a> のメモ。</p>
<p>データキューブについて。
LinkedInではPinotを使って事前にキューブ化されている。</p>
<p>ディメンジョン・ヒートマップの利用。
あるディメンジョンにおける問題の原因分析に有用。</p>
<p>変化の検出の仕方について。</p>
<p>ただし単独のディメンジョンの問題を検出するだけでは不十分。
複数のディメンジョンにまたがって分析してわかる問題を検出したい。</p>
<p>ディメンジョンをツリー構成。
ベースラインと現在の値でツリーを構成。</p>
<p>ノードの親子関係を利用。
各ノードとその親ノードの変化を式に組み入れることで、木構造に基づく傾向（？）を考慮しながら、
変化の重大さを算出する。</p>
<p>上記仕組みを利用することで、データマネージャのバグ、機械学習モデルサービングにおけるバグを見つけた。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/01/16/ThirdEye-LinkedIns-business-wide-monitoring-platform/" data-id="clt1hv4dx00b41vqr0ftu2xtd" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/01/16/Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management/">Automating ML model training and deployments via metadata-driven data, infrastructure, feature engineering, and model management</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/01/16/Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management/">
            <time datetime="2020-01-15T15:05:53.000Z" itemprop="datePublished">2020-01-16</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Comcast/">Comcast</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://conferences.oreilly.com/strata/strata-ny-2019/public/schedule/detail/77284" target="_blank" rel="noopener">Automating
ML model training and deployments via metadata-driven data,
infrastructure, feature engineering, and model management</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>以下の内容が記載されている。</p>
<p>ワークフロー。 よくあるワークフローなので特筆なし。</p>
<p>無数のストリームイベント。 数億のレコードアップデート。
数千万の顧客アカウント。</p>
<p>メタデータ管理、特徴量ストア、モデルサービング、パイプラインオートメーション。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/01/16/Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management/" data-id="clt1hv4a9001v1vqrjthy24fd" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Questioning-the-Lambda-Architecture" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/01/13/Questioning-the-Lambda-Architecture/">Questioning the Lambda Architecture</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/01/13/Questioning-the-Lambda-Architecture/">
            <time datetime="2020-01-13T12:50:11.000Z" itemprop="datePublished">2020-01-13</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/">Stream Processing</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Kappa-Architecture/">Kappa Architecture</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Kappa-Architecture/">Kappa Architecture</a>, <a class="tag-link" href="/memo-blog/tags/Lambda-Architecture/">Lambda Architecture</a>, <a class="tag-link" href="/memo-blog/tags/Stream-Processing/">Stream Processing</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#まとめ" id="toc-まとめ">まとめ</a></li>
<li><a href="#気になった文言を引用" id="toc-気になった文言を引用">気になった文言を引用</a></li>
<li><a href="#所感" id="toc-所感">所感</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" target="_blank" rel="noopener">Questioning
the Lambda Architecture</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p><a href="https://www.oreilly.com/radar/questioning-the-lambda-architecture/" target="_blank" rel="noopener">Questioning
the Lambda Architecture</a> にてJay
Krepsが初めて言及したとされているようだ。
過去に読んだが忘れたので改めて、いかにメモを記載する。</p>
<h2><span id="まとめ">まとめ</span></h2>
<p>主張としては、ストリーム処理はKafkaの登場により、十分に使用に耐えうるものになったため、
バッチ処理と両方使うのではなく、ストリーム処理1本で勝負できるのでは？ということだった。</p>
<h2><span id="気になった文言を引用">気になった文言を引用</span></h2>
<p>ラムダアーキテクチャについて：</p>
<blockquote>
<p>The Lambda Architecture is an approach to building stream processing
applications on top of MapReduce and Storm or similar systems.</p>
</blockquote>
<figure>
<img src="https://dmgpayxepw99m.cloudfront.net/lambda-16338c9225c8e6b0c33a3f953133a4cb.png" alt="Lambdaアーキテクチャのイメージ">
<figcaption aria-hidden="true">Lambdaアーキテクチャのイメージ</figcaption>
</figure>
<p>バッチ処理とストリーム処理で2回ロジックを書く。：</p>
<blockquote>
<p>You implement your transformation logic twice, once in the batch
system and once in the stream processing system.</p>
</blockquote>
<p>レコメンデーションシステムを例にとる：</p>
<blockquote>
<p>A good example would be a news recommendation system that needs to
crawl various news sources, process and normalize all the input, and
then index, rank, and store it for serving.</p>
</blockquote>
<p>データを取り込み、イミュターブルなものとして扱うことはありだと思う：</p>
<blockquote>
<p>I’ve written some of my thoughts about capturing and transforming
immutable data streams</p>
</blockquote>
<blockquote>
<p>I have found that many people who attempt to build real-time data
processing systems don’t put much thought into this problem and end-up
with a system that simply cannot evolve quickly because it has no
convenient way to handle reprocessing.</p>
</blockquote>
<p>リアルタイム処理が本質的に近似であり、バッチ処理よりも弱く、損失しがち、という意見があるよね、と。：</p>
<blockquote>
<p>One is that real-time processing is inherently approximate, less
powerful, and more lossy than batch processing.</p>
</blockquote>
<p>ラムダアーキテクチャの利点にCAP定理との比較が持ち出されることを引き合いに出し、ラムダアーキテクチャがCAP定例を克服するようなものではない旨を説明。：</p>
<blockquote>
<p>Long story short, although there are definitely latency/availability
trade-offs in stream processing, this is an architecture for
asynchronous processing, so the results being computed are not kept
immediately consistent with the incoming data. The CAP theorem, sadly,
remains intact.</p>
</blockquote>
<p>結局、StormとHadoopの両方で同じ結果を生み出すアプリケーションを
実装するのがしんどいという話。：</p>
<blockquote>
<p>Programming in distributed frameworks like Storm and Hadoop is
complex.</p>
</blockquote>
<p>ひとつの解法は抽象化。：</p>
<blockquote>
<p>Summingbird</p>
</blockquote>
<p>とはいえ、2重運用はしんどい。デバッグなど。：</p>
<blockquote>
<p>the operational burden of running and debugging two systems is going
to be very high.</p>
</blockquote>
<p>結局のところ、両方を同時に使わないでくれ、という結論：</p>
<blockquote>
<p>These days, my advice is to use a batch processing framework like
MapReduce if you aren’t latency sensitive, and use a stream processing
framework if you are, but not to try to do both at the same time unless
you absolutely must.</p>
</blockquote>
<p>ストリーム処理はヒストリカルデータの高スループットでの処理に向かない、という話もあるが…：</p>
<blockquote>
<p>When I’ve discussed this with people, they sometimes tell me that
stream processing feels inappropriate for high-throughput processing of
historical data.</p>
</blockquote>
<p>バッチ処理もストリーム処理も抽象化の仕方は、DAGをベースにしたものであり、
その点では共通である、と。：</p>
<blockquote>
<p>But there is no reason this should be true. The fundamental
abstraction in stream processing is data flow DAGs, which are exactly
the same underlying abstraction in a traditional data warehouse (a la
Volcano) as well as being the fundamental abstraction in the MapReduce
successor Tez.</p>
</blockquote>
<p>ということでKafka。：</p>
<blockquote>
<p>Use Kafka</p>
</blockquote>
<figure>
<img src="https://dmgpayxepw99m.cloudfront.net/kappa-61d0afc292912b61ce62517fa2bd4309.png" alt="提案アーキテクチャ">
<figcaption aria-hidden="true">提案アーキテクチャ</figcaption>
</figure>
<p>Kafkaに入れた後は、HDFS等に簡単に保存できる。：</p>
<blockquote>
<p>Kafka has good integration with Hadoop, so mirroring any Kafka topic
into HDFS is easy.</p>
</blockquote>
<p>このあと少し、Kafkaの説明が続く。</p>
<p>この時点では、Event Sourcing、CQRSについての言及あり。</p>
<blockquote>
<p>Indeed, a lot of people are familiar with similar patterns that go by
the name Event Sourcing or CQRS.</p>
</blockquote>
<p>LinkedInにて、JayはSamzaを利用。</p>
<blockquote>
<p>I know this approach works well using Samza as the stream processing
system because we do it at LinkedIn.</p>
</blockquote>
<p>提案手法の難点として、一時的に2倍の出力ストレージサイズが必要になる。</p>
<blockquote>
<p>However, my proposal requires temporarily having 2x the storage space
in the output database and requires a database that supports high-volume
writes for the re-load.</p>
</blockquote>
<p>単純さを大事にする。：</p>
<blockquote>
<p>So, in cases where simplicity is important, consider this approach as
an alternative to the Lambda Architecture.</p>
</blockquote>
<h2><span id="所感">所感</span></h2>
<p>当時よりも、最近のワークロードは複雑なものも含めて期待されるようになっており、
ますます「バッチ処理とストリーム処理で同じ処理を実装する」というのがしんどくなっている印象。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/01/13/Questioning-the-Lambda-Architecture/" data-id="clt1hv4d5008w1vqrphmgvc90" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-ML-Ops-Machine-Learning-as-an-Engineering-Discipline" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/01/11/ML-Ops-Machine-Learning-as-an-Engineering-Discipline/">ML Ops: Machine Learning as an Engineering Discipline</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/01/11/ML-Ops-Machine-Learning-as-an-Engineering-Discipline/">
            <time datetime="2020-01-11T13:26:40.000Z" itemprop="datePublished">2020-01-11</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/OpML/">OpML</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/ML-Ops/">ML Ops</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#感想" id="toc-感想">感想</a></li>
<li><a href="#気になる文言の抜粋" id="toc-気になる文言の抜粋">気になる文言の抜粋</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://towardsdatascience.com/ml-ops-machine-learning-as-an-engineering-discipline-b86ca4874a3F" target="_blank" rel="noopener">ML
Ops Machine Learning as an Engineering Discipline</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>読んでみた感想をまとめる。</p>
<h2><span id="感想">感想</span></h2>
<p>結論としては、まとめ表がよくまとまっているのでそれでよい気がする。
この表をベースに、アクティビティから必要なものを追加するか？ -&gt; <a href="https://miro.medium.com/max/2870/1*hlukfeyP-I209WBEMsBSEA.png" target="_blank" rel="noopener">まとめ表</a></p>
<p>演繹的ではなく、帰納的な手法であるため、もとになったコードとデータの両方が重要。</p>
<p>データサイエンティスト、MLエンジニア、DevOpsエンジニア、データエンジニア。
MLエンジニア。あえてエンジニアと称しているのは、ソフトウェア開発のスキルを有していることを期待するから。</p>
<p>フェアネスの考慮、というか機械学習の倫理考慮をどうやって機械的に実現するのか、というのはかねてより気になっていた。
フェアネスの考慮などを達成するためには、テストデータセットの作り方、メトリクスの作り方に工夫するとよい。つまり、男女でそれぞれ個別にもテストするなど。
まだ一面ではあるが、参考になった。</p>
<h2><span id="気になる文言の抜粋">気になる文言の抜粋</span></h2>
<p>以下の文言が印象に残った。
ほかのレポートでも言われていることに見える。</p>
<blockquote>
<p>Deeplearning.ai reports that “only 22 percent of companies using
machine learning have successfully deployed a model”.</p>
</blockquote>
<p>このレポートがどれか気になった。 <a href="https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents" target="_blank" rel="noopener">Deeplearning.aiのレポート</a>
か。</p>
<p>確かに以下のように記載されている。</p>
<blockquote>
<p>Although AI budgets are on the rise, only 22 percent of companies
using machine learning have successfully deployed a model, the study
found.</p>
</blockquote>
<p>データが大切論：</p>
<blockquote>
<p>ML is not just code, it’s code plus data</p>
</blockquote>
<blockquote>
<p>training data, which will affect the behavior of the model in
production</p>
</blockquote>
<figure>
<img src="https://miro.medium.com/max/2890/1*XKk_Dc9fNLZ8VgL0v6WpYw.png" alt="ML = Code + Dataの図">
<figcaption aria-hidden="true">ML = Code + Dataの図</figcaption>
</figure>
<blockquote>
<p>It never stops changing, and you can’t control how it will
change.</p>
</blockquote>
<p>確かに、データはコントロールできない。</p>
<blockquote>
<p>Data Engineering</p>
</blockquote>
<figure>
<img src="https://miro.medium.com/max/2443/1*rCyvV8hAhAhqNjkLt7Wi7g.png" alt="ML Opsの概念図">
<figcaption aria-hidden="true">ML Opsの概念図</figcaption>
</figure>
<p>チーム構成について言及あり。：</p>
<blockquote>
<p>But the most likely scenario right now is that a successful team
would include a Data Scientist or ML Engineer, a DevOps Engineer and a
Data Engineer.</p>
</blockquote>
<p>データサイエンティストのほかに、明確にMLエンジニア、DevOpsエンジニア、データエンジニアを入れている。
基盤エンジニアはデータエンジニアに含まれるのだろうか。</p>
<blockquote>
<p>Even if an organization includes all necessary skills, it won’t be
successful if they don’t work closely together.</p>
</blockquote>
<p>ノートブックに殴り書かれたコードは不十分の話：</p>
<blockquote>
<p>getting a model to work great in a messy notebook is not enough.</p>
</blockquote>
<blockquote>
<p>ML Engineers</p>
</blockquote>
<p>データパイプラインの話：</p>
<blockquote>
<p>data pipeline</p>
</blockquote>
<p>殴り書きのコードではなく、適切なパイプラインはメリットいくつかあるよね、と。：</p>
<blockquote>
<p>Switching to proper data pipelines provides many advantages in code
reuse, run time visibility, management and scalability.</p>
</blockquote>
<p>トレーニングとサービングの両方でパイプラインがあるけど、
入力データや変換内容は、場合によっては微妙に異なる可能性がある、と。：</p>
<blockquote>
<p>Most models will need 2 versions of the pipeline: one for training
and one for serving.</p>
</blockquote>
<figure>
<img src="https://miro.medium.com/max/2920/1*U7Efc4rSPsXDTeKRzM86eg.png" alt="ML Pipelineは特定のデータから独立しているためCICDと連携可能">
<figcaption aria-hidden="true">ML
Pipelineは特定のデータから独立しているためCICDと連携可能</figcaption>
</figure>
<blockquote>
<p>For example, the training pipeline usually runs over batch files that
contain all features, while the serving pipeline often runs online and
receives only part of the features in the requests, retrieving the rest
from a database.</p>
</blockquote>
<p>いくつかTensorFlow関係のツールが紹介されている。確認したほうがよさそう。：</p>
<blockquote>
<p>TensorFlow Pipeline</p>
</blockquote>
<blockquote>
<p>TensorFlow Transform</p>
</blockquote>
<p>バージョン管理について：</p>
<blockquote>
<p>In ML, we also need to track model versions, along with the data used
to train it, and some meta-information like training
hyperparameters.</p>
</blockquote>
<blockquote>
<p>Models and metadata can be tracked in a standard version control
system like Git, but data is often too large and mutable for that to be
efficient and practical.</p>
</blockquote>
<p>コードのラインサイクルと、モデルのライフサイクルは異なる：</p>
<blockquote>
<p>It’s also important to avoid tying the model lifecycle to the code
lifecycle, since model training often happens on a different
schedule.</p>
</blockquote>
<blockquote>
<p>It’s also necessary to version data and tie each trained model to the
exact versions of code, data and hyperparameters that were used.</p>
</blockquote>
<blockquote>
<p>Having comprehensive automated tests can give great confidence to a
team, accelerating the pace of production deployments dramatically.</p>
</blockquote>
<p>モデルの検証は本質的に、統計に基づくものになる。というのも、そもそもモデルの出力が確率的だし、入力データも変動する。：</p>
<blockquote>
<p>model validation tests need to be necessarily statistical in
nature</p>
</blockquote>
<blockquote>
<p>Just as good unit tests must test several cases, model validation
needs to be done individually for relevant segments of the data, known
as slices.</p>
</blockquote>
<blockquote>
<p>Data validation is analogous to unit testing in the code domain.</p>
</blockquote>
<blockquote>
<p>ML pipelines should also validate higher level statistical properties
of the input.</p>
</blockquote>
<blockquote>
<p>TensorFlow Data Validation</p>
</blockquote>
<blockquote>
<p>Therefore, in addition to monitoring standard metrics like latency,
traffic, errors and saturation, we also need to monitor model prediction
performance.</p>
</blockquote>
<blockquote>
<p>An obvious challenge with monitoring model performance is that we
usually don’t have a verified label to compare our model’s predictions
to, since the model works on new data.</p>
</blockquote>
<figure>
<img src="https://miro.medium.com/max/2870/1*hlukfeyP-I209WBEMsBSEA.png" alt="まとめ表">
<figcaption aria-hidden="true">まとめ表</figcaption>
</figure>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/01/11/ML-Ops-Machine-Learning-as-an-Engineering-Discipline/" data-id="clt1hv4cj00721vqrvkk3b3v9" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/memo-blog/page/5/">&laquo; 前</a><a class="page-number" href="/memo-blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/memo-blog/page/4/">4</a><a class="page-number" href="/memo-blog/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/memo-blog/page/7/">7</a><a class="page-number" href="/memo-blog/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/memo-blog/page/21/">21</a><a class="extend next" rel="next" href="/memo-blog/page/7/">次 &raquo;</a>
    </nav>
</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">最近の記事</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/09/20/Dataspace-Protocol-of-EDC/" class="title">Dataspace Protocol of EDC</a></p>
                            <p class="item-date"><time datetime="2023-09-20T14:07:49.000Z" itemprop="datePublished">2023-09-20</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/09/09/Generate-OpenAPI-Spec-of-EDC-Connector/" class="title">Generate OpenAPI Spec of EDC Connector</a></p>
                            <p class="item-date"><time datetime="2023-09-09T13:26:58.000Z" itemprop="datePublished">2023-09-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Open-API/">Open API</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/09/07/OpenAPI-Generator-for-Flask/" class="title">OpenAPI Generator for Flask</a></p>
                            <p class="item-date"><time datetime="2023-09-07T13:10:58.000Z" itemprop="datePublished">2023-09-07</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/08/31/IDS-Dataspace-Protocol/" class="title">IDS Dataspace Protocol</a></p>
                            <p class="item-date"><time datetime="2023-08-31T06:56:27.000Z" itemprop="datePublished">2023-08-31</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/SDK/">SDK</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/08/27/How-to-create-SDK/" class="title">How to create SDK (WIP)</a></p>
                            <p class="item-date"><time datetime="2023-08-27T05:11:45.000Z" itemprop="datePublished">2023-08-27</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">カテゴリ</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/">Clipping</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/AI/">AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Camera/">Camera</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Camera/Lighting/">Lighting</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Cloud/">Cloud</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Database/">Database</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Kafka/">Kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/List/">List</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/List/Research/">Research</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Management/">Management</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/PostgreSQL/">PostgreSQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Stream-Processing/">Stream Processing</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Uber/">Uber</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Vim/">Vim</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Windows-Tools/">Windows Tools</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/">Home server</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/File-server/">File server</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Nature-Remo/">Nature Remo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Network/">Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Remote-desktop/">Remote desktop</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/">Ubuntu</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/Adobe-Reader/">Adobe Reader</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/Gnome/">Gnome</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/KVM/">KVM</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/OneDrive/">OneDrive</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/vim/">vim</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Video-processing/">Video processing</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><span class="category-list-count">159</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Alluxio/">Alluxio</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/BaaS/">BaaS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Configuration-Management/">Configuration Management</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Configuration-Management/Ansible/">Ansible</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Catalog/">Data Catalog</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Catalog/CKAN/">CKAN</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/">Data Collaboration</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/">Delta Sharing</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/X-Road/">X-Road</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/">Data Engineering</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Lineage/">Data Lineage</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Transformation/">Data Transformation</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Mesh/">Data Mesh</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/">Data Processing Engine</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/EDC/">EDC</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/IDS/">IDS</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Documentation/">Documentation</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Flask/">Flask</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/">GPD Pocket</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/">Device</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/Bluetooth/">Bluetooth</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/HBase/">HBase</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/">Hadoop</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/Ambari/">Ambari</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/BigTop/">BigTop</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/HDP/">HDP</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hexo/">Hexo</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Home-Network/">Home Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper/">Hyper</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper/Plugin/">Plugin</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper-V/">Hyper-V</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/">Keyboard</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/Corne-Chocolate/">Corne Chocolate</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/QMK/">QMK</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Kubernetes/">Kubernetes</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><span class="category-list-count">25</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Analytics-Zoo/">Analytics Zoo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/AutoML/">AutoML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Data-Platform/">Data Platform</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Flow-Engine/">Flow Engine</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/MLflow/">MLflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/">Model</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Cross-Validation/">Cross Validation</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Data-Leakage/">Data Leakage</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Partial-Dependency-Plot/">Partial Dependency Plot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/XGBoost/">XGBoost</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/">Model Management</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/Clipper/">Clipper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/OpML/">OpML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Preparation/">Preparation</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Software-Engineering-Patterns/">Software Engineering Patterns</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Stream-Processing/">Stream Processing</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/">Visualization</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/Seaborn/">Seaborn</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Word2Vec/">Word2Vec</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><span class="category-list-count">15</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a><span class="category-list-count">14</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Metadata-Management/">Metadata Management</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Monitering/">Monitering</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-API/">Open API</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/">Open Data</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/Scraping/">Scraping</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/Tellus/">Tellus</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Pinot/">Pinot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Power-Grid-Data/">Power Grid Data</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/">Python</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/Jupyter/">Jupyter</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/Pipenv/">Pipenv</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/pyenv/">pyenv</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/SDK/">SDK</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/SQLAlchemy/">SQLAlchemy</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Scala/">Scala</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Scala/SBT/">SBT</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Spark/">Spark</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Spark/Spark-Summit/">Spark Summit</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><span class="category-list-count">13</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/">Delta Lake</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Hudi/">Hudi</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Minio/">Minio</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/">Stream Processing</a><span class="category-list-count">9</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Apache-Edgent/">Apache Edgent</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Kappa-Architecture/">Kappa Architecture</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/MillWheel/">MillWheel</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Twitter-Heron/">Twitter Heron</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/">Tools</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Git/">Git</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Intellij/">Intellij</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Selenium/">Selenium</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/tmux/">tmux</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/">WSL</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/CentOS/">CentOS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Docker/">Docker</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Terminal-tool/">Terminal tool</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Vagrant/">Vagrant</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/X-Window/">X Window</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/">Windows</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Ansible/">Ansible</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Hyper-V/">Hyper-V</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Zeppelin/">Zeppelin</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/ZooKeeper/">ZooKeeper</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/vim/">vim</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/">Research</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/AWS/">AWS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Conference/">Conference</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Conference/DevSumi/">DevSumi</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Data-Analytics/">Data Analytics</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Data-Analytics/Tools/">Tools</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/">Machine Learning</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/BigDL/">BigDL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/TensorFlow/">TensorFlow</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/NVM/">NVM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/SX-Aurora-Frovedis/">SX-Aurora/Frovedis</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Trends/">Trends</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Video-processing/">Video processing</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Video-processing/BlazeIt/">BlazeIt</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Visualization/">Visualization</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Visualization/Superset/">Superset</a><span class="category-list-count">1</span></li></ul></li></ul></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">アーカイブ</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2023/09/">9月 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2023/08/">8月 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/05/">5月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/02/">2月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/01/">1月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/10/">10月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/09/">9月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/08/">8月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/07/">7月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/06/">6月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/05/">5月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/04/">4月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/02/">2月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/01/">1月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/12/">12月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/11/">11月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/10/">10月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/09/">9月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/08/">8月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/07/">7月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/06/">6月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/05/">5月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/04/">4月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/03/">3月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/02/">2月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/01/">1月 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/12/">12月 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/11/">11月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/10/">10月 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/09/">9月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/08/">8月 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/07/">7月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/06/">6月 2019</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/05/">5月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/04/">4月 2019</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/03/">3月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/02/">2月 2019</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/01/">1月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/12/">12月 2018</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/11/">11月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/10/">10月 2018</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/09/">9月 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">タグ</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AI/">AI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AWS/">AWS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Academia/">Academia</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Adobe-Reader/">Adobe Reader</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Alluxio/">Alluxio</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ambari/">Ambari</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Analytics-Zoo/">Analytics Zoo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ansible/">Ansible</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Edgent/">Apache Edgent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Hudi/">Apache Hudi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Kafka/">Apache Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Spark/">Apache Spark</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apple/">Apple</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AutoML/">AutoML</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Automagica/">Automagica</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Autonomous-Database/">Autonomous Database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BaaS/">BaaS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Behavioral-Economics/">Behavioral Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Big-Data/">Big Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BigDL/">BigDL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BigTop/">BigTop</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BlazeIt/">BlazeIt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Blog/">Blog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Bluetooth/">Bluetooth</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CDC/">CDC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Camera/">Camera</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CentOS/">CentOS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CentOS7/">CentOS7</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CircleCI/">CircleCI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Clipper/">Clipper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Clipping/">Clipping</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Cloud/">Cloud</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Comcast/">Comcast</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Computing-resource/">Computing resource</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Conference/">Conference</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Configuration-Management/">Configuration Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Corne-Chocolate/">Corne Chocolate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Cross-Validation/">Cross Validation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/DB/">DB</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dask/">Dask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Analysis/">Data Analysis</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Analytics/">Data Analytics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Lake/">Data Lake</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Leakage/">Data Leakage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Lineage/">Data Lineage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Mesh/">Data Mesh</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Platform/">Data Platform</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Processing-Engine/">Data Processing Engine</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Spaces/">Data Spaces</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Transformation/">Data Transformation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dataspace-Protocol/">Dataspace Protocol</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Delta-Lake/">Delta Lake</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Delta-Sharing/">Delta Sharing</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/DevSumi/">DevSumi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Docker/">Docker</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dockerfile/">Dockerfile</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Druid/">Druid</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/EDC/">EDC</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Flask/">Flask</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Flow-Engine/">Flow Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Frovedis/">Frovedis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GIS/">GIS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GNOME/">GNOME</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GPD-Pocket/">GPD Pocket</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Git/">Git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GitHub-Actions/">GitHub Actions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Google/">Google</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Goverment/">Goverment</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Graceful-Shutdown/">Graceful Shutdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Gradle/">Gradle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HBase/">HBase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HDP/">HDP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HPC/">HPC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hadoop/">Hadoop</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hexo/">Hexo</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hexo-Plugin/">Hexo Plugin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hyper/">Hyper</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hyper-V/">Hyper-V</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IDS/">IDS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IEEE/">IEEE</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IPv6/">IPv6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Icarus/">Icarus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Incident/">Incident</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Intel/">Intel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Intellij/">Intellij</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/JSON/">JSON</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Jupyter/">Jupyter</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/KVM/">KVM</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafak-Connect/">Kafak Connect</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka/">Kafka</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Connect/">Kafka Connect</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Streams/">Kafka Streams</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Summit/">Kafka Summit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kaggle/">Kaggle</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kappa-Architecture/">Kappa Architecture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Keyboard/">Keyboard</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kinesis/">Kinesis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kubernetes/">Kubernetes</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Lambda-Architecture/">Lambda Architecture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Lighthing/">Lighthing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/LinkedIn/">LinkedIn</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MATE-Desktop/">MATE Desktop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ML-Model-Management/">ML Model Management</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ML-Ops/">ML Ops</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MLflow/">MLflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning-Lifecycle/">Machine Learning Lifecycle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning-Lifecycle/">Machine Learning Lifecycle/</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Management/">Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Map/">Map</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Markdown/">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Messaging-System/">Messaging System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Metadata-Management/">Metadata Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MillWheel/">MillWheel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Minikube/">Minikube</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Minio/">Minio</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Model/">Model</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Model-Management/">Model Management</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Monitering/">Monitering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Mouse/">Mouse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/NERDTree/">NERDTree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/NVM/">NVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Nature-Remo/">Nature Remo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Network/">Network</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OLAP/">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OneDrive/">OneDrive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpML/">OpML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-API/">Open API</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-Data/">Open Data</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-Messaging-Benchmark/">Open Messaging Benchmark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpenAPI/">OpenAPI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpenML/">OpenML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Oracle/">Oracle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PAPIDS/">PAPIDS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PDF/">PDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pandoc/">Pandoc</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Paper/">Paper</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Parquet/">Parquet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Partial-Dependency-Plot/">Partial Dependency Plot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pinot/">Pinot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pipenv/">Pipenv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PostgreSQL/">PostgreSQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Power-Grid-Data/">Power Grid Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PowerShell/">PowerShell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Preparation/">Preparation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Profiler/">Profiler</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pulsar/">Pulsar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PySpark/">PySpark</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Python/">Python</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Python3/">Python3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/QMK/">QMK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Query-Engine/">Query Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/RDBMS/">RDBMS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/RPA/">RPA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Redshift/">Redshift</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Research-later/">Research later</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/S3/">S3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SBT/">SBT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SDK/">SDK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SQLAlchemy/">SQLAlchemy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SQLite/">SQLite</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SX-Aurora/">SX-Aurora</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Samba/">Samba</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Scala/">Scala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Scraping/">Scraping</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Security/">Security</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Session/">Session</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Slenium/">Slenium</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Smart-Home/">Smart Home</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Snowflake/">Snowflake</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Software-Engineering-Patterns/">Software Engineering Patterns</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Spark/">Spark</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Spark-Summit/">Spark Summit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Sphinx/">Sphinx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Statistic/">Statistic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Stonebraker/">Stonebraker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage/">Storage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Engine/">Storage Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Layer-Software/">Storage Layer Software</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Stream-Processing/">Stream Processing</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Superset/">Superset</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Supervision/">Supervision</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Tellus/">Tellus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/TensorFlow/">TensorFlow</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/TensorFlowOnSpark/">TensorFlowOnSpark</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Tools/">Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Trends/">Trends</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Troubleshoot/">Troubleshoot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Twitter/">Twitter</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Twitter-Heron/">Twitter Heron</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Uber/">Uber</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/VMWare/">VMWare</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vagrant/">Vagrant</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vault/">Vault</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vector-Engine/">Vector Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Video-Processing/">Video Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vim/">Vim</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Visualization/">Visualization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WSL/">WSL</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Web/">Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WhereHows/">WhereHows</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WiFi/">WiFi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WiFi6/">WiFi6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Windows/">Windows</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Windows-Tools/">Windows Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Word/">Word</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Word2Vec/">Word2Vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/X-Window/">X Window</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/X-Road/">X-Road</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/XGBoost/">XGBoost</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Zeppelin/">Zeppelin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ZooKeeper/">ZooKeeper</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/dein/">dein</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/dstat/">dstat</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/fsync/">fsync</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/git/">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/keyboard/">keyboard</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/libvirt/">libvirt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/markdown/">markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/nltk/">nltk</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/pandoc/">pandoc</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/pyenv/">pyenv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/tmux/">tmux</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/vim/">vim</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/windows/">windows</a><span class="tag-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">タグクラウド</h3>
        <div class="widget tagcloud">
            <a href="/memo-blog/tags/AI/" style="font-size: 10px;">AI</a> <a href="/memo-blog/tags/AWS/" style="font-size: 10px;">AWS</a> <a href="/memo-blog/tags/Academia/" style="font-size: 10px;">Academia</a> <a href="/memo-blog/tags/Adobe-Reader/" style="font-size: 10px;">Adobe Reader</a> <a href="/memo-blog/tags/Alluxio/" style="font-size: 10.77px;">Alluxio</a> <a href="/memo-blog/tags/Ambari/" style="font-size: 10px;">Ambari</a> <a href="/memo-blog/tags/Analytics-Zoo/" style="font-size: 10px;">Analytics Zoo</a> <a href="/memo-blog/tags/Ansible/" style="font-size: 11.54px;">Ansible</a> <a href="/memo-blog/tags/Apache-Edgent/" style="font-size: 10px;">Apache Edgent</a> <a href="/memo-blog/tags/Apache-Hudi/" style="font-size: 10px;">Apache Hudi</a> <a href="/memo-blog/tags/Apache-Kafka/" style="font-size: 10px;">Apache Kafka</a> <a href="/memo-blog/tags/Apache-Spark/" style="font-size: 11.54px;">Apache Spark</a> <a href="/memo-blog/tags/Apple/" style="font-size: 10px;">Apple</a> <a href="/memo-blog/tags/AutoML/" style="font-size: 10.77px;">AutoML</a> <a href="/memo-blog/tags/Automagica/" style="font-size: 10px;">Automagica</a> <a href="/memo-blog/tags/Autonomous-Database/" style="font-size: 10px;">Autonomous Database</a> <a href="/memo-blog/tags/BaaS/" style="font-size: 10px;">BaaS</a> <a href="/memo-blog/tags/Behavioral-Economics/" style="font-size: 10px;">Behavioral Economics</a> <a href="/memo-blog/tags/Big-Data/" style="font-size: 10px;">Big Data</a> <a href="/memo-blog/tags/BigDL/" style="font-size: 10.77px;">BigDL</a> <a href="/memo-blog/tags/BigTop/" style="font-size: 10.77px;">BigTop</a> <a href="/memo-blog/tags/BlazeIt/" style="font-size: 10px;">BlazeIt</a> <a href="/memo-blog/tags/Blog/" style="font-size: 10px;">Blog</a> <a href="/memo-blog/tags/Bluetooth/" style="font-size: 10px;">Bluetooth</a> <a href="/memo-blog/tags/CDC/" style="font-size: 10px;">CDC</a> <a href="/memo-blog/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/memo-blog/tags/CentOS/" style="font-size: 10px;">CentOS</a> <a href="/memo-blog/tags/CentOS7/" style="font-size: 11.54px;">CentOS7</a> <a href="/memo-blog/tags/CircleCI/" style="font-size: 10px;">CircleCI</a> <a href="/memo-blog/tags/Clipper/" style="font-size: 10px;">Clipper</a> <a href="/memo-blog/tags/Clipping/" style="font-size: 10px;">Clipping</a> <a href="/memo-blog/tags/Cloud/" style="font-size: 10px;">Cloud</a> <a href="/memo-blog/tags/Comcast/" style="font-size: 10px;">Comcast</a> <a href="/memo-blog/tags/Computing-resource/" style="font-size: 10px;">Computing resource</a> <a href="/memo-blog/tags/Conference/" style="font-size: 10px;">Conference</a> <a href="/memo-blog/tags/Configuration-Management/" style="font-size: 10px;">Configuration Management</a> <a href="/memo-blog/tags/Corne-Chocolate/" style="font-size: 10px;">Corne Chocolate</a> <a href="/memo-blog/tags/Cross-Validation/" style="font-size: 10px;">Cross Validation</a> <a href="/memo-blog/tags/DB/" style="font-size: 10px;">DB</a> <a href="/memo-blog/tags/Dask/" style="font-size: 10.77px;">Dask</a> <a href="/memo-blog/tags/Data-Analysis/" style="font-size: 10.77px;">Data Analysis</a> <a href="/memo-blog/tags/Data-Analytics/" style="font-size: 10px;">Data Analytics</a> <a href="/memo-blog/tags/Data-Lake/" style="font-size: 10px;">Data Lake</a> <a href="/memo-blog/tags/Data-Leakage/" style="font-size: 10px;">Data Leakage</a> <a href="/memo-blog/tags/Data-Lineage/" style="font-size: 10px;">Data Lineage</a> <a href="/memo-blog/tags/Data-Mesh/" style="font-size: 10px;">Data Mesh</a> <a href="/memo-blog/tags/Data-Platform/" style="font-size: 10px;">Data Platform</a> <a href="/memo-blog/tags/Data-Processing-Engine/" style="font-size: 10.77px;">Data Processing Engine</a> <a href="/memo-blog/tags/Data-Spaces/" style="font-size: 12.31px;">Data Spaces</a> <a href="/memo-blog/tags/Data-Transformation/" style="font-size: 10.77px;">Data Transformation</a> <a href="/memo-blog/tags/Dataspace-Protocol/" style="font-size: 10px;">Dataspace Protocol</a> <a href="/memo-blog/tags/Delta-Lake/" style="font-size: 16.92px;">Delta Lake</a> <a href="/memo-blog/tags/Delta-Sharing/" style="font-size: 12.31px;">Delta Sharing</a> <a href="/memo-blog/tags/DevSumi/" style="font-size: 10px;">DevSumi</a> <a href="/memo-blog/tags/Docker/" style="font-size: 14.62px;">Docker</a> <a href="/memo-blog/tags/Dockerfile/" style="font-size: 10.77px;">Dockerfile</a> <a href="/memo-blog/tags/Druid/" style="font-size: 10px;">Druid</a> <a href="/memo-blog/tags/EDC/" style="font-size: 11.54px;">EDC</a> <a href="/memo-blog/tags/Flask/" style="font-size: 12.31px;">Flask</a> <a href="/memo-blog/tags/Flow-Engine/" style="font-size: 10px;">Flow Engine</a> <a href="/memo-blog/tags/Frovedis/" style="font-size: 10px;">Frovedis</a> <a href="/memo-blog/tags/GIS/" style="font-size: 10px;">GIS</a> <a href="/memo-blog/tags/GNOME/" style="font-size: 10px;">GNOME</a> <a href="/memo-blog/tags/GPD-Pocket/" style="font-size: 10px;">GPD Pocket</a> <a href="/memo-blog/tags/Git/" style="font-size: 10.77px;">Git</a> <a href="/memo-blog/tags/GitHub-Actions/" style="font-size: 10px;">GitHub Actions</a> <a href="/memo-blog/tags/Google/" style="font-size: 10px;">Google</a> <a href="/memo-blog/tags/Goverment/" style="font-size: 10px;">Goverment</a> <a href="/memo-blog/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/memo-blog/tags/Gradle/" style="font-size: 10px;">Gradle</a> <a href="/memo-blog/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/memo-blog/tags/HDP/" style="font-size: 10px;">HDP</a> <a href="/memo-blog/tags/HPC/" style="font-size: 10px;">HPC</a> <a href="/memo-blog/tags/Hadoop/" style="font-size: 11.54px;">Hadoop</a> <a href="/memo-blog/tags/Hexo/" style="font-size: 16.15px;">Hexo</a> <a href="/memo-blog/tags/Hexo-Plugin/" style="font-size: 10px;">Hexo Plugin</a> <a href="/memo-blog/tags/Hyper/" style="font-size: 10.77px;">Hyper</a> <a href="/memo-blog/tags/Hyper-V/" style="font-size: 10.77px;">Hyper-V</a> <a href="/memo-blog/tags/IDS/" style="font-size: 10px;">IDS</a> <a href="/memo-blog/tags/IEEE/" style="font-size: 10.77px;">IEEE</a> <a href="/memo-blog/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/memo-blog/tags/Icarus/" style="font-size: 10px;">Icarus</a> <a href="/memo-blog/tags/Incident/" style="font-size: 10px;">Incident</a> <a href="/memo-blog/tags/Intel/" style="font-size: 10px;">Intel</a> <a href="/memo-blog/tags/Intellij/" style="font-size: 10.77px;">Intellij</a> <a href="/memo-blog/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/memo-blog/tags/Java/" style="font-size: 10px;">Java</a> <a href="/memo-blog/tags/Jupyter/" style="font-size: 10.77px;">Jupyter</a> <a href="/memo-blog/tags/KVM/" style="font-size: 11.54px;">KVM</a> <a href="/memo-blog/tags/Kafak-Connect/" style="font-size: 10px;">Kafak Connect</a> <a href="/memo-blog/tags/Kafka/" style="font-size: 19.23px;">Kafka</a> <a href="/memo-blog/tags/Kafka-Connect/" style="font-size: 10px;">Kafka Connect</a> <a href="/memo-blog/tags/Kafka-Streams/" style="font-size: 10.77px;">Kafka Streams</a> <a href="/memo-blog/tags/Kafka-Summit/" style="font-size: 10px;">Kafka Summit</a> <a href="/memo-blog/tags/Kaggle/" style="font-size: 14.62px;">Kaggle</a> <a href="/memo-blog/tags/Kappa-Architecture/" style="font-size: 10px;">Kappa Architecture</a> <a href="/memo-blog/tags/Keyboard/" style="font-size: 10.77px;">Keyboard</a> <a href="/memo-blog/tags/Kinesis/" style="font-size: 10px;">Kinesis</a> <a href="/memo-blog/tags/Kubernetes/" style="font-size: 11.54px;">Kubernetes</a> <a href="/memo-blog/tags/Lambda-Architecture/" style="font-size: 10px;">Lambda Architecture</a> <a href="/memo-blog/tags/Lighthing/" style="font-size: 10px;">Lighthing</a> <a href="/memo-blog/tags/LinkedIn/" style="font-size: 12.31px;">LinkedIn</a> <a href="/memo-blog/tags/MATE-Desktop/" style="font-size: 10px;">MATE Desktop</a> <a href="/memo-blog/tags/ML-Model-Management/" style="font-size: 10.77px;">ML Model Management</a> <a href="/memo-blog/tags/ML-Ops/" style="font-size: 10px;">ML Ops</a> <a href="/memo-blog/tags/MLflow/" style="font-size: 10px;">MLflow</a> <a href="/memo-blog/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/memo-blog/tags/Machine-Learning-Lifecycle/" style="font-size: 10px;">Machine Learning Lifecycle</a> <a href="/memo-blog/tags/Machine-Learning-Lifecycle/" style="font-size: 10px;">Machine Learning Lifecycle/</a> <a href="/memo-blog/tags/Management/" style="font-size: 10px;">Management</a> <a href="/memo-blog/tags/Map/" style="font-size: 10px;">Map</a> <a href="/memo-blog/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/memo-blog/tags/Messaging-System/" style="font-size: 10px;">Messaging System</a> <a href="/memo-blog/tags/Metadata-Management/" style="font-size: 10px;">Metadata Management</a> <a href="/memo-blog/tags/MillWheel/" style="font-size: 10px;">MillWheel</a> <a href="/memo-blog/tags/Minikube/" style="font-size: 10px;">Minikube</a> <a href="/memo-blog/tags/Minio/" style="font-size: 10.77px;">Minio</a> <a href="/memo-blog/tags/Model/" style="font-size: 12.31px;">Model</a> <a href="/memo-blog/tags/Model-Management/" style="font-size: 12.31px;">Model Management</a> <a href="/memo-blog/tags/Monitering/" style="font-size: 10px;">Monitering</a> <a href="/memo-blog/tags/Mouse/" style="font-size: 10px;">Mouse</a> <a href="/memo-blog/tags/NERDTree/" style="font-size: 10px;">NERDTree</a> <a href="/memo-blog/tags/NVM/" style="font-size: 10px;">NVM</a> <a href="/memo-blog/tags/Nature-Remo/" style="font-size: 10px;">Nature Remo</a> <a href="/memo-blog/tags/Network/" style="font-size: 10px;">Network</a> <a href="/memo-blog/tags/OLAP/" style="font-size: 10px;">OLAP</a> <a href="/memo-blog/tags/OneDrive/" style="font-size: 10px;">OneDrive</a> <a href="/memo-blog/tags/OpML/" style="font-size: 10px;">OpML</a> <a href="/memo-blog/tags/Open-API/" style="font-size: 10px;">Open API</a> <a href="/memo-blog/tags/Open-Data/" style="font-size: 10.77px;">Open Data</a> <a href="/memo-blog/tags/Open-Messaging-Benchmark/" style="font-size: 10px;">Open Messaging Benchmark</a> <a href="/memo-blog/tags/OpenAPI/" style="font-size: 10px;">OpenAPI</a> <a href="/memo-blog/tags/OpenML/" style="font-size: 10px;">OpenML</a> <a href="/memo-blog/tags/Oracle/" style="font-size: 10px;">Oracle</a> <a href="/memo-blog/tags/PAPIDS/" style="font-size: 10px;">PAPIDS</a> <a href="/memo-blog/tags/PDF/" style="font-size: 10px;">PDF</a> <a href="/memo-blog/tags/Pandoc/" style="font-size: 10px;">Pandoc</a> <a href="/memo-blog/tags/Paper/" style="font-size: 18.46px;">Paper</a> <a href="/memo-blog/tags/Parquet/" style="font-size: 10px;">Parquet</a> <a href="/memo-blog/tags/Partial-Dependency-Plot/" style="font-size: 10px;">Partial Dependency Plot</a> <a href="/memo-blog/tags/Pinot/" style="font-size: 10px;">Pinot</a> <a href="/memo-blog/tags/Pipenv/" style="font-size: 10px;">Pipenv</a> <a href="/memo-blog/tags/PostgreSQL/" style="font-size: 10px;">PostgreSQL</a> <a href="/memo-blog/tags/Power-Grid-Data/" style="font-size: 10px;">Power Grid Data</a> <a href="/memo-blog/tags/PowerShell/" style="font-size: 10px;">PowerShell</a> <a href="/memo-blog/tags/Preparation/" style="font-size: 10.77px;">Preparation</a> <a href="/memo-blog/tags/Profiler/" style="font-size: 10px;">Profiler</a> <a href="/memo-blog/tags/Pulsar/" style="font-size: 10px;">Pulsar</a> <a href="/memo-blog/tags/PySpark/" style="font-size: 12.31px;">PySpark</a> <a href="/memo-blog/tags/Python/" style="font-size: 17.69px;">Python</a> <a href="/memo-blog/tags/Python3/" style="font-size: 10px;">Python3</a> <a href="/memo-blog/tags/QMK/" style="font-size: 10px;">QMK</a> <a href="/memo-blog/tags/Query-Engine/" style="font-size: 10px;">Query Engine</a> <a href="/memo-blog/tags/RDBMS/" style="font-size: 10.77px;">RDBMS</a> <a href="/memo-blog/tags/RPA/" style="font-size: 10px;">RPA</a> <a href="/memo-blog/tags/Redshift/" style="font-size: 10px;">Redshift</a> <a href="/memo-blog/tags/Research-later/" style="font-size: 10px;">Research later</a> <a href="/memo-blog/tags/S3/" style="font-size: 10px;">S3</a> <a href="/memo-blog/tags/SBT/" style="font-size: 10px;">SBT</a> <a href="/memo-blog/tags/SDK/" style="font-size: 10px;">SDK</a> <a href="/memo-blog/tags/SQLAlchemy/" style="font-size: 10.77px;">SQLAlchemy</a> <a href="/memo-blog/tags/SQLite/" style="font-size: 10px;">SQLite</a> <a href="/memo-blog/tags/SX-Aurora/" style="font-size: 10px;">SX-Aurora</a> <a href="/memo-blog/tags/Samba/" style="font-size: 10px;">Samba</a> <a href="/memo-blog/tags/Scala/" style="font-size: 10px;">Scala</a> <a href="/memo-blog/tags/Scraping/" style="font-size: 10px;">Scraping</a> <a href="/memo-blog/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/memo-blog/tags/Security/" style="font-size: 10px;">Security</a> <a href="/memo-blog/tags/Session/" style="font-size: 10px;">Session</a> <a href="/memo-blog/tags/Slenium/" style="font-size: 10px;">Slenium</a> <a href="/memo-blog/tags/Smart-Home/" style="font-size: 10px;">Smart Home</a> <a href="/memo-blog/tags/Snowflake/" style="font-size: 10px;">Snowflake</a> <a href="/memo-blog/tags/Software-Engineering-Patterns/" style="font-size: 10px;">Software Engineering Patterns</a> <a href="/memo-blog/tags/Spark/" style="font-size: 15.38px;">Spark</a> <a href="/memo-blog/tags/Spark-Summit/" style="font-size: 10px;">Spark Summit</a> <a href="/memo-blog/tags/Sphinx/" style="font-size: 10px;">Sphinx</a> <a href="/memo-blog/tags/Statistic/" style="font-size: 10px;">Statistic</a> <a href="/memo-blog/tags/Stonebraker/" style="font-size: 10px;">Stonebraker</a> <a href="/memo-blog/tags/Storage/" style="font-size: 10px;">Storage</a> <a href="/memo-blog/tags/Storage-Engine/" style="font-size: 10px;">Storage Engine</a> <a href="/memo-blog/tags/Storage-Layer/" style="font-size: 11.54px;">Storage Layer</a> <a href="/memo-blog/tags/Storage-Layer-Software/" style="font-size: 10px;">Storage Layer Software</a> <a href="/memo-blog/tags/Stream-Processing/" style="font-size: 18.46px;">Stream Processing</a> <a href="/memo-blog/tags/Superset/" style="font-size: 10px;">Superset</a> <a href="/memo-blog/tags/Supervision/" style="font-size: 10px;">Supervision</a> <a href="/memo-blog/tags/Tellus/" style="font-size: 10px;">Tellus</a> <a href="/memo-blog/tags/TensorFlow/" style="font-size: 11.54px;">TensorFlow</a> <a href="/memo-blog/tags/TensorFlowOnSpark/" style="font-size: 10.77px;">TensorFlowOnSpark</a> <a href="/memo-blog/tags/Tools/" style="font-size: 10px;">Tools</a> <a href="/memo-blog/tags/Trends/" style="font-size: 10px;">Trends</a> <a href="/memo-blog/tags/Troubleshoot/" style="font-size: 10px;">Troubleshoot</a> <a href="/memo-blog/tags/Twitter/" style="font-size: 10px;">Twitter</a> <a href="/memo-blog/tags/Twitter-Heron/" style="font-size: 10px;">Twitter Heron</a> <a href="/memo-blog/tags/Uber/" style="font-size: 10.77px;">Uber</a> <a href="/memo-blog/tags/Ubuntu/" style="font-size: 14.62px;">Ubuntu</a> <a href="/memo-blog/tags/VMWare/" style="font-size: 10px;">VMWare</a> <a href="/memo-blog/tags/Vagrant/" style="font-size: 10.77px;">Vagrant</a> <a href="/memo-blog/tags/Vault/" style="font-size: 10px;">Vault</a> <a href="/memo-blog/tags/Vector-Engine/" style="font-size: 10px;">Vector Engine</a> <a href="/memo-blog/tags/Video-Processing/" style="font-size: 10px;">Video Processing</a> <a href="/memo-blog/tags/Vim/" style="font-size: 10.77px;">Vim</a> <a href="/memo-blog/tags/Visualization/" style="font-size: 10.77px;">Visualization</a> <a href="/memo-blog/tags/WSL/" style="font-size: 13.85px;">WSL</a> <a href="/memo-blog/tags/Web/" style="font-size: 10px;">Web</a> <a href="/memo-blog/tags/WhereHows/" style="font-size: 10px;">WhereHows</a> <a href="/memo-blog/tags/WiFi/" style="font-size: 10px;">WiFi</a> <a href="/memo-blog/tags/WiFi6/" style="font-size: 10px;">WiFi6</a> <a href="/memo-blog/tags/Windows/" style="font-size: 13.08px;">Windows</a> <a href="/memo-blog/tags/Windows-Tools/" style="font-size: 10px;">Windows Tools</a> <a href="/memo-blog/tags/Word/" style="font-size: 10px;">Word</a> <a href="/memo-blog/tags/Word2Vec/" style="font-size: 10px;">Word2Vec</a> <a href="/memo-blog/tags/X-Window/" style="font-size: 10px;">X Window</a> <a href="/memo-blog/tags/X-Road/" style="font-size: 10px;">X-Road</a> <a href="/memo-blog/tags/XGBoost/" style="font-size: 10px;">XGBoost</a> <a href="/memo-blog/tags/Zeppelin/" style="font-size: 10px;">Zeppelin</a> <a href="/memo-blog/tags/ZooKeeper/" style="font-size: 12.31px;">ZooKeeper</a> <a href="/memo-blog/tags/bug/" style="font-size: 10px;">bug</a> <a href="/memo-blog/tags/dein/" style="font-size: 10px;">dein</a> <a href="/memo-blog/tags/dstat/" style="font-size: 10px;">dstat</a> <a href="/memo-blog/tags/fsync/" style="font-size: 10px;">fsync</a> <a href="/memo-blog/tags/git/" style="font-size: 10px;">git</a> <a href="/memo-blog/tags/keyboard/" style="font-size: 10px;">keyboard</a> <a href="/memo-blog/tags/libvirt/" style="font-size: 10px;">libvirt</a> <a href="/memo-blog/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/memo-blog/tags/nltk/" style="font-size: 10px;">nltk</a> <a href="/memo-blog/tags/pandoc/" style="font-size: 11.54px;">pandoc</a> <a href="/memo-blog/tags/pyenv/" style="font-size: 10px;">pyenv</a> <a href="/memo-blog/tags/tmux/" style="font-size: 10.77px;">tmux</a> <a href="/memo-blog/tags/vim/" style="font-size: 13.85px;">vim</a> <a href="/memo-blog/tags/windows/" style="font-size: 10px;">windows</a>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">リンク</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fas fa-angle-up"></div>
</aside>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2024 dobachi<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
        


    
        <script src="/memo-blog/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/memo-blog/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    



<!-- Custom Scripts -->
<script src="/memo-blog/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>