<!DOCTYPE html>
<html lang="ja">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>memo-blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="This is just a memo">
<meta property="og:type" content="website">
<meta property="og:title" content="memo-blog">
<meta property="og:url" content="https://dobachi.github.io/memo-blog/page/6/index.html">
<meta property="og:site_name" content="memo-blog">
<meta property="og:description" content="This is just a memo">
<meta property="og:locale" content="ja">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="memo-blog">
<meta name="twitter:description" content="This is just a memo">
    

    
        <link rel="alternate" href="/" title="memo-blog" type="application/atom+xml">
    

    

    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fontawesome.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fa-brands.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fa-solid.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/memo-blog/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/memo-blog/css/style.css">

    <script src="/memo-blog/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/memo-blog/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/memo-blog/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-155235180-1', 'auto');
ga('send', 'pageview');

</script>
    
    
    


</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/memo-blog/" id="logo">
                <i class="logo"></i>
                <span class="site-title">memo-blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/memo-blog/.">Home</a>
                
                    <a class="main-nav-link" href="/memo-blog/archives">Archives</a>
                
                    <a class="main-nav-link" href="/memo-blog/categories">Categories</a>
                
                    <a class="main-nav-link" href="/memo-blog/tags">Tags</a>
                
                    <a class="main-nav-link" href="/memo-blog/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/memo-blog/css/images/avatar.png" />
                            <i class="fas fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="検索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fas fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '投稿',
            PAGES: 'Pages',
            CATEGORIES: 'カテゴリ',
            TAGS: 'タグ',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/memo-blog/',
        CONTENT_URL: '/memo-blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/memo-blog/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/memo-blog/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="検索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile" class="profile-fixed">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/memo-blog/css/images/avatar.png" />
            <h2 id="name">dobachi</h2>
            <h3 id="title">man of leisure</h3>
            <span id="location"><i class="fas fa-map-marker-alt" style="padding-right: 5px"></i>Tokyo, Japan</span>
            <a id="follow" target="_blank" href="https://github.com/dobachi">フォローする</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                215
                <span>投稿</span>
            </div>
            <div class="article-info-block">
                232
                <span>タグ</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="http://github.com/dobachi" target="_blank" title="github" class=tooltip>
                            <i class="fab fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/memo-blog/" target="_blank" title="rss" class=tooltip>
                            <i class="fab fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main">
    <article id="post-Install-Bigtop-RPMs-using-Yum" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/07/21/Install-Bigtop-RPMs-using-Yum/">Install Bigtop RPMs using Yum</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/07/21/Install-Bigtop-RPMs-using-Yum/">
            <time datetime="2020-07-21T13:50:06.000Z" itemprop="datePublished">2020-07-21</time>
        </a>
    </div>


                        
                        
                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+1.4.0+Release" target="_blank" rel="noopener">Apache
Bigtop 1.4.0のパッケージバージョン</a></li>
<li><a href="https://downloads.apache.org/bigtop/" target="_blank" rel="noopener">レポジトリ関連の資材置き場</a></li>
<li><a href="https://downloads.apache.org/bigtop/bigtop-1.4.0/repos/centos7/bigtop.repo" target="_blank" rel="noopener">CentOS7のbigtop.repo</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>今回は、2020/7/21時点の最新バージョンである <a href="https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+1.4.0+Release" target="_blank" rel="noopener">Apache
Bigtop 1.4.0のパッケージバージョン</a> の
Hadoopをインストールできるかどうかを試してみることとする。</p>
<p>Yumのrepoファイルは <a href="https://downloads.apache.org/bigtop/" target="_blank" rel="noopener">レポジトリ関連の資材置き場</a>
以下にある。 例えば、今回はCentOS7を利用することにするので、 <a href="https://downloads.apache.org/bigtop/bigtop-1.4.0/repos/centos7/bigtop.repo" target="_blank" rel="noopener">CentOS7のbigtop.repo</a>
あたりを利用する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /etc/yum.repos.d</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo wget https://downloads.apache.org/bigtop/bigtop-1.4.0/repos/centos7/bigtop.repo</span></span><br></pre></td></tr></table></figure>
<p>ひとまずパッケージが見つかるかどうか、確認。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo yum search hadoop-conf-pseudo</span></span><br><span class="line">読み込んだプラグイン:fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: d36uatko69830t.cloudfront.net</span><br><span class="line"> * epel: d2lzkl7pfhq30w.cloudfront.net</span><br><span class="line"> * extras: d36uatko69830t.cloudfront.net</span><br><span class="line"> * updates: d36uatko69830t.cloudfront.net</span><br><span class="line">=========================================== N/S matched: hadoop-conf-pseudo ============================================</span><br><span class="line">hadoop-conf-pseudo.x86_64 : Pseudo-distributed Hadoop configuration</span><br></pre></td></tr></table></figure>
<p>確認できたので、試しにインストール。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo yum install hadoop-conf-pseudo</span></span><br></pre></td></tr></table></figure>
<p>自分の手元の環境では、依存関係で以下のパッケージがインストールされた。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">========================================================================================================================</span><br><span class="line"> Package                                    アーキテクチャー   バージョン                      リポジトリー        容量</span><br><span class="line">========================================================================================================================</span><br><span class="line">インストール中:</span><br><span class="line"> hadoop-conf-pseudo                         x86_64             2.8.5-1.el7                     bigtop              20 k</span><br><span class="line">依存性関連でのインストールをします:</span><br><span class="line"> at                                         x86_64             3.1.13-24.el7                   base                51 k</span><br><span class="line"> bc                                         x86_64             1.06.95-13.el7                  base               115 k</span><br><span class="line"> bigtop-groovy                              noarch             2.4.10-1.el7                    bigtop             9.8 M</span><br><span class="line"> bigtop-jsvc                                x86_64             1.0.15-1.el7                    bigtop              29 k</span><br><span class="line"> bigtop-utils                               noarch             1.4.0-1.el7                     bigtop              11 k</span><br><span class="line"> cups-client                                x86_64             1:1.6.3-43.el7                  base               152 k</span><br><span class="line"> ed                                         x86_64             1.9-4.el7                       base                72 k</span><br><span class="line"> hadoop                                     x86_64             2.8.5-1.el7                     bigtop              24 M</span><br><span class="line"> hadoop-hdfs                                x86_64             2.8.5-1.el7                     bigtop              24 M</span><br><span class="line"> hadoop-hdfs-datanode                       x86_64             2.8.5-1.el7                     bigtop             5.7 k</span><br><span class="line"> hadoop-hdfs-namenode                       x86_64             2.8.5-1.el7                     bigtop             5.8 k</span><br><span class="line"> hadoop-hdfs-secondarynamenode              x86_64             2.8.5-1.el7                     bigtop             5.8 k</span><br><span class="line"> hadoop-mapreduce                           x86_64             2.8.5-1.el7                     bigtop              34 M</span><br><span class="line"> hadoop-mapreduce-historyserver             x86_64             2.8.5-1.el7                     bigtop             5.8 k</span><br><span class="line"> hadoop-yarn                                x86_64             2.8.5-1.el7                     bigtop              20 M</span><br><span class="line"> hadoop-yarn-nodemanager                    x86_64             2.8.5-1.el7                     bigtop             5.7 k</span><br><span class="line"> hadoop-yarn-resourcemanager                x86_64             2.8.5-1.el7                     bigtop             5.6 k</span><br><span class="line"> libpcap                                    x86_64             14:1.5.3-12.el7                 base               139 k</span><br><span class="line"> m4                                         x86_64             1.4.16-10.el7                   base               256 k</span><br><span class="line"> mailx                                      x86_64             12.5-19.el7                     base               245 k</span><br><span class="line"> nmap-ncat                                  x86_64             2:6.40-19.el7                   base               206 k</span><br><span class="line"> patch                                      x86_64             2.7.1-12.el7_7                  base               111 k</span><br><span class="line"> psmisc                                     x86_64             22.20-16.el7                    base               141 k</span><br><span class="line"> redhat-lsb-core                            x86_64             4.1-27.el7.centos.1             base                38 k</span><br><span class="line"> redhat-lsb-submod-security                 x86_64             4.1-27.el7.centos.1             base                15 k</span><br><span class="line"> spax                                       x86_64             1.5.2-13.el7                    base               260 k</span><br><span class="line"> time                                       x86_64             1.7-45.el7                      base                30 k</span><br><span class="line"> zookeeper                                  x86_64             3.4.6-1.el7                     bigtop             7.0 M</span><br></pre></td></tr></table></figure>
<p>initスクリプトがインストールされていることがわかる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -1 /etc/init.d/</span></span><br><span class="line">README</span><br><span class="line">functions</span><br><span class="line">hadoop-hdfs-datanode</span><br><span class="line">hadoop-hdfs-namenode</span><br><span class="line">hadoop-hdfs-secondarynamenode</span><br><span class="line">hadoop-mapreduce-historyserver</span><br><span class="line">hadoop-yarn-nodemanager</span><br><span class="line">hadoop-yarn-resourcemanager</span><br><span class="line">netconsole</span><br><span class="line">network</span><br></pre></td></tr></table></figure>
<p>ひとまずHDFSをフォーマット。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo -u hdfs hdfs namenode -format</span></span><br></pre></td></tr></table></figure>
<p>あとは、上記の各種Hadoopサービスを立ち上げれば良い。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/07/21/Install-Bigtop-RPMs-using-Yum/" data-id="cm0jr5kk5005p18qbhtf4hsj9" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Delta-Lake-0-7-0" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/06/19/Delta-Lake-0-7-0/">Delta Lake 0.7.0</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/06/19/Delta-Lake-0-7-0/">
            <time datetime="2020-06-19T00:46:13.000Z" itemprop="datePublished">2020-06-19</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/">Delta Lake</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Delta-Lake/">Delta Lake</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#sql-ddlへの対応やhive-メタストアの対応" id="toc-sql-ddlへの対応やhive-メタストアの対応">SQL DDLへの対応やHive
メタストアの対応</a>
<ul>
<li><a href="#起動時のオプション例" id="toc-起動時のオプション例">起動時のオプション例</a></li>
<li><a href="#パーサの呼び出し流れ" id="toc-パーサの呼び出し流れ">パーサの呼び出し流れ</a></li>
<li><a href="#カスタムカタログ" id="toc-カスタムカタログ">カスタムカタログ</a></li>
<li><a href="#scalaやpythonでの例" id="toc-scalaやpythonでの例">ScalaやPythonでの例</a></li>
<li><a href="#sqlでのマージ処理" id="toc-sqlでのマージ処理">SQLでのマージ処理</a></li>
</ul></li>
<li><a href="#presto-athena用のメタデータの自動生成" id="toc-presto-athena用のメタデータの自動生成">Presto /
Athena用のメタデータの自動生成</a></li>
<li><a href="#テーブル履歴の切り詰めの管理" id="toc-テーブル履歴の切り詰めの管理">テーブル履歴の切り詰めの管理</a></li>
<li><a href="#ユーザメタデータ" id="toc-ユーザメタデータ">ユーザメタデータ</a></li>
<li><a href="#azureのdata-lake-storage-gen2" id="toc-azureのdata-lake-storage-gen2">AzureのData Lake Storage
Gen2</a></li>
<li><a href="#ストリーム処理のone-timeトリガの改善" id="toc-ストリーム処理のone-timeトリガの改善">ストリーム処理のone-timeトリガの改善</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://github.com/delta-io/delta/releases" target="_blank" rel="noopener">Delta Lake
リリースノート</a></li>
<li><a href="https://docs.delta.io/0.7.0/delta-batch.html" target="_blank" rel="noopener">0.7.0のテーブル読み書き</a></li>
<li><a href="https://docs.delta.io/0.6.1/delta-batch.html" target="_blank" rel="noopener">0.6.1のテーブル読み書き</a></li>
<li><a href="https://github.com/delta-io/delta/issues/307#issuecomment-582186407" target="_blank" rel="noopener">Spark3系でないとHiveメタストアに対応できない理由</a></li>
<li><a href="https://github.com/delta-io/delta/blob/master/examples/scala/src/main/scala/example/QuickstartSQL.scala#L58" target="_blank" rel="noopener">SQLを用いたマージの例</a></li>
<li><a href="https://docs.delta.io/0.7.0/delta-batch.html#table-properties" target="_blank" rel="noopener">Table
Properties</a></li>
<li><a href="https://docs.delta.io/0.7.0/delta-batch.html#table-properties" target="_blank" rel="noopener">Table
Properties</a></li>
<li><a href="https://docs.delta.io/0.7.0/delta-storage.html#azure-data-lake-storage-gen2" target="_blank" rel="noopener">0.7.0で対応したAzure
Data Lake Storage Gen2</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>0.7.0が出たので、本リリースの特徴を確認する。</p>
<h2><span id="sql-ddlへの対応やhiveメタストアの対応">SQL DDLへの対応やHive
メタストアの対応</span></h2>
<p>0.6系まではScala、Python APIのみであったが、SQL DDLにも対応した。 <a href="https://docs.delta.io/0.7.0/delta-batch.html" target="_blank" rel="noopener">0.7.0のテーブル読み書き</a>
と <a href="https://docs.delta.io/0.6.1/delta-batch.html" target="_blank" rel="noopener">0.6.1のテーブル読み書き</a>
を見比べると、SQLの例が載っていることがわかる。
対応するSQL構文については
<code>src/main/antlr4/io/delta/sql/parser/DeltaSqlBase.g4</code>
あたりを見ると良い。</p>
<p>なお、 <a href="https://github.com/delta-io/delta/issues/307#issuecomment-582186407" target="_blank" rel="noopener">Spark3系でないとHiveメタストアに対応できない理由</a>
を見る限り、 Spark3系のAPI（や、DataSourceV2も、かな）を使わないと、Data
SourceのカスタムAPIを利用できないため、
これまでHiveメタストアのような外部メタストアと連携したDelta
Lakeのメタデータ管理ができなかった、とのこと。</p>
<p>なお、今回の対応でSparkのカタログ機能を利用することになったので、起動時もしくはSparkSession生成時の
オプション指定が必要になった。
その代わり、ライブラリの明示的なインポートが不要であり、クエリはDelta
Lakeのパーサで解釈された後、
解釈できないようであれば通常のパーサで処理されるようになる。</p>
<h3><span id="起動時のオプション例">起動時のオプション例</span></h3>
<p>例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf <span class="string">"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"</span> --conf <span class="string">"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"</span></span></span><br></pre></td></tr></table></figure>
<p>なお、ここでは <code>SparkSessionExtensions</code>
を利用し、SparkSession生成時にカスタムルール等を挿入している。
この機能は2020/06/19時点でSpark本体側でExperimentalであることに注意。
今後もSpark本体側の仕様変更に引きずられる可能性はある。</p>
<h3><span id="パーサの呼び出し流れ">パーサの呼び出し流れ</span></h3>
<p>セッション拡張機能を利用し、パーサが差し替えられている。</p>
<p>io/delta/sql/DeltaSparkSessionExtension.scala:73</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeltaSparkSessionExtension</span> <span class="keyword">extends</span> (<span class="params"><span class="type">SparkSessionExtensions</span> =&gt; <span class="type">Unit</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(extensions: <span class="type">SparkSessionExtensions</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    extensions.injectParser &#123; (session, parser) =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">DeltaSqlParser</span>(parser)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>io.delta.sql.parser.DeltaSqlParser</code> クラスでは
デリゲート用のパーサを受け取り、自身のパーサで処理できなかった場合に処理をデリゲートパーサに渡す。</p>
<p>io/delta/sql/parser/DeltaSqlParser.scala:66</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeltaSqlParser</span>(<span class="params">val delegate: <span class="type">ParserInterface</span></span>) <span class="keyword">extends</span> <span class="title">ParserInterface</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> builder = <span class="keyword">new</span> <span class="type">DeltaSqlAstBuilder</span></span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>例えば、 <code>SparkSession</code> の <code>sql</code>
メソッドを使って呼び出す場合を例にする。 このとき、内部では、
<code>org.apache.spark.sql.catalyst.parser.ParserInterface#parsePlan</code>
メソッドが呼ばれて、 渡されたクエリ文 <code>sqlText</code>
が処理される。</p>
<p>org/apache/spark/sql/SparkSession.scala:601</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sql</span></span>(sqlText: <span class="type">String</span>): <span class="type">DataFrame</span> = withActive &#123;</span><br><span class="line">  <span class="keyword">val</span> tracker = <span class="keyword">new</span> <span class="type">QueryPlanningTracker</span></span><br><span class="line">  <span class="keyword">val</span> plan = tracker.measurePhase(<span class="type">QueryPlanningTracker</span>.<span class="type">PARSING</span>) &#123;</span><br><span class="line">    sessionState.sqlParser.parsePlan(sqlText)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Dataset</span>.ofRows(self, plan, tracker)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>この <code>parsePlan</code>
がoverrideされており、以下のように定義されている。</p>
<p>io/delta/sql/parser/DeltaSqlParser.scala:69</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">parsePlan</span></span>(sqlText: <span class="type">String</span>): <span class="type">LogicalPlan</span> = parse(sqlText) &#123; parser =&gt;</span><br><span class="line">  builder.visit(parser.singleStatement()) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> plan: <span class="type">LogicalPlan</span> =&gt; plan</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; delegate.parsePlan(sqlText)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>まずは <code>io.delta.sql.parser.DeltaSqlParser#parse</code>
メソッドを利用してパースがここ見られるが、
LogicalPlanが戻らなかったときは、デリゲート用パーサが呼び出されるようになっている。</p>
<h3><span id="カスタムカタログ">カスタムカタログ</span></h3>
<p>Spark3ではDataSourvV2の中で、プラガブルなカタログに対応した。 Delta
Lake
0.7.0はこれを利用し、カスタムカタログを用いる。（これにより、Hiveメタストアを経由してDelta
Lake形式のデータを読み書きできるようになっている）
使われているカタログは
<code>org.apache.spark.sql.delta.catalog.DeltaCatalog</code> である。
（SparkSessionのインスタンス生成する際、もしくは起動時のオプション指定）</p>
<p>当該カタログ内部では、例えば
<code>org.apache.spark.sql.delta.catalog.DeltaCatalog#createDeltaTable</code>
メソッドが定義されており、
<code>org.apache.spark.sql.delta.catalog.DeltaCatalog#createTable</code>
※ しようとするときなどに呼び出されるようになっている。</p>
<p>※
<code>org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension#createTable</code>
をoverrideしている</p>
<p>なお、このクラスもデリゲート用のカタログを用いるようになっている。
<code>org.apache.spark.sql.delta.catalog.DeltaCatalog#createTable</code>
メソッドは以下のようになっており、 データソースが delta
出ない場合は、親クラスの <code>createTable</code>
（つまり標準的なもの）が呼び出されるようになっている。</p>
<p>org/apache/spark/sql/delta/catalog/DeltaCatalog.scala:149</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createTable</span></span>(</span><br><span class="line">    ident: <span class="type">Identifier</span>,</span><br><span class="line">    schema: <span class="type">StructType</span>,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Transform</span>],</span><br><span class="line">    properties: util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Table</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">DeltaSourceUtils</span>.isDeltaDataSourceName(getProvider(properties))) &#123;</span><br><span class="line">    createDeltaTable(</span><br><span class="line">      ident, schema, partitions, properties, sourceQuery = <span class="type">None</span>, <span class="type">TableCreationModes</span>.<span class="type">Create</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">super</span>.createTable(ident, schema, partitions, properties)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3><span id="scalaやpythonでの例">ScalaやPythonでの例</span></h3>
<p>代表的にScalaの例を出す。公式サイトには以下のように載っている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">"delta"</span>).saveAsTable(<span class="string">"events"</span>)      <span class="comment">// create table in the metastore</span></span><br><span class="line"></span><br><span class="line">df.write.format(<span class="string">"delta"</span>).save(<span class="string">"/delta/events"</span>)  <span class="comment">// create table by path</span></span><br></pre></td></tr></table></figure>
<p>Hiveメタストア経由で書き込むケースと、ストレージ上に直接書き出すケースが載っている。</p>
<h3><span id="sqlでのマージ処理">SQLでのマージ処理</span></h3>
<p><a href="https://github.com/delta-io/delta/blob/master/examples/scala/src/main/scala/example/QuickstartSQL.scala#L58" target="_blank" rel="noopener">SQLを用いたマージの例</a>
の通り、Delta Lakeの特徴であるマージ機能もSQLから呼び出させる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">s""</span><span class="string">"MERGE INTO $tableName USING newData</span></span><br><span class="line"><span class="string">    ON $&#123;tableName&#125;.id = newData.id</span></span><br><span class="line"><span class="string">    WHEN MATCHED THEN</span></span><br><span class="line"><span class="string">      UPDATE SET $&#123;tableName&#125;.id = newData.id</span></span><br><span class="line"><span class="string">    WHEN NOT MATCHED THEN INSERT *</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>)</span><br></pre></td></tr></table></figure>
<p>Spark SQLのカタログに登録されたDelta
LakeのテーブルからDeltaTableを生成することもできる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables.<span class="type">DeltaTable</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> tbl = <span class="type">DeltaTable</span>.forName(tableName)</span><br></pre></td></tr></table></figure>
<h2><span id="presto-athena用のメタデータの自動生成">Presto /
Athena用のメタデータの自動生成</span></h2>
<p>Delta
LakeはPresto、Athena用のメタデータを生成できるが、更新があった際にパーティションごとに自動で再生成できるようになった。</p>
<h2><span id="テーブル履歴の切り詰めの管理">テーブル履歴の切り詰めの管理</span></h2>
<p>Delta Lakeは更新の履歴を保持することも特徴の一つだが、
データ本体とログのそれぞれの切り詰め対象期間を指定できる。</p>
<p>CREATEやALTER句内で、TBLPROPERTIESとして指定することになっている。
例えば以下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">s"CREATE TABLE <span class="subst">$tableName</span>(id LONG) USING delta TBLPROPERTIES ('delta.logRetentionDuration' = 'interval 1 day', 'delta.deletedFileRetentionDuration' = 'interval 1 day')"</span>)</span><br><span class="line">spark.sql(<span class="string">s"ALTER TABLE <span class="subst">$tableName</span> SET TBLPROPERTIES ('delta.logRetentionDuration' = 'interval 1 day', 'delta.deletedFileRetentionDuration' = 'interval 1 day')"</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="ユーザメタデータ">ユーザメタデータ</span></h2>
<p>spark.databricks.delta.commitInfo.userMetadata
プロパティを利用して、ユーザメタデータを付与できる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.option(<span class="string">"spark.databricks.delta.commitInfo.userMetadata"</span>, <span class="string">"test"</span>).format(<span class="string">"delta"</span>).mode(<span class="string">"append"</span>).save(<span class="string">"/tmp/test"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"SET spark.databricks.delta.commitInfo.userMetadata=test"</span>)</span><br><span class="line">scala&gt; spark.sql(<span class="string">s"INSERT INTO <span class="subst">$tableName</span> VALUES 0, 1, 2, 3, 4"</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="azureのdata-lake-storagegen2">AzureのData Lake Storage
Gen2</span></h2>
<p>対応した。</p>
<p>しかし、 <a href="https://docs.delta.io/0.7.0/delta-storage.html#azure-data-lake-storage-gen2" target="_blank" rel="noopener">0.7.0で対応したAzure
Data Lake Storage Gen2</a> の通り、
前提となっている各種ソフトウェアバージョンは高め。</p>
<h2><span id="ストリーム処理のone-timeトリガの改善">ストリーム処理のone-timeトリガの改善</span></h2>
<p>DataStreamReaderのオプションでmaxFilesPerTriggerを設定しているとしても、
one-time
triggerでは一度に溜まったすべてのデータを読み込むようになった。（Spark
3系の話）  <!-- vim: set et tw=0 ts=2 sw=2: --></p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/06/19/Delta-Lake-0-7-0/" data-id="cm0jr5kt2015p18qbc69yxy43" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-About-tream-table-theory" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/05/20/About-tream-table-theory/">About tream table theory</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/05/20/About-tream-table-theory/">
            <time datetime="2020-05-19T15:12:43.000Z" itemprop="datePublished">2020-05-20</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Kafka/">Kafka</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#tyler-akidauらによるapache-beamにまつわる情報" id="toc-tyler-akidauらによるapache-beamにまつわる情報">Tyler
AkidauらによるApache Beamにまつわる情報</a></li>
<li><a href="#matthias-j.-saxguozhang-wangらによる論文" id="toc-matthias-j.-saxguozhang-wangらによる論文">Matthias J.
Sax、Guozhang Wangらによる論文</a>
<ul>
<li><a href="#概要" id="toc-概要">概要</a></li>
<li><a href="#introduction" id="toc-introduction">1
Introduction</a></li>
<li><a href="#background" id="toc-background">2 Background</a></li>
<li><a href="#duality-of-streams-and-tables" id="toc-duality-of-streams-and-tables">3 Duality of streams and
tables</a></li>
<li><a href="#stream-processing-operators" id="toc-stream-processing-operators">4 STREAM PROCESSING
OPERATORS</a></li>
<li><a href="#case-study-apache-kafka" id="toc-case-study-apache-kafka">5 CASE STUDY: APACHE KAFKA</a></li>
<li><a href="#related-work" id="toc-related-work">6 RELATED
WORK</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://www.slideshare.net/Hadoop_Summit/foundations-of-streaming-sql-stream-table-theory" target="_blank" rel="noopener">Foundations
of streaming SQL stream &amp; table theory</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="tylerakidauらによるapache-beamにまつわる情報">Tyler
AkidauらによるApache Beamにまつわる情報</span></h2>
<p><a href="https://www.slideshare.net/Hadoop_Summit/foundations-of-streaming-sql-stream-table-theory#43" target="_blank" rel="noopener">Foundations
of streaming SQL stream &amp; table theory (General theory)</a> の通り、
ストリームデータとテーブルデータの関係性を論じたもの。</p>
<p>上記の内容の全体は、 <a href="http://shop.oreilly.com/product/0636920073994.do" target="_blank" rel="noopener">OReillyのStreaming
Systems</a> に記載あり。</p>
<p>これらは、ストリーム・テーブル理論をベースに、Apache
Beamを利用したストリームデータ処理・活用システムについて論じたもの。</p>
<h2><span id="matthias-jsax-guozhang-wangらによる論文">Matthias J.
Sax、Guozhang Wangらによる論文</span></h2>
<p><a href="https://dl.acm.org/doi/10.1145/3242153.3242155" target="_blank" rel="noopener">Streams and
Tables Two Sides of the Same Coin</a></p>
<h3><span id="概要">概要</span></h3>
<p>ストリームデータの処理モデルを提案。
論理・物理の間の一貫性の崩れに対処する。</p>
<h3><span id="1-introduction">1 Introduction</span></h3>
<p>データストリームのチャレンジ</p>
<ul>
<li>物理的なアウト・オブ・オーダへの対応</li>
<li>処理のオペレータがステートフルでないといけない、レイテンシ、正しさ、処理コストなどのトレードオフがある、など</li>
<li>分散処理への対応</li>
</ul>
<p>物理的な並びが保証されない条件下で、
十分に表現力のあるオペレータ（モデル）を立案する。</p>
<h3><span id="2-background">2 Background</span></h3>
<p>ストリームデータのモデル。</p>
<p>ポイント。レコード配下の構成。</p>
<ul>
<li>オフセット：物理の並び</li>
<li>タイムスタンプ：論理の並び（生成時刻）</li>
<li>キー</li>
<li>バリュー</li>
</ul>
<h3><span id="3-duality-of-streams-andtables">3 Duality of streams and
tables</span></h3>
<p>レイテンシは、データストリームの特性に依存する。</p>
<p>論理、物理の並びの差異を解決するため、結果を継続的に更新するモデルを提案した。</p>
<p>Dual Streaming Model</p>
<p>DUality of streams and tables</p>
<p>テーブルは、テーブルのバージョンのコレクションと考えることもできる。</p>
<h3><span id="4-stream-processing-operators">4 STREAM PROCESSING OPERATORS</span></h3>
<p>モデル定義を解説。
フィルターなどのステートレスな処理から、アグリゲーションなどのステートフルな処理まで。</p>
<p>out-of-order
なレコードが届いても、最終的な出力結果テーブルがout-of-orderレコードがないときと同一になるように動くことを期待する。</p>
<p>Watermarkと似たような機構として、「retention time」を導入。
結果テーブル内の「現在時刻 - retention
time」よりも古い結果をメンテナンス対象から外す。</p>
<p>Stream - Table Joinのケースで、「遅れテーブル更新」が生じると、
下流システムに対して結果の上書きデータを出力する必要がある。</p>
<h3><span id="5-case-study-apache-kafka">5 CASE STUDY: APACHE KAFKA</span></h3>
<p>Kafka Streamsの例。</p>
<p>Kafka Streamsの利用企業：The New York Times, Pinterest, LINE,
Trivago, etc</p>
<p>Kafka Streamsのオペレーションはすべてノンブロッキングであり、
レコードを受領次第処理し、KTableにマテリアライズされたり、Topicに書き戻したりされる。</p>
<p>Window集計の例を題材に、retention timeを利用。 retention
timeを長くするほどストレージを利用するが、
out-of-orderレコードに対してロバストになる。 また、retention
timeが長いほど、ウィンドウ結果が確定したと言えるタイミングが遅くなる。</p>
<h3><span id="6-related-work">6 RELATED WORK</span></h3>
<p>Relationを取り扱うモデル、out-of-orderを取り扱うモデル、テーブルバージョン管理を取り扱うモデルなど。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/05/20/About-tream-table-theory/" data-id="cm0jr5kin001418qbc8kzztev" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Handle-pictures-in-delta-lake" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/05/12/Handle-pictures-in-delta-lake/">Handle pictures in delta lake and hudi</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/05/12/Handle-pictures-in-delta-lake/">
            <time datetime="2020-05-12T13:58:43.000Z" itemprop="datePublished">2020-05-12</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/">Delta Lake</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Delta-Lake/">Delta Lake</a>, <a class="tag-link" href="/memo-blog/tags/Spark/">Spark</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#delta-lake" id="toc-delta-lake">Delta Lake</a>
<ul>
<li><a href="#まずはjpgをsparkで読み込む" id="toc-まずはjpgをsparkで読み込む">まずはjpgをSparkで読み込む</a></li>
<li><a href="#delta-lakeで扱う" id="toc-delta-lakeで扱う">Delta
Lakeで扱う</a>
<ul>
<li><a href="#書き込み" id="toc-書き込み">書き込み</a></li>
<li><a href="#読み出し" id="toc-読み出し">読み出し</a></li>
<li><a href="#更新" id="toc-更新">更新</a></li>
</ul></li>
<li><a href="#parquetの内容を確認する" id="toc-parquetの内容を確認する">Parquetの内容を確認する</a>
<ul>
<li><a href="#スキーマ" id="toc-スキーマ">スキーマ</a></li>
<li><a href="#メタデータ" id="toc-メタデータ">メタデータ</a></li>
</ul></li>
<li><a href="#pythonで画像処理してみる" id="toc-pythonで画像処理してみる">Pythonで画像処理してみる</a></li>
</ul></li>
<li><a href="#hudi" id="toc-hudi">Hudi</a>
<ul>
<li><a href="#画像読み込み" id="toc-画像読み込み">画像読み込み</a></li>
<li><a href="#書き込み-1" id="toc-書き込み-1">書き込み</a></li>
<li><a href="#読み込み" id="toc-読み込み">読み込み</a></li>
<li><a href="#更新-1" id="toc-更新-1">更新</a></li>
<li><a href="#parquetの内容を確認する-1" id="toc-parquetの内容を確認する-1">Parquetの内容を確認する</a>
<ul>
<li><a href="#スキーマ-1" id="toc-スキーマ-1">スキーマ</a></li>
<li><a href="#メタデータ-1" id="toc-メタデータ-1">メタデータ</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://spark.apache.org/docs/latest/ml-datasource#image-data-source" target="_blank" rel="noopener">Spark公式ドキュメントのImage
data source</a></li>
<li><a href="https://www.kaggle.com/scolianni/mnistasjpg/data" target="_blank" rel="noopener">Kaggleのmnistデータ</a></li>
<li><a href="https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/" target="_blank" rel="noopener">How
to Load and Manipulate Images for Deep Learning in Python With
PIL/Pillow</a></li>
<li><a href="https://hudi.apache.org/docs/quick-start-guide.html#setup" target="_blank" rel="noopener">Hudiのquick-start-guide</a></li>
<li><a href="https://github.com/apache/parquet-mr" target="_blank" rel="noopener">parquet-mr</a></li>
</ul>
<h1><span id="delta-lake">Delta Lake</span></h1>
<h2><span id="まずはjpgをsparkで読み込む">まずはjpgをSparkで読み込む</span></h2>
<p>予め、Delta Lakeを読み込みながら起動する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.11:0.6.0</span></span><br></pre></td></tr></table></figure>
<p><a href="https://spark.apache.org/docs/latest/ml-datasource#image-data-source" target="_blank" rel="noopener">Spark公式ドキュメントのImage
data source</a> を利用し、jpgをDataFrameとして読み込んでみる。
データは、 <a href="https://www.kaggle.com/scolianni/mnistasjpg/data" target="_blank" rel="noopener">Kaggleのmnistデータ</a>
を利用。このデータはjpgになっているので便利。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> home = sys.env(<span class="string">"HOME"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> imageDir = <span class="string">"Downloads/mnist_jpg/trainingSet/trainingSet/0"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"image"</span>).option(<span class="string">"dropInvalid"</span>, <span class="literal">true</span>).load(home + <span class="string">"/"</span> + imageDir)</span><br></pre></td></tr></table></figure>
<p>データをDataFrameにする定義ができた。 内容は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">"image.origin"</span>, <span class="string">"image.width"</span>, <span class="string">"image.height"</span>).show(<span class="number">3</span>, truncate=<span class="literal">false</span>)</span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">|origin                                                                         |width|height|</span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">|file:<span class="comment">///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_20188.jpg|28   |28    |</span></span><br><span class="line">|file:<span class="comment">///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_12634.jpg|28   |28    |</span></span><br><span class="line">|file:<span class="comment">///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_26812.jpg|28   |28    |</span></span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br></pre></td></tr></table></figure>
<p>公式ドキュメントのとおりだが、カラムは以下の通り。</p>
<ul>
<li>origin: StringType (represents the file path of the image)</li>
<li>height: IntegerType (height of the image)</li>
<li>width: IntegerType (width of the image)</li>
<li>nChannels: IntegerType (number of image channels)</li>
<li>mode: IntegerType (OpenCV-compatible type)</li>
<li>data: BinaryType (Image bytes in OpenCV-compatible order: row-wise
BGR in most cases)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- image: struct (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- origin: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- height: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- width: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- nChannels: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- mode: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- data: binary (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<h2><span id="delta-lakeで扱う">Delta Lakeで扱う</span></h2>
<h3><span id="書き込み">書き込み</span></h3>
<p>これをDelta LakeのData Sourceで書き出してみる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> deltaImagePath = <span class="string">"/tmp/delta-lake-image"</span></span><br><span class="line">scala&gt; df.write.format(<span class="string">"delta"</span>).save(deltaImagePath)</span><br></pre></td></tr></table></figure>
<h3><span id="読み出し">読み出し</span></h3>
<p>できた。試しにDeltaTableとして読み出してみる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(deltaImagePath)</span><br></pre></td></tr></table></figure>
<h3><span id="更新">更新</span></h3>
<p>mergeを試す。
マージのためにキーを明示的に与えつつデータを準備する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> targetDf = df.select($<span class="string">"image.origin"</span>, $<span class="string">"image"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> targetImagePath = <span class="string">"/tmp/delta-table"</span></span><br><span class="line">scala&gt; targetDf.write.format(<span class="string">"delta"</span>).save(targetImagePath)</span><br><span class="line">scala&gt; <span class="keyword">val</span> sourcePath = <span class="string">"Downloads/mnist_jpg/trainingSet/trainingSet/1"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> sourceDf = spark.read.format(<span class="string">"image"</span>).option(<span class="string">"dropInvalid"</span>, <span class="literal">true</span>).load(home + <span class="string">"/"</span> + sourcePath).select($<span class="string">"image.origin"</span>, $<span class="string">"image"</span>)</span><br></pre></td></tr></table></figure>
<p>Delta Tableを定義。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(targetImagePath)</span><br><span class="line">scala&gt; deltaTable</span><br><span class="line">        .as(<span class="string">"target"</span>)</span><br><span class="line">        .merge(</span><br><span class="line">          sourceDf.as(<span class="string">"source"</span>),</span><br><span class="line">          <span class="string">"target.origin = source.origin"</span>)</span><br><span class="line">        .whenMatched</span><br><span class="line">        .updateExpr(<span class="type">Map</span>(</span><br><span class="line">          <span class="string">"image"</span> -&gt; <span class="string">"source.image"</span>))</span><br><span class="line">        .whenNotMatched</span><br><span class="line">        .insertExpr(<span class="type">Map</span>(</span><br><span class="line">          <span class="string">"origin"</span> -&gt; <span class="string">"source.origin"</span>,</span><br><span class="line">          <span class="string">"image"</span> -&gt; <span class="string">"source.image"</span>))</span><br><span class="line">        .execute()</span><br></pre></td></tr></table></figure>
<p>実際に、追加データが入っていることが確かめられる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; deltaTable.toDF.where($<span class="string">"origin"</span> contains <span class="string">"trainingSet/1"</span>).select(<span class="string">"image.origin"</span>, <span class="string">"image.width"</span>, <span class="string">"image.height"</span>).show(<span class="number">3</span>, truncate=<span class="literal">false</span>)</span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">|origin                                                                         |width|height|</span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">|file:<span class="comment">///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/1/img_10266.jpg|28   |28    |</span></span><br><span class="line">|file:<span class="comment">///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/1/img_10665.jpg|28   |28    |</span></span><br><span class="line">|file:<span class="comment">///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/1/img_10772.jpg|28   |28    |</span></span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">only showing top <span class="number">3</span> rows</span><br></pre></td></tr></table></figure>
<p>SparkのImage Data
Sourceは、メタデータと画像本体（バイナリ）を構造で扱っているだけなので、
実は特に工夫することなく扱えた。</p>
<p>公式ドキュメントによると、OpenCV形式バイナリで保持されている。
これから先は、Python使って処理したいので、改めて開き直す。</p>
<h2><span id="parquetの内容を確認する">Parquetの内容を確認する</span></h2>
<p><a href="https://github.com/apache/parquet-mr" target="_blank" rel="noopener">parquet-mr</a> の
<code>parquet-tools</code> を利用して、Parquetの内容を確認する。</p>
<p>なお、Hudi
v0.5.2におけるParquetのバージョンは以下の通り、1.10.1である。</p>
<p>pom.xml:84</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parquet.version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">parquet.version</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>当該バージョンに対応するタグをチェックアウトし、予めparquet-toolsをパッケージしておくこと。</p>
<h3><span id="スキーマ">スキーマ</span></h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -jar target/parquet-tools-1.10.1.jar schema /tmp/delta-lake-image/part-00000-c3d03d70-1785-435f-b055-b2a78903e732-c000.snappy.parquet</span></span><br><span class="line">message spark_schema &#123;</span><br><span class="line">  optional group image &#123;</span><br><span class="line">    optional binary origin (UTF8);</span><br><span class="line">    optional int32 height;</span><br><span class="line">    optional int32 width;</span><br><span class="line">    optional int32 nChannels;</span><br><span class="line">    optional int32 mode;</span><br><span class="line">    optional binary data;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>シンプルにParquet内に保持されていることがわかる。 Spark
SQLのParquet取扱機能を利用している。</p>
<h3><span id="メタデータ">メタデータ</span></h3>
<p>ファイル名やクリエイタなど。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file:        file:/tmp/delta-lake-image/part-00000-c3d03d70-1785-435f-b055-b2a78903e732-c000.snappy.parquet</span><br><span class="line">creator:     parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)</span><br><span class="line">extra:       org.apache.spark.sql.parquet.row.metadata = &#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;image&quot;,&quot;type&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;origin&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;height&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;width&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;nChannels&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;mode&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;data&quot;,&quot;type&quot;:&quot;binary&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;]&#125;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;]&#125;</span><br></pre></td></tr></table></figure>
<p>ファイルスキーマ</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">file schema: spark_schema</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">image:       OPTIONAL F:6</span><br><span class="line">.origin:     OPTIONAL BINARY O:UTF8 R:0 D:2</span><br><span class="line">.height:     OPTIONAL INT32 R:0 D:2</span><br><span class="line">.width:      OPTIONAL INT32 R:0 D:2</span><br><span class="line">.nChannels:  OPTIONAL INT32 R:0 D:2</span><br><span class="line">.mode:       OPTIONAL INT32 R:0 D:2</span><br><span class="line">.data:       OPTIONAL BINARY R:0 D:2</span><br></pre></td></tr></table></figure>
<p>Rowグループ</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">row group 1: RC:32 TS:29939 OFFSET:4</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">image:</span><br><span class="line">.origin:      BINARY SNAPPY DO:0 FPO:4 SZ:620/2838/4.58 VC:32 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_10203.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9098.jpg, num_nulls: 0]</span><br><span class="line">.height:      INT32 SNAPPY DO:0 FPO:624 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0]</span><br><span class="line">.width:       INT32 SNAPPY DO:0 FPO:698 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0]</span><br><span class="line">.nChannels:   INT32 SNAPPY DO:0 FPO:772 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 1, max: 1, num_nulls: 0]</span><br><span class="line">.mode:        INT32 SNAPPY DO:0 FPO:846 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0]</span><br><span class="line">.data:        BINARY SNAPPY DO:0 FPO:920 SZ:21175/26821/1.27 VC:32 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 0x00</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<h2><span id="pythonで画像処理してみる">Pythonで画像処理してみる</span></h2>
<p>とりあえずOpenCVをインストールしたPython環境のJupyterでPySparkを起動。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> PYSPARK_DRIVER_PYTHON_OPTS=<span class="string">"notebook --ip 0.0.0.0"</span> /opt/spark/default/bin/pyspark --packages io.delta:delta-core_2.11:0.6.0 --conf spark.pyspark.driver.python=<span class="variable">$HOME</span>/venv/jupyter/bin/jupyter</span></span><br></pre></td></tr></table></figure>
<p>この後はJupyter内で処理する。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> delta.tables <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line">delta_image_path = <span class="string">"/tmp/delta-lake-image"</span></span><br><span class="line">deltaTable = DeltaTable.forPath(spark, delta_image_path)</span><br><span class="line">deltaTable.toDF().select(<span class="string">"image.origin"</span>, <span class="string">"image.width"</span>, <span class="string">"image.height"</span>).show(<span class="number">3</span>, truncate=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>とりあえず読み込めたことがわかる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">|origin                                                                         |width|height|</span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">|file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_20188.jpg|28   |28    |</span><br><span class="line">|file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_12634.jpg|28   |28    |</span><br><span class="line">|file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_26812.jpg|28   |28    |</span><br><span class="line">+-------------------------------------------------------------------------------+-----+------+</span><br><span class="line">only showing top 3 rows</span><br></pre></td></tr></table></figure>
<p>中のデータを利用できることを確かめるため、1件だけ取り出して処理してみる。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = deltaTable.toDF().where(<span class="string">"image.origin == 'file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg'"</span>).select(<span class="string">"image.origin"</span>, <span class="string">"image.width"</span>, <span class="string">"image.height"</span>, <span class="string">"image.nChannels"</span>, <span class="string">"image.data"</span>).collect()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>とりあえずエッジ抽出をしてみる。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">nparr = np.frombuffer(img.data, np.uint8).reshape(img.height, img.width, img.nChannels)</span><br><span class="line">edges = cv2.Canny(nparr,<span class="number">100</span>,<span class="number">200</span>)</span><br><span class="line">plt.imshow(edges)</span><br></pre></td></tr></table></figure>
<p>Jupyter上でエッジ抽出されたことが確かめられただろうか。</p>
<figure>
<img src="/memo-blog/images/3LsygT3y9k3jCjbJ-EB945.png" alt="エッジ抽出された様">
<figcaption aria-hidden="true">エッジ抽出された様</figcaption>
</figure>
<p>これをSparkでUDFで実行するようにする。
（とりあえず雑にライブラリをインポート…）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_edge</span><span class="params">(data, h, w, c)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="keyword">import</span> cv2</span><br><span class="line">    nparr = np.frombuffer(data, np.uint8).reshape(h, w, c)</span><br><span class="line">    edge = cv2.Canny(nparr,<span class="number">100</span>,<span class="number">200</span>)</span><br><span class="line">    <span class="keyword">return</span> edge.tobytes()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> BinaryType</span><br><span class="line"></span><br><span class="line">get_edge_udf = udf(get_edge, BinaryType())</span><br></pre></td></tr></table></figure>
<p>エッジ抽出されたndarrayをバイト列に変換し、Spark上ではBinaryTypeで扱うように指定した。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with_edge = deltaTable.toDF().select(<span class="string">"image.origin"</span>, <span class="string">"image.width"</span>, <span class="string">"image.height"</span>, <span class="string">"image.nChannels"</span>, get_edge_udf(<span class="string">"image.data"</span>, <span class="string">"image.height"</span>, <span class="string">"image.width"</span>, <span class="string">"image.nChannels"</span>).alias(<span class="string">"edge"</span>), <span class="string">"image.data"</span>)</span><br><span class="line"></span><br><span class="line">with_edge.show(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+--------------------+-----+------+---------+--------------------+--------------------+</span><br><span class="line">|              origin|width|height|nChannels|                edge|                data|</span><br><span class="line">+--------------------+-----+------+---------+--------------------+--------------------+</span><br><span class="line">|file:///home/cent...|   <span class="number">28</span>|    <span class="number">28</span>|        <span class="number">1</span>|[<span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0.</span>..|[<span class="number">05</span> <span class="number">00</span> <span class="number">04</span> <span class="number">08</span> <span class="number">00</span> <span class="number">0.</span>..|</span><br><span class="line">|file:///home/cent...|   <span class="number">28</span>|    <span class="number">28</span>|        <span class="number">1</span>|[<span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0.</span>..|[<span class="number">05</span> <span class="number">00</span> <span class="number">0</span>B <span class="number">00</span> <span class="number">00</span> <span class="number">0.</span>..|</span><br><span class="line">|file:///home/cent...|   <span class="number">28</span>|    <span class="number">28</span>|        <span class="number">1</span>|[<span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0.</span>..|[<span class="number">06</span> <span class="number">00</span> <span class="number">02</span> <span class="number">00</span> <span class="number">02</span> <span class="number">0.</span>..|</span><br><span class="line">+--------------------+-----+------+---------+--------------------+--------------------+</span><br><span class="line">only showing top <span class="number">3</span> rows</span><br></pre></td></tr></table></figure>
<p>こんな感じで、OpenCVを用いた処理をDataFrameに対して実行できる。</p>
<p>なお、当然ながら処理されたデータを取り出せば、また画像として表示も可能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_img = with_edge.where(<span class="string">"origin == 'file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg'"</span>).select(<span class="string">"origin"</span>, <span class="string">"width"</span>, <span class="string">"height"</span>, <span class="string">"nChannels"</span>, <span class="string">"data"</span>, <span class="string">"edge"</span>).collect()[<span class="number">0</span>]</span><br><span class="line">new_nparr = np.frombuffer(new_img.edge, np.uint8).reshape(new_img.height, new_img.width)</span><br><span class="line">plt.imshow(new_nparr)</span><br></pre></td></tr></table></figure>
<h1><span id="hudi">Hudi</span></h1>
<h2><span id="画像読み込み">画像読み込み</span></h2>
<p><a href="#まずはjpgをsparkで読み込む">まずはjpgをSparkで読み込む</a>
と同様に データを読み込む。</p>
<p><a href="https://hudi.apache.org/docs/quick-start-guide.html#setup" target="_blank" rel="noopener">Hudiのquick-start-guide</a>
を参考にしながら、まずはHudiを読み込みながら、シェルを起動する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /opt/spark/default/bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \</span></span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'</span><br></pre></td></tr></table></figure>
<p>ライブラリをインポート。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceReadOptions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceWriteOptions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.config.<span class="type">HoodieWriteConfig</span>._</span><br></pre></td></tr></table></figure>
<p>画像データを読み込み</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tableName = <span class="string">"hudi_images"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> basePath = <span class="string">"file:///tmp/hudi_images"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> home = sys.env(<span class="string">"HOME"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> imageDir = <span class="string">"Downloads/mnist_jpg/trainingSet/trainingSet/*"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"image"</span>).option(<span class="string">"dropInvalid"</span>, <span class="literal">true</span>).load(home + <span class="string">"/"</span> + imageDir)</span><br></pre></td></tr></table></figure>
<p>今回は、Hudiにおけるパーティション構成を確認するため、上記のようにワイルドカード指定した。</p>
<h2><span id="書き込み">書き込み</span></h2>
<p>Hudi書き込み用にデータを変換。パーティション用キーなどを定義する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, udf&#125;</span><br><span class="line">scala&gt; <span class="keyword">val</span> partitionPath = udf((str: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">         str.split(<span class="string">"/"</span>).takeRight(<span class="number">2</span>).head</span><br><span class="line">       &#125;)</span><br><span class="line">scala&gt; <span class="keyword">val</span> hudiDf = df.select($<span class="string">"image.origin"</span>, partitionPath($<span class="string">"image.origin"</span>) as <span class="string">"partitionpath"</span>, $<span class="string">"*"</span>)</span><br></pre></td></tr></table></figure>
<p>書き込み。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; hudiDf.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">         option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">         option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"origin"</span>).</span><br><span class="line">         option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"origin"</span>).</span><br><span class="line">         option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">         mode(<span class="type">Overwrite</span>).</span><br><span class="line">         save(basePath)</span><br></pre></td></tr></table></figure>
<p>書き込まれたデータは以下のような感じ。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -R /tmp/hudi_images/</span></span><br><span class="line">/tmp/hudi_images/:</span><br><span class="line">0  1  2  3  4  5  6  7  8  9</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/0:</span><br><span class="line">86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-29-14074_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/1:</span><br><span class="line">4912032f-3365-4852-b2ae-91ffb5ea9806-0_1-29-14075_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/2:</span><br><span class="line">0636bb93-dd32-40f1-b5c8-328130386528-0_2-29-14076_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/3:</span><br><span class="line">6f0154cd-4a9f-4001-935b-47d067416797-0_3-29-14077_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/4:</span><br><span class="line">b6f29c2c-7df0-481a-8149-2bbc93e92e2c-0_4-29-14078_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/5:</span><br><span class="line">24115172-10db-4c85-808a-bf4048e0f533-0_5-29-14079_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/6:</span><br><span class="line">64823f4b-26fb-4679-87e8-cd81d01b1181-0_6-29-14080_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/7:</span><br><span class="line">d0c097e3-9ab7-477a-b591-593c6cb7880f-0_7-29-14081_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/8:</span><br><span class="line">13a48b38-cbfb-4076-957d-9c580765bfca-0_8-29-14082_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/9:</span><br><span class="line">5d3ba9ae-8ede-410f-95f3-e8d69e8c0478-0_9-29-14083_20200517135104.parquet</span><br></pre></td></tr></table></figure>
<p>メタデータは以下のような感じ。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -1 /tmp/hudi_images/.hoodie/</span></span><br><span class="line">20200517135104.clean</span><br><span class="line">20200517135104.clean.inflight</span><br><span class="line">20200517135104.clean.requested</span><br><span class="line">20200517135104.commit</span><br><span class="line">20200517135104.commit.requested</span><br><span class="line">20200517135104.inflight</span><br><span class="line">20200517140006.clean</span><br><span class="line">20200517140006.clean.inflight</span><br><span class="line">20200517140006.clean.requested</span><br><span class="line">20200517140006.commit</span><br><span class="line">20200517140006.commit.requested</span><br><span class="line">20200517140006.inflight</span><br><span class="line">archived</span><br><span class="line">hoodie.properties</span><br></pre></td></tr></table></figure>
<h2><span id="読み込み">読み込み</span></h2>
<p>読み込んで見る。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> hudiImageDf = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*"</span>)</span><br></pre></td></tr></table></figure>
<p>スキーマは以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; hudiImageDf.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- _hoodie_commit_time: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- _hoodie_commit_seqno: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- _hoodie_record_key: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- _hoodie_partition_path: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- _hoodie_file_name: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- origin: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- partitionpath: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- image: struct (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- origin: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- height: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- width: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- nChannels: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- mode: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- data: binary (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p>すべてのただの書き込みAPIを試しただけだが、想定通り書き込み・読み込みできている。</p>
<h2><span id="更新">更新</span></h2>
<p>Hudiといえば、テーブルを更新可能であることも特徴のひとつなので、試しに更新をしてみる。
元は同じデータであるが、「0」の画像データを新しいデータとしてDataFrameを定義し、更新する。
意図的に、一部のデータだけ更新する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> updateImageDir = <span class="string">"Downloads/mnist_jpg/trainingSet/trainingSet/0"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> updateDf = spark.read.format(<span class="string">"image"</span>).option(<span class="string">"dropInvalid"</span>, <span class="literal">true</span>).load(home + <span class="string">"/"</span> + updateImageDir)</span><br><span class="line">scala&gt; <span class="keyword">val</span> updateHudiDf = updateDf.select($<span class="string">"image.origin"</span>, partitionPath($<span class="string">"image.origin"</span>) as <span class="string">"partitionpath"</span>, $<span class="string">"*"</span>)</span><br><span class="line">scala&gt; updateHudiDf.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">         option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">         option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"origin"</span>).</span><br><span class="line">         option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"origin"</span>).</span><br><span class="line">         option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">         mode(<span class="type">Append</span>).</span><br><span class="line">         save(basePath)</span><br></pre></td></tr></table></figure>
<p>以下のように、「0」のデータが更新されていることがわかる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -R /tmp/hudi_images/</span></span><br><span class="line">/tmp/hudi_images/:</span><br><span class="line">0  1  2  3  4  5  6  7  8  9</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/0:</span><br><span class="line">86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-29-14074_20200517135104.parquet</span><br><span class="line">86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_images/1:</span><br><span class="line">4912032f-3365-4852-b2ae-91ffb5ea9806-0_1-29-14075_20200517135104.parquet</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>つづいて、更新されたことを確かめるため、改めて読み込みDataFrameを定義し、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> updatedHudiImageDf = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*"</span>)</span><br></pre></td></tr></table></figure>
<p>試しに、差分がわかるように「1」と「0」のデータをそれぞれ読んで見る。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; updatedHudiImageDf.select($<span class="string">"_hoodie_commit_time"</span>, $<span class="string">"_hoodie_commit_seqno"</span>, $<span class="string">"_hoodie_partition_path"</span>).filter(<span class="string">"partitionpath == 1"</span>).show(<span class="number">3</span>)</span><br><span class="line">+-------------------+--------------------+----------------------+</span><br><span class="line">|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_partition_path|</span><br><span class="line">+-------------------+--------------------+----------------------+</span><br><span class="line">|     <span class="number">20200517135104</span>|  <span class="number">20200517135104</span>_1_1|                     <span class="number">1</span>|</span><br><span class="line">|     <span class="number">20200517135104</span>| <span class="number">20200517135104</span>_1_12|                     <span class="number">1</span>|</span><br><span class="line">|     <span class="number">20200517135104</span>| <span class="number">20200517135104</span>_1_45|                     <span class="number">1</span>|</span><br><span class="line">+-------------------+--------------------+----------------------+</span><br><span class="line">only showing top <span class="number">3</span> rows</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; updatedHudiImageDf.select($<span class="string">"_hoodie_commit_time"</span>, $<span class="string">"_hoodie_commit_seqno"</span>, $<span class="string">"_hoodie_partition_path"</span>).filter(<span class="string">"partitionpath == 0"</span>).show(<span class="number">3</span>)</span><br><span class="line">+-------------------+--------------------+----------------------+</span><br><span class="line">|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_partition_path|</span><br><span class="line">+-------------------+--------------------+----------------------+</span><br><span class="line">|     <span class="number">20200517140006</span>|<span class="number">20200517140006</span>_0_...|                     <span class="number">0</span>|</span><br><span class="line">|     <span class="number">20200517140006</span>|<span class="number">20200517140006</span>_0_...|                     <span class="number">0</span>|</span><br><span class="line">|     <span class="number">20200517140006</span>|<span class="number">20200517140006</span>_0_...|                     <span class="number">0</span>|</span><br><span class="line">+-------------------+--------------------+----------------------+</span><br><span class="line">only showing top <span class="number">3</span> rows</span><br></pre></td></tr></table></figure>
<p>上記のように、更新されたことがわかる。最新の更新日時は、
<code>2020/05/17 14:00:06UTC</code> である。</p>
<p>ここで読み込まれたデータも、Delta
Lake同様に画像を元にしたベクトルデータとして扱える。 Pythonであれば
<code>ndarray</code> に変換して用いれば良い。</p>
<h2><span id="parquetの内容を確認する">Parquetの内容を確認する</span></h2>
<p><a href="https://github.com/apache/parquet-mr" target="_blank" rel="noopener">parquet-mr</a> の
<code>parquet-tools</code> を利用して、Parquetの内容を確認する。</p>
<p>なお、Hudi
v0.5.2におけるParquetのバージョンは以下の通り、1.10.1である。</p>
<p>pom.xml:84</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parquet.version</span>&gt;</span>1.10.1<span class="tag">&lt;/<span class="name">parquet.version</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>当該バージョンに対応するタグをチェックアウトし、予めparquet-toolsをパッケージしておくこと。</p>
<h3><span id="スキーマ">スキーマ</span></h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -jar target/parquet-tools-1.10.1.jar schema /tmp/hudi_images/0/86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquet</span></span><br><span class="line">message hoodie.hudi_images.hudi_images_record &#123;</span><br><span class="line">  optional binary _hoodie_commit_time (UTF8);</span><br><span class="line">  optional binary _hoodie_commit_seqno (UTF8);</span><br><span class="line">  optional binary _hoodie_record_key (UTF8);</span><br><span class="line">  optional binary _hoodie_partition_path (UTF8);</span><br><span class="line">  optional binary _hoodie_file_name (UTF8);</span><br><span class="line">  optional binary origin (UTF8);</span><br><span class="line">  optional binary partitionpath (UTF8);</span><br><span class="line">  optional group image &#123;</span><br><span class="line">    optional binary origin (UTF8);</span><br><span class="line">    optional int32 height;</span><br><span class="line">    optional int32 width;</span><br><span class="line">    optional int32 nChannels;</span><br><span class="line">    optional int32 mode;</span><br><span class="line">    optional binary data;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Hudiが独自に付加したカラムが追加されていることが見て取れる。</p>
<h3><span id="メタデータ">メタデータ</span></h3>
<p>つづいて、メタデータを確認する。</p>
<p>ファイル名やクリエイタなど。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file:                   file:/tmp/hudi_images/0/86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquet</span><br><span class="line">creator:                parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)</span><br></pre></td></tr></table></figure>
<p>ブルームフィルタはここでは省略</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">extra:                  org.apache.hudi.bloomfilter = ///</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>レコードキーやAvroスキーマ</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">extra:                  hoodie_min_record_key = file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg</span><br><span class="line">extra:                  parquet.avro.schema = &#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_images_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_images&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;_hoodie_commit_time&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_commit_seqno&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_record_key&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_partition_path&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_file_name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;origin&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;image&quot;,&quot;type&quot;:[&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;image&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_images.hudi_images_record&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;origin&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;height&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;width&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;nChannels&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;mode&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;data&quot;,&quot;type&quot;:[&quot;bytes&quot;,&quot;null&quot;]&#125;]&#125;,&quot;null&quot;]&#125;]&#125;</span><br><span class="line">extra:                  writer.model.name = avro</span><br><span class="line">extra:                  hoodie_max_record_key = file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg</span><br></pre></td></tr></table></figure>
<p>上記の通り、Hudiでは <code>parquet-avro</code>
を利用し、Avro形式でParquet内のデータを保持する。</p>
<p>スキーマ詳細</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">file schema:            hoodie.hudi_images.hudi_images_record</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">_hoodie_commit_time:    OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">_hoodie_commit_seqno:   OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">_hoodie_record_key:     OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">_hoodie_partition_path: OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">_hoodie_file_name:      OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">origin:                 OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">partitionpath:          OPTIONAL BINARY O:UTF8 R:0 D:1</span><br><span class="line">image:                  OPTIONAL F:6</span><br><span class="line">.origin:                OPTIONAL BINARY O:UTF8 R:0 D:2</span><br><span class="line">.height:                OPTIONAL INT32 R:0 D:2</span><br><span class="line">.width:                 OPTIONAL INT32 R:0 D:2</span><br><span class="line">.nChannels:             OPTIONAL INT32 R:0 D:2</span><br><span class="line">.mode:                  OPTIONAL INT32 R:0 D:2</span><br><span class="line">.data:                  OPTIONAL BINARY R:0 D:2</span><br></pre></td></tr></table></figure>
<p>Rowグループ</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">row group 1:            RC:4132 TS:4397030 OFFSET:4</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">_hoodie_commit_time:     BINARY GZIP DO:0 FPO:4 SZ:165/127/0.77 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 20200517140006, max: 20200517140006, num_nulls: 0]</span><br><span class="line">_hoodie_commit_seqno:    BINARY GZIP DO:0 FPO:169 SZ:10497/107513/10.24 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 20200517140006_0_42001, max: 20200517140006_0_46132, num_nulls: 0]</span><br><span class="line">_hoodie_record_key:      BINARY GZIP DO:0 FPO:10666 SZ:16200/342036/21.11 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg, num_nulls: 0]</span><br><span class="line">_hoodie_partition_path:  BINARY GZIP DO:0 FPO:26866 SZ:100/62/0.62 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0]</span><br><span class="line">_hoodie_file_name:       BINARY GZIP DO:0 FPO:26966 SZ:448/419/0.94 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquet, max: 86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquet, num_nulls: 0]</span><br><span class="line">origin:                  BINARY GZIP DO:0 FPO:27414 SZ:16200/342036/21.11 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg, num_nulls: 0]</span><br><span class="line">partitionpath:           BINARY GZIP DO:0 FPO:43614 SZ:100/62/0.62 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0]</span><br><span class="line">image:</span><br><span class="line">.origin:                 BINARY GZIP DO:0 FPO:43714 SZ:16200/342036/21.11 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg, num_nulls: 0]</span><br><span class="line">.height:                 INT32 GZIP DO:0 FPO:59914 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0]</span><br><span class="line">.width:                  INT32 GZIP DO:0 FPO:60025 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0]</span><br><span class="line">.nChannels:              INT32 GZIP DO:0 FPO:60136 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 1, max: 1, num_nulls: 0]</span><br><span class="line">.mode:                   INT32 GZIP DO:0 FPO:60247 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0]</span><br><span class="line">.data:                   BINARY GZIP DO:0 FPO:60358 SZ:1609341/3262447/2.03 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 0x00</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/05/12/Handle-pictures-in-delta-lake/" data-id="cm0jr5kvx01f618qben4dw8d8" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Research-about-BigTop" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/04/24/Research-about-BigTop/">Research about BigTop</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/04/24/Research-about-BigTop/">
            <time datetime="2020-04-24T12:51:36.000Z" itemprop="datePublished">2020-04-24</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/">Hadoop</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/BigTop/">BigTop</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/BigTop/">BigTop</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#trunkのビルド方法" id="toc-trunkのビルド方法">Trunkのビルド方法</a></li>
<li><a href="#デプロイ周りを読み解いてみる" id="toc-デプロイ周りを読み解いてみる">デプロイ周りを読み解いてみる</a>
<ul>
<li><a href="#dockerベースのデプロイを試す" id="toc-dockerベースのデプロイを試す">Dockerベースのデプロイを試す</a></li>
<li><a href="#ambariのモジュールを確認する" id="toc-ambariのモジュールを確認する">Ambariのモジュールを確認する。</a></li>
</ul></li>
<li><a href="#ホットissue" id="toc-ホットissue">ホットIssue</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/BIGTOP/Index" target="_blank" rel="noopener">公式Wikiトップページ</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/BIGTOP/How+to+build+Bigtop-trunk" target="_blank" rel="noopener">How
to build Bigtop-trunk</a></li>
<li><a href="https://github.com/dobachi/bigtop/tree/reuse_images" target="_blank" rel="noopener">reuse_images</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/BIGTOP/Deployment+and+Integration+Testing" target="_blank" rel="noopener">Deployment
and Integration Testing</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+Provisioner+User+Guide" target="_blank" rel="noopener">Dockerベースのデプロイ方法</a></li>
<li><a href="https://github.com/apache/bigtop/blob/master/bigtop-deploy/puppet/README.md" target="_blank" rel="noopener">Puppetベースのデプロイ方法</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="trunkのビルド方法">Trunkのビルド方法</span></h2>
<p><a href="https://cwiki.apache.org/confluence/display/BIGTOP/How+to+build+Bigtop-trunk" target="_blank" rel="noopener">How
to build Bigtop-trunk</a>
を見ると、現在ではDocker上でビルドする方法が推奨されているようだ。
ただ、</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./gradlew spark-pkg-ind</span></span><br></pre></td></tr></table></figure>
<p>で実行されるとき、都度Mavenキャッシュのない状態から開始される。
これが待ち時間長いので工夫が必要。</p>
<p>なお、上記コマンドで実行されるタスクは、以下の通り。</p>
<p>packages.gradle:629</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">task</span> <span class="string">"$target-pkg-ind"</span> (</span><br><span class="line">          <span class="keyword">description</span>: <span class="string">"Invoking a native binary packaging for $target in Docker. Usage: \$ ./gradlew "</span> +</span><br><span class="line">                  <span class="string">"-POS=[centos-7|fedora-26|debian-9|ubuntu-16.04|opensuse-42.3] "</span> +</span><br><span class="line">                  <span class="string">"-Pprefix=[trunk|1.2.1|1.2.0|1.1.0|...] $target-pkg-ind "</span> +</span><br><span class="line">                  <span class="string">"-Pnexus=[true|false]"</span>,</span><br><span class="line">          <span class="keyword">group</span>: PACKAGES_GROUP) <span class="keyword">doLast</span> &#123;</span><br><span class="line">    <span class="keyword">def</span> _prefix = <span class="keyword">project</span>.hasProperty(<span class="string">"prefix"</span>) ? prefix : <span class="string">"trunk"</span></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>bigtop-ci/build.sh</code>
がタスク内で実行されるスクリプトである。
この中でDockerが呼び出されてビルドが実行される。</p>
<p>これをいじれば、一応ビルド時に使ったDockerコンテナを残すことができそうではある。</p>
<p>-&gt; <a href="https://github.com/dobachi/bigtop/tree/reuse_images" target="_blank" rel="noopener">reuse_images</a>
というブランチに、コンテナンをリユースする機能をつけた。</p>
<h2><span id="デプロイ周りを読み解いてみる">デプロイ周りを読み解いてみる</span></h2>
<p><a href="https://cwiki.apache.org/confluence/display/BIGTOP/Deployment+and+Integration+Testing" target="_blank" rel="noopener">Deployment
and Integration Testing</a> を見る限り、 <a href="https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+Provisioner+User+Guide" target="_blank" rel="noopener">Dockerベースのデプロイ方法</a>
と <a href="https://github.com/apache/bigtop/blob/master/bigtop-deploy/puppet/README.md" target="_blank" rel="noopener">Puppetベースのデプロイ方法</a>
があるように見える。</p>
<h3><span id="dockerベースのデプロイを試す">Dockerベースのデプロイを試す</span></h3>
<p><a href="https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+Provisioner+User+Guide" target="_blank" rel="noopener">Dockerベースのデプロイ方法</a>
に従って、Dockerベースでデプロイしてみる。
ソースコードは20200427時点でのmasterを利用。</p>
<p>Ubuntu16でDocker、Java、Ruby環境を整え、以下を実行した。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ./gradlew -Pconfig=config_ubuntu-16.04.yaml -Pnum_instances=1 docker-provisioner</span></span><br></pre></td></tr></table></figure>
<p>なお、20200427時点で <a href="https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+Provisioner+User+Guide" target="_blank" rel="noopener">Dockerベースのデプロイ方法</a>
ではコンフィグファイルが <code>config_ubuntu_xenial.yaml</code>
となっていたが、 BIGTOP-2814 の際に変更されたのに合わせた。</p>
<p>プロビジョニング完了後、コンテナを確認し、接続。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                              COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">7311ae86181a        bigtop/puppet:trunk-ubuntu-16.04   "/sbin/init"        4 minutes ago       Up 4 minutes                            20200426_151302_r30103_bigtop_1</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo docker <span class="built_in">exec</span> -it 20200426_151302_r30103_bigtop_1 bash</span></span><br></pre></td></tr></table></figure>
<p>以下のように <code>hdfs</code> コマンドを実行できることがわかる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@7311ae86181a:/# hdfs dfs -ls /</span><br><span class="line">Found 7 items</span><br><span class="line">drwxr-xr-x   - hdfs  hadoop          0 2020-04-26 15:15 /apps</span><br><span class="line">drwxrwxrwx   - hdfs  hadoop          0 2020-04-26 15:15 /benchmarks</span><br><span class="line">drwxr-xr-x   - hbase hbase           0 2020-04-26 15:15 /hbase</span><br><span class="line">drwxr-xr-x   - solr  solr            0 2020-04-26 15:15 /solr</span><br><span class="line">drwxrwxrwt   - hdfs  hadoop          0 2020-04-26 15:15 /tmp</span><br><span class="line">drwxr-xr-x   - hdfs  hadoop          0 2020-04-26 15:15 /user</span><br><span class="line">drwxr-xr-x   - hdfs  hadoop          0 2020-04-26 15:15 /var</span><br></pre></td></tr></table></figure>
<p>なお、 <code>-Pnum_instances=1</code>
の値を3に変更すると、コンテナが3個立ち上がる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ./gradlew -Pconfig=config_ubuntu-16.04.yaml -Pnum_instances=3 docker-provisioner</span></span><br></pre></td></tr></table></figure>
<p>試しに、<code>dfsadmin</code> を実行してみる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@53211353deec:/# sudo -u hdfs hdfs dfsadmin -report</span><br><span class="line">Configured Capacity: 374316318720 (348.61 GB)</span><br><span class="line">Present Capacity: 345302486224 (321.59 GB)</span><br><span class="line">DFS Remaining: 345300606976 (321.59 GB)</span><br><span class="line">DFS Used: 1879248 (1.79 MB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">Under replicated blocks: 0</span><br><span class="line">Blocks with corrupt replicas: 0</span><br><span class="line">Missing blocks: 0</span><br><span class="line">Missing blocks (with replication factor 1): 0</span><br><span class="line">Pending deletion blocks: 0</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Live datanodes (3):</span><br><span class="line"></span><br><span class="line">Name: 172.17.0.2:50010 (53211353deec.bigtop.apache.org)</span><br><span class="line">Hostname: 53211353deec.bigtop.apache.org</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 124772106240 (116.20 GB)</span><br><span class="line">DFS Used: 626416 (611.73 KB)</span><br><span class="line">Non DFS Used: 9637679376 (8.98 GB)</span><br><span class="line">DFS Remaining: 115100246016 (107.20 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 92.25%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Sun Apr 26 15:33:12 UTC 2020</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>また、 <code>--stack</code>
オプションを利用し、デプロイするコンポーネントを指定できる。
ここではHadoopに加え、Sparkをプロビジョニングしてみる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ./gradlew -Pconfig=config_ubuntu-16.04.yaml -Pstack=hdfs,yarn,spark -Pnum_instances=1 docker-provisioner</span></span><br></pre></td></tr></table></figure>
<p>コンテナに接続し、Sparkを動かす。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo docker ps</span></span><br><span class="line">CONTAINER ID        IMAGE                              COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">56e1ff5670be        bigtop/puppet:trunk-ubuntu-16.04   "/sbin/init"        3 minutes ago       Up 3 minutes                            20200426_155010_r21667_bigtop_1</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo docker <span class="built_in">exec</span> -it 20200426_155010_r21667_bigtop_1 bash</span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@56e1ff5670be:/# spark-shell</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sparkContext.master</span></span><br><span class="line">res1: String = yarn</span><br></pre></td></tr></table></figure>
<p>上記のように、マスタ=YARNで起動していることがわかる。 参考までに、
<code>spark-env.sh</code> は以下の通り。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root@56e1ff5670be:/# cat /etc/spark/conf/spark-env.sh</span><br><span class="line">export SPARK_HOME=$&#123;SPARK_HOME:-/usr/lib/spark&#125;</span><br><span class="line">export SPARK_LOG_DIR=$&#123;SPARK_LOG_DIR:-/var/log/spark&#125;</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=NONE"</span><br><span class="line">export HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/lib/hadoop&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-/etc/hadoop/conf&#125;</span><br><span class="line">export HIVE_CONF_DIR=$&#123;HIVE_CONF_DIR:-/etc/hive/conf&#125;</span><br><span class="line"></span><br><span class="line">export STANDALONE_SPARK_MASTER_HOST=56e1ff5670be.bigtop.apache.org</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">export SPARK_MASTER_IP=$STANDALONE_SPARK_MASTER_HOST</span><br><span class="line">export SPARK_MASTER_URL=yarn</span><br><span class="line">export SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line">export SPARK_WORKER_DIR=$&#123;SPARK_WORKER_DIR:-/var/run/spark/work&#125;</span><br><span class="line">export SPARK_WORKER_PORT=7078</span><br><span class="line">export SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line">export SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br></pre></td></tr></table></figure>
<p>なお、構築したクラスタを破棄する際には以下の通り。（公式ドキュメントの通り）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ./gradlew docker-provisioner-destroy</span></span><br></pre></td></tr></table></figure>
<h4><span id="仕様確認">仕様確認</span></h4>
<p>上記のプロビジョナのタスクは、 <code>build.gradle</code>
内にある。</p>
<p>build.gradle:263</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">task &quot;docker-provisioner&quot;(type:Exec,</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>上記の中で、 <code>./docker-hadoop.sh</code> が用いられている。</p>
<p>build.gradle:296</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> command = [</span><br><span class="line">    <span class="string">'./docker-hadoop.sh'</span>,</span><br><span class="line">    <span class="string">'-C'</span>, _config,</span><br><span class="line">    <span class="string">'--create'</span>, _num_instances,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><code>--create</code> オプションが用いられているので、
<code>create</code> 関数が呼ばれる。</p>
<p>provisioner/docker/docker-hadoop.sh:387</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ "$READY_TO_LAUNCH" = true ]; then</span><br><span class="line">    create $NUM_INSTANCES</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>
<p><code>create</code> 内では、以下のように <code>docker-compose</code>
が用いられている。</p>
<p>provisioner/docker/docker-hadoop.sh:78</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -p $PROVISION_ID up -d --scale bigtop=$1 --no-recreate</span><br></pre></td></tr></table></figure>
<p>また、コンポーネントの内容に応じて、Puppetマニフェスト（正確には、hieraファイル）が生成されるようになっている。</p>
<p>provisioner/docker/docker-hadoop.sh:101</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generate-config "$hadoop_head_node" "$repo" "$components"</span><br></pre></td></tr></table></figure>
<p>また、最終的には、 <code>provision</code> 関数、さらに
<code>bigtop-puppet</code>
関数を通じ、各コンテナ内でpuppetが実行されるようになっている。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bigtop-puppet() &#123;</span><br><span class="line">    <span class="keyword">if</span> docker exec $<span class="number">1</span> bash -c <span class="string">"puppet --version"</span> | <span class="keyword">grep</span> ^<span class="number">3</span> &gt;<span class="regexp">/dev/</span><span class="keyword">null</span> ; then</span><br><span class="line">      future=<span class="string">"--parser future"</span></span><br><span class="line">    fi</span><br><span class="line">    docker exec $<span class="number">1</span> bash -c <span class="string">"puppet apply --detailed-exitcodes $future --modulepath=/bigtop-home/bigtop-deploy/puppet/modules:/etc/puppet/modules:/usr/share/puppet/modules /bigtop-home/bigtop-deploy/puppet/manifests"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3><span id="ambariのモジュールを確認する">Ambariのモジュールを確認する。</span></h3>
<p><code>bigtop-deploy/puppet/modules/ambari/manifests/init.pp</code>
にAmbariデプロイ用のモジュールがある。</p>
<p>内容は短い。</p>
<figure class="highlight puppet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title">ambari</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">class</span> <span class="title">deploy</span> ($<span class="title">roles</span>) &#123;</span><br><span class="line">    if (<span class="string">"ambari-server"</span> in <span class="variable">$roles</span>) &#123;</span><br><span class="line">      include ambari::server</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (<span class="string">"ambari-agent"</span> in <span class="variable">$roles</span>) &#123;</span><br><span class="line">      include ambari::agent</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">class</span> <span class="title">server</span> &#123;</span><br><span class="line">    <span class="keyword">package</span> &#123; <span class="string">"ambari-server"</span>:</span><br><span class="line">      <span class="attr">ensure</span> =&gt; latest,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">exec</span> &#123;</span><br><span class="line">        <span class="string">"mpack install"</span>:</span><br><span class="line">           <span class="attr">command</span> =&gt; <span class="string">"/bin/bash -c 'echo yes | /usr/sbin/ambari-server install-mpack --purge --verbose --mpack=/var/lib/ambari-server/resources/odpi-ambari-mpack-1.0.0.0-SNAPSHOT.tar.gz'"</span>,</span><br><span class="line">           <span class="attr">require</span> =&gt; [ Package[<span class="string">"ambari-server"</span>] ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">exec</span> &#123;</span><br><span class="line">        <span class="string">"server setup"</span>:</span><br><span class="line">           <span class="attr">command</span> =&gt; <span class="string">"/usr/sbin/ambari-server setup -j $(readlink -f /usr/bin/java | sed 's@jre/bin/java@@') -s"</span>,</span><br><span class="line">           <span class="attr">require</span> =&gt; [ Package[<span class="string">"ambari-server"</span>], Package[<span class="string">"jdk"</span>], Exec[<span class="string">"mpack install"</span>] ]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">service</span> &#123; <span class="string">"ambari-server"</span>:</span><br><span class="line">        <span class="attr">ensure</span> =&gt; <span class="literal">running</span>,</span><br><span class="line">        <span class="attr">require</span> =&gt; [ Package[<span class="string">"ambari-server"</span>], Exec[<span class="string">"server setup"</span>] ],</span><br><span class="line">        <span class="attr">hasrestart</span> =&gt; <span class="keyword">true</span>,</span><br><span class="line">        <span class="attr">hasstatus</span> =&gt; <span class="keyword">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class agent($server_host = "localhost") &#123;</span><br><span class="line">    <span class="keyword">package</span> &#123; <span class="string">"ambari-agent"</span>:</span><br><span class="line">      <span class="attr">ensure</span> =&gt; latest,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">file</span> &#123;</span><br><span class="line">      <span class="string">"/etc/ambari-agent/conf/ambari-agent.ini"</span>:</span><br><span class="line">        <span class="attr">content</span> =&gt; template(<span class="string">'ambari/ambari-agent.ini'</span>),</span><br><span class="line">        <span class="attr">require</span> =&gt; [Package[<span class="string">"ambari-agent"</span>]],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">service</span> &#123; <span class="string">"ambari-agent"</span>:</span><br><span class="line">        <span class="attr">ensure</span> =&gt; <span class="literal">running</span>,</span><br><span class="line">        <span class="attr">require</span> =&gt; [ Package[<span class="string">"ambari-agent"</span>], File[<span class="string">"/etc/ambari-agent/conf/ambari-agent.ini"</span>] ],</span><br><span class="line">        <span class="attr">hasrestart</span> =&gt; <span class="keyword">true</span>,</span><br><span class="line">        <span class="attr">hasstatus</span> =&gt; <span class="keyword">true</span>,</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、上記の通り、サーバとエージェントそれぞれのマニフェストが存在する。
ODPiのMpackをインストールしているのが特徴。</p>
<p>逆にいうと、それをインストールしないと使えるVersionが存在しない。（ベンダ製のVDFを読み込めば使えるが）
また、ODPiのMpackをインストールした上でクラスタを構成しようとしたところ、
ODPiのレポジトリが見当たらなかった。
プライベートレポジトリを立てる必要があるのだろうか。</p>
<p>いったん、公式のAmbariをインストールした上で動作確認することにする。</p>
<h2><span id="ホットissue">ホットIssue</span></h2>
<p><a href="https://issues.apache.org/jira/browse/BIGTOP-3123" target="_blank" rel="noopener">BIGTOP-3123</a>
が1.5リリース向けのIssue</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/04/24/Research-about-BigTop/" data-id="cm0jr5kto017718qbb322x5mg" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Storage-Layer-Software-for-Machine-Learning" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/">機械学習向けのFeature StoreないしStorage Layer Software</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/">
            <time datetime="2020-04-10T02:43:58.000Z" itemprop="datePublished">2020-04-10</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/Storage-Layer-Software/">Storage Layer Software</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a>
<ul>
<li><a href="#プロダクト" id="toc-プロダクト">プロダクト</a></li>
<li><a href="#企業アーキテクチャ" id="toc-企業アーキテクチャ">企業アーキテクチャ</a></li>
<li><a href="#まとめ" id="toc-まとめ">まとめ</a></li>
</ul></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#傾向" id="toc-傾向">傾向</a></li>
<li><a href="#feature-storeとして挙げられている特徴機能" id="toc-feature-storeとして挙げられている特徴機能">Feature
Storeとして挙げられている特徴・機能</a>
<ul>
<li><a href="#主にfeatuer-storeとしての特徴" id="toc-主にfeatuer-storeとしての特徴">主に、featuer
storeとしての特徴</a></li>
<li><a href="#rawデータストアを含めた特徴" id="toc-rawデータストアを含めた特徴">rawデータストアを含めた特徴</a></li>
<li><a href="#特徴量エンジニアリングの例" id="toc-特徴量エンジニアリングの例">特徴量エンジニアリングの例</a></li>
<li><a href="#feature-storeにおける画像の取扱は" id="toc-feature-storeにおける画像の取扱は">feature
storeにおける画像の取扱は？</a></li>
</ul></li>
<li><a href="#feastにおけるデータフロー概要" id="toc-feastにおけるデータフロー概要">Feastにおけるデータフロー概要</a></li>
<li><a href="#hopsworksにおけるfeature-store" id="toc-hopsworksにおけるfeature-store">hopsworksにおけるfeature
store</a></li>
<li><a href="#ストレージ製品の動向" id="toc-ストレージ製品の動向">ストレージ製品の動向</a>
<ul>
<li><a href="#netapp" id="toc-netapp">Netapp</a></li>
<li><a href="#dell-emc" id="toc-dell-emc">Dell EMC</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<h2><span id="プロダクト">プロダクト</span></h2>
<ul>
<li>Feast
<ul>
<li><a href="https://github.com/gojek/feast" target="_blank" rel="noopener">Feast</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li>メモ
<ul>
<li>Feature Store for Machine Learning https://feast.dev</li>
<li>GoJek/Google released Feast in early 2019 and it is built around
Google Cloud services: Big Query (offline) and Big Table (online) and
Redis (low-latency), using Beam for feature engineering.</li>
</ul></li>
</ul></li>
<li><a href="https://delta.io/" target="_blank" rel="noopener">Delta Lake</a>
<ul>
<li>メモ
<ul>
<li>Delta Lake is an open-source storage layer that brings ACID
transactions to Apache Spark™ and big data workloads.</li>
</ul></li>
</ul></li>
<li>Hopsworks
<ul>
<li>メモ
<ul>
<li>The Platform for Data-Intensive AI</li>
<li>Feature Storeに限らない</li>
</ul></li>
<li><a href="https://www.hopsworks.ai/" target="_blank" rel="noopener">Hopsworks</a></li>
<li><a href="https://github.com/logicalclocks/hopsworks" target="_blank" rel="noopener">HopsworksのGitHub</a></li>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li><a href="https://metaflow.org/" target="_blank" rel="noopener">Metaflow</a>
<ul>
<li>メモ
<ul>
<li>Netflixの機械学習パイプライン管理用のライブラリ</li>
<li>特徴量を保存するためのライブラリを内容（割と素朴に保存する仕組み）</li>
</ul></li>
</ul></li>
<li><a href="https://github.com/uber/petastorm" target="_blank" rel="noopener">Petastorm</a>
<ul>
<li>メモ
<ul>
<li>いろいろなフレームワークから利用できるデータ入出力のためのライブラリ
Parquetを利用する。</li>
</ul></li>
</ul></li>
<li><a href="https://www.zadara.com/" target="_blank" rel="noopener">Zadara</a>
<ul>
<li>どちらかということ純粋にストレージ</li>
</ul></li>
<li>Netapp
<ul>
<li>メモ
<ul>
<li>いわゆる「ストレージ」におけるアプローチの例</li>
</ul></li>
<li><a href="https://www.netapp.com/us/solutions/applications/ai-deep-learning.aspx" target="_blank" rel="noopener">Accelerated
AI and deep learning pipelines across edge, core, and cloud</a></li>
<li><a href="https://www.netapp.com/us/media/wp-7271.pdf" target="_blank" rel="noopener">Edge to Core
to Cloud Architecture for AI</a></li>
</ul></li>
<li>Dell EMC
<ul>
<li>メモ
<ul>
<li>単独の技術というより、コンピューティングと合わせてのソリューション、サーバ</li>
</ul></li>
<li><a href="https://www.dellemc.com/resources/en-us/asset/analyst-reports/products/storage/h17841_ar_enterprise_machine_and_deep_learning_with_intelligent_storage.pdf" target="_blank" rel="noopener">ENTERPRISE
MACHINE &amp; DEEP LEARNING WITH INTELLIGENT STORAGE</a></li>
</ul></li>
<li>IBM
<ul>
<li>メモ
<ul>
<li>Watsonの名のもとに様々なソリューションを集結</li>
</ul></li>
<li><a href="https://www.ibm.com/it-infrastructure/storage/ai-infrastructure" target="_blank" rel="noopener">IBM
Storage for AI and big data</a></li>
<li><a href="https://www.ibm.com/downloads/cas/JPKRD1R0" target="_blank" rel="noopener">IBM Spectrum
Storage for AI with Power Systems</a></li>
</ul></li>
<li>Kafka
<ul>
<li>メモ
<ul>
<li>推論用のイベントをやり取りするためのハブとして用いる</li>
<li>最近開発されているTiered
Storage機能を利用し、長期保存用のストレージと組み合わせた使い方が可能になる
つまり学習データ（ヒストリカルデータ）を含めて、Kafkaでデータをサーブすることが描かれている。</li>
</ul></li>
<li><a href="https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/" target="_blank" rel="noopener">Streaming
Machine Learning with Tiered Storage and Without a Data Lake</a>
<ul>
<li>Tiered Storageの紹介と機械学習への応用例</li>
</ul></li>
</ul></li>
<li><a href="https://www.usenix.org/conference/atc19/presentation/liang" target="_blank" rel="noopener">Cognitive
SSD</a>
<ul>
<li>メモ
<ul>
<li>USENIX ATC'19</li>
<li>非構造データを記憶装置の階層間で移動するのが無駄。そこでSSDのNAND
Flash横にDeep Learning用、グラフ検索用のエンジンを積む</li>
</ul></li>
</ul></li>
<li>Ignite
<ul>
<li><a href="https://ignite.apache.org/features/machinelearning.html" target="_blank" rel="noopener">Ignite
Machine Larning</a></li>
</ul></li>
<li>Bandana
<ul>
<li><a href="https://research.fb.com/publications/bandana-using-non-volatile-memory-for-storing-deep-learning-models/" target="_blank" rel="noopener">Bandana
Using Non-Volatile Memory for Storing Deep Learning Models</a></li>
<li>Facebook Research</li>
<li>深層学習モデルをストアするためのストレージの提案。
NVMを活用。一緒に読み込まれるベクトルを物理的近く配置し、プリフェッチの効果を向上。
キャッシュポリシーをシミュレーションに基づいて最適化。</li>
</ul></li>
</ul>
<h2><span id="企業アーキテクチャ">企業アーキテクチャ</span></h2>
<ul>
<li><a href="https://www.slideshare.net/Alluxio/pinterest-big-data-machine-learning-platform-at-pinterest" target="_blank" rel="noopener">Pinterest
- Big Data Machine Learning Platform at Pinterest</a></li>
<li>Michelangelo
<ul>
<li>メモ
<ul>
<li>UberのMLプラットフォーム。必ずしもFeature Storeに限らない。</li>
<li>Feature
Storeに関して特筆すると、「online」と「offline」のデータを統合して扱う、という発想。</li>
</ul></li>
<li><a href="https://eng.uber.com/michelangelo-machine-learning-platform/" target="_blank" rel="noopener">Michelangelo_0</a></li>
<li><a href="https://eng.uber.com/michelangelo-machine-learning-model-representation/" target="_blank" rel="noopener">Michelangelo_1</a></li>
</ul></li>
<li>Twiter</li>
<li>特徴量をライブラリとして保持、アプリケーションから使いやすくした？</li>
<li>Comcas
<ul>
<li>ReidsをFeature Storeに利用</li>
</ul></li>
<li>Pinterest
<ul>
<li><a href="https://www.slideshare.net/Alluxio/pinterest-big-data-machine-learning-platform-at-pinterest" target="_blank" rel="noopener">Pinterest
- Big Data Machine Learning Platform at Pinterest</a></li>
</ul></li>
<li>Zipline
<ul>
<li>メモ
<ul>
<li>特徴量エンジニアリングパイプラインを補助するライブラリ</li>
<li>バックフィルが特徴？</li>
<li>OSSではないように見える</li>
</ul></li>
<li><a href="https://www.slideshare.net/KarthikMurugesan2/airbnb-zipline-airbnbs-machine-learning-data-management-platform" target="_blank" rel="noopener">Zipline_0</a></li>
<li><a href="https://www.slideshare.net/databricks/ziplineairbnbs-declarative-feature-engineering-framework" target="_blank" rel="noopener">Zipline_1</a></li>
</ul></li>
</ul>
<h2><span id="まとめ">まとめ</span></h2>
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a>
<ul>
<li>割とよくまとまっている。観点が参考になる。</li>
</ul></li>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a>
<ul>
<li>プロダクトは、 <a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for
ML</a> と重なっているが、考察が載っている。</li>
</ul></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a>
<ul>
<li>Feature Storeの要件を整理しようとしている</li>
</ul></li>
<li><a href="https://www.cognizant.com/whitepapers/accelerating-machine-learning-as-a-service-with-automated-feature-engineering-codex4971.pdf" target="_blank" rel="noopener">Accelerating
Machine Learning as a Service with Automated Feature Engineering</a>
<ul>
<li>Feature Storeの定義、ビジネスメリットまで言及されている。</li>
</ul></li>
<li><a href="https://qumulo.com/wp-content/uploads/2019/11/data-storage-architectures-for-machine-learning-and-artificial-intelligence.pdf" target="_blank" rel="noopener">Data
Storage Architectures for Machine Learning and Artificial
Intelligence</a>
<ul>
<li>ベンダリストが載っていて便利そう。発行が2019/11なので比較的最近。</li>
<li>AI/ML向けのストレージアーキテクチャを「2層型」、「1層型」で分けている。
<ul>
<li>2層型は性能層と容量層に別れる。また性能層は1層型として用いられることもある。</li>
<li>各層に用いられる、ベンダ製品を例示している。</li>
</ul></li>
<li>ベンダリスト
<ul>
<li>Dell EMC（Isilon、ECS）</li>
<li>Qumulo</li>
<li>WekaIO</li>
<li>Scality RING</li>
<li>DataDirect Networks</li>
<li>IBM（Spectrum Scale、COS）</li>
<li>Minio</li>
<li>Netapp</li>
<li>OpenIO</li>
<li>Pavilion Data Systems</li>
<li>Pure Storage AIRI</li>
<li>Quobyte</li>
<li>VAST Data</li>
</ul></li>
</ul></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="傾向">傾向</span></h2>
<ul>
<li>Google Big Query、Big
Table、Redisあたりを特徴量置き場として使っている例が見られた。</li>
</ul>
<h2><span id="featurestoreとして挙げられている特徴機能">Feature
Storeとして挙げられている特徴・機能</span></h2>
<h3><span id="主に-featuerstoreとしての特徴">主に、featuer
storeとしての特徴</span></h3>
<h4><span id="機能分析補助">機能・分析補助</span></h4>
<ul>
<li>オンライン・オフライン統合（一貫性の実現、共通API）
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a>
<ul>
<li>Hopsworksではオンライン用にMySQL、オフライン用にHiveを利用</li>
<li>また一方でHudiにも対応</li>
</ul></li>
</ul></li>
<li>バージョンニング、point-in-time
correctness（特定のタイミングのレコードに対するラベルの更新）、タイムトラベル
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>ストレージレイヤ（実体の保存方法）
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
</ul></li>
<li>自動特徴量分析、ドキュメンテーション、特徴量のテスト
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a>
<ul>
<li>HopsworksではDeequを使った特徴量のユニットテストが可能</li>
</ul></li>
</ul></li>
<li>マルチテナンシ（ネームスペース、リソース)
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
</ul></li>
<li>読み書きAPI
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
</ul></li>
<li>アクセス管理
<ul>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>クエリプランナ（複数の特徴量グループの自動結合）
<ul>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>必要な特徴量だけ選んでデータセットを定義（DBMSでいうビュー）
<ul>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
</ul>
<h4><span id="計算">計算</span></h4>
<ul>
<li>遅延評価（必要なタイイングでの計算実行）
<ul>
<li><a href="https://medium.com/@changshe/rethinking-feature-stores-74963c2596f0" target="_blank" rel="noopener">Rethinking
Feature Stores</a></li>
</ul></li>
<li>自動再計算（auto backfill）
<ul>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
</ul>
<h4><span id="性能">性能</span></h4>
<ul>
<li>オンラインFeature Storeとしての良いレスポンス、スケーラビリティ
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>オフラインデータストアの性能（スケーラビリティ）
<ul>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>特徴量サービングの分散化・非中央集権化（サービングのコピー）
<ul>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a></li>
</ul></li>
</ul>
<h4><span id="連係">連係</span></h4>
<ul>
<li>特徴量エンジニアリング手段との連係
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
</ul></li>
<li>共通フォーマットでのデータのマテリアライズ、複数のフレームワークから読み書き可能なフォーマット
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
<li>メタデータ管理との統合、データカタログ、登録・探索、探索用のGUI
<ul>
<li><a href="http://featurestore.org/" target="_blank" rel="noopener">Feature Stores for ML</a></li>
<li><a href="https://towardsdatascience.com/feature-stores-components-of-a-data-science-factory-f0f1f73d39b8" target="_blank" rel="noopener">Feature
Stores Components of a Data Science Factory</a></li>
<li><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a></li>
<li><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a></li>
</ul></li>
</ul>
<h3><span id="rawデータストアを含めた特徴">rawデータストアを含めた特徴</span></h3>
<ul>
<li>画像、動画、音声など非テキストデータとテキストデータの統合的な取り扱い</li>
</ul>
<h3><span id="特徴量エンジニアリングの例">特徴量エンジニアリングの例</span></h3>
<p><a href="https://www.logicalclocks.com/blog/feature-store-the-missing-data-layer-in-ml-pipelines" target="_blank" rel="noopener">Hopsworks
Feature Store The missing data layer in ML pipelines?</a>
に一例が載っていたのでついでに転記。</p>
<ul>
<li>Converting categorical data into numeric data;</li>
<li>Normalizing data (to alleviate ill-conditioned optimization when
features originate from different distributions);</li>
<li>One-hot-encoding/binarization;</li>
<li>Feature binning (e.g., convert continuous features into
discrete);</li>
<li>Feature hashing (e.g., to reduce the memory footprint of
one-hot-encoded features);</li>
<li>Computing polynomial features;</li>
<li>Representation learning (e.g., extract features using clustering,
embeddings, or generative models);</li>
<li>Computing aggregate features (e.g., count, min, max, stdev).</li>
</ul>
<h3><span id="featurestoreにおける画像の取扱は">feature
storeにおける画像の取扱は？</span></h3>
<p>feature
storeのレベルになると行列化されているので、画像を特別なものとして扱わない？
rawデータストア上では画像は画像として扱う。</p>
<h2><span id="feastにおけるデータフロー概要">Feastにおけるデータフロー概要</span></h2>
<p>※Feastから幾つか図を引用。</p>
<p><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644" target="_blank" rel="noopener">Feast
Bridging ML Models and Data</a> に載っていたイメージ。</p>
<figure>
<img src="/memo-blog/images/IhhINZxPsyQx25yZ-F676F.png" alt="Feastのデータフローから引用">
<figcaption aria-hidden="true">Feastのデータフローから引用</figcaption>
</figure>
<p>データオーナ側はストリームデータ（Kafka）、DWH（BigQuery）、File（BigQuery）が書かれている。
また真ん中にはApache
Beamが書かれており、ストリームETLを経ながらデータがサービングシステムに渡されている。
データは基本的にはストリームとして扱うようだ。</p>
<p>また特徴量を取得するときは以下のようにする。</p>
<figure>
<img src="/memo-blog/images/IhhINZxPsyQx25yZ-A94FA.png" alt="特徴量の取得">
<figcaption aria-hidden="true">特徴量の取得</figcaption>
</figure>
<h2><span id="hopsworksにおけるfeaturestore">hopsworksにおけるfeature
store</span></h2>
<p>※Hopsworksから幾つか図を引用。</p>
<p><a href="https://hopsworks.readthedocs.io/en/1.1/featurestore/featurestore.html" target="_blank" rel="noopener">Hopsworksの公式ドキュメントのFeature
Store</a> に掲載されていたイメージは以下の通り。
Rawデータストアとは異なる位置づけ。</p>
<figure>
<img src="/memo-blog/images/BSBzSj1E5pwx3uqN-C9B87.png" alt="hopsworksでのfeature storeの位置づけ">
<figcaption aria-hidden="true">hopsworksでのfeature
storeの位置づけ</figcaption>
</figure>
<p>Feastでも言われているが、データエンジニアとデータサイエンティストの間にあるもの、とされている。</p>
<p>データストアする部分の全体アーキテクチャ。</p>
<figure>
<img src="/memo-blog/images/BSBzSj1E5pwx3uqN-EA1F7.png" alt="feature storeのアーキテクチャ">
<figcaption aria-hidden="true">feature
storeのアーキテクチャ</figcaption>
</figure>
<figure>
<img src="/memo-blog/images/BSBzSj1E5pwx3uqN-86B00.png" alt="feature storeのレイヤ構成">
<figcaption aria-hidden="true">feature storeのレイヤ構成</figcaption>
</figure>
<p>複数のコンポーネントを組み合わせて、ひとつのfeature
storeを構成しているようである。</p>
<h2><span id="ストレージ製品の動向">ストレージ製品の動向</span></h2>
<h3><span id="netapp">Netapp</span></h3>
<p><a href="https://www.netapp.com/us/solutions/applications/ai-deep-learning.aspx" target="_blank" rel="noopener">Accelerated
AI and deep learning pipelines across edge, core, and cloud</a>
では、</p>
<ul>
<li>Create a smooth, secure flow of data for your AI workloads.</li>
<li>Unify AI compute and data silos across sites and regions.​</li>
<li>Your data, always available: right place, right time.</li>
</ul>
<p>が挙げられている。
また、クラウド・オンプレ、エッジ・センタを統合する、というのが重要なアピールポイントに見えた。
詳しくは、 <a href="https://www.netapp.com/us/media/wp-7271.pdf" target="_blank" rel="noopener">Edge to
Core to Cloud Architecture for AI</a> を読めばわかりそう。</p>
<h3><span id="dell-emc">Dell EMC</span></h3>
<p>単独の技術というより、コンピューティングの工夫を含めてのソリューションのようにみえる。
<a href="https://www.dellemc.com/resources/en-us/asset/analyst-reports/products/storage/h17841_ar_enterprise_machine_and_deep_learning_with_intelligent_storage.pdf" target="_blank" rel="noopener">ENTERPRISE
MACHINE &amp; DEEP LEARNING WITH INTELLIGENT STORAGE</a>
に思想が書いてありそう。まだ読んでいない。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/" data-id="cm0jr5ktq017c18qb9kw8hyox" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Hudi" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/03/25/Hudi/">Hudi</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/03/25/Hudi/">
            <time datetime="2020-03-25T14:40:12.000Z" itemprop="datePublished">2020-03-25</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Hudi/">Hudi</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Apache-Hudi/">Apache Hudi</a>, <a class="tag-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#公式ドキュメント" id="toc-公式ドキュメント">公式ドキュメント</a></li>
<li><a href="#クイックスタートから確認version-0.5.2前提" id="toc-クイックスタートから確認version-0.5.2前提">クイックスタートから確認（version
0.5.2前提）</a>
<ul>
<li><a href="#org.apache.hudi.defaultsourcecreaterelation書き込み" id="toc-org.apache.hudi.defaultsourcecreaterelation書き込み">org.apache.hudi.DefaultSource#createRelation（書き込み）</a></li>
<li><a href="#org.apache.hudi.defaultsourcecreaterelation読み込み" id="toc-org.apache.hudi.defaultsourcecreaterelation読み込み">org.apache.hudi.DefaultSource#createRelation（読み込み）</a></li>
<li><a href="#incrementalrelation" id="toc-incrementalrelation">IncrementalRelation</a></li>
</ul></li>
</ul></li>
<li><a href="#hudiへの書き込み" id="toc-hudiへの書き込み">Hudiへの書き込み</a>
<ul>
<li><a href="#オペレーション種類" id="toc-オペレーション種類">オペレーション種類</a></li>
<li><a href="#deltastreamer" id="toc-deltastreamer">DeltaStreamer</a>
<ul>
<li><a href="#動作確認" id="toc-動作確認">動作確認</a></li>
<li><a href="#実装確認" id="toc-実装確認">実装確認</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><p><a href="https://hudi.apache.org/" target="_blank" rel="noopener">公式ドキュメント</a></p></li>
<li><p><a href="https://hudi.apache.org/docs/quick-start-guide.html" target="_blank" rel="noopener">Quick Start
Guide</a></p></li>
<li><p><a href="https://hudi.apache.org/docs/writing_data.html" target="_blank" rel="noopener">Writing
Hudi Tables</a></p></li>
<li><p><a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a></p></li>
<li><p><a href="https://github.com/apurvam/streams-prototyping" target="_blank" rel="noopener">apurvam
streams-prototyping</a></p></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="公式ドキュメント">公式ドキュメント</span></h2>
<p>載っている特徴は、以下の通り。</p>
<ul>
<li>Upsert support with fast, pluggable indexing.</li>
<li>Atomically publish data with rollback support.</li>
<li>Snapshot isolation between writer &amp; queries.</li>
<li>Savepoints for data recovery.</li>
<li>Manages file sizes, layout using statistics.</li>
<li>Async compaction of row &amp; columnar data.</li>
<li>Timeline metadata to track lineage.</li>
</ul>
<h2><span id="クイックスタートから確認version052前提">クイックスタートから確認（version
0.5.2前提）</span></h2>
<p><a href="https://hudi.apache.org/docs/quick-start-guide.html" target="_blank" rel="noopener">Quick
Start Guide</a> を参考に進める。</p>
<p>公式ドキュメントではSpark2.4.4を利用しているが、ここでは2.4.5を利用する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> SPARK_HOME=/opt/spark/default</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-shell \</span></span><br><span class="line">  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.2-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \</span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'</span><br></pre></td></tr></table></figure>
<p>必要なライブラリをインポート</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">QuickstartUtils</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceReadOptions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.<span class="type">DataSourceWriteOptions</span>._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.hudi.config.<span class="type">HoodieWriteConfig</span>._</span><br><span class="line">scala&gt; </span><br><span class="line">scala&gt; <span class="keyword">val</span> tableName = <span class="string">"hudi_trips_cow"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> basePath = <span class="string">"file:///tmp/hudi_trips_cow"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> dataGen = <span class="keyword">new</span> <span class="type">DataGenerator</span></span><br></pre></td></tr></table></figure>
<p>ダミーデータには
<code>org.apache.hudi.QuickstartUtils.DataGenerator</code>
クラスを利用する。 以下の例では、
<code>org.apache.hudi.QuickstartUtils.DataGenerator#generateInserts</code>
メソッドを利用しデータを生成するが、 どういうレコードが生成されるかは、
<code>org.apache.hudi.QuickstartUtils.DataGenerator#generateRandomValue</code>
メソッドあたりを見るとわかる。</p>
<p>ダミーデータを生成し、Spark DataFrameに変換。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> inserts = convertToStringList(dataGen.generateInserts(<span class="number">10</span>))</span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(inserts, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>中身は以下。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show</span><br><span class="line">+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">|          begin_lat|          begin_lon|    driver|            end_lat|            end_lon|              fare|       partitionpath|    rider| ts|                uuid|</span><br><span class="line">+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">| <span class="number">0.4726905879569653</span>|<span class="number">0.46157858450465483</span>|driver<span class="number">-213</span>|  <span class="number">0.754803407008858</span>| <span class="number">0.9671159942018241</span>|<span class="number">34.158284716382845</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">28432</span>dec<span class="number">-53</span>eb<span class="number">-402.</span>..|</span><br><span class="line">| <span class="number">0.6100070562136587</span>| <span class="number">0.8779402295427752</span>|driver<span class="number">-213</span>| <span class="number">0.3407870505929602</span>| <span class="number">0.5030798142293655</span>|  <span class="number">43.4923811219014</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">1</span>bd3905e-a6c4<span class="number">-404.</span>..|</span><br><span class="line">| <span class="number">0.5731835407930634</span>| <span class="number">0.4923479652912024</span>|driver<span class="number">-213</span>|<span class="number">0.08988581780930216</span>|<span class="number">0.42520899698713666</span>| <span class="number">64.27696295884016</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|c9cc8f4b-acee<span class="number">-413.</span>..|</span><br><span class="line">|<span class="number">0.21624150367601136</span>|<span class="number">0.14285051259466197</span>|driver<span class="number">-213</span>| <span class="number">0.5890949624813784</span>| <span class="number">0.0966823831927115</span>| <span class="number">93.56018115236618</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">4</span>be1c199<span class="number">-86</span>dc<span class="number">-489.</span>..|</span><br><span class="line">|   <span class="number">0.40613510977307</span>| <span class="number">0.5644092139040959</span>|driver<span class="number">-213</span>|  <span class="number">0.798706304941517</span>|<span class="number">0.02698359227182834</span>|<span class="number">17.851135255091155</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">83</span>f4d3df<span class="number">-46</span>c1<span class="number">-48</span>a...|</span><br><span class="line">| <span class="number">0.8742041526408587</span>| <span class="number">0.7528268153249502</span>|driver<span class="number">-213</span>| <span class="number">0.9197827128888302</span>|  <span class="number">0.362464770874404</span>|<span class="number">19.179139106643607</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|cb8b392d-c9d0<span class="number">-445.</span>..|</span><br><span class="line">| <span class="number">0.1856488085068272</span>| <span class="number">0.9694586417848392</span>|driver<span class="number">-213</span>|<span class="number">0.38186367037201974</span>|<span class="number">0.25252652214479043</span>| <span class="number">33.92216483948643</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">66</span>aaf87d<span class="number">-4786</span><span class="number">-4</span>d0...|</span><br><span class="line">| <span class="number">0.0750588760043035</span>|<span class="number">0.03844104444445928</span>|driver<span class="number">-213</span>|<span class="number">0.04376353354538354</span>| <span class="number">0.6346040067610669</span>| <span class="number">66.62084366450246</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|c5a335f5-c57f<span class="number">-4</span>f5...|</span><br><span class="line">|  <span class="number">0.651058505660742</span>| <span class="number">0.8192868687714224</span>|driver<span class="number">-213</span>|<span class="number">0.20714896002914462</span>|<span class="number">0.06224031095826987</span>| <span class="number">41.06290929046368</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">53026</span>eda<span class="number">-28</span>c4<span class="number">-4</span>d8...|</span><br><span class="line">|<span class="number">0.11488393157088261</span>| <span class="number">0.6273212202489661</span>|driver<span class="number">-213</span>| <span class="number">0.7454678537511295</span>| <span class="number">0.3954939864908973</span>| <span class="number">27.79478688582596</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|cd42df54<span class="number">-5215</span><span class="number">-402.</span>..|</span><br><span class="line">+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">         options(getQuickstartWriteConfigs).</span><br><span class="line">         option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"ts"</span>).</span><br><span class="line">         option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"uuid"</span>).</span><br><span class="line">         option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">         option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">         mode(<span class="type">Overwrite</span>).</span><br><span class="line">         save(basePath)</span><br></pre></td></tr></table></figure>
<p>なお、生成されたファイルは以下の通り。
<code>PARTITIONPATH_FIELD_OPT_KEY</code>
で指定したカラムをパーティションキーとして用いていることがわかる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -R /tmp/hudi_trips_cow/</span></span><br><span class="line">/tmp/hudi_trips_cow/:</span><br><span class="line">americas  asia</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas:</span><br><span class="line">brazil  united_states</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/brazil:</span><br><span class="line">sao_paulo</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/brazil/sao_paulo:</span><br><span class="line">ae28c85a-38f0-487f-a42d-3a0babc9d321-0_0-21-25_20200329002247.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/united_states:</span><br><span class="line">san_francisco</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/americas/united_states/san_francisco:</span><br><span class="line">849db286-1cbe-4a1f-b544-9939893e99f8-0_1-21-26_20200329002247.parquet</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/asia:</span><br><span class="line">india</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/asia/india:</span><br><span class="line">chennai</span><br><span class="line"></span><br><span class="line">/tmp/hudi_trips_cow/asia/india/chennai:</span><br><span class="line">2ebfbab0-4f8f-42db-b79e-1c0cbcc3cf39-0_2-21-27_20200329002247.parquet</span><br></pre></td></tr></table></figure>
<p>保存したデータを読み出してみる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsSnapshotDF = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*/*/*"</span>)</span><br><span class="line">scala&gt; tripsSnapshotDF.createOrReplaceTempView(<span class="string">"hudi_trips_snapshot"</span>)</span><br></pre></td></tr></table></figure>
<p>中身は以下の通り。
元データに対し、Hudiのカラムが追加されていることがわかる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; tripsSnapshotDF.show</span><br><span class="line">+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|          begin_lat|          begin_lon|    driver|            end_lat|            end_lon|              fare|       partitionpath|    rider| ts|                uuid|</span><br><span class="line">+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_1|<span class="number">7695</span>c291<span class="number">-8530</span><span class="number">-473.</span>..|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...|<span class="number">0.21624150367601136</span>|<span class="number">0.14285051259466197</span>|driver<span class="number">-213</span>| <span class="number">0.5890949624813784</span>| <span class="number">0.0966823831927115</span>| <span class="number">93.56018115236618</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">7695</span>c291<span class="number">-8530</span><span class="number">-473.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_3|<span class="number">2</span>f06fcd2<span class="number">-8296</span><span class="number">-423.</span>..|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...| <span class="number">0.5731835407930634</span>| <span class="number">0.4923479652912024</span>|driver<span class="number">-213</span>|<span class="number">0.08988581780930216</span>|<span class="number">0.42520899698713666</span>| <span class="number">64.27696295884016</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">2</span>f06fcd2<span class="number">-8296</span><span class="number">-423.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_5|<span class="number">6</span>ebc4028<span class="number">-9</span>aae<span class="number">-420.</span>..|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...| <span class="number">0.8742041526408587</span>| <span class="number">0.7528268153249502</span>|driver<span class="number">-213</span>| <span class="number">0.9197827128888302</span>|  <span class="number">0.362464770874404</span>|<span class="number">19.179139106643607</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">6</span>ebc4028<span class="number">-9</span>aae<span class="number">-420.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_6|<span class="number">8</span>bf60390-ad41<span class="number">-4</span>b0...|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...|<span class="number">0.11488393157088261</span>| <span class="number">0.6273212202489661</span>|driver<span class="number">-213</span>| <span class="number">0.7454678537511295</span>| <span class="number">0.3954939864908973</span>| <span class="number">27.79478688582596</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">8</span>bf60390-ad41<span class="number">-4</span>b0...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_1_7|<span class="number">762e8</span>cb2<span class="number">-8806</span><span class="number">-47</span>d...|  americas/united_s...|<span class="number">849</span>db286<span class="number">-1</span>cbe<span class="number">-4</span>a1...| <span class="number">0.1856488085068272</span>| <span class="number">0.9694586417848392</span>|driver<span class="number">-213</span>|<span class="number">0.38186367037201974</span>|<span class="number">0.25252652214479043</span>| <span class="number">33.92216483948643</span>|americas/united_s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">762e8</span>cb2<span class="number">-8806</span><span class="number">-47</span>d...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_0_8|<span class="number">28622337</span>-d76b<span class="number">-442.</span>..|  americas/brazil/s...|ae28c85a<span class="number">-38</span>f0<span class="number">-487.</span>..| <span class="number">0.6100070562136587</span>| <span class="number">0.8779402295427752</span>|driver<span class="number">-213</span>| <span class="number">0.3407870505929602</span>| <span class="number">0.5030798142293655</span>|  <span class="number">43.4923811219014</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">28622337</span>-d76b<span class="number">-442.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_0_9|<span class="number">33</span>aec15d<span class="number">-356</span>f<span class="number">-475.</span>..|  americas/brazil/s...|ae28c85a<span class="number">-38</span>f0<span class="number">-487.</span>..| <span class="number">0.0750588760043035</span>|<span class="number">0.03844104444445928</span>|driver<span class="number">-213</span>|<span class="number">0.04376353354538354</span>| <span class="number">0.6346040067610669</span>| <span class="number">66.62084366450246</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">33</span>aec15d<span class="number">-356</span>f<span class="number">-475.</span>..|</span><br><span class="line">|     <span class="number">20200329002247</span>| <span class="number">20200329002247</span>_0_10|<span class="number">2</span>d71c9a3<span class="number">-26</span>a3<span class="number">-40</span>b...|  americas/brazil/s...|ae28c85a<span class="number">-38</span>f0<span class="number">-487.</span>..| <span class="number">0.4726905879569653</span>|<span class="number">0.46157858450465483</span>|driver<span class="number">-213</span>|  <span class="number">0.754803407008858</span>| <span class="number">0.9671159942018241</span>|<span class="number">34.158284716382845</span>|americas/brazil/s...|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">2</span>d71c9a3<span class="number">-26</span>a3<span class="number">-40</span>b...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_2_2|a997a8f0<span class="number">-4</span>ab6<span class="number">-4</span>d5...|    asia/india/chennai|<span class="number">2</span>ebfbab0<span class="number">-4</span>f8f<span class="number">-42</span>d...|   <span class="number">0.40613510977307</span>| <span class="number">0.5644092139040959</span>|driver<span class="number">-213</span>|  <span class="number">0.798706304941517</span>|<span class="number">0.02698359227182834</span>|<span class="number">17.851135255091155</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|a997a8f0<span class="number">-4</span>ab6<span class="number">-4</span>d5...|</span><br><span class="line">|     <span class="number">20200329002247</span>|  <span class="number">20200329002247</span>_2_4|<span class="number">271</span>de424-a0f8<span class="number">-427.</span>..|    asia/india/chennai|<span class="number">2</span>ebfbab0<span class="number">-4</span>f8f<span class="number">-42</span>d...|  <span class="number">0.651058505660742</span>| <span class="number">0.8192868687714224</span>|driver<span class="number">-213</span>|<span class="number">0.20714896002914462</span>|<span class="number">0.06224031095826987</span>| <span class="number">41.06290929046368</span>|  asia/india/chennai|rider<span class="number">-213</span>|<span class="number">0.0</span>|<span class="number">271</span>de424-a0f8<span class="number">-427.</span>..|</span><br><span class="line">+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+</span><br></pre></td></tr></table></figure>
<p>上記の通り、SparkのData Source機能を利用している。
中では、<code>org.apache.hudi.DefaultSource#createRelation</code>
メソッドが用いられる。</p>
<p>つづいて、更新を試す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> updates = convertToStringList(dataGen.generateUpdates(<span class="number">10</span>))</span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.json(spark.sparkContext.parallelize(updates, <span class="number">2</span>))</span><br><span class="line">scala&gt; df.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">         options(getQuickstartWriteConfigs).</span><br><span class="line">         option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"ts"</span>).</span><br><span class="line">         option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"uuid"</span>).</span><br><span class="line">         option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">         option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">         mode(<span class="type">Append</span>).</span><br><span class="line">         save(basePath)</span><br></pre></td></tr></table></figure>
<p>もう一度、DataFrameとして読み出すと、レコードが追加されていることを確かめられる。（ここでは省略）
この後の、 <code>incremental</code>
クエリタイプの実験のため、上記の更新を幾度か実行しておく。</p>
<p>つづいて、 <code>incremental</code> クエリタイプで読み出す。</p>
<p>一度読み出し、最初のコミット時刻を取り出す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*/*/*"</span>).</span><br><span class="line">         createOrReplaceTempView(<span class="string">"hudi_trips_snapshot"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> commits = spark.sql(<span class="string">"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime"</span>).map(k =&gt; k.getString(<span class="number">0</span>)).take(<span class="number">50</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> beginTime = commits(commits.length - <span class="number">2</span>) <span class="comment">// commit time we are interested in</span></span><br></pre></td></tr></table></figure>
<p>今回は、初回書き込みに加えて2回更新したので、 <code>commits</code>
は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; commits</span><br><span class="line">res12: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(<span class="number">20200330002239</span>, <span class="number">20200330002354</span>, <span class="number">20200330003142</span>)</span><br></pre></td></tr></table></figure>
<p>また、今回「読み込みの最初」とするコミットは、以下の通り。
つまり、2回目の更新時。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; beginTime</span><br><span class="line">res13: <span class="type">String</span> = <span class="number">20200330002354</span></span><br></pre></td></tr></table></figure>
<p>では、 <code>incremental</code> クエリタイプで読み出す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsIncrementalDF = spark.read.format(<span class="string">"hudi"</span>).</span><br><span class="line">         option(<span class="type">QUERY_TYPE_OPT_KEY</span>, <span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>).</span><br><span class="line">         option(<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>, beginTime).</span><br><span class="line">         load(basePath)</span><br><span class="line">scala&gt; tripsIncrementalDF.createOrReplaceTempView(<span class="string">"hudi_trips_incremental"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br></pre></td></tr></table></figure>
<p>結果は以下のようなイメージ。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|_hoodie_commit_time|              fare|           begin_lon|          begin_lat| ts|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">87.68271062363665</span>|  <span class="number">0.9273857651526887</span>| <span class="number">0.1620033132033215</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">40.44073446276323</span>|<span class="number">9.842943407509797E-4</span>|<span class="number">0.47631824594751015</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">45.39370966816483</span>|    <span class="number">0.65888271115305</span>| <span class="number">0.8535610661589833</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">47.332186591003044</span>|  <span class="number">0.8006023508896579</span>| <span class="number">0.9025851737325563</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">93.34457064050349</span>|  <span class="number">0.6331319396951335</span>| <span class="number">0.5375953886834237</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">31.065524210209226</span>|  <span class="number">0.7608842984578864</span>| <span class="number">0.9514417909802292</span>|<span class="number">0.0</span>|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br></pre></td></tr></table></figure>
<p>なお、ここでbeginTimeを1遡ることにすると…。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> beginTime = commits(commits.length - <span class="number">3</span>) <span class="comment">// commit time we are interested in</span></span><br></pre></td></tr></table></figure>
<p>以下のように、2回目のコミットも含まれるようになる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|_hoodie_commit_time|              fare|           begin_lon|          begin_lat| ts|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">87.68271062363665</span>|  <span class="number">0.9273857651526887</span>| <span class="number">0.1620033132033215</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">40.44073446276323</span>|<span class="number">9.842943407509797E-4</span>|<span class="number">0.47631824594751015</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">45.39370966816483</span>|    <span class="number">0.65888271115305</span>| <span class="number">0.8535610661589833</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">47.332186591003044</span>|  <span class="number">0.8006023508896579</span>| <span class="number">0.9025851737325563</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">39.09858962414072</span>| <span class="number">0.08151154133724581</span>|<span class="number">0.21729959707372848</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>| <span class="number">93.34457064050349</span>|  <span class="number">0.6331319396951335</span>| <span class="number">0.5375953886834237</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">80.87869643345753</span>|  <span class="number">0.0748253615757305</span>| <span class="number">0.9787639413761751</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330003142</span>|<span class="number">31.065524210209226</span>|  <span class="number">0.7608842984578864</span>| <span class="number">0.9514417909802292</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>|<span class="number">21.602186045036387</span>|   <span class="number">0.772134626462835</span>| <span class="number">0.3291184473506418</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">43.41497201940956</span>|  <span class="number">0.6226833057042072</span>| <span class="number">0.5501675314928346</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">35.71294622426758</span>|  <span class="number">0.6696123015022845</span>| <span class="number">0.7318572150654761</span>|<span class="number">0.0</span>|</span><br><span class="line">|     <span class="number">20200330002354</span>| <span class="number">67.30906296028802</span>| <span class="number">0.16768228612130764</span>|<span class="number">0.29666655980198253</span>|<span class="number">0.0</span>|</span><br><span class="line">+-------------------+------------------+--------------------+-------------------+---+</span><br></pre></td></tr></table></figure>
<h3><span id="orgapachehudidefaultsourcecreaterelation書き込み">org.apache.hudi.DefaultSource#createRelation（書き込み）</span></h3>
<p>クイックスタートで、例えば更新などする際の動作を確認する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.write.format(<span class="string">"hudi"</span>).</span><br><span class="line">     |   options(getQuickstartWriteConfigs).</span><br><span class="line">     |   option(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>, <span class="string">"ts"</span>).</span><br><span class="line">     |   option(<span class="type">RECORDKEY_FIELD_OPT_KEY</span>, <span class="string">"uuid"</span>).</span><br><span class="line">     |   option(<span class="type">PARTITIONPATH_FIELD_OPT_KEY</span>, <span class="string">"partitionpath"</span>).</span><br><span class="line">     |   option(<span class="type">TABLE_NAME</span>, tableName).</span><br><span class="line">     |   mode(<span class="type">Append</span>).</span><br><span class="line">     |   save(basePath)</span><br></pre></td></tr></table></figure>
<p>のような例を実行する際、内部的には
<code>org.apache.hudi.DefaultSource#createRelation</code>
が呼ばれる。</p>
<p>org/apache/hudi/DefaultSource.scala:85</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">                            mode: <span class="type">SaveMode</span>,</span><br><span class="line">                            optParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">                            df: <span class="type">DataFrame</span>): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> parameters = <span class="type">HoodieSparkSqlWriter</span>.parametersWithWriteDefaults(optParams)</span><br><span class="line">  <span class="type">HoodieSparkSqlWriter</span>.write(sqlContext, mode, parameters, df)</span><br><span class="line">  createRelation(sqlContext, parameters, df.schema)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記メソッド内では、
<code>org.apache.hudi.HoodieSparkSqlWriter$#write</code>
メソッドが呼ばれており、 これが書き込みの実態である。 なお、その下の
<code>org.apache.hudi.DefaultSource#createRelation</code>
は、読み込み時に呼ばれるものと同一。</p>
<p>ここでは <code>org.apache.hudi.HoodieSparkSqlWriter#write</code>
メソッドを確認する。
当該メソッドの冒頭では、オペレーションの判定などいくつか前処理が行われた後、
以下の箇所から実際に書き出す処理が定義されている。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:85</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> (writeStatuses, writeClient: <span class="type">HoodieWriteClient</span>[<span class="type">HoodieRecordPayload</span>[<span class="type">Nothing</span>]]) =</span><br><span class="line">  <span class="keyword">if</span> (!operation.equalsIgnoreCase(<span class="type">DELETE_OPERATION_OPT_VAL</span>)) &#123;</span><br><span class="line">  <span class="comment">// register classes &amp; schemas</span></span><br><span class="line">  <span class="keyword">val</span> structName = <span class="string">s"<span class="subst">$&#123;tblName.get&#125;</span>_record"</span></span><br><span class="line">  <span class="keyword">val</span> nameSpace = <span class="string">s"hoodie.<span class="subst">$&#123;tblName.get&#125;</span>"</span></span><br><span class="line">  sparkContext.getConf.registerKryoClasses(</span><br><span class="line">    <span class="type">Array</span>(classOf[org.apache.avro.generic.<span class="type">GenericData</span>],</span><br><span class="line">      classOf[org.apache.avro.<span class="type">Schema</span>]))</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="type">AvroConversionUtils</span>.convertStructTypeToAvroSchema(df.schema, structName, nameSpace)</span><br><span class="line">  sparkContext.getConf.registerAvroSchemas(schema)</span><br><span class="line"></span><br><span class="line">  (snip)</span><br></pre></td></tr></table></figure>
<p>まず <code>delete</code>
オペレーションかどうかで処理が別れるが、上記の例では <code>upsert</code>
オペレーションなので一旦そのまま読み進める。
ネームスペース（データベースやテーブル？）を取得した後、SparkのStructTypeで保持されたスキーマ情報を、AvroのSchemaに変換する。
変換されたスキーマをSparkで登録する。</p>
<p>つづいて、DataFrameをRDDに変換する。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:97</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Convert to RDD[HoodieRecord]</span></span><br><span class="line"><span class="keyword">val</span> keyGenerator = <span class="type">DataSourceUtils</span>.createKeyGenerator(toProperties(parameters))</span><br><span class="line"><span class="keyword">val</span> genericRecords: <span class="type">RDD</span>[<span class="type">GenericRecord</span>] = <span class="type">AvroConversionUtils</span>.createRdd(df, structName, nameSpace)</span><br><span class="line"><span class="keyword">val</span> hoodieAllIncomingRecords = genericRecords.map(gr =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> orderingVal = <span class="type">DataSourceUtils</span>.getNestedFieldValAsString(</span><br><span class="line">    gr, parameters(<span class="type">PRECOMBINE_FIELD_OPT_KEY</span>), <span class="literal">false</span>).asInstanceOf[<span class="type">Comparable</span>[_]]</span><br><span class="line">  <span class="type">DataSourceUtils</span>.createHoodieRecord(gr,</span><br><span class="line">    orderingVal, keyGenerator.getKey(gr), parameters(<span class="type">PAYLOAD_CLASS_OPT_KEY</span>))</span><br><span class="line">&#125;).toJavaRDD()</span><br></pre></td></tr></table></figure>
<p>RDDに一度変換した後、mapメソッドで加工する。</p>
<p>まず、 <code>genericRecords</code>
の内容は以下のようなものが含まれる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;GenericRecord[1]@27822&#125; </span><br><span class="line"> 0 = &#123;GenericData$Record@27827&#125; &quot;&#123;&quot;begin_lat&quot;: 0.09632451474505643, &quot;begin_lon&quot;: 0.8989273848550128, &quot;driver&quot;: &quot;driver-164&quot;, &quot;end_lat&quot;: 0.6431885917325862, &quot;end_lon&quot;: 0.6664889106258252, &quot;fare&quot;: 86.865568091804, &quot;partitionpath&quot;: &quot;americas/brazil/sao_paulo&quot;, &quot;rider&quot;: &quot;rider-164&quot;, &quot;ts&quot;: 0.0, &quot;uuid&quot;: &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;&#125;&quot;</span><br><span class="line">  schema = &#123;Schema$RecordSchema@27835&#125; &quot;&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_trips_cow_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_trips_cow&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;begin_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;begin_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;driver&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;fare&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;rider&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;&quot;</span><br><span class="line">  values = &#123;Object[10]@27836&#125; </span><br><span class="line">   0 = &#123;Double@27838&#125; 0.09632451474505643</span><br><span class="line">   1 = &#123;Double@27839&#125; 0.8989273848550128</span><br><span class="line">   2 = &#123;Utf8@27840&#125; &quot;driver-164&quot;</span><br><span class="line">   3 = &#123;Double@27841&#125; 0.6431885917325862</span><br><span class="line">   4 = &#123;Double@27842&#125; 0.6664889106258252</span><br><span class="line">   5 = &#123;Double@27843&#125; 86.865568091804</span><br><span class="line">   6 = &#123;Utf8@27844&#125; &quot;americas/brazil/sao_paulo&quot;</span><br><span class="line">   7 = &#123;Utf8@27845&#125; &quot;rider-164&quot;</span><br><span class="line">   8 = &#123;Double@27846&#125; 0.0</span><br><span class="line">   9 = &#123;Utf8@27847&#125; &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;</span><br></pre></td></tr></table></figure>
<p>上記の通り、これは入ロクレコードそのものである。
その後、mapメソッドを使ってHudiで利用するキーを含む、Hudiのレコード形式に変換する。</p>
<p>変換された <code>hoodieAllIncomingRecords</code>
は以下のような内容になる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;Wrappers$SeqWrapper@27881&#125;  size = 1</span><br><span class="line"> 0 = &#123;HoodieRecord@27883&#125; &quot;HoodieRecord&#123;key=HoodieKey &#123; recordKey=5d49cfb5-0db4-4172-bff4-e581eb1f9783 partitionPath=americas/brazil/sao_paulo&#125;, currentLocation=&apos;null&apos;, newLocation=&apos;null&apos;&#125;&quot;</span><br><span class="line">  key = &#123;HoodieKey@27892&#125; &quot;HoodieKey &#123; recordKey=5d49cfb5-0db4-4172-bff4-e581eb1f9783 partitionPath=americas/brazil/sao_paulo&#125;&quot;</span><br><span class="line">   recordKey = &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;</span><br><span class="line">   partitionPath = &quot;americas/brazil/sao_paulo&quot;</span><br><span class="line">  data = &#123;OverwriteWithLatestAvroPayload@27893&#125; </span><br><span class="line">   recordBytes = &#123;byte[142]@27895&#125; </span><br><span class="line">   orderingVal = &quot;0.0&quot;</span><br><span class="line">  currentLocation = null</span><br><span class="line">  newLocation = null</span><br><span class="line">  sealed = false</span><br></pre></td></tr></table></figure>
<p>上記の例の通り、ペイロードは
<code>org.apache.hudi.common.model.OverwriteWithLatestAvroPayload</code>
で保持される。</p>
<p>その後、いくつかモードの確認が行われた後、もしテーブルがなければ
<code>org.apache.hudi.common.table.HoodieTableMetaClient#initTableType</code>
を用いて テーブルを初期化する。</p>
<p>その後、重複レコードを必要に応じて落とす。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:132</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> hoodieRecords =</span><br><span class="line">  <span class="keyword">if</span> (parameters(<span class="type">INSERT_DROP_DUPS_OPT_KEY</span>).toBoolean) &#123;</span><br><span class="line">    <span class="type">DataSourceUtils</span>.dropDuplicates(</span><br><span class="line">      jsc,</span><br><span class="line">      hoodieAllIncomingRecords,</span><br><span class="line">      mapAsJavaMap(parameters), client.getTimelineServer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    hoodieAllIncomingRecords</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>レコードが空かどうかを改めて確認しつつ、 最後に書き込み実施。
<code>org.apache.hudi.DataSourceUtils#doWriteOperation</code>
が実態である。</p>
<p>org/apache/hudi/HoodieSparkSqlWriter.scala:147</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val writeStatuses = DataSourceUtils.doWriteOperation(client, hoodieRecords, commitTime, operation)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.DataSourceUtils#doWriteOperation</code>
メソッドは以下の通り。</p>
<p>org/apache/hudi/DataSourceUtils.java:162</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JavaRDD&lt;WriteStatus&gt; <span class="title">doWriteOperation</span><span class="params">(HoodieWriteClient client, JavaRDD&lt;HoodieRecord&gt; hoodieRecords,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                    String commitTime, String operation)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) &#123;</span><br><span class="line">    <span class="keyword">return</span> client.bulkInsert(hoodieRecords, commitTime);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation.equals(DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())) &#123;</span><br><span class="line">    <span class="keyword">return</span> client.insert(hoodieRecords, commitTime);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// default is upsert</span></span><br><span class="line">    <span class="keyword">return</span> client.upsert(hoodieRecords, commitTime);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>今回の例だと、 <code>upseart</code> オペレーションなので
<code>org.apache.hudi.client.HoodieWriteClient#upsert</code>
メソッドが呼ばれる。 このメソッドは以下のとおりだが、ポイントは、
<code>org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal</code>
メソッドである。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;WriteStatus&gt; <span class="title">upsert</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, <span class="keyword">final</span> String commitTime)</span> </span>&#123;</span><br><span class="line">  HoodieTable&lt;T&gt; table = getTableAndInitCtx(OperationType.UPSERT);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// De-dupe/merge if needed</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords =</span><br><span class="line">        combineOnCondition(config.shouldCombineBeforeUpsert(), records, config.getUpsertShuffleParallelism());</span><br><span class="line"></span><br><span class="line">    Timer.Context indexTimer = metrics.getIndexCtx();</span><br><span class="line">    <span class="comment">// perform index loop up to get existing location of records</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; taggedRecords = getIndex().tagLocation(dedupedRecords, jsc, table);</span><br><span class="line">    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == <span class="keyword">null</span> ? <span class="number">0L</span> : indexTimer.stop()));</span><br><span class="line">    <span class="keyword">return</span> upsertRecordsInternal(taggedRecords, commitTime, table, <span class="keyword">true</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> HoodieUpsertException) &#123;</span><br><span class="line">      <span class="keyword">throw</span> (HoodieUpsertException) e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(<span class="string">"Failed to upsert for commit time "</span> + commitTime, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal</code>
メソッド内のポイントは、 以下の箇所。
<code>org.apache.spark.api.java.AbstractJavaRDDLike#mapPartitionsWithIndex</code>
メソッドで、upsertやinsertの処理を定義している。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:470</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;WriteStatus&gt; writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -&gt; &#123;</span><br><span class="line">  <span class="keyword">if</span> (isUpsert) &#123;</span><br><span class="line">    <span class="keyword">return</span> hoodieTable.handleUpsertPartition(commitTime, partition, recordItr, partitioner);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> hoodieTable.handleInsertPartition(commitTime, partition, recordItr, partitioner);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;, <span class="keyword">true</span>).flatMap(List::iterator);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime);</span><br></pre></td></tr></table></figure>
<p>ここでは、
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleUpsertPartition</code>
メソッドを確認してみる。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:253</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String commitTime, Integer partition, Iterator recordItr,</span><br><span class="line">    Partitioner partitioner) &#123;</span><br><span class="line">  UpsertPartitioner upsertPartitioner = (UpsertPartitioner) partitioner;</span><br><span class="line">  BucketInfo binfo = upsertPartitioner.getBucketInfo(partition);</span><br><span class="line">  BucketType btype = binfo.bucketType;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (btype.equals(BucketType.INSERT)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleInsert(commitTime, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (btype.equals(BucketType.UPDATE)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleUpdate(commitTime, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(<span class="string">"Unknown bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    String msg = <span class="string">"Error upserting bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition;</span><br><span class="line">    LOG.error(msg, t);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(msg, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>真ん中あたりに、INSERTかUPDATEかで条件分岐しているが、ここでは例としてINSERT側を確認する。
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsert</code>
メソッドがポイントとなる。
なお、当該メッソッドには同期的な実装と、非同期的な実装があるよう。
ここでは上記呼び出しに基づき、非同期的な実装の方を確認する。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:233</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String commitTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records</span></span><br><span class="line">  <span class="keyword">if</span> (!recordItr.hasNext()) &#123;</span><br><span class="line">    LOG.info(<span class="string">"Empty partition"</span>);</span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, commitTime, <span class="keyword">this</span>, idPfx);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>戻り値が、
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable</code>
クラスのインスタンスになっていることがわかる。 このイテレータは、
<code>org.apache.hudi.client.utils.LazyIterableIterator</code>
アブストラクトクラスを継承している。
<code>org.apache.hudi.client.utils.LazyIterableIterator</code>
では、nextメソッドが</p>
<p>org/apache/hudi/client/utils/LazyIterableIterator.java:116</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> O <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> computeNext();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(ex);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>のように定義されており、実態が
<code>org.apache.hudi.client.utils.LazyIterableIterator#computeNext</code>
であることがわかる。 当該メソッドは、
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#CopyOnWriteLazyInsertIterable</code>
クラスではオーバーライドされており、 以下のように定義されている。</p>
<p>org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java:93</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> List&lt;WriteStatus&gt; <span class="title">computeNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Executor service used for launching writer thread.</span></span><br><span class="line">  BoundedInMemoryExecutor&lt;HoodieRecord&lt;T&gt;, HoodieInsertValueGenResult&lt;HoodieRecord&gt;, List&lt;WriteStatus&gt;&gt; bufferedIteratorExecutor =</span><br><span class="line">      <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> Schema schema = <span class="keyword">new</span> Schema.Parser().parse(hoodieConfig.getSchema());</span><br><span class="line">    bufferedIteratorExecutor =</span><br><span class="line">        <span class="keyword">new</span> SparkBoundedInMemoryExecutor&lt;&gt;(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema));</span><br><span class="line">    <span class="keyword">final</span> List&lt;WriteStatus&gt; result = bufferedIteratorExecutor.execute();</span><br><span class="line">    <span class="keyword">assert</span> result != <span class="keyword">null</span> &amp;&amp; !result.isEmpty() &amp;&amp; !bufferedIteratorExecutor.isRemaining();</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieException(e);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> != bufferedIteratorExecutor) &#123;</span><br><span class="line">      bufferedIteratorExecutor.shutdownNow();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>どうやら内部でFutureパターンを利用し、非同期化して書き込みを行っているようだ。（これが筋よしなのかどうかは要議論。update、つまりマージも同様になっている。）
処理内容を知る上でポイントとなるのは、</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bufferedIteratorExecutor =</span><br><span class="line">    <span class="keyword">new</span> SparkBoundedInMemoryExecutor&lt;&gt;(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema));</span><br></pre></td></tr></table></figure>
<p>の箇所。
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#getInsertHandler</code>
あたり。 中で用いられている、
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.CopyOnWriteInsertHandler</code>
クラスがポイントとなる。
このクラスは、書き込みデータのキュー（要確認）からレコードを受け取って、処理していると考えられる。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">consumeOneRecord</span><span class="params">(HoodieInsertValueGenResult&lt;HoodieRecord&gt; payload)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> HoodieRecord insertPayload = payload.record;</span><br><span class="line">  <span class="comment">// lazily initialize the handle, for the first time</span></span><br><span class="line">  <span class="keyword">if</span> (handle == <span class="keyword">null</span>) &#123;</span><br><span class="line">    handle = <span class="keyword">new</span> HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, insertPayload.getPartitionPath(),</span><br><span class="line">        getNextFileId(idPrefix));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (handle.canWrite(payload.record)) &#123;</span><br><span class="line">    <span class="comment">// write the payload, if the handle has capacity</span></span><br><span class="line">    handle.write(insertPayload, payload.insertValue, payload.exception);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// handle is full.</span></span><br><span class="line">    statuses.add(handle.close());</span><br><span class="line">    <span class="comment">// Need to handle the rejected payload &amp; open new handle</span></span><br><span class="line">    handle = <span class="keyword">new</span> HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, insertPayload.getPartitionPath(),</span><br><span class="line">        getNextFileId(idPrefix));</span><br><span class="line">    handle.write(insertPayload, payload.insertValue, payload.exception); <span class="comment">// we should be able to write 1 payload.</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下の方にある <code>org.apache.hudi.io.HoodieCreateHandle</code>
クラスを用いているあたりがポイント。 そのwriteメソッドは以下の通り。
<code>org.apache.hudi.io.storage.HoodieStorageWriter#writeAvroWithMetadata</code>
を用いて書き出しているように見える。 （実際には
<code>org.apache.hudi.io.storage.HoodieParquetWriter</code> ）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(HoodieRecord record, Option&lt;IndexedRecord&gt; avroRecord)</span> </span>&#123;</span><br><span class="line">  Option recordMetadata = record.getData().getMetadata();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (avroRecord.isPresent()) &#123;</span><br><span class="line">      <span class="comment">// Convert GenericRecord to GenericRecord with hoodie commit metadata in schema</span></span><br><span class="line">      IndexedRecord recordWithMetadataInSchema = rewriteRecord((GenericRecord) avroRecord.get());</span><br><span class="line">      storageWriter.writeAvroWithMetadata(recordWithMetadataInSchema, record);</span><br><span class="line">      <span class="comment">// update the new location of record, so we know where to find it next</span></span><br><span class="line">      record.unseal();</span><br><span class="line">      record.setNewLocation(<span class="keyword">new</span> HoodieRecordLocation(instantTime, writeStatus.getFileId()));</span><br><span class="line">      record.seal();</span><br><span class="line">      recordsWritten++;</span><br><span class="line">      insertRecordsWritten++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      recordsDeleted++;</span><br><span class="line">    &#125;</span><br><span class="line">    writeStatus.markSuccess(record, recordMetadata);</span><br><span class="line">    <span class="comment">// deflate record payload after recording success. This will help users access payload as a</span></span><br><span class="line">    <span class="comment">// part of marking</span></span><br><span class="line">    <span class="comment">// record successful.</span></span><br><span class="line">    record.deflate();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    <span class="comment">// Not throwing exception from here, since we don't want to fail the entire job</span></span><br><span class="line">    <span class="comment">// for a single record</span></span><br><span class="line">    writeStatus.markFailure(record, t, recordMetadata);</span><br><span class="line">    LOG.error(<span class="string">"Error writing record "</span> + record, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.io.storage.HoodieParquetWriter#writeAvroWithMetadata</code>
メソッドは以下の通りである。 つまり、
<code>org.apache.parquet.hadoop.ParquetWriter#write</code>
を用いてParquet内に、Avroレコードを書き出していることがわかる。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">writeAvroWithMetadata</span><span class="params">(R avroRecord, HoodieRecord record)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  String seqId =</span><br><span class="line">      HoodieRecord.generateSequenceId(commitTime, TaskContext.getPartitionId(), recordIndex.getAndIncrement());</span><br><span class="line">  HoodieAvroUtils.addHoodieKeyToRecord((GenericRecord) avroRecord, record.getRecordKey(), record.getPartitionPath(),</span><br><span class="line">      file.getName());</span><br><span class="line">  HoodieAvroUtils.addCommitMetadataToRecord((GenericRecord) avroRecord, commitTime, seqId);</span><br><span class="line">  <span class="keyword">super</span>.write(avroRecord);</span><br><span class="line">  writeSupport.add(record.getRecordKey());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>今回のクイックスタートの例では、 <code>avroRecord</code>
には以下のような内容が含まれていた。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;GenericData$Record@18566&#125; &quot;&#123;&quot;_hoodie_commit_time&quot;: &quot;20200331002133&quot;, &quot;_hoodie_commit_seqno&quot;: &quot;20200331002133_0_44&quot;, &quot;_hoodie_record_key&quot;: &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;, &quot;_hoodie_partition_path&quot;: &quot;asia/india/chennai&quot;, &quot;_hoodie_file_name&quot;: &quot;317a54b0-70b8-4bdc-bfde-12ba4fde982b-0_0-207-301_20200331002133.parquet&quot;, &quot;begin_lat&quot;: 0.4789745387904072, &quot;begin_lon&quot;: 0.14781856144057215, &quot;driver&quot;: &quot;driver-022&quot;, &quot;end_lat&quot;: 0.10509642405359532, &quot;end_lon&quot;: 0.07682825311613706, &quot;fare&quot;: 30.429177017810616, &quot;partitionpath&quot;: &quot;asia/india/chennai&quot;, &quot;rider&quot;: &quot;rider-022&quot;, &quot;ts&quot;: 0.0, &quot;uuid&quot;: &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;&#125;&quot;</span><br><span class="line"> schema = &#123;Schema$RecordSchema@18582&#125; &quot;&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_trips_cow_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_trips_cow&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;_hoodie_commit_time&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_commit_seqno&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_record_key&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_partition_path&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_file_name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;begin_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;begin_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;driver&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;fare&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;rider&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;&quot;</span><br><span class="line"> values = &#123;Object[15]@18583&#125; </span><br><span class="line">  0 = &quot;20200331002133&quot;</span><br><span class="line">  1 = &quot;20200331002133_0_44&quot;</span><br><span class="line">  2 = &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;</span><br><span class="line">  3 = &quot;asia/india/chennai&quot;</span><br><span class="line">  4 = &quot;317a54b0-70b8-4bdc-bfde-12ba4fde982b-0_0-207-301_20200331002133.parquet&quot;</span><br><span class="line">  5 = &#123;Double@18596&#125; 0.4789745387904072</span><br><span class="line">  6 = &#123;Double@18597&#125; 0.14781856144057215</span><br><span class="line">  7 = &#123;Utf8@18598&#125; &quot;driver-022&quot;</span><br><span class="line">  8 = &#123;Double@18599&#125; 0.10509642405359532</span><br><span class="line">  9 = &#123;Double@18600&#125; 0.07682825311613706</span><br><span class="line">  10 = &#123;Double@18601&#125; 30.429177017810616</span><br><span class="line">  11 = &#123;Utf8@18602&#125; &quot;asia/india/chennai&quot;</span><br><span class="line">  12 = &#123;Utf8@18603&#125; &quot;rider-022&quot;</span><br><span class="line">  13 = &#123;Double@18604&#125; 0.0</span><br><span class="line">  14 = &#123;Utf8@18605&#125; &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;</span><br></pre></td></tr></table></figure>
<h3><span id="orgapachehudidefaultsourcecreaterelation読み込み">org.apache.hudi.DefaultSource#createRelation（読み込み）</span></h3>
<p>当該メソッドのポイントを確認する。</p>
<p><code>hoodie.datasource.query.type</code>
の種類によって返すRelationが変わる。</p>
<p>org/apache/hudi/DefaultSource.scala:60</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (parameters(<span class="type">QUERY_TYPE_OPT_KEY</span>).equals(<span class="type">QUERY_TYPE_SNAPSHOT_OPT_VAL</span>)) &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (parameters(<span class="type">QUERY_TYPE_OPT_KEY</span>).equals(<span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>)) &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">HoodieException</span>(<span class="string">"Invalid query type :"</span> + parameters(<span class="type">QUERY_TYPE_OPT_KEY</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、 <code>snapshot</code> 、もしくは
<code>incremental</code> クエリタイプである。 なお、以下の通り、
<code>MERGE_ON_READ</code> テーブルに対する <code>snapshot</code>
クエリタイプは利用できない。 もし使いたければ、SparkのData
Source機能ではなく、Hiveテーブルとして読み込むこと。</p>
<p>org/apache/hudi/DefaultSource.scala:69</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log.warn(<span class="string">"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. "</span> +</span><br><span class="line">  <span class="string">"Please query the Hive table registered using Spark SQL."</span>)</span><br></pre></td></tr></table></figure>
<p>まずクエリタイプが <code>snapshot</code> である場合は、
以下の通り、Parquetとして読み込みが定義され、Relationが返される。</p>
<p>org/apache/hudi/DefaultSource.scala:72</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataSource</span>.apply(</span><br><span class="line">  sparkSession = sqlContext.sparkSession,</span><br><span class="line">  userSpecifiedSchema = <span class="type">Option</span>(schema),</span><br><span class="line">  className = <span class="string">"parquet"</span>,</span><br><span class="line">  options = parameters)</span><br><span class="line">  .resolveRelation()</span><br></pre></td></tr></table></figure>
<p>例えば、クイックスタートの例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsSnapshotDF = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*/*/*"</span>)</span><br><span class="line">scala&gt; tripsSnapshotDF.createOrReplaceTempView(<span class="string">"hudi_trips_snapshot"</span>)</span><br></pre></td></tr></table></figure>
<p>では、こちらのタイプ。
ParquetベースのRelation（実際には、HadoopFsRelation）が返される。
上記の例では、当該RelationのrootPathsに、以下のような値が含まれる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rootPaths = &#123;$colon$colon@14885&#125; &quot;::&quot; size = 6</span><br><span class="line"> 0 = &#123;Path@15421&#125; &quot;file:/tmp/hudi_trips_cow/americas/brazil/sao_paulo/ae28c85a-38f0-487f-a42d-3a0babc9d321-0_0-21-25_20200329002247.parquet&quot;</span><br><span class="line"> 1 = &#123;Path@15422&#125; &quot;file:/tmp/hudi_trips_cow/americas/brazil/sao_paulo/.hoodie_partition_metadata&quot;</span><br><span class="line"> 2 = &#123;Path@15423&#125; &quot;file:/tmp/hudi_trips_cow/americas/united_states/san_francisco/849db286-1cbe-4a1f-b544-9939893e99f8-0_1-21-26_20200329002247.parquet&quot;</span><br><span class="line"> 3 = &#123;Path@15424&#125; &quot;file:/tmp/hudi_trips_cow/americas/united_states/san_francisco/.hoodie_partition_metadata&quot;</span><br><span class="line"> 4 = &#123;Path@15425&#125; &quot;file:/tmp/hudi_trips_cow/asia/india/chennai/2ebfbab0-4f8f-42db-b79e-1c0cbcc3cf39-0_2-21-27_20200329002247.parquet&quot;</span><br><span class="line"> 5 = &#123;Path@15426&#125; &quot;file:/tmp/hudi_trips_cow/asia/india/chennai/.hoodie_partition_metadata&quot;</span><br></pre></td></tr></table></figure>
<p>次にクエリタイプが <code>incremental</code> である場合は、
以下の通り、
<code>org.apache.hudi.IncrementalRelation#IncrementalRelation</code>
が返される。</p>
<p>org/apache/hudi/DefaultSource.scala:79</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">IncrementalRelation</span>(sqlContext, path.get, optParams, schema)</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> tripsIncrementalDF = spark.read.format(<span class="string">"hudi"</span>).</span><br><span class="line">         option(<span class="type">QUERY_TYPE_OPT_KEY</span>, <span class="type">QUERY_TYPE_INCREMENTAL_OPT_VAL</span>).</span><br><span class="line">         option(<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>, beginTime).</span><br><span class="line">         load(basePath)</span><br><span class="line">scala&gt; tripsIncrementalDF.createOrReplaceTempView(<span class="string">"hudi_trips_incremental"</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; spark.sql(<span class="string">"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare &gt; 20.0"</span>).show()</span><br></pre></td></tr></table></figure>
<p>では、
<code>org.apache.hudi.IncrementalRelation#IncrementalRelation</code>
が戻り値として返される。</p>
<h3><span id="incrementalrelation">IncrementalRelation</span></h3>
<h4><span id="コンストラクタ">コンストラクタ</span></h4>
<p>Parquetをファイルを単純に読めば良いのと比べて、格納された最新データを返すようにしないとならないので
それなりに複雑なRelationとなっている。</p>
<p>以下、簡単にコンストラクタのポイントを確認する。</p>
<p>最初にメタデータを取得するクライアント。
コミット、セーブポイント、コンパクションなどの情報を得られるようになる。</p>
<p>org/apache/hudi/IncrementalRelation.scala:51</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> metaClient = <span class="keyword">new</span> <span class="type">HoodieTableMetaClient</span>(sqlContext.sparkContext.hadoopConfiguration, basePath, <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例では、 <code>metaPath</code> は、
<code>file:/tmp/hudi_trips_cow/.hoodie</code> だった。</p>
<p>続いてテーブル情報のインスタンスを取得する。
テーブル情報からタイムラインを取り出す。</p>
<p>org/apache/hudi/IncrementalRelation.scala:57</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> hoodieTable = <span class="type">HoodieTable</span>.getHoodieTable(metaClient, <span class="type">HoodieWriteConfig</span>.newBuilder().withPath(basePath).build(),</span><br><span class="line">  sqlContext.sparkContext)</span><br><span class="line"><span class="keyword">val</span> commitTimeline = hoodieTable.getMetaClient.getCommitTimeline.filterCompletedInstants()</span><br><span class="line"><span class="keyword">if</span> (commitTimeline.empty()) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">HoodieException</span>(<span class="string">"No instants to incrementally pull"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (!optParams.contains(<span class="type">DataSourceReadOptions</span>.<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>)) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">HoodieException</span>(<span class="string">s"Specify the begin instant time to pull from using "</span> +</span><br><span class="line">    <span class="string">s"option <span class="subst">$&#123;DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY&#125;</span>"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例で実際に生成されたタイムラインは以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instants = &#123;ArrayList@25586&#125;  size = 3</span><br><span class="line"> 0 = &#123;HoodieInstant@25589&#125; &quot;[20200330002239__commit__COMPLETED]&quot;</span><br><span class="line"> 1 = &#123;HoodieInstant@25590&#125; &quot;[20200330002354__commit__COMPLETED]&quot;</span><br><span class="line"> 2 = &#123;HoodieInstant@25591&#125; &quot;[20200330003142__commit__COMPLETED]&quot;</span><br></pre></td></tr></table></figure>
<p>オプションとして与えられた「はじめ」と「おわり」から、
対象となるタイムラインを構成する。
タイムライン上、最も新しいインスタンスを取得し、
Parquetファイルからスキーマを読んでいる。</p>
<p>org/apache/hudi/IncrementalRelation.scala:68</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lastInstant = commitTimeline.lastInstant().get()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> commitsToReturn = commitTimeline.findInstantsInRange(</span><br><span class="line">  optParams(<span class="type">DataSourceReadOptions</span>.<span class="type">BEGIN_INSTANTTIME_OPT_KEY</span>),</span><br><span class="line">  optParams.getOrElse(<span class="type">DataSourceReadOptions</span>.<span class="type">END_INSTANTTIME_OPT_KEY</span>, lastInstant.getTimestamp))</span><br><span class="line">  .getInstants.iterator().toList</span><br><span class="line"></span><br><span class="line"><span class="comment">// use schema from a file produced in the latest instant</span></span><br><span class="line"><span class="keyword">val</span> latestSchema = &#123;</span><br><span class="line">  <span class="comment">// use last instant if instant range is empty</span></span><br><span class="line">  <span class="keyword">val</span> instant = commitsToReturn.lastOption.getOrElse(lastInstant)</span><br><span class="line">  <span class="keyword">val</span> latestMeta = <span class="type">HoodieCommitMetadata</span></span><br><span class="line">        .fromBytes(commitTimeline.getInstantDetails(instant).get, classOf[<span class="type">HoodieCommitMetadata</span>])</span><br><span class="line">  <span class="keyword">val</span> metaFilePath = latestMeta.getFileIdAndFullPaths(basePath).values().iterator().next()</span><br><span class="line">  <span class="type">AvroConversionUtils</span>.convertAvroSchemaToStructType(<span class="type">ParquetUtils</span>.readAvroSchema(</span><br><span class="line">    sqlContext.sparkContext.hadoopConfiguration, <span class="keyword">new</span> <span class="type">Path</span>(metaFilePath)))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>クイックスタートの例では、 <code>commitsToReturn</code>
は以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;$colon$colon@25626&#125; &quot;::&quot; size = 1</span><br><span class="line"> 0 = &#123;HoodieInstant@25591&#125; &quot;[20200330003142__commit__COMPLETED]&quot;</span><br><span class="line">  state = &#123;HoodieInstant$State@25602&#125; &quot;COMPLETED&quot;</span><br><span class="line">  action = &quot;commit&quot;</span><br><span class="line">  timestamp = &quot;20200330003142&quot;</span><br></pre></td></tr></table></figure>
<p>また、少々気になるのは、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">AvroConversionUtils</span>.convertAvroSchemaToStructType(<span class="type">ParquetUtils</span>.readAvroSchema(</span><br><span class="line">  sqlContext.sparkContext.hadoopConfiguration, <span class="keyword">new</span> <span class="type">Path</span>(metaFilePath)))</span><br></pre></td></tr></table></figure>
<p>という箇所で、もともとParquet形式のデータからAvro形式のスキーマを取り出し、それをさらにSparkのStructTypeに変換しているところ。
実際にParquetのfooterから取り出したスキーマ情報を、AvroのSchemaに変換しているのは以下の箇所。</p>
<p>org/apache/hudi/common/util/ParquetUtils.java:140</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public static <span class="type">Schema</span> readAvroSchema(<span class="type">Configuration</span> configuration, <span class="type">Path</span> parquetFilePath) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">AvroSchemaConverter</span>().convert(readSchema(configuration, parquetFilePath));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Parquet自身にAvroへの変換器
<code>org.apache.parquet.avro.AvroSchemaConverter</code>
が備わっているので便利？</li>
<li>SparkのData
Source機能でDataFrame化してからスキーマを取り出すと、一度読み込みが生じていしまうから非効率？</li>
</ul>
<p>という理由が想像されるが、やや回りくどいような印象を持った。
★要確認</p>
<p>本編に戻る。続いてフィルタを定義。</p>
<p>org/apache/hudi/IncrementalRelation.scala:86</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> filters = &#123;</span><br><span class="line">  <span class="keyword">if</span> (optParams.contains(<span class="type">DataSourceReadOptions</span>.<span class="type">PUSH_DOWN_INCR_FILTERS_OPT_KEY</span>)) &#123;</span><br><span class="line">    <span class="keyword">val</span> filterStr = optParams.getOrElse(</span><br><span class="line">      <span class="type">DataSourceReadOptions</span>.<span class="type">PUSH_DOWN_INCR_FILTERS_OPT_KEY</span>,</span><br><span class="line">      <span class="type">DataSourceReadOptions</span>.<span class="type">DEFAULT_PUSH_DOWN_FILTERS_OPT_VAL</span>)</span><br><span class="line">    filterStr.split(<span class="string">","</span>).filter(!_.isEmpty)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">Array</span>[<span class="type">String</span>]()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ここまでがコンストラクタ。</p>
<h4><span id="buildscan">buildScan</span></h4>
<p>実際にSparkのData
Sourceで読み込むときに用いられる読み込みの手段が定義されている。
以下にポイントを述べる。</p>
<p>org/apache/hudi/IncrementalRelation.scala:99</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span></span>(): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>ファイルIDとフルPATHのマップを作る。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileIdToFullPath = mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line"><span class="keyword">for</span> (commit &lt;- commitsToReturn) &#123;</span><br><span class="line">  <span class="keyword">val</span> metadata: <span class="type">HoodieCommitMetadata</span> = <span class="type">HoodieCommitMetadata</span>.fromBytes(commitTimeline.getInstantDetails(commit)</span><br><span class="line">    .get, classOf[<span class="type">HoodieCommitMetadata</span>])</span><br><span class="line">  fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記マップに対し、必要に応じてフィルタを適用する。</p>
<p>org/apache/hudi/IncrementalRelation.scala:106</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pathGlobPattern = optParams.getOrElse(</span><br><span class="line">  <span class="type">DataSourceReadOptions</span>.<span class="type">INCR_PATH_GLOB_OPT_KEY</span>,</span><br><span class="line">  <span class="type">DataSourceReadOptions</span>.<span class="type">DEFAULT_INCR_PATH_GLOB_OPT_VAL</span>)</span><br><span class="line"><span class="keyword">val</span> filteredFullPath = <span class="keyword">if</span>(!pathGlobPattern.equals(<span class="type">DataSourceReadOptions</span>.<span class="type">DEFAULT_INCR_PATH_GLOB_OPT_VAL</span>)) &#123;</span><br><span class="line">  <span class="keyword">val</span> globMatcher = <span class="keyword">new</span> <span class="type">GlobPattern</span>(<span class="string">"*"</span> + pathGlobPattern)</span><br><span class="line">  fileIdToFullPath.filter(p =&gt; globMatcher.matches(p._2))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  fileIdToFullPath</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>コンストラクタで定義されたフィルタを適用しながら、
対象となるParquetファイルを読み込み、RDDを生成する。</p>
<p>org/apache/hudi/IncrementalRelation.scala:117</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.sparkContext.hadoopConfiguration.unset(<span class="string">"mapreduce.input.pathFilter.class"</span>)</span><br><span class="line"><span class="keyword">val</span> sOpts = optParams.filter(p =&gt; !p._1.equalsIgnoreCase(<span class="string">"path"</span>))</span><br><span class="line"><span class="keyword">if</span> (filteredFullPath.isEmpty) &#123;</span><br><span class="line">  sqlContext.sparkContext.emptyRDD[<span class="type">Row</span>]</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  log.info(<span class="string">"Additional Filters to be applied to incremental source are :"</span> + filters)</span><br><span class="line">  filters.foldLeft(sqlContext.read.options(sOpts)</span><br><span class="line">    .schema(latestSchema)</span><br><span class="line">    .parquet(filteredFullPath.values.toList: _*)</span><br><span class="line">    .filter(<span class="type">String</span>.format(<span class="string">"%s &gt;= '%s'"</span>, <span class="type">HoodieRecord</span>.<span class="type">COMMIT_TIME_METADATA_FIELD</span>, commitsToReturn.head.getTimestamp))</span><br><span class="line">    .filter(<span class="type">String</span>.format(<span class="string">"%s &lt;= '%s'"</span>,</span><br><span class="line">      <span class="type">HoodieRecord</span>.<span class="type">COMMIT_TIME_METADATA_FIELD</span>, commitsToReturn.last.getTimestamp)))((e, f) =&gt; e.filter(f))</span><br><span class="line">    .toDF().rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1><span id="hudiへの書き込み">Hudiへの書き込み</span></h1>
<p><a href="https://hudi.apache.org/docs/writing_data.html" target="_blank" rel="noopener">Writing Hudi
Tables</a> をベースに調べる。</p>
<h2><span id="オペレーション種類">オペレーション種類</span></h2>
<p>書き込みのオペレーション種類は、upsert、insert、bulk_insert。
クイックスタートにはbulk_insertはなかった。</p>
<h2><span id="deltastreamer">DeltaStreamer</span></h2>
<p>ユーティリティとして付属するDeltaStreamerを用いると、
Kafka等からデータを取り込める。
Avro等のスキーマのデータを読み取れる。</p>
<h3><span id="動作確認">動作確認</span></h3>
<h4><span id="パッケージ化">パッケージ化</span></h4>
<p><a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a> の手順に基づくと、
ビルドされたユーティリティを使うことになるので、
予めパッケージ化しておく。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p ~/Sources</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/Sources</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/apache/incubator-hudi.git incubator-hudi-052</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> incubator-hudi-052</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> git checkout -b release-0.5.2-incubating refs/tags/release-0.5.2-incubating</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mvn clean package -DskipTests -DskipITs</span></span><br></pre></td></tr></table></figure>
<h4><span id="実行">実行</span></h4>
<p><a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a>に基づくと、Confluentメンバが作成した （ <a href="https://github.com/apurvam/streams-prototyping" target="_blank" rel="noopener">apurvam
streams-prototyping</a> ）サンプルデータ作成用のAvroスキーマと Confluent
PlatformのKSQLのユーティリティを 使ってサンプルデータを作成する。</p>
<p>ついては。予めConfluent Platformをインストールしておくこと。</p>
<p>まずはスキーマをダウンロードする。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl https://raw.githubusercontent.com/apurvam/streams-prototyping/master/src/main/resources/impressions.avro &gt; /tmp/impressions.avro</span></span><br></pre></td></tr></table></figure>
<p>テストデータを生成する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ksql-datagen schema=/tmp/impressions.avro format=avro topic=impressions key=impressionid</span></span><br></pre></td></tr></table></figure>
<p>別の端末を開き、ユーティリティを起動する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> SPARK_HOME=/opt/spark/default</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/Sources/incubator-hudi-052</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.5.2-incubating.jar \</span></span><br><span class="line">  --props file://$&#123;PWD&#125;/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \</span><br><span class="line">  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \</span><br><span class="line">  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \</span><br><span class="line">  --source-ordering-field impresssiontime \</span><br><span class="line">  --target-base-path file:\/\/\/tmp/hudi-deltastreamer-op \</span><br><span class="line">  --target-table uber.impressions \</span><br><span class="line">  --table-type COPY_ON_WRITE \</span><br><span class="line">  --op BULK_INSERT</span><br></pre></td></tr></table></figure>
<p>なお、 <a href="https://hudi.apache.org/docs/writing_data.html#deltastreamer" target="_blank" rel="noopener">公式ドキュメントのData
Streamer</a> から2箇所修正した。（JarファイルPATH、
<code>--table-type</code> オプション追加。</p>
<p>Kafkaから読み込んで書き出したデータ（
<code>/tmp/hudi-deltastreamer-op</code> ）を確認してみる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="variable">$&#123;SPARK_HOME&#125;</span>/bin/spark-shell \</span></span><br><span class="line">  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.2-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \</span><br><span class="line">  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'</span><br></pre></td></tr></table></figure>
<p>シェルが起動したら、以下の通り読み込んで見る。 なお、ここでは
<code>userid</code>
がパーティションキーとなっているので、ロード時にそれを指定した。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> basePath = <span class="string">"file:///tmp/hudi-deltastreamer-op"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> impressionDF = spark.</span><br><span class="line">         read.</span><br><span class="line">         format(<span class="string">"hudi"</span>).</span><br><span class="line">         load(basePath + <span class="string">"/*/*"</span>)</span><br></pre></td></tr></table></figure>
<p>内容は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; impressionDF.show</span><br><span class="line">+-------------------+--------------------+------------------+----------------------+--------------------+---------------+--------------+-------+-----+</span><br><span class="line">|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|impresssiontime|  impressionid| userid| adid|</span><br><span class="line">+-------------------+--------------------+------------------+----------------------+--------------------+---------------+--------------+-------+-----+</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_106|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586096500438</span>|impression_106|user_83|ad_57|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_107|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586096464324</span>|impression_107|user_83|ad_11|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_111|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586096366450</span>|impression_111|user_83|ad_14|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_111|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586099019181</span>|impression_111|user_83|ad_38|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_116|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586099146437</span>|impression_116|user_83|ad_48|</span><br><span class="line">|     <span class="number">20200406002420</span>|<span class="number">20200406002420</span>_1_...|    impression_121|               user_83|fb381e12-f9ec<span class="number">-4</span>fb...|  <span class="number">1586098316334</span>|impression_121|user_83|ad_26|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<h3><span id="実装確認">実装確認</span></h3>
<p><code>org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer</code>
クラスの実装を確認する。</p>
<p>まずmainは以下の通り。</p>
<p>org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java:298</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> Config cfg = <span class="keyword">new</span> Config();</span><br><span class="line">  JCommander cmd = <span class="keyword">new</span> JCommander(cfg, <span class="keyword">null</span>, args);</span><br><span class="line">  <span class="keyword">if</span> (cfg.help || args.length == <span class="number">0</span>) &#123;</span><br><span class="line">    cmd.usage();</span><br><span class="line">    System.exit(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Map&lt;String, String&gt; additionalSparkConfigs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg);</span><br><span class="line">  JavaSparkContext jssc =</span><br><span class="line">      UtilHelpers.buildSparkContext(<span class="string">"delta-streamer-"</span> + cfg.targetTableName, cfg.sparkMaster, additionalSparkConfigs);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> HoodieDeltaStreamer(cfg, jssc).sync();</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    jssc.stop();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、
<code>org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer#sync</code>
メソッドがエントリポイント。</p>
<p>org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java:116</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sync</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (cfg.continuousMode) &#123;</span><br><span class="line">    deltaSyncService.start(<span class="keyword">this</span>::onDeltaSyncShutdown);</span><br><span class="line">    deltaSyncService.waitForShutdown();</span><br><span class="line">    LOG.info(<span class="string">"Delta Sync shutting down"</span>);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    LOG.info(<span class="string">"Delta Streamer running only single round"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      deltaSyncService.getDeltaSync().syncOnce();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">      LOG.error(<span class="string">"Got error running delta sync once. Shutting down"</span>, ex);</span><br><span class="line">      <span class="keyword">throw</span> ex;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      deltaSyncService.close();</span><br><span class="line">      LOG.info(<span class="string">"Shut down delta streamer"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、 <code>continous</code>
モードかどうかで動作が変わる。</p>
<p>ここでは一旦、ワンショットの場合を確認する。</p>
<p>上記の通り、
<code>org.apache.hudi.utilities.deltastreamer.DeltaSync#syncOnce</code>
メソッドがエントリポイント。
当該メソッドは以下のようにシンプルな内容。</p>
<p>org/apache/hudi/utilities/deltastreamer/DeltaSync.java:218</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Option&lt;String&gt; <span class="title">syncOnce</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  Option&lt;String&gt; scheduledCompaction = Option.empty();</span><br><span class="line">  HoodieDeltaStreamerMetrics metrics = <span class="keyword">new</span> HoodieDeltaStreamerMetrics(getHoodieClientConfig(schemaProvider));</span><br><span class="line">  Timer.Context overallTimerContext = metrics.getOverallTimerContext();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Refresh Timeline</span></span><br><span class="line">  refreshTimeline();</span><br><span class="line"></span><br><span class="line">  Pair&lt;SchemaProvider, Pair&lt;String, JavaRDD&lt;HoodieRecord&gt;&gt;&gt; srcRecordsWithCkpt = readFromSource(commitTimelineOpt);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">null</span> != srcRecordsWithCkpt) &#123;</span><br><span class="line">    <span class="comment">// this is the first input batch. If schemaProvider not set, use it and register Avro Schema and start</span></span><br><span class="line">    <span class="comment">// compactor</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == schemaProvider) &#123;</span><br><span class="line">      <span class="comment">// Set the schemaProvider if not user-provided</span></span><br><span class="line">      <span class="keyword">this</span>.schemaProvider = srcRecordsWithCkpt.getKey();</span><br><span class="line">      <span class="comment">// Setup HoodieWriteClient and compaction now that we decided on schema</span></span><br><span class="line">      setupWriteClient();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    scheduledCompaction = writeToSink(srcRecordsWithCkpt.getRight().getRight(),</span><br><span class="line">        srcRecordsWithCkpt.getRight().getLeft(), metrics, overallTimerContext);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Clear persistent RDDs</span></span><br><span class="line">  jssc.getPersistentRDDs().values().forEach(JavaRDD::unpersist);</span><br><span class="line">  <span class="keyword">return</span> scheduledCompaction;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最初にメトリクスの準備、データソースから読み出してRDD化する定義（
<code>org.apache.hudi.utilities.deltastreamer.DeltaSync#readFromSource</code>
メソッド） その後、
<code>org.apache.hudi.utilities.deltastreamer.DeltaSync#writeToSink</code>
メソッドにより、定義されたRDDの内容を実際に書き込む。</p>
<p>ここでは上記メソッドを確認する。</p>
<p>まず与えられたRDDから重複排除する。</p>
<p>org/apache/hudi/utilities/deltastreamer/DeltaSync.java:352</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> Option&lt;String&gt; <span class="title">writeToSink</span><span class="params">(JavaRDD&lt;HoodieRecord&gt; records, String checkpointStr,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     HoodieDeltaStreamerMetrics metrics, Timer.Context overallTimerContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    Option&lt;String&gt; scheduledCompactionInstant = Option.empty();</span><br><span class="line">    <span class="comment">// filter dupes if needed</span></span><br><span class="line">    <span class="keyword">if</span> (cfg.filterDupes) &#123;</span><br><span class="line">      <span class="comment">// turn upserts to insert</span></span><br><span class="line">      cfg.operation = cfg.operation == Operation.UPSERT ? Operation.INSERT : cfg.operation;</span><br><span class="line">      records = DataSourceUtils.dropDuplicates(jssc, records, writeClient.getConfig());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isEmpty = records.isEmpty();</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>その後実際の書き込みになるが、そのとき採用したオペレーション種類によって動作が異なる。</p>
<p>org/apache/hudi/utilities/deltastreamer/DeltaSync.java:369</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (cfg.operation == Operation.INSERT) &#123;</span><br><span class="line">  writeStatusRDD = writeClient.insert(records, instantTime);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (cfg.operation == Operation.UPSERT) &#123;</span><br><span class="line">  writeStatusRDD = writeClient.upsert(records, instantTime);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (cfg.operation == Operation.BULK_INSERT) &#123;</span><br><span class="line">  writeStatusRDD = writeClient.bulkInsert(records, instantTime);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> HoodieDeltaStreamerException(<span class="string">"Unknown operation :"</span> + cfg.operation);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="bulkinsert">bulkInsert</span></h4>
<p>ここではためしに
<code>org.apache.hudi.client.HoodieWriteClient#bulkInsert</code>
メソッドを確認してみる。</p>
<p>当該メソッドでは、最初にCOPY_ON_WRITEかMERGE_ON_READかに応じて、それぞれの種類のテーブル情報を取得する。
その後、
<code>org.apache.hudi.client.HoodieWriteClient#bulkInsertInternal</code>
メソッドを使ってデータを書き込む。</p>
<p>なお、その間で重複排除されているが、上記の通り、もともと重複排除しているはずなので、要確認。（重複排除のロジックが異なるのかどうか、など）
パット見た感じ、
<code>org.apache.hudi.DataSourceUtils#dropDuplicates</code>
メソッドはロケーション情報（インデックス？）がない場合をドロップする。
<code>org.apache.hudi.client.HoodieWriteClient#combineOnCondition</code>
メソッドはキーに基づきreduce処理する。 という違いがあるようだ。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:300</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;WriteStatus&gt; <span class="title">bulkInsert</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, <span class="keyword">final</span> String instantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">    Option&lt;UserDefinedBulkInsertPartitioner&gt; bulkInsertPartitioner)</span> </span>&#123;</span><br><span class="line">  HoodieTable&lt;T&gt; table = getTableAndInitCtx(WriteOperationType.BULK_INSERT);</span><br><span class="line">  setOperationType(WriteOperationType.BULK_INSERT);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// De-dupe/merge if needed</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords =</span><br><span class="line">        combineOnCondition(config.shouldCombineBeforeInsert(), records, config.getInsertShuffleParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bulkInsertInternal(dedupedRecords, instantTime, table, bulkInsertPartitioner);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> HoodieInsertException) &#123;</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieInsertException(<span class="string">"Failed to bulk insert for commit time "</span> + instantTime, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、
<code>org.apache.hudi.client.HoodieWriteClient#bulkInsertInternal</code>
メソッドが中で用いられている。
当該メソッドでは、再パーティションないしソートが行われた後、書き込みが実行される。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:412</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;WriteStatus&gt; writeStatusRDD = repartitionedRecords</span><br><span class="line">    .mapPartitionsWithIndex(<span class="keyword">new</span> BulkInsertMapFunction&lt;T&gt;(instantTime, config, table, fileIDPrefixes), <span class="keyword">true</span>)</span><br><span class="line">    .flatMap(List::iterator);</span><br></pre></td></tr></table></figure>
<p>ポイントは、<code>org.apache.hudi.execution.BulkInsertMapFunction</code>
クラスである。 このクラスが関数として渡されている。
<code>org.apache.hudi.execution.BulkInsertMapFunction#call</code>
メソッドは以下の通り。</p>
<p>org/apache/hudi/execution/BulkInsertMapFunction.java:52</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; call(Integer partition, Iterator&lt;HoodieRecord&lt;T&gt;&gt; sortedRecordItr) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(sortedRecordItr, config, instantTime, hoodieTable,</span><br><span class="line">      fileIDPrefixes.get(partition), hoodieTable.getSparkTaskContextSupplier());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable</code>
クラスについては、別の節で書いたとおり。</p>
<h4><span id="insert">insert</span></h4>
<p><code>org.apache.hudi.client.HoodieWriteClient#insert</code>
メソッド。</p>
<p>大まかな構造は、 <code>bulkInsert</code> と同様。 ポイントは、
<code>org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal</code>
メソッド。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:229</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;WriteStatus&gt; <span class="title">insert</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, <span class="keyword">final</span> String instantTime)</span> </span>&#123;</span><br><span class="line">  HoodieTable&lt;T&gt; table = getTableAndInitCtx(WriteOperationType.INSERT);</span><br><span class="line">  setOperationType(WriteOperationType.INSERT);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// De-dupe/merge if needed</span></span><br><span class="line">    JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords =</span><br><span class="line">        combineOnCondition(config.shouldCombineBeforeInsert(), records, config.getInsertShuffleParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> upsertRecordsInternal(dedupedRecords, instantTime, table, <span class="keyword">false</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    <span class="keyword">if</span> (e <span class="keyword">instanceof</span> HoodieInsertException) &#123;</span><br><span class="line">      <span class="keyword">throw</span> e;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieInsertException(<span class="string">"Failed to insert for commit time "</span> + instantTime, e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、挿入対象のデータを表すRDDを引数に取り、データを書き込む。
これは、upsertのときと同じメソッドである。第4引数でinsertかupsertかを分ける。</p>
<p>当該メソッドは以下の通り。 <code>bulkInsert</code>
と同様にリパーティションなどを経て、
<code>org.apache.hudi.table.HoodieTable#handleUpsertPartition</code>
が呼び出される。</p>
<p>org/apache/hudi/client/HoodieWriteClient.java:457</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">private</span> JavaRDD&lt;WriteStatus&gt; <span class="title">upsertRecordsInternal</span><span class="params">(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; preppedRecords, String instantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">      HoodieTable&lt;T&gt; hoodieTable, <span class="keyword">final</span> <span class="keyword">boolean</span> isUpsert)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">    JavaRDD&lt;WriteStatus&gt; writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (isUpsert) &#123;</span><br><span class="line">        <span class="keyword">return</span> hoodieTable.handleUpsertPartition(instantTime, partition, recordItr, partitioner);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> hoodieTable.handleInsertPartition(instantTime, partition, recordItr, partitioner);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="keyword">true</span>).flatMap(List::iterator);</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.hudi.table.HoodieTable#handleUpsertPartition</code>
と <code>org.apache.hudi.table.HoodieTable#handleInsertPartition</code>
が用いられている。 今回は、insertなので後者。</p>
<p>なお、
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsertPartition</code>
は以下の通り、実態としては
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleUpsertPartition</code>
である。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsertPartition(String instantTime, Integer partition, Iterator recordItr,</span><br><span class="line">                                                         Partitioner partitioner) &#123;</span><br><span class="line">  <span class="keyword">return</span> handleUpsertPartition(instantTime, partition, recordItr, partitioner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当該メソッドは以下の通り。
insertやupsertでは、RDDひとつを1バケットと表現している。
バケットの情報から、insertやupdateの情報を取得して用いる。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String instantTime, Integer partition, Iterator recordItr,</span><br><span class="line">                                                         Partitioner partitioner) &#123;</span><br><span class="line">  UpsertPartitioner upsertPartitioner = (UpsertPartitioner) partitioner;</span><br><span class="line">  BucketInfo binfo = upsertPartitioner.getBucketInfo(partition);</span><br><span class="line">  BucketType btype = binfo.bucketType;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (btype.equals(BucketType.INSERT)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleInsert(instantTime, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (btype.equals(BucketType.UPDATE)) &#123;</span><br><span class="line">      <span class="keyword">return</span> handleUpdate(instantTime, binfo.partitionPath, binfo.fileIdPrefix, recordItr);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(<span class="string">"Unknown bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    String msg = <span class="string">"Error upserting bucketType "</span> + btype + <span class="string">" for partition :"</span> + partition;</span><br><span class="line">    LOG.error(msg, t);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> HoodieUpsertException(msg, t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例えば、insertの場合は、
<code>org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsert</code>
が呼び出される。 当該メソッドでは、戻り値として
<code>org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#CopyOnWriteLazyInsertIterable</code>
が返される。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:186</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records</span></span><br><span class="line">  <span class="keyword">if</span> (!recordItr.hasNext()) &#123;</span><br><span class="line">    LOG.info(<span class="string">"Empty partition"</span>);</span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, <span class="keyword">this</span>, idPfx, sparkTaskContextSupplier);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>このメソッドについては上記ですでに説明したとおり。</p>
<h4><span id="hoodiecopyonwritetableと-hoodiemergeonreadtable">HoodieCopyOnWriteTable
と HoodieMergeOnReadTable</span></h4>
<p>テーブルの種類によって、書き込みの実装上どういう違いがあるかを確認する。</p>
<p>例えば、<code>handleInsert</code>
メソッドを確認する。なお、当該メソッドには同期的、非同期的な処理方式がそれぞれ実装されている。</p>
<p>HoodieCopyOnWriteTableの場合は以下の通り。</p>
<p>org/apache/hudi/table/HoodieCopyOnWriteTable.java:186</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records</span></span><br><span class="line">  <span class="keyword">if</span> (!recordItr.hasNext()) &#123;</span><br><span class="line">    LOG.info(<span class="string">"Empty partition"</span>);</span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, <span class="keyword">this</span>, idPfx, sparkTaskContextSupplier);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String partitionPath, String fileId,</span><br><span class="line">    Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) &#123;</span><br><span class="line">  HoodieCreateHandle createHandle =</span><br><span class="line">      <span class="keyword">new</span> HoodieCreateHandle(config, instantTime, <span class="keyword">this</span>, partitionPath, fileId, recordItr, sparkTaskContextSupplier);</span><br><span class="line">  createHandle.write();</span><br><span class="line">  <span class="keyword">return</span> Collections.singletonList(Collections.singletonList(createHandle.close())).iterator();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上が非同期的な方式、下が同期的な方式と見られる。
なお、実装上は同期的な処理方式は今は使われていないようにもみえるが、要確認。</p>
<p>HoodieMergeOnReadTableの場合は、非同期的な処理だけoverrideされている。</p>
<p>org/apache/hudi/table/HoodieMergeOnReadTable.java:120</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr)</span><br><span class="line">    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// If canIndexLogFiles, write inserts to log files else write inserts to parquet files</span></span><br><span class="line">  <span class="keyword">if</span> (index.canIndexLogFiles()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MergeOnReadLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, <span class="keyword">this</span>, idPfx, sparkTaskContextSupplier);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">super</span>.handleInsert(instantTime, idPfx, recordItr);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/03/25/Hudi/" data-id="cm0jr5kwx01gq18qbrtrc4lj7" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-pandoc-template-and-css" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/03/06/pandoc-template-and-css/">pandoc template and css</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/03/06/pandoc-template-and-css/">
            <time datetime="2020-03-06T13:46:59.000Z" itemprop="datePublished">2020-03-06</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Documentation/">Documentation</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/pandoc/">pandoc</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://qiita.com/cawpea/items/cea1243e106ababd15e7" target="_blank" rel="noopener">Pandocを使ってMarkdownを整形されたHTMLに変換する</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>Pandocのバージョンは、 2.9.2を使用。</p>
<p><a href="https://qiita.com/cawpea/items/cea1243e106ababd15e7" target="_blank" rel="noopener">Pandocを使ってMarkdownを整形されたHTMLに変換する</a>
を参考に、テンプレートを作成してCSSを用いた。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p ~/.pandoc/templates</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pandoc -D html5 &gt; ~/.pandoc/templates/mytemplate.html</span></span><br></pre></td></tr></table></figure>
<p>テンプレートを適当にいじる。</p>
<p>その後、HTMLを以下のように生成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pandoc --css ./pandoc-github.css --template=mytemplate -i ./README.md -o ./README.html</span></span><br></pre></td></tr></table></figure>
<p>なお、GitHub風になるCSSは、 <a href="https://gist.github.com/dashed/6714393" target="_blank" rel="noopener">dashed/github-pandoc.css</a>
に公開されていたものを利用。 <code>--css</code>
はcssのURLを表すだけなので、上記の例ではREADME.htmlと同じディレクトリに
<code>pandoc-github.css</code> があることを期待する。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/03/06/pandoc-template-and-css/" data-id="cm0jr5krz014318qbmb1mks0j" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Flow-Engine-for-ML" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/02/16/Flow-Engine-for-ML/">Flow Engine for ML</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/02/16/Flow-Engine-for-ML/">
            <time datetime="2020-02-16T13:31:14.000Z" itemprop="datePublished">2020-02-16</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Flow-Engine/">Flow Engine</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Flow-Engine/">Flow Engine</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a>
<ul>
<li><a href="#総合" id="toc-総合">総合</a></li>
<li><a href="#azkaban" id="toc-azkaban">Azkaban</a></li>
</ul></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#よく名前の挙がるもの" id="toc-よく名前の挙がるもの">よく名前の挙がるもの</a></li>
<li><a href="#機械的な検索" id="toc-機械的な検索">機械的な検索</a>
<ul>
<li><a href="#airflowの代替" id="toc-airflowの代替">Airflowの代替</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<h2><span id="総合">総合</span></h2>
<ul>
<li><a href="https://alternativeto.net/software/apache-airflow/" target="_blank" rel="noopener">alternativetoでAirflowを検索した結果</a></li>
</ul>
<h2><span id="azkaban">Azkaban</span></h2>
<ul>
<li><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html#flow-2-0-basics" target="_blank" rel="noopener">Azkabanのフロー書き方</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>機械学習で利用されるフロー管理ツールを軽くさらってみる。</p>
<h2><span id="よく名前の挙がるもの">よく名前の挙がるもの</span></h2>
<ul>
<li>Apache Airflow</li>
<li>DigDag</li>
<li>Oozie</li>
</ul>
<h2><span id="機械的な検索">機械的な検索</span></h2>
<h3><span id="airflowの代替">Airflowの代替</span></h3>
<p><a href="https://alternativeto.net/software/apache-airflow/" target="_blank" rel="noopener">alternativetoでAirflowを検索した結果</a>
では以下の通り。</p>
<ul>
<li>RunDeck
<ul>
<li>OSSだが商用版もある。自動化ツール。ワークフローも管理できるようだ</li>
</ul></li>
<li>StackStorm
<ul>
<li>どちらかというとIFTTTみたいなものか？</li>
</ul></li>
<li>Zenaton
<ul>
<li>ワークフローエンジン。JavaScriptで記述できるようだ</li>
</ul></li>
<li>Apache Oozie
<ul>
<li>Hadoopエコシステムのワークフローエンジン</li>
</ul></li>
<li>Azkaban
<ul>
<li>ワークフローエンジン</li>
<li><a href="https://azkaban.readthedocs.io/en/latest/createFlows.html#flow-2-0-basics" target="_blank" rel="noopener">Azkabanのフロー書き方</a>
の通り、YAMLで書ける。</li>
<li>LinkedIn が主に開発</li>
</ul></li>
<li>Metaflow ★
<ul>
<li>ワークフローエンジン</li>
<li>機械学習にフォーカス</li>
<li>Netflix と AWS が主に開発</li>
</ul></li>
<li>luigi
<ul>
<li>ワークフローエンジン</li>
<li>Pythonモジュール</li>
<li>Spotify が主に開発</li>
</ul></li>
</ul>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/02/16/Flow-Engine-for-ML/" data-id="cm0jr5kjj003z18qb3pycnqda" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Starting-Kafka-Streams" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2020/02/14/Starting-Kafka-Streams/">Kafka Streamsの始め方</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2020/02/14/Starting-Kafka-Streams/">
            <time datetime="2020-02-14T05:56:58.000Z" itemprop="datePublished">2020-02-14</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Kafka/">Kafka</a>, <a class="tag-link" href="/memo-blog/tags/Kafka-Streams/">Kafka Streams</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#はじめに読む文献" id="toc-はじめに読む文献">はじめに読む文献</a></li>
<li><a href="#レファレンスとして使う文献" id="toc-レファレンスとして使う文献">レファレンスとして使う文献</a></li>
<li><a href="#環境準備" id="toc-環境準備">環境準備</a></li>
<li><a href="#プロジェクト作成" id="toc-プロジェクト作成">プロジェクト作成</a>
<ul>
<li><a href="#wordcountpipe.java" id="toc-wordcountpipe.java">wordcount/Pipe.java</a></li>
<li><a href="#wordcountlinesplit.java" id="toc-wordcountlinesplit.java">wordcount/LineSplit.java</a></li>
<li><a href="#wordcountwordcount.java" id="toc-wordcountwordcount.java">wordcount/WordCount.java</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">公式ドキュメント</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/tutorial" target="_blank" rel="noopener">公式チュートリアル</a></li>
<li><a href="https://docs.confluent.io/current/installation/index.html" target="_blank" rel="noopener">Confluent
Platformドキュメント</a></li>
<li><a href="https://docs.confluent.io/current/cli/index.html" target="_blank" rel="noopener">Confluent
CLI</a></li>
<li><a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KStream.html#groupBy-org.apache.kafka.streams.kstream.KeyValueMapper-" target="_blank" rel="noopener">公式API説明（groupBy）</a></li>
<li><a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html#count-org.apache.kafka.streams.kstream.Materialized-" target="_blank" rel="noopener">公式API説明（count）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/quickstart#quickstart_streams_process" target="_blank" rel="noopener">公式ドキュメント（Step
5: Process some data）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/" target="_blank" rel="noopener">公式ドキュメント（Developer
Guide）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams DSL）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/processor-api.html#streams-developer-guide-processor-api" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Processor API）</a></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/testing.html" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Test Utils）</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>まとまった情報が無いような気がするので、初心者向けのメモをここに書いておくことにする。</p>
<h2><span id="はじめに読む文献">はじめに読む文献</span></h2>
<ul>
<li><a href="https://kafka.apache.org/24/documentation/streams/tutorial" target="_blank" rel="noopener">公式チュートリアル</a>
<ul>
<li>最初にこのあたりを読み、イメージをつかむのが良い</li>
</ul></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/" target="_blank" rel="noopener">公式ドキュメント（Developer
Guide）</a>
<ul>
<li>つづいて開発者ガイドを読むと良い</li>
</ul></li>
</ul>
<h2><span id="レファレンスとして使う文献">レファレンスとして使う文献</span></h2>
<ul>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams DSL）</a>
<ul>
<li>チュートリアルを終えたあとくらいに使用し始めると良い</li>
</ul></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/processor-api.html#streams-developer-guide-processor-api" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Processor API）</a>
<ul>
<li>Kafka Streams DSLでは対応しきれないときにProcessor
APIを用いるときに使う</li>
</ul></li>
<li><a href="https://kafka.apache.org/24/documentation/streams/developer-guide/testing.html" target="_blank" rel="noopener">公式ドキュメント（Kafka
Streams Test Utils）</a>
<ul>
<li>Kafka Streamsのテスト作るときに使用</li>
</ul></li>
</ul>
<h2><span id="環境準備">環境準備</span></h2>
<p>Apache Kafka、もしくはConfluent
Platformで環境構築しておくことを前提とする。 Apache Kafkaであれば、 <a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">公式ドキュメント</a>
のインストール手順。 Confluent Platformであれば、 <a href="https://docs.confluent.io/current/installation/index.html" target="_blank" rel="noopener">Confluent
Platformドキュメント</a>のインストール手順。</p>
<p>また、Confluent Platformを用いるときは、 <a href="https://docs.confluent.io/current/cli/index.html" target="_blank" rel="noopener">Confluent
CLI</a> をインストールしておくと便利である。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> confluent <span class="built_in">local</span> start</span></span><br></pre></td></tr></table></figure>
<p>だけでKafka関連のサービスを開発用にローカル環境に起動できる。
具体的には、以下のサービスを立ち上げられる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">control-center is [UP]</span><br><span class="line">ksql-server is [UP]</span><br><span class="line">connect is [UP]</span><br><span class="line">kafka-rest is [UP]</span><br><span class="line">schema-registry is [UP]</span><br><span class="line">kafka is [UP]</span><br><span class="line">zookeeper is [UP]</span><br></pre></td></tr></table></figure>
<p>ちなみに、
<code>org.apache.kafka.connect.cli.ConnectDistributed</code>
が意外とメモリを使用するので注意。</p>
<p>また、デフォルトでは <code>/tmp</code>
以下にワーキングディレクトリを作成する。 また実行時には
<code>/tmp/confluent.current</code>
を作成し、その時に使用しているワーキングディレクトリを識別できるようになっている。
tmpwatch等により、ワーキングディレクトリを乱してしまい、
<code>confluent local start</code>
によりKafkaクラスタを起動できなくなったときは、
<code>/tmp/confluent.current</code>
を削除してもう一度起動すると良い。</p>
<p>以降の説明では、Confluent
Platformをインストールしたものとして説明する。</p>
<h2><span id="プロジェクト作成">プロジェクト作成</span></h2>
<p><a href="https://kafka.apache.org/24/documentation/streams/tutorial" target="_blank" rel="noopener">公式チュートリアル</a>
が最初は参考になるはず。</p>
<p>MavenのArchetypeを使い、プロジェクトを生成する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn archetype:generate \</span></span><br><span class="line">    -DarchetypeGroupId=org.apache.kafka \</span><br><span class="line">    -DarchetypeArtifactId=streams-quickstart-java \</span><br><span class="line">    -DarchetypeVersion=2.4.0 \</span><br><span class="line">    -DgroupId=net.dobachi.kafka.streams.examples \</span><br><span class="line">    -DartifactId=firstapp \</span><br><span class="line">    -Dversion=0.1 \</span><br><span class="line">    -Dpackage=wordcount</span><br></pre></td></tr></table></figure>
<p>適宜パッケージ名などを変更して用いること。</p>
<p>雛形に基づいたプロジェクトには、簡単なアプリが含まれている。
最初はこれらを修正しながら、アプリの書き方に慣れるとよい。</p>
<h3><span id="wordcountpipejava">wordcount/Pipe.java</span></h3>
<p>Kafka
Streamsのアプリは通常のJavaアプリと同様に、1プロセスからスタンドアローンで起動する。
ここでは、Pipe.javaの内容を確認しよう。
以下、ポイントとなるソースコードとその説明を並べる。</p>
<p>wordcount/Pipe.java:36</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"streams-pipe"</span>);</span><br><span class="line">props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br></pre></td></tr></table></figure>
<p>メインの中では最初にストリームを作るための設定が定義される。
上記の例では、ストリーム処理アプリの名前、Kafkaクラスタのブートストラップサーバ（つまり、Broker）、
またキーやバリューのデフォルトのシリアライゼーションの仕組みを指定します。
今回はキー・バリューともにStringであることがわかります。</p>
<p>wordcount/Pipe.java:42</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamsBuilder builder = <span class="keyword">new</span> StreamsBuilder();</span><br><span class="line"></span><br><span class="line">builder.stream(<span class="string">"streams-plaintext-input"</span>).to(<span class="string">"streams-pipe-output"</span>);</span><br></pre></td></tr></table></figure>
<p>つづいて、ストリームのビルダをインスタンス化。
このとき、入力・出力トピックを指定する。</p>
<p>wordcount/Pipe.java:46</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> Topology topology = builder.build();</span><br><span class="line"><span class="keyword">final</span> KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(topology, props);</span><br><span class="line"><span class="keyword">final</span> CountDownLatch latch = <span class="keyword">new</span> CountDownLatch(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>ビルダでストリームをビルドし、トポロジを定義する。</p>
<p>wordcount/Pipe.java:46</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// attach shutdown handler to catch control-c</span></span><br><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="string">"streams-shutdown-hook"</span>) &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        streams.close();</span><br><span class="line">        latch.countDown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>シャットダウンフックを定義。</p>
<p>wordcount/Pipe.java:59</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    streams.start();</span><br><span class="line">    latch.await();</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    System.exit(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">System.exit(<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>ストリーム処理を開始。</p>
<p>上記アプリを実行するには、事前に</p>
<ul>
<li>streams-plaintext-input</li>
<li>streams-pipe-output</li>
</ul>
<p>の2種類のトピックを生成しておく。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic streams-plaintext-input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic streams-pipe-output</span></span><br></pre></td></tr></table></figure>
<p>トピックが作られたかどうかは、以下のように確認する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics --list --zookeeper localhost:2181</span></span><br></pre></td></tr></table></figure>
<p>なお、ユーザが明示的に作るトピックの他にも、Kafkaの動作等のために作られるトピックがあるので、
上記コマンドを実行するとずらーっと出力されるはず。</p>
<p>コンパイル、パッケージングする。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies</span></span><br></pre></td></tr></table></figure>
<p>入力ファイルを作成し、入ロトピックに書き込み。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -e <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; /tmp/file-input.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input</span></span><br></pre></td></tr></table></figure>
<p>アプリを実行する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.Pipe</span></span><br></pre></td></tr></table></figure>
<p>別のターミナルを改めて開き、コンソール上に出力トピックの内容を出力する。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning  --property print.key=<span class="literal">true</span> --topic streams-pipe-output</span></span><br></pre></td></tr></table></figure>
<p>以下のような結果が見られるはずである。なお、今回はキーを使用しないアプリだから、左側（キーを表示する場所）には
<code>null</code> が並ぶ。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">null    all streams lead to kafka</span><br><span class="line">null    hello kafka streams</span><br><span class="line">null    join kafka summit</span><br></pre></td></tr></table></figure>
<p>さて、ここでキーを使うようにしてみる。 今回使用したアプリをコピーし、
<code>wordcount/PipeWithKey.java</code> を作る。</p>
<p>ここで変更点は以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">--- src/main/java/wordcount/Pipe.java   2020-02-14 15:23:23.808282200 +0900</span><br><span class="line">+++ src/main/java/wordcount/PipeWithKey.java    2020-02-14 16:54:17.623090500 +0900</span><br><span class="line">@@ -17,10 +17,8 @@</span><br><span class="line"> package wordcount;</span><br><span class="line"></span><br><span class="line"> import org.apache.kafka.common.serialization.Serdes;</span><br><span class="line">-import org.apache.kafka.streams.KafkaStreams;</span><br><span class="line">-import org.apache.kafka.streams.StreamsBuilder;</span><br><span class="line">-import org.apache.kafka.streams.StreamsConfig;</span><br><span class="line">-import org.apache.kafka.streams.Topology;</span><br><span class="line">+import org.apache.kafka.streams.*;</span><br><span class="line">+import org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"></span><br><span class="line"> import java.util.Properties;</span><br><span class="line"> import java.util.concurrent.CountDownLatch;</span><br><span class="line">@@ -30,7 +28,7 @@</span><br><span class="line">  * that reads from a source topic &quot;streams-plaintext-input&quot;, where the values of messages represent lines of text,</span><br><span class="line">  * and writes the messages as-is into a sink topic &quot;streams-pipe-output&quot;.</span><br><span class="line">  */</span><br><span class="line">-public class Pipe &#123;</span><br><span class="line">+public class PipeWithKey &#123;</span><br><span class="line"></span><br><span class="line">     public static void main(String[] args) throws Exception &#123;</span><br><span class="line">         Properties props = new Properties();</span><br><span class="line">@@ -41,7 +39,8 @@</span><br><span class="line"></span><br><span class="line">         final StreamsBuilder builder = new StreamsBuilder();</span><br><span class="line"></span><br><span class="line">-        builder.stream(&quot;streams-plaintext-input&quot;).to(&quot;streams-pipe-output&quot;);</span><br><span class="line">+        KStream&lt;String, String&gt; raw = builder.stream(&quot;streams-plaintext-input&quot;);</span><br><span class="line">+        raw.map((key, value ) -&gt; new KeyValue&lt;&gt;(value.split(&quot; &quot;)[0], value)).to(&quot;streams-pipe-output&quot;);</span><br><span class="line"></span><br><span class="line">         final Topology topology = builder.build();</span><br><span class="line">         final KafkaStreams streams = new KafkaStreams(topology, props);</span><br></pre></td></tr></table></figure>
<p>主な変更は、ストリームビルダから定義されたストリームをいったん、
<code>raw</code> にバインドし、
mapメソッドを使って変換している箇所である。
ここでは、バリューをスペースで区切り、先頭の単語をキーとすることにした。</p>
<p>このアプリをコンパイル、パッケージ化し実行すると、以下のような結果が得られる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.PipeWithKey</span></span><br></pre></td></tr></table></figure>
<p>実行結果の例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all     all streams lead to kafka</span><br><span class="line">hello   hello kafka streams</span><br><span class="line">join    join kafka summit</span><br></pre></td></tr></table></figure>
<h3><span id="wordcountlinesplitjava">wordcount/LineSplit.java</span></h3>
<p>先程作成したPipeWithKeyとほぼ同じ。 実行すると、
<code>streams-linesplit-output</code>
というトピックに結果が出力される。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.LineSplit</span></span><br></pre></td></tr></table></figure>
<p>結果の例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning  --property print.key=<span class="literal">true</span> --topic streams-linesplit-output</span></span><br><span class="line">null    all</span><br><span class="line">null    streams</span><br><span class="line">null    lead</span><br><span class="line">null    to</span><br><span class="line">null    kafka</span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<h3><span id="wordcountwordcountjava">wordcount/WordCount.java</span></h3>
<p>最後にWordCountを確認する。
ほぼ他のアプリと同じだが、ポイントはストリームを加工する定義の部分である。</p>
<p>wordcount/WordCount.java:53</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">builder.&lt;String, String&gt;stream(<span class="string">"streams-plaintext-input"</span>)</span><br><span class="line">       .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(<span class="string">"\\W+"</span>)))</span><br><span class="line">       .groupBy((key, value) -&gt; value)</span><br><span class="line">       .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, <span class="keyword">byte</span>[]&gt;&gt;as(<span class="string">"counts-store"</span>))</span><br><span class="line">       .toStream()</span><br><span class="line">       .to(<span class="string">"streams-wordcount-output"</span>, Produced.with(Serdes.String(), Serdes.Long()));</span><br></pre></td></tr></table></figure>
<p>以下、上記実装を説明する。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">builder.&lt;String, String&gt;stream(<span class="string">"streams-plaintext-input"</span>)</span><br></pre></td></tr></table></figure>
<p>ストリームビルダを利用し、入力トピックからストリームを定義</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(<span class="string">"\\W+"</span>)))</span><br></pre></td></tr></table></figure>
<p>バリューに入っている文字列をスペース等で分割し、配列にする。
合わせて配列をflattenする。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.groupBy((key, value) -&gt; value)</span><br></pre></td></tr></table></figure>
<p>キーバリューから新しいキーを生成し、新しいキーに基づいてグループ化する。
今回の例では、分割されて生成された単語（バリューに入っている）をキーとしてグループ化する。
詳しくは、 <a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KStream.html#groupBy-org.apache.kafka.streams.kstream.KeyValueMapper-" target="_blank" rel="noopener">公式API説明（groupBy）</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, <span class="keyword">byte</span>[]&gt;&gt;as(<span class="string">"counts-store"</span>))</span><br></pre></td></tr></table></figure>
<p>groupByにより生成された <code>KGroupedStream</code> の
<code>count</code> メソッドを呼び出し、 キーごとの合計値を求める。
今回はキーはString型であり、合計値はLong型。
また集計結果を保持するストアは <code>counts-store</code>
という名前とする。 詳しくは、 <a href="https://kafka.apache.org/24/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html#count-org.apache.kafka.streams.kstream.Materialized-" target="_blank" rel="noopener">公式API説明（count）</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.toStream()</span><br><span class="line">.to(<span class="string">"streams-wordcount-output"</span>, Produced.with(Serdes.String(), Serdes.Long()));</span><br></pre></td></tr></table></figure>
<p><code>count</code> の結果は <code>KTable</code>
になるので、これをストリームに変換し、出力先トピックを指定する。</p>
<p>実行してみる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.WordCount</span></span><br></pre></td></tr></table></figure>
<p>別のターミナルを改めて立ち上げ、入力トピックに書き込む。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input</span></span><br></pre></td></tr></table></figure>
<p>出力は以下のようになる。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning  --property print.key=<span class="literal">true</span> --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --topic streams-wordcount-output</span></span><br><span class="line">all     19</span><br><span class="line">lead    19</span><br><span class="line">to      19</span><br><span class="line">hello   19</span><br><span class="line">streams 38</span><br><span class="line">join    19</span><br><span class="line">kafka   57</span><br><span class="line">summit  19</span><br></pre></td></tr></table></figure>
<p>なお、ここでは <code>kafka-console-consumer</code> にプロパティ
<code>value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</code>
を渡した。 アプリケーションでは集計した値はLong型だったためである。
詳しくは、 <a href="https://kafka.apache.org/24/documentation/streams/quickstart#quickstart_streams_process" target="_blank" rel="noopener">公式ドキュメント（Step
5: Process some data）</a> 参照。</p>
<p>なお、指定しない場合は入力されたバイト列がそのまま標準出力に渡されるようになっている。
その結果、期待する出力が得られないことになるので注意。</p>
<p>kafka/tools/ConsoleConsumer.scala:512</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(deserializer: <span class="type">Option</span>[<span class="type">Deserializer</span>[_]], sourceBytes: <span class="type">Array</span>[<span class="type">Byte</span>], topic: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> nonNullBytes = <span class="type">Option</span>(sourceBytes).getOrElse(<span class="string">"null"</span>.getBytes(<span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>))</span><br><span class="line">  <span class="keyword">val</span> convertedBytes = deserializer.map(_.deserialize(topic, nonNullBytes).toString.</span><br><span class="line">    getBytes(<span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>)).getOrElse(nonNullBytes)</span><br><span class="line">  output.write(convertedBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、別の方法として <code>WordCount</code>
の実装を修正する方法がある。以下、参考までに修正方法を紹介する。</p>
<p>想定と異なる表示だが、これは今回バリューの方にLongを用いたため。
kafka-console-consumer
で表示させるために以下のように実装を修正する。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@@ -50,12 +44,15 @@ public class WordCount &#123;</span><br><span class="line"></span><br><span class="line">         final StreamsBuilder builder = new StreamsBuilder();</span><br><span class="line"></span><br><span class="line"><span class="deletion">-        builder.&lt;String, String&gt;stream("streams-plaintext-input")</span></span><br><span class="line"><span class="addition">+        KStream&lt;String, Long&gt; wordCount = builder.&lt;String, String&gt;stream("streams-plaintext-input")</span></span><br><span class="line">                .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split("\\W+")))</span><br><span class="line">                .groupBy((key, value) -&gt; value)</span><br><span class="line"><span class="deletion">-               .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as("counts-store"))</span></span><br><span class="line"><span class="deletion">-               .toStream()</span></span><br><span class="line"><span class="deletion">-               .to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));</span></span><br><span class="line"><span class="addition">+               .count(Materialized.as("counts-store"))</span></span><br><span class="line"><span class="addition">+               .toStream();</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        wordCount.foreach((key, value) -&gt; System.out.println("key: " + key + ", value: " + value));</span></span><br><span class="line"><span class="addition">+</span></span><br><span class="line"><span class="addition">+        wordCount.map((key, value) -&gt; new KeyValue&lt;&gt;(key, String.valueOf(value))).to("streams-wordcount-output", Produced.with(Serdes.String(), Serdes.String()));</span></span><br></pre></td></tr></table></figure>
<p>つまり、もともと <code>to</code>
で終えていたところを、いったん変数にバインドし、 <code>foreach</code>
を使ってストリームの内容を標準出力に表示させるようにしている。 また、
<code>map</code>
メソッドを利用し、バリューの型をLongからStringに変換してから
<code>to</code> で書き出すようにしている。</p>
<p>上記修正を加えた上で、改めてパッケージ化して実行したところ、以下のような表示が得られる。</p>
<p>kafka-console-consumer での表示例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">all     9</span><br><span class="line">lead    9</span><br><span class="line">to      9</span><br><span class="line">hello   9</span><br><span class="line">streams 18</span><br><span class="line">join    9</span><br><span class="line">kafka   27</span><br><span class="line">summit  9</span><br></pre></td></tr></table></figure>
<p>ストリーム処理アプリの標準出力例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">key: all, value: 9</span><br><span class="line">key: lead, value: 9</span><br><span class="line">key: to, value: 9</span><br><span class="line">key: hello, value: 9</span><br><span class="line">key: streams, value: 18</span><br><span class="line">key: join, value: 9</span><br><span class="line">key: kafka, value: 27</span><br><span class="line">key: summit, value: 9</span><br></pre></td></tr></table></figure>
<p>無事に表示できたことが確かめられただろうか。</p>
<!-- vim: set et tw=0 ts=2 sw=2: -->

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2020/02/14/Starting-Kafka-Streams/" data-id="cm0jr5kv401cu18qb1aafmv7k" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/memo-blog/page/5/">&laquo; 前</a><a class="page-number" href="/memo-blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/memo-blog/page/4/">4</a><a class="page-number" href="/memo-blog/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/memo-blog/page/7/">7</a><a class="page-number" href="/memo-blog/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/memo-blog/page/22/">22</a><a class="extend next" rel="next" href="/memo-blog/page/7/">次 &raquo;</a>
    </nav>
</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">最近の記事</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a></p>
                            <p class="item-title"><a href="/memo-blog/2024/09/01/Data-Sovereignty/" class="title">Data Sovereignty</a></p>
                            <p class="item-date"><time datetime="2024-09-01T12:40:49.000Z" itemprop="datePublished">2024-09-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Ansible/">Ansible</a></p>
                            <p class="item-title"><a href="/memo-blog/2024/08/25/Use-HOME-in-case-of-using-sudo-ansible/" class="title">Use HOME in case of using sudo [ansible]</a></p>
                            <p class="item-date"><time datetime="2024-08-25T12:30:22.000Z" itemprop="datePublished">2024-08-25</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Dataspace-Connector/">Dataspace Connector</a></p>
                            <p class="item-title"><a href="/memo-blog/2024/07/14/Management-Domain/" class="title">Management Domain of EDC</a></p>
                            <p class="item-date"><time datetime="2024-07-14T03:09:34.000Z" itemprop="datePublished">2024-07-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Dataspace-Connector/">Dataspace Connector</a></p>
                            <p class="item-title"><a href="/memo-blog/2024/07/07/MVD-of-EDC/" class="title">MVD_of_EDC</a></p>
                            <p class="item-date"><time datetime="2024-07-06T16:16:32.000Z" itemprop="datePublished">2024-07-07</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Dataspace-Connector/">Dataspace Connector</a></p>
                            <p class="item-title"><a href="/memo-blog/2024/02/24/EDCSample/" class="title">EDCSample</a></p>
                            <p class="item-date"><time datetime="2024-02-24T13:15:17.000Z" itemprop="datePublished">2024-02-24</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">カテゴリ</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/">Clipping</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/AI/">AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Camera/">Camera</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Camera/Lighting/">Lighting</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Cloud/">Cloud</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Database/">Database</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Kafka/">Kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/List/">List</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/List/Research/">Research</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Management/">Management</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/PostgreSQL/">PostgreSQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Stream-Processing/">Stream Processing</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Uber/">Uber</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Vim/">Vim</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Windows-Tools/">Windows Tools</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/">Home server</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/File-server/">File server</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Nature-Remo/">Nature Remo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Network/">Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Remote-desktop/">Remote desktop</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/">Ubuntu</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/Adobe-Reader/">Adobe Reader</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/Gnome/">Gnome</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/KVM/">KVM</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/OneDrive/">OneDrive</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/vim/">vim</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Video-processing/">Video processing</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><span class="category-list-count">164</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Alluxio/">Alluxio</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Ansible/">Ansible</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/BaaS/">BaaS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Configuration-Management/">Configuration Management</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Configuration-Management/Ansible/">Ansible</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Catalog/">Data Catalog</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Catalog/CKAN/">CKAN</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/">Data Collaboration</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/">Delta Sharing</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/X-Road/">X-Road</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/">Data Engineering</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Lineage/">Data Lineage</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Transformation/">Data Transformation</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Mesh/">Data Mesh</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/">Data Processing Engine</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/EDC/">EDC</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/IDS/">IDS</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Dataspace-Connector/">Dataspace Connector</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Dataspace-Connector/Eclipse-Dataspace-Components/">Eclipse Dataspace Components</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Documentation/">Documentation</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Flask/">Flask</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/">GPD Pocket</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/">Device</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/Bluetooth/">Bluetooth</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/HBase/">HBase</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/">Hadoop</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/Ambari/">Ambari</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/BigTop/">BigTop</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/HDP/">HDP</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hexo/">Hexo</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Home-Network/">Home Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper/">Hyper</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper/Plugin/">Plugin</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper-V/">Hyper-V</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/">Keyboard</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/Corne-Chocolate/">Corne Chocolate</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/QMK/">QMK</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Kubernetes/">Kubernetes</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><span class="category-list-count">25</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Analytics-Zoo/">Analytics Zoo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/AutoML/">AutoML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Data-Platform/">Data Platform</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Flow-Engine/">Flow Engine</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/MLflow/">MLflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/">Model</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Cross-Validation/">Cross Validation</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Data-Leakage/">Data Leakage</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Partial-Dependency-Plot/">Partial Dependency Plot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/XGBoost/">XGBoost</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/">Model Management</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/Clipper/">Clipper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/OpML/">OpML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Preparation/">Preparation</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Software-Engineering-Patterns/">Software Engineering Patterns</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Stream-Processing/">Stream Processing</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/">Visualization</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/Seaborn/">Seaborn</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Word2Vec/">Word2Vec</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><span class="category-list-count">15</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a><span class="category-list-count">14</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Metadata-Management/">Metadata Management</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Monitering/">Monitering</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-API/">Open API</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/">Open Data</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/Scraping/">Scraping</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/Tellus/">Tellus</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Pinot/">Pinot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Power-Grid-Data/">Power Grid Data</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/">Python</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/Jupyter/">Jupyter</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/Pipenv/">Pipenv</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/pyenv/">pyenv</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/SDK/">SDK</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/SQLAlchemy/">SQLAlchemy</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Scala/">Scala</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Scala/SBT/">SBT</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Spark/">Spark</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Spark/Spark-Summit/">Spark Summit</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><span class="category-list-count">13</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/">Delta Lake</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Hudi/">Hudi</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Minio/">Minio</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/">Stream Processing</a><span class="category-list-count">9</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Apache-Edgent/">Apache Edgent</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Kappa-Architecture/">Kappa Architecture</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/MillWheel/">MillWheel</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Twitter-Heron/">Twitter Heron</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/">Tools</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Git/">Git</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Intellij/">Intellij</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Selenium/">Selenium</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/tmux/">tmux</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/">WSL</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/CentOS/">CentOS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Docker/">Docker</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Terminal-tool/">Terminal tool</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Vagrant/">Vagrant</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/X-Window/">X Window</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/">Windows</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Ansible/">Ansible</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Hyper-V/">Hyper-V</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Zeppelin/">Zeppelin</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/ZooKeeper/">ZooKeeper</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/vim/">vim</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/">Research</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/AWS/">AWS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Conference/">Conference</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Conference/DevSumi/">DevSumi</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Data-Analytics/">Data Analytics</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Data-Analytics/Tools/">Tools</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/">Machine Learning</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/BigDL/">BigDL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/TensorFlow/">TensorFlow</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/NVM/">NVM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/SX-Aurora-Frovedis/">SX-Aurora/Frovedis</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Trends/">Trends</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Video-processing/">Video processing</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Video-processing/BlazeIt/">BlazeIt</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Visualization/">Visualization</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Visualization/Superset/">Superset</a><span class="category-list-count">1</span></li></ul></li></ul></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">アーカイブ</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2024/09/">9月 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2024/08/">8月 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2024/07/">7月 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2024/02/">2月 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2023/09/">9月 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2023/08/">8月 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/05/">5月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/02/">2月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/01/">1月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/10/">10月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/09/">9月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/08/">8月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/07/">7月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/06/">6月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/05/">5月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/04/">4月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/02/">2月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/01/">1月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/12/">12月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/11/">11月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/10/">10月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/09/">9月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/08/">8月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/07/">7月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/06/">6月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/05/">5月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/04/">4月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/03/">3月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/02/">2月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/01/">1月 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/12/">12月 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/11/">11月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/10/">10月 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/09/">9月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/08/">8月 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/07/">7月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/06/">6月 2019</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/05/">5月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/04/">4月 2019</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/03/">3月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/02/">2月 2019</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/01/">1月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/12/">12月 2018</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/11/">11月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/10/">10月 2018</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/09/">9月 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">タグ</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AI/">AI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AWS/">AWS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Academia/">Academia</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Adobe-Reader/">Adobe Reader</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Alluxio/">Alluxio</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ambari/">Ambari</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Analytics-Zoo/">Analytics Zoo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ansible/">Ansible</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Edgent/">Apache Edgent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Hudi/">Apache Hudi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Kafka/">Apache Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Spark/">Apache Spark</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apple/">Apple</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AutoML/">AutoML</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Automagica/">Automagica</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Autonomous-Database/">Autonomous Database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BaaS/">BaaS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Behavioral-Economics/">Behavioral Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Big-Data/">Big Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BigDL/">BigDL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BigTop/">BigTop</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BlazeIt/">BlazeIt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Blog/">Blog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Bluetooth/">Bluetooth</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CDC/">CDC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Camera/">Camera</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CentOS/">CentOS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CentOS7/">CentOS7</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CircleCI/">CircleCI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Clipper/">Clipper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Clipping/">Clipping</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Cloud/">Cloud</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Comcast/">Comcast</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Computing-resource/">Computing resource</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Conference/">Conference</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Configuration-Management/">Configuration Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Corne-Chocolate/">Corne Chocolate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Cross-Validation/">Cross Validation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/DB/">DB</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dask/">Dask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Analysis/">Data Analysis</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Analytics/">Data Analytics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Lake/">Data Lake</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Leakage/">Data Leakage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Lineage/">Data Lineage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Mesh/">Data Mesh</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Platform/">Data Platform</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Processing-Engine/">Data Processing Engine</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Spaces/">Data Spaces</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Transformation/">Data Transformation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dataspace-Connector/">Dataspace Connector</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dataspace-Protocol/">Dataspace Protocol</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Delta-Lake/">Delta Lake</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Delta-Sharing/">Delta Sharing</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/DevSumi/">DevSumi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Docker/">Docker</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dockerfile/">Dockerfile</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Druid/">Druid</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/EDC/">EDC</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Flask/">Flask</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Flow-Engine/">Flow Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Frovedis/">Frovedis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GIS/">GIS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GNOME/">GNOME</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GPD-Pocket/">GPD Pocket</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Git/">Git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GitHub-Actions/">GitHub Actions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Google/">Google</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Goverment/">Goverment</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Graceful-Shutdown/">Graceful Shutdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Gradle/">Gradle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HBase/">HBase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HDP/">HDP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HPC/">HPC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hadoop/">Hadoop</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hexo/">Hexo</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hexo-Plugin/">Hexo Plugin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hyper/">Hyper</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hyper-V/">Hyper-V</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IDS/">IDS</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IEEE/">IEEE</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IPv6/">IPv6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Icarus/">Icarus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Incident/">Incident</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Intel/">Intel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Intellij/">Intellij</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/JSON/">JSON</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Jupyter/">Jupyter</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/KVM/">KVM</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafak-Connect/">Kafak Connect</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka/">Kafka</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Connect/">Kafka Connect</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Streams/">Kafka Streams</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Summit/">Kafka Summit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kaggle/">Kaggle</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kappa-Architecture/">Kappa Architecture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Keyboard/">Keyboard</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kinesis/">Kinesis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kubernetes/">Kubernetes</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Lambda-Architecture/">Lambda Architecture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Lighthing/">Lighthing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/LinkedIn/">LinkedIn</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MATE-Desktop/">MATE Desktop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ML-Model-Management/">ML Model Management</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ML-Ops/">ML Ops</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MLflow/">MLflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning-Lifecycle/">Machine Learning Lifecycle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning-Lifecycle/">Machine Learning Lifecycle/</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Management/">Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Map/">Map</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Markdown/">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Messaging-System/">Messaging System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Metadata-Management/">Metadata Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MillWheel/">MillWheel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Minikube/">Minikube</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Minio/">Minio</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Model/">Model</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Model-Management/">Model Management</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Monitering/">Monitering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Mouse/">Mouse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/NERDTree/">NERDTree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/NVM/">NVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Nature-Remo/">Nature Remo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Network/">Network</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OLAP/">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OneDrive/">OneDrive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpML/">OpML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-API/">Open API</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-Data/">Open Data</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-Messaging-Benchmark/">Open Messaging Benchmark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpenAPI/">OpenAPI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpenML/">OpenML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Oracle/">Oracle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PAPIDS/">PAPIDS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PDF/">PDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pandoc/">Pandoc</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Paper/">Paper</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Parquet/">Parquet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Partial-Dependency-Plot/">Partial Dependency Plot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pinot/">Pinot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pipenv/">Pipenv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PostgreSQL/">PostgreSQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Power-Grid-Data/">Power Grid Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PowerShell/">PowerShell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Preparation/">Preparation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Profiler/">Profiler</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pulsar/">Pulsar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PySpark/">PySpark</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Python/">Python</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Python3/">Python3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/QMK/">QMK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Query-Engine/">Query Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/RDBMS/">RDBMS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/RPA/">RPA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Redshift/">Redshift</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Research-later/">Research later</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/S3/">S3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SBT/">SBT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SDK/">SDK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SQLAlchemy/">SQLAlchemy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SQLite/">SQLite</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SX-Aurora/">SX-Aurora</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Samba/">Samba</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Scala/">Scala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Scraping/">Scraping</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Security/">Security</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Session/">Session</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Slenium/">Slenium</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Smart-Home/">Smart Home</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Snowflake/">Snowflake</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Software-Engineering-Patterns/">Software Engineering Patterns</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Spark/">Spark</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Spark-Summit/">Spark Summit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Sphinx/">Sphinx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Statistic/">Statistic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Stonebraker/">Stonebraker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage/">Storage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Engine/">Storage Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Layer-Software/">Storage Layer Software</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Stream-Processing/">Stream Processing</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Superset/">Superset</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Supervision/">Supervision</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Tellus/">Tellus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/TensorFlow/">TensorFlow</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/TensorFlowOnSpark/">TensorFlowOnSpark</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Tools/">Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Trends/">Trends</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Troubleshoot/">Troubleshoot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Twitter/">Twitter</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Twitter-Heron/">Twitter Heron</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Uber/">Uber</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/VMWare/">VMWare</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vagrant/">Vagrant</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vault/">Vault</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vector-Engine/">Vector Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Video-Processing/">Video Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vim/">Vim</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Visualization/">Visualization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WSL/">WSL</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Web/">Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WhereHows/">WhereHows</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WiFi/">WiFi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WiFi6/">WiFi6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Windows/">Windows</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Windows-Tools/">Windows Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Word/">Word</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Word2Vec/">Word2Vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/X-Window/">X Window</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/X-Road/">X-Road</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/XGBoost/">XGBoost</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Zeppelin/">Zeppelin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ZooKeeper/">ZooKeeper</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ansible/">ansible</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/dein/">dein</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/dstat/">dstat</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/fsync/">fsync</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/git/">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/keyboard/">keyboard</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/libvirt/">libvirt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/markdown/">markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/nltk/">nltk</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/pandoc/">pandoc</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/pyenv/">pyenv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/tmux/">tmux</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/vim/">vim</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/windows/">windows</a><span class="tag-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">タグクラウド</h3>
        <div class="widget tagcloud">
            <a href="/memo-blog/tags/AI/" style="font-size: 10px;">AI</a> <a href="/memo-blog/tags/AWS/" style="font-size: 10px;">AWS</a> <a href="/memo-blog/tags/Academia/" style="font-size: 10px;">Academia</a> <a href="/memo-blog/tags/Adobe-Reader/" style="font-size: 10px;">Adobe Reader</a> <a href="/memo-blog/tags/Alluxio/" style="font-size: 10.71px;">Alluxio</a> <a href="/memo-blog/tags/Ambari/" style="font-size: 10px;">Ambari</a> <a href="/memo-blog/tags/Analytics-Zoo/" style="font-size: 10px;">Analytics Zoo</a> <a href="/memo-blog/tags/Ansible/" style="font-size: 11.43px;">Ansible</a> <a href="/memo-blog/tags/Apache-Edgent/" style="font-size: 10px;">Apache Edgent</a> <a href="/memo-blog/tags/Apache-Hudi/" style="font-size: 10px;">Apache Hudi</a> <a href="/memo-blog/tags/Apache-Kafka/" style="font-size: 10px;">Apache Kafka</a> <a href="/memo-blog/tags/Apache-Spark/" style="font-size: 11.43px;">Apache Spark</a> <a href="/memo-blog/tags/Apple/" style="font-size: 10px;">Apple</a> <a href="/memo-blog/tags/AutoML/" style="font-size: 10.71px;">AutoML</a> <a href="/memo-blog/tags/Automagica/" style="font-size: 10px;">Automagica</a> <a href="/memo-blog/tags/Autonomous-Database/" style="font-size: 10px;">Autonomous Database</a> <a href="/memo-blog/tags/BaaS/" style="font-size: 10px;">BaaS</a> <a href="/memo-blog/tags/Behavioral-Economics/" style="font-size: 10px;">Behavioral Economics</a> <a href="/memo-blog/tags/Big-Data/" style="font-size: 10px;">Big Data</a> <a href="/memo-blog/tags/BigDL/" style="font-size: 10.71px;">BigDL</a> <a href="/memo-blog/tags/BigTop/" style="font-size: 10.71px;">BigTop</a> <a href="/memo-blog/tags/BlazeIt/" style="font-size: 10px;">BlazeIt</a> <a href="/memo-blog/tags/Blog/" style="font-size: 10px;">Blog</a> <a href="/memo-blog/tags/Bluetooth/" style="font-size: 10px;">Bluetooth</a> <a href="/memo-blog/tags/CDC/" style="font-size: 10px;">CDC</a> <a href="/memo-blog/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/memo-blog/tags/CentOS/" style="font-size: 10px;">CentOS</a> <a href="/memo-blog/tags/CentOS7/" style="font-size: 11.43px;">CentOS7</a> <a href="/memo-blog/tags/CircleCI/" style="font-size: 10px;">CircleCI</a> <a href="/memo-blog/tags/Clipper/" style="font-size: 10px;">Clipper</a> <a href="/memo-blog/tags/Clipping/" style="font-size: 10px;">Clipping</a> <a href="/memo-blog/tags/Cloud/" style="font-size: 10px;">Cloud</a> <a href="/memo-blog/tags/Comcast/" style="font-size: 10px;">Comcast</a> <a href="/memo-blog/tags/Computing-resource/" style="font-size: 10px;">Computing resource</a> <a href="/memo-blog/tags/Conference/" style="font-size: 10px;">Conference</a> <a href="/memo-blog/tags/Configuration-Management/" style="font-size: 10px;">Configuration Management</a> <a href="/memo-blog/tags/Corne-Chocolate/" style="font-size: 10px;">Corne Chocolate</a> <a href="/memo-blog/tags/Cross-Validation/" style="font-size: 10px;">Cross Validation</a> <a href="/memo-blog/tags/DB/" style="font-size: 10px;">DB</a> <a href="/memo-blog/tags/Dask/" style="font-size: 10.71px;">Dask</a> <a href="/memo-blog/tags/Data-Analysis/" style="font-size: 10.71px;">Data Analysis</a> <a href="/memo-blog/tags/Data-Analytics/" style="font-size: 10px;">Data Analytics</a> <a href="/memo-blog/tags/Data-Lake/" style="font-size: 10px;">Data Lake</a> <a href="/memo-blog/tags/Data-Leakage/" style="font-size: 10px;">Data Leakage</a> <a href="/memo-blog/tags/Data-Lineage/" style="font-size: 10px;">Data Lineage</a> <a href="/memo-blog/tags/Data-Mesh/" style="font-size: 10px;">Data Mesh</a> <a href="/memo-blog/tags/Data-Platform/" style="font-size: 10px;">Data Platform</a> <a href="/memo-blog/tags/Data-Processing-Engine/" style="font-size: 10.71px;">Data Processing Engine</a> <a href="/memo-blog/tags/Data-Spaces/" style="font-size: 12.86px;">Data Spaces</a> <a href="/memo-blog/tags/Data-Transformation/" style="font-size: 10.71px;">Data Transformation</a> <a href="/memo-blog/tags/Dataspace-Connector/" style="font-size: 11.43px;">Dataspace Connector</a> <a href="/memo-blog/tags/Dataspace-Protocol/" style="font-size: 10px;">Dataspace Protocol</a> <a href="/memo-blog/tags/Delta-Lake/" style="font-size: 17.14px;">Delta Lake</a> <a href="/memo-blog/tags/Delta-Sharing/" style="font-size: 12.14px;">Delta Sharing</a> <a href="/memo-blog/tags/DevSumi/" style="font-size: 10px;">DevSumi</a> <a href="/memo-blog/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/memo-blog/tags/Dockerfile/" style="font-size: 10.71px;">Dockerfile</a> <a href="/memo-blog/tags/Druid/" style="font-size: 10px;">Druid</a> <a href="/memo-blog/tags/EDC/" style="font-size: 13.57px;">EDC</a> <a href="/memo-blog/tags/Flask/" style="font-size: 12.14px;">Flask</a> <a href="/memo-blog/tags/Flow-Engine/" style="font-size: 10px;">Flow Engine</a> <a href="/memo-blog/tags/Frovedis/" style="font-size: 10px;">Frovedis</a> <a href="/memo-blog/tags/GIS/" style="font-size: 10px;">GIS</a> <a href="/memo-blog/tags/GNOME/" style="font-size: 10px;">GNOME</a> <a href="/memo-blog/tags/GPD-Pocket/" style="font-size: 10px;">GPD Pocket</a> <a href="/memo-blog/tags/Git/" style="font-size: 10.71px;">Git</a> <a href="/memo-blog/tags/GitHub-Actions/" style="font-size: 10px;">GitHub Actions</a> <a href="/memo-blog/tags/Google/" style="font-size: 10px;">Google</a> <a href="/memo-blog/tags/Goverment/" style="font-size: 10px;">Goverment</a> <a href="/memo-blog/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/memo-blog/tags/Gradle/" style="font-size: 10px;">Gradle</a> <a href="/memo-blog/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/memo-blog/tags/HDP/" style="font-size: 10px;">HDP</a> <a href="/memo-blog/tags/HPC/" style="font-size: 10px;">HPC</a> <a href="/memo-blog/tags/Hadoop/" style="font-size: 11.43px;">Hadoop</a> <a href="/memo-blog/tags/Hexo/" style="font-size: 16.43px;">Hexo</a> <a href="/memo-blog/tags/Hexo-Plugin/" style="font-size: 10px;">Hexo Plugin</a> <a href="/memo-blog/tags/Hyper/" style="font-size: 10.71px;">Hyper</a> <a href="/memo-blog/tags/Hyper-V/" style="font-size: 10.71px;">Hyper-V</a> <a href="/memo-blog/tags/IDS/" style="font-size: 12.14px;">IDS</a> <a href="/memo-blog/tags/IEEE/" style="font-size: 10.71px;">IEEE</a> <a href="/memo-blog/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/memo-blog/tags/Icarus/" style="font-size: 10px;">Icarus</a> <a href="/memo-blog/tags/Incident/" style="font-size: 10px;">Incident</a> <a href="/memo-blog/tags/Intel/" style="font-size: 10px;">Intel</a> <a href="/memo-blog/tags/Intellij/" style="font-size: 10.71px;">Intellij</a> <a href="/memo-blog/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/memo-blog/tags/Java/" style="font-size: 10px;">Java</a> <a href="/memo-blog/tags/Jupyter/" style="font-size: 10.71px;">Jupyter</a> <a href="/memo-blog/tags/KVM/" style="font-size: 11.43px;">KVM</a> <a href="/memo-blog/tags/Kafak-Connect/" style="font-size: 10px;">Kafak Connect</a> <a href="/memo-blog/tags/Kafka/" style="font-size: 19.29px;">Kafka</a> <a href="/memo-blog/tags/Kafka-Connect/" style="font-size: 10px;">Kafka Connect</a> <a href="/memo-blog/tags/Kafka-Streams/" style="font-size: 10.71px;">Kafka Streams</a> <a href="/memo-blog/tags/Kafka-Summit/" style="font-size: 10px;">Kafka Summit</a> <a href="/memo-blog/tags/Kaggle/" style="font-size: 15px;">Kaggle</a> <a href="/memo-blog/tags/Kappa-Architecture/" style="font-size: 10px;">Kappa Architecture</a> <a href="/memo-blog/tags/Keyboard/" style="font-size: 10.71px;">Keyboard</a> <a href="/memo-blog/tags/Kinesis/" style="font-size: 10px;">Kinesis</a> <a href="/memo-blog/tags/Kubernetes/" style="font-size: 11.43px;">Kubernetes</a> <a href="/memo-blog/tags/Lambda-Architecture/" style="font-size: 10px;">Lambda Architecture</a> <a href="/memo-blog/tags/Lighthing/" style="font-size: 10px;">Lighthing</a> <a href="/memo-blog/tags/LinkedIn/" style="font-size: 12.14px;">LinkedIn</a> <a href="/memo-blog/tags/MATE-Desktop/" style="font-size: 10px;">MATE Desktop</a> <a href="/memo-blog/tags/ML-Model-Management/" style="font-size: 10.71px;">ML Model Management</a> <a href="/memo-blog/tags/ML-Ops/" style="font-size: 10px;">ML Ops</a> <a href="/memo-blog/tags/MLflow/" style="font-size: 10px;">MLflow</a> <a href="/memo-blog/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/memo-blog/tags/Machine-Learning-Lifecycle/" style="font-size: 10px;">Machine Learning Lifecycle</a> <a href="/memo-blog/tags/Machine-Learning-Lifecycle/" style="font-size: 10px;">Machine Learning Lifecycle/</a> <a href="/memo-blog/tags/Management/" style="font-size: 10px;">Management</a> <a href="/memo-blog/tags/Map/" style="font-size: 10px;">Map</a> <a href="/memo-blog/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/memo-blog/tags/Messaging-System/" style="font-size: 10px;">Messaging System</a> <a href="/memo-blog/tags/Metadata-Management/" style="font-size: 10px;">Metadata Management</a> <a href="/memo-blog/tags/MillWheel/" style="font-size: 10px;">MillWheel</a> <a href="/memo-blog/tags/Minikube/" style="font-size: 10px;">Minikube</a> <a href="/memo-blog/tags/Minio/" style="font-size: 10.71px;">Minio</a> <a href="/memo-blog/tags/Model/" style="font-size: 12.14px;">Model</a> <a href="/memo-blog/tags/Model-Management/" style="font-size: 12.14px;">Model Management</a> <a href="/memo-blog/tags/Monitering/" style="font-size: 10px;">Monitering</a> <a href="/memo-blog/tags/Mouse/" style="font-size: 10px;">Mouse</a> <a href="/memo-blog/tags/NERDTree/" style="font-size: 10px;">NERDTree</a> <a href="/memo-blog/tags/NVM/" style="font-size: 10px;">NVM</a> <a href="/memo-blog/tags/Nature-Remo/" style="font-size: 10px;">Nature Remo</a> <a href="/memo-blog/tags/Network/" style="font-size: 10px;">Network</a> <a href="/memo-blog/tags/OLAP/" style="font-size: 10px;">OLAP</a> <a href="/memo-blog/tags/OneDrive/" style="font-size: 10px;">OneDrive</a> <a href="/memo-blog/tags/OpML/" style="font-size: 10px;">OpML</a> <a href="/memo-blog/tags/Open-API/" style="font-size: 10px;">Open API</a> <a href="/memo-blog/tags/Open-Data/" style="font-size: 10.71px;">Open Data</a> <a href="/memo-blog/tags/Open-Messaging-Benchmark/" style="font-size: 10px;">Open Messaging Benchmark</a> <a href="/memo-blog/tags/OpenAPI/" style="font-size: 10px;">OpenAPI</a> <a href="/memo-blog/tags/OpenML/" style="font-size: 10px;">OpenML</a> <a href="/memo-blog/tags/Oracle/" style="font-size: 10px;">Oracle</a> <a href="/memo-blog/tags/PAPIDS/" style="font-size: 10px;">PAPIDS</a> <a href="/memo-blog/tags/PDF/" style="font-size: 10px;">PDF</a> <a href="/memo-blog/tags/Pandoc/" style="font-size: 10px;">Pandoc</a> <a href="/memo-blog/tags/Paper/" style="font-size: 18.57px;">Paper</a> <a href="/memo-blog/tags/Parquet/" style="font-size: 10px;">Parquet</a> <a href="/memo-blog/tags/Partial-Dependency-Plot/" style="font-size: 10px;">Partial Dependency Plot</a> <a href="/memo-blog/tags/Pinot/" style="font-size: 10px;">Pinot</a> <a href="/memo-blog/tags/Pipenv/" style="font-size: 10px;">Pipenv</a> <a href="/memo-blog/tags/PostgreSQL/" style="font-size: 10px;">PostgreSQL</a> <a href="/memo-blog/tags/Power-Grid-Data/" style="font-size: 10px;">Power Grid Data</a> <a href="/memo-blog/tags/PowerShell/" style="font-size: 10px;">PowerShell</a> <a href="/memo-blog/tags/Preparation/" style="font-size: 10.71px;">Preparation</a> <a href="/memo-blog/tags/Profiler/" style="font-size: 10px;">Profiler</a> <a href="/memo-blog/tags/Pulsar/" style="font-size: 10px;">Pulsar</a> <a href="/memo-blog/tags/PySpark/" style="font-size: 12.14px;">PySpark</a> <a href="/memo-blog/tags/Python/" style="font-size: 17.86px;">Python</a> <a href="/memo-blog/tags/Python3/" style="font-size: 10px;">Python3</a> <a href="/memo-blog/tags/QMK/" style="font-size: 10px;">QMK</a> <a href="/memo-blog/tags/Query-Engine/" style="font-size: 10px;">Query Engine</a> <a href="/memo-blog/tags/RDBMS/" style="font-size: 10.71px;">RDBMS</a> <a href="/memo-blog/tags/RPA/" style="font-size: 10px;">RPA</a> <a href="/memo-blog/tags/Redshift/" style="font-size: 10px;">Redshift</a> <a href="/memo-blog/tags/Research-later/" style="font-size: 10px;">Research later</a> <a href="/memo-blog/tags/S3/" style="font-size: 10px;">S3</a> <a href="/memo-blog/tags/SBT/" style="font-size: 10px;">SBT</a> <a href="/memo-blog/tags/SDK/" style="font-size: 10px;">SDK</a> <a href="/memo-blog/tags/SQLAlchemy/" style="font-size: 10.71px;">SQLAlchemy</a> <a href="/memo-blog/tags/SQLite/" style="font-size: 10px;">SQLite</a> <a href="/memo-blog/tags/SX-Aurora/" style="font-size: 10px;">SX-Aurora</a> <a href="/memo-blog/tags/Samba/" style="font-size: 10px;">Samba</a> <a href="/memo-blog/tags/Scala/" style="font-size: 10px;">Scala</a> <a href="/memo-blog/tags/Scraping/" style="font-size: 10px;">Scraping</a> <a href="/memo-blog/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/memo-blog/tags/Security/" style="font-size: 10px;">Security</a> <a href="/memo-blog/tags/Session/" style="font-size: 10px;">Session</a> <a href="/memo-blog/tags/Slenium/" style="font-size: 10px;">Slenium</a> <a href="/memo-blog/tags/Smart-Home/" style="font-size: 10px;">Smart Home</a> <a href="/memo-blog/tags/Snowflake/" style="font-size: 10px;">Snowflake</a> <a href="/memo-blog/tags/Software-Engineering-Patterns/" style="font-size: 10px;">Software Engineering Patterns</a> <a href="/memo-blog/tags/Spark/" style="font-size: 15.71px;">Spark</a> <a href="/memo-blog/tags/Spark-Summit/" style="font-size: 10px;">Spark Summit</a> <a href="/memo-blog/tags/Sphinx/" style="font-size: 10px;">Sphinx</a> <a href="/memo-blog/tags/Statistic/" style="font-size: 10px;">Statistic</a> <a href="/memo-blog/tags/Stonebraker/" style="font-size: 10px;">Stonebraker</a> <a href="/memo-blog/tags/Storage/" style="font-size: 10px;">Storage</a> <a href="/memo-blog/tags/Storage-Engine/" style="font-size: 10px;">Storage Engine</a> <a href="/memo-blog/tags/Storage-Layer/" style="font-size: 11.43px;">Storage Layer</a> <a href="/memo-blog/tags/Storage-Layer-Software/" style="font-size: 10px;">Storage Layer Software</a> <a href="/memo-blog/tags/Stream-Processing/" style="font-size: 18.57px;">Stream Processing</a> <a href="/memo-blog/tags/Superset/" style="font-size: 10px;">Superset</a> <a href="/memo-blog/tags/Supervision/" style="font-size: 10px;">Supervision</a> <a href="/memo-blog/tags/Tellus/" style="font-size: 10px;">Tellus</a> <a href="/memo-blog/tags/TensorFlow/" style="font-size: 11.43px;">TensorFlow</a> <a href="/memo-blog/tags/TensorFlowOnSpark/" style="font-size: 10.71px;">TensorFlowOnSpark</a> <a href="/memo-blog/tags/Tools/" style="font-size: 10px;">Tools</a> <a href="/memo-blog/tags/Trends/" style="font-size: 10px;">Trends</a> <a href="/memo-blog/tags/Troubleshoot/" style="font-size: 10px;">Troubleshoot</a> <a href="/memo-blog/tags/Twitter/" style="font-size: 10px;">Twitter</a> <a href="/memo-blog/tags/Twitter-Heron/" style="font-size: 10px;">Twitter Heron</a> <a href="/memo-blog/tags/Uber/" style="font-size: 10.71px;">Uber</a> <a href="/memo-blog/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/memo-blog/tags/VMWare/" style="font-size: 10px;">VMWare</a> <a href="/memo-blog/tags/Vagrant/" style="font-size: 10.71px;">Vagrant</a> <a href="/memo-blog/tags/Vault/" style="font-size: 10px;">Vault</a> <a href="/memo-blog/tags/Vector-Engine/" style="font-size: 10px;">Vector Engine</a> <a href="/memo-blog/tags/Video-Processing/" style="font-size: 10px;">Video Processing</a> <a href="/memo-blog/tags/Vim/" style="font-size: 10.71px;">Vim</a> <a href="/memo-blog/tags/Visualization/" style="font-size: 10.71px;">Visualization</a> <a href="/memo-blog/tags/WSL/" style="font-size: 14.29px;">WSL</a> <a href="/memo-blog/tags/Web/" style="font-size: 10px;">Web</a> <a href="/memo-blog/tags/WhereHows/" style="font-size: 10px;">WhereHows</a> <a href="/memo-blog/tags/WiFi/" style="font-size: 10px;">WiFi</a> <a href="/memo-blog/tags/WiFi6/" style="font-size: 10px;">WiFi6</a> <a href="/memo-blog/tags/Windows/" style="font-size: 13.57px;">Windows</a> <a href="/memo-blog/tags/Windows-Tools/" style="font-size: 10px;">Windows Tools</a> <a href="/memo-blog/tags/Word/" style="font-size: 10px;">Word</a> <a href="/memo-blog/tags/Word2Vec/" style="font-size: 10px;">Word2Vec</a> <a href="/memo-blog/tags/X-Window/" style="font-size: 10px;">X Window</a> <a href="/memo-blog/tags/X-Road/" style="font-size: 10px;">X-Road</a> <a href="/memo-blog/tags/XGBoost/" style="font-size: 10px;">XGBoost</a> <a href="/memo-blog/tags/Zeppelin/" style="font-size: 10px;">Zeppelin</a> <a href="/memo-blog/tags/ZooKeeper/" style="font-size: 12.14px;">ZooKeeper</a> <a href="/memo-blog/tags/ansible/" style="font-size: 10px;">ansible</a> <a href="/memo-blog/tags/bug/" style="font-size: 10px;">bug</a> <a href="/memo-blog/tags/dein/" style="font-size: 10px;">dein</a> <a href="/memo-blog/tags/dstat/" style="font-size: 10px;">dstat</a> <a href="/memo-blog/tags/fsync/" style="font-size: 10px;">fsync</a> <a href="/memo-blog/tags/git/" style="font-size: 10px;">git</a> <a href="/memo-blog/tags/keyboard/" style="font-size: 10px;">keyboard</a> <a href="/memo-blog/tags/libvirt/" style="font-size: 10px;">libvirt</a> <a href="/memo-blog/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/memo-blog/tags/nltk/" style="font-size: 10px;">nltk</a> <a href="/memo-blog/tags/pandoc/" style="font-size: 11.43px;">pandoc</a> <a href="/memo-blog/tags/pyenv/" style="font-size: 10px;">pyenv</a> <a href="/memo-blog/tags/tmux/" style="font-size: 10.71px;">tmux</a> <a href="/memo-blog/tags/vim/" style="font-size: 14.29px;">vim</a> <a href="/memo-blog/tags/windows/" style="font-size: 10px;">windows</a>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">リンク</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fas fa-angle-up"></div>
</aside>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2024 dobachi<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
        


    
        <script src="/memo-blog/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/memo-blog/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    



<!-- Custom Scripts -->
<script src="/memo-blog/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>