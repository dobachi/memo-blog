<!DOCTYPE html>
<html lang=ja>
<head>
    <meta charset="utf-8">
    
    <title>memo-blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="This is just a memo">
<meta property="og:type" content="website">
<meta property="og:title" content="memo-blog">
<meta property="og:url" content="https://dobachi.github.io/memo-blog/page/13/index.html">
<meta property="og:site_name" content="memo-blog">
<meta property="og:description" content="This is just a memo">
<meta property="og:locale" content="ja">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="memo-blog">
<meta name="twitter:description" content="This is just a memo">
    

    
        <link rel="alternate" href="/" title="memo-blog" type="application/atom+xml" />
    

    

    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fontawesome.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fa-brands.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/font-awesome5/css/fa-solid.min.css">
    <link rel="stylesheet" href="/memo-blog/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/memo-blog/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/memo-blog/css/style.css">

    <script src="/memo-blog/libs/jquery/2.1.3/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/memo-blog/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/memo-blog/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-155235180-1', 'auto');
ga('send', 'pageview');

</script>
    
    
    


</head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/memo-blog/" id="logo">
                <i class="logo"></i>
                <span class="site-title">memo-blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/memo-blog/.">Home</a>
                
                    <a class="main-nav-link" href="/memo-blog/archives">Archives</a>
                
                    <a class="main-nav-link" href="/memo-blog/categories">Categories</a>
                
                    <a class="main-nav-link" href="/memo-blog/tags">Tags</a>
                
                    <a class="main-nav-link" href="/memo-blog/about">About</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/memo-blog/css/images/avatar.png" />
                            <i class="fas fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="検索" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fas fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '投稿',
            PAGES: 'Pages',
            CATEGORIES: 'カテゴリ',
            TAGS: 'タグ',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/memo-blog/',
        CONTENT_URL: '/memo-blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/memo-blog/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/memo-blog/.">Home</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/memo-blog/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="検索" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile" class="profile-fixed">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/memo-blog/css/images/avatar.png" />
            <h2 id="name">dobachi</h2>
            <h3 id="title">man of leisure</h3>
            <span id="location"><i class="fas fa-map-marker-alt" style="padding-right: 5px"></i>Tokyo, Japan</span>
            <a id="follow" target="_blank" href="https://github.com/dobachi">フォローする</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                209
                <span>投稿</span>
            </div>
            <div class="article-info-block">
                229
                <span>タグ</span>
            </div>
        </div>
        
        <div class="profile-block social-links">
            <table>
                <tr>
                    
                    
                    <td>
                        <a href="http://github.com/dobachi" target="_blank" title="github" class=tooltip>
                            <i class="fab fa-github"></i>
                        </a>
                    </td>
                    
                    <td>
                        <a href="/memo-blog/" target="_blank" title="rss" class=tooltip>
                            <i class="fab fa-rss"></i>
                        </a>
                    </td>
                    
                </tr>
            </table>
        </div>
        
    </div>
</aside>

            
            <section id="main">
    <article id="post-Alluxio-Security" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/05/10/Alluxio-Security/">Alluxio Security</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/05/10/Alluxio-Security/">
            <time datetime="2019-05-10T07:07:48.000Z" itemprop="datePublished">2019-05-10</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Alluxio/">Alluxio</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Alluxio/">Alluxio</a>, <a class="tag-link" href="/memo-blog/tags/Security/">Security</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#ユーザ認証" id="toc-ユーザ認証">ユーザ認証</a>
<ul>
<li><a href="#kerberos認証はサポートされていない" id="toc-kerberos認証はサポートされていない">Kerberos認証はサポートされていない</a></li>
<li><a href="#simpleモードの認証" id="toc-simpleモードの認証">SIMPLEモードの認証</a></li>
<li><a href="#customモードの認証" id="toc-customモードの認証">CUSTOMモードの認証</a></li>
</ul></li>
<li><a href="#監査ログ" id="toc-監査ログ">監査ログ</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://docs.alluxio.io/os/user/stable/en/advanced/Security.html" target="_blank" rel="noopener">Alluxio公式ドキュメントのセキュリティに関する説明</a></li>
<li><a href="https://docs.alluxio.io/os/user/1.5/en/Security.html#authentication" target="_blank" rel="noopener">公式ドキュメントのAuthenticationの説明</a></li>
<li><a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/LoginModuleConfiguration.java#L81" target="_blank" rel="noopener">Kerberosがサポートされていない？</a></li>
<li><a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/AlluxioLoginModule.java#L55" target="_blank" rel="noopener">AlluxioのJAAS実装では、login時に必ずTrueを返す？</a></li>
<li><a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/AlluxioLoginModule.java#L84" target="_blank" rel="noopener">commitでも特に認証をしていないように見える？</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p><a href="https://docs.alluxio.io/os/user/stable/en/advanced/Security.html" target="_blank" rel="noopener">Alluxio公式ドキュメントのセキュリティに関する説明</a>
を眺めてみる。</p>
<p>提供機能は以下の通りか。</p>
<ul>
<li>認証</li>
<li>認可
<ul>
<li>POSIXパーミッション相当の認可機能を提供する</li>
</ul></li>
<li>Impersonation
<ul>
<li>システムユーザなどが複数のユーザを演じることが可能</li>
</ul></li>
<li>監査ログの出力</li>
</ul>
<h2><span id="ユーザ認証">ユーザ認証</span></h2>
<p>基本的には、2019/05/10現在ではSIMPLEモードが用いられるようである。
<code>alluxio.security.login.username</code>
プロパティで指定されたユーザ名か、そうでなければOS上のユーザ名がログインユーザ名として用いられる。</p>
<p><a href="https://docs.alluxio.io/os/user/1.5/en/Security.html#authentication" target="_blank" rel="noopener">公式ドキュメントのAuthenticationの説明</a>
によると、</p>
<blockquote>
<p>JAAS (Java Authentication and Authorization Service) is used to
determine who is currently executing the process.</p>
</blockquote>
<p>とのこと。設定をどうするのか、という点はあまり記載されていない。</p>
<h3><span id="kerberos認証はサポートされていない">Kerberos認証はサポートされていない</span></h3>
<p>なお、 <a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/LoginModuleConfiguration.java#L81" target="_blank" rel="noopener">Kerberosがサポートされていない？</a>
を見る限り、2019/05/10現在でKerberosがサポートされていない。</p>
<h3><span id="simpleモードの認証">SIMPLEモードの認証</span></h3>
<p>また、 <a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/AlluxioLoginModule.java#L55" target="_blank" rel="noopener">AlluxioのJAAS実装では、login時に必ずTrueを返す？</a>
を見ると、2019/05/10現在ではユーザ・パスワード認証がloginメソッド内で定義されていないように見える。
また、念のために <a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/AlluxioLoginModule.java#L84" target="_blank" rel="noopener">commitでも特に認証をしていないように見える？</a>
を見ても、commitメソッド内でも何かしら認証しているように見えない？</p>
<p>ただ、 <a href="https://github.com/Alluxio/alluxio/blob/master/core/common/src/main/java/alluxio/security/login/AlluxioLoginModule.java#L86" target="_blank" rel="noopener">mSubject.getPrincipalsしている箇所</a>
があるので、そこを確認したほうが良さそう。</p>
<p>関連する実装を確認する。</p>
<p>そこでまずは、 <code>alluxio.security.LoginUser#login</code>
メソッドを確認する。 当該メソッドではSubjectインスタンスを生成し、
<code>alluxio.security.LoginUser#createLoginContext</code> メソッド
内部で <code>javax.security.auth.login.LoginContext</code>
クラスのインスタンス生成に用いている。</p>
<p>alluxio/security/LoginUser.java:80 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Subject subject = new Subject();</span><br><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">  // Use the class loader of User.class to construct the LoginContext. LoginContext uses this</span><br><span class="line">  // class loader to dynamically instantiate login modules. This enables</span><br><span class="line">  // Subject#getPrincipals to use reflection to search for User.class instances.</span><br><span class="line">  LoginContext loginContext = createLoginContext(authType, subject, User.class.getClassLoader(),</span><br><span class="line">      new LoginModuleConfiguration(), conf);</span><br><span class="line">  loginContext.login();</span><br></pre></td></tr></table></figure></p>
<p>ちなみに、 <code>alluxio.security.authentication.AuthType</code>
を確認している中で、
SIMPLEモードのときはクライアント側、サーバ側ともにVerify処理をしない旨のコメントを見つけた。</p>
<p>alluxio/security/authentication/AuthType.java:26 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * User is aware in Alluxio. On the client side, the login username is determined by the</span><br><span class="line"> * &quot;alluxio.security.login.username&quot; property, or the OS user upon failure.</span><br><span class="line"> * On the server side, the verification of client user is disabled.</span><br><span class="line"> */</span><br><span class="line">SIMPLE,</span><br></pre></td></tr></table></figure></p>
<p>現時点でわかったことをまとめると、SIMPLEモード時はプロパティで渡された情報や、そうでなければ
OSユーザから得られた情報を用いてユーザ名を用いるようになっている。</p>
<h3><span id="customモードの認証">CUSTOMモードの認証</span></h3>
<p>一方、CUSTOMモード時はサーバ側で任意のVersify処理を実行する旨のコメントが記載されていた。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * User is aware in Alluxio. On the client side, the login username is determined by the</span><br><span class="line"> * &quot;alluxio.security.login.username&quot; property, or the OS user upon failure.</span><br><span class="line"> * On the server side, the user is verified by a Custom authentication provider</span><br><span class="line"> * (Specified by property &quot;alluxio.security.authentication.custom.provider.class&quot;).</span><br><span class="line"> */</span><br><span class="line">CUSTOM,</span><br></pre></td></tr></table></figure>
<p>ユーザをVerifyしたいときは、CUSTOMモードを使い、サーバ側で確認せよ、ということか。
ただ、CUSTOMモードは <a href="https://docs.alluxio.io/os/user/stable/en/advanced/Security.html" target="_blank" rel="noopener">Alluxio公式ドキュメントのセキュリティに関する説明</a>
において、</p>
<blockquote>
<p>CUSTOM Authentication is enabled. Alluxio file system can know the
user accessing it, and use customized AuthenticationProvider to verify
the user is the one he/she claims.</p>
<p>Experimental. This mode is only used in tests currently.</p>
</blockquote>
<p>と記載されており、「Experimental」であることから、積極的には使いづらい状況に見える。（2019/05/10現在）</p>
<p>関連事項として、
<code>alluxio.security.authentication.AuthenticationProvider</code>
を見ると、
<code>alluxio.security.authentication.custom.provider.class</code>
プロパティで渡された クラス名を用いて CustomAuthenticationProvider
をインスタンス生成するようにしているように見える。</p>
<p>alluxio/security/authentication/AuthenticationProvider.java:44
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">switch (authType) &#123;</span><br><span class="line">  case SIMPLE:</span><br><span class="line">    return new SimpleAuthenticationProvider();</span><br><span class="line">  case CUSTOM:</span><br><span class="line">    String customProviderName =</span><br><span class="line">        conf.get(PropertyKey.SECURITY_AUTHENTICATION_CUSTOM_PROVIDER_CLASS);</span><br><span class="line">    return new CustomAuthenticationProvider(customProviderName);</span><br><span class="line">  default:</span><br><span class="line">    throw new AuthenticationException(&quot;Unsupported AuthType: &quot; + authType.getAuthName());</span><br></pre></td></tr></table></figure></p>
<p>なお、テストには以下のような実装が見られ、CUSTOMモードの使い方を想像できる。</p>
<ul>
<li>alluxio.security.authentication.PlainSaslServerCallbackHandlerTest.NameMatchAuthenticationProvider</li>
<li>alluxio.security.authentication.GrpcSecurityTest.ExactlyMatchAuthenticationProvider</li>
<li>alluxio.server.auth.MasterClientAuthenticationIntegrationTest.NameMatchAuthenticationProvider</li>
<li>alluxio.security.authentication.CustomAuthenticationProviderTest.MockAuthenticationProvider</li>
</ul>
<p>ExactlyMatchAuthenticationProviderを用いたテストは以下の通り。</p>
<p>alluxio/security/authentication/GrpcSecurityTest.java:78
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public void testCustomAuthentication() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">  mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_TYPE, AuthType.CUSTOM.getAuthName());</span><br><span class="line">  mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_CUSTOM_PROVIDER_CLASS,</span><br><span class="line">      ExactlyMatchAuthenticationProvider.class.getName());</span><br><span class="line">  GrpcServer server = createServer(AuthType.CUSTOM);</span><br><span class="line">  server.start();</span><br><span class="line">  GrpcChannelBuilder channelBuilder =</span><br><span class="line">      GrpcChannelBuilder.newBuilder(getServerConnectAddress(server), mConfiguration);</span><br><span class="line">  channelBuilder.setCredentials(ExactlyMatchAuthenticationProvider.USERNAME,</span><br><span class="line">      ExactlyMatchAuthenticationProvider.PASSWORD, null).build();</span><br><span class="line">  server.shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>alluxio/security/authentication/GrpcSecurityTest.java:93
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public void testCustomAuthenticationFails() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">  mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_TYPE, AuthType.CUSTOM.getAuthName());</span><br><span class="line">  mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_CUSTOM_PROVIDER_CLASS,</span><br><span class="line">      ExactlyMatchAuthenticationProvider.class.getName());</span><br><span class="line">  GrpcServer server = createServer(AuthType.CUSTOM);</span><br><span class="line">  server.start();</span><br><span class="line">  GrpcChannelBuilder channelBuilder =</span><br><span class="line">      GrpcChannelBuilder.newBuilder(getServerConnectAddress(server), mConfiguration);</span><br><span class="line">  mThrown.expect(UnauthenticatedException.class);</span><br><span class="line">  channelBuilder.setCredentials(&quot;fail&quot;, &quot;fail&quot;, null).build();</span><br><span class="line">  server.shutdown();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>参考までに、SIMPLEモードで用いられる
<code>alluxio.security.authentication.plain.SimpleAuthenticationProvider</code>
では、
実際に以下のように何もしないauthenticateメソッドが定義されている。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public void authenticate(String user, String password) throws AuthenticationException &#123;</span><br><span class="line">  // no-op authentication</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2><span id="監査ログ">監査ログ</span></h2>
<p><a href="https://docs.alluxio.io/os/user/stable/en/advanced/Security.html" target="_blank" rel="noopener">Alluxio公式ドキュメントのセキュリティに関する説明</a>
の「AUDITING」にログのエントリが記載されている。
2019/05/10時点では、以下の通り。</p>
<ul>
<li>succeeded:
<ul>
<li>True if the command has succeeded. To succeed, it must also have
been allowed.</li>
</ul></li>
<li>allowed:
<ul>
<li>True if the command has been allowed. Note that a command can still
fail even if it has been allowed.</li>
</ul></li>
<li>ugi:
<ul>
<li>User group information, including username, primary group, and
authentication type.</li>
</ul></li>
<li>ip:
<ul>
<li>Client IP address.</li>
</ul></li>
<li>cmd:
<ul>
<li>Command issued by the user.</li>
</ul></li>
<li>src:
<ul>
<li>Path of the source file or directory.</li>
</ul></li>
<li>dst:
<ul>
<li>Path of the destination file or directory. If not applicable, the
value is null.</li>
</ul></li>
<li>perm:
<ul>
<li>User:group:mask or null if not applicable.</li>
</ul></li>
</ul>
<p>HDFSにおける監査ログ相当の内容が出力されるようだ。
これが、下層にあるストレージによらずに出力されるとしたら、
Alluxioによる抽象化層でAuditログを取る、という方針も悪くないか？</p>
<p>ただ、そのときにはどのパス（URI？）に、どのストレージをマウントしたか、という情報もセットで保存しておく必要があるだろう。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/05/10/Alluxio-Security/" data-id="clmdqacs8015h1vs3vbeco31v" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Storage-Layer-Storage-Engine" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/05/10/Storage-Layer-Storage-Engine/">Storage Layer ? Storage Engine ?</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/05/10/Storage-Layer-Storage-Engine/">
            <time datetime="2019-05-10T05:36:50.000Z" itemprop="datePublished">2019-05-10</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Storage-Engine/">Storage Engine</a>, <a class="tag-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a>
<ul>
<li><a href="#データベースエンジン-あるいは-ストレージエンジン" id="toc-データベースエンジン-あるいは-ストレージエンジン">データベースエンジン
あるいは ストレージエンジン</a></li>
<li><a href="#apache-parquet" id="toc-apache-parquet">Apache
Parquet</a></li>
<li><a href="#delta-lake" id="toc-delta-lake">Delta Lake</a></li>
<li><a href="#apache-hudi" id="toc-apache-hudi">Apache Hudi</a></li>
<li><a href="#alluxio" id="toc-alluxio">Alluxio</a></li>
<li><a href="#aws-aurora" id="toc-aws-aurora">AWS Aurora</a></li>
<li><a href="#apache-hbase" id="toc-apache-hbase">Apache HBase</a></li>
<li><a href="#apache-kudu" id="toc-apache-kudu">Apache Kudu</a></li>
</ul></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#動機" id="toc-動機">動機</a></li>
<li><a href="#どういうユースケースを想定するか" id="toc-どういうユースケースを想定するか">どういうユースケースを想定するか？</a></li>
<li><a href="#どういう特徴を持つストレージを探るか" id="toc-どういう特徴を持つストレージを探るか">どういう特徴を持つストレージを探るか？</a></li>
<li><a href="#データベースエンジン-あるいは-ストレージエンジン-1" id="toc-データベースエンジン-あるいは-ストレージエンジン-1">データベースエンジン
あるいは ストレージエンジン</a></li>
<li><a href="#apache-parquet-1" id="toc-apache-parquet-1">Apache
Parquet</a></li>
<li><a href="#delta-lake-1" id="toc-delta-lake-1">Delta Lake</a></li>
<li><a href="#apache-hudi-1" id="toc-apache-hudi-1">Apache Hudi</a></li>
<li><a href="#alluxio-1" id="toc-alluxio-1">Alluxio</a></li>
<li><a href="#aws-auroraのバックエンドストレージもしくはストレージエンジン" id="toc-aws-auroraのバックエンドストレージもしくはストレージエンジン">AWS
Auroraのバックエンドストレージ（もしくはストレージエンジン）</a></li>
<li><a href="#apache-hbase-1" id="toc-apache-hbase-1">Apache
HBase</a></li>
<li><a href="#apache-kudu-1" id="toc-apache-kudu-1">Apache Kudu</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<h2><span id="データベースエンジンあるいは-ストレージエンジン">データベースエンジン
あるいは ストレージエンジン</span></h2>
<ul>
<li><a href="https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3" target="_blank" rel="noopener">jp.wikipediaのデータベースエンジン</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.6/ja/storage-engines.html" target="_blank" rel="noopener">MySQLの代替ストレージエンジン</a></li>
<li><a href="https://dev.mysql.com/doc/refman/5.6/ja/innodb-introduction.html" target="_blank" rel="noopener">InnoDBイントロダクション</a></li>
</ul>
<h2><span id="apache-parquet">Apache Parquet</span></h2>
<ul>
<li><a href="https://parquet.apache.org/" target="_blank" rel="noopener">Parquetの公式ウェブサイト</a></li>
</ul>
<h2><span id="delta-lake">Delta Lake</span></h2>
<ul>
<li><a href="https://delta.io/" target="_blank" rel="noopener">Delta Lake公式ウェブサイト</a></li>
</ul>
<h2><span id="apache-hudi">Apache Hudi</span></h2>
<ul>
<li><a href="https://hudi.incubator.apache.org/" target="_blank" rel="noopener">Apache
Hudiの公式ウェブサイト</a></li>
<li><a href="https://hudi.incubator.apache.org/images/hudi_intro_1.png" target="_blank" rel="noopener">Hudiのイメージを表す図</a></li>
</ul>
<h2><span id="alluxio">Alluxio</span></h2>
<ul>
<li><a href="https://www.alluxio.io/" target="_blank" rel="noopener">Alluxio公式ウェブサイト</a></li>
</ul>
<h2><span id="aws-aurora">AWS Aurora</span></h2>
<ul>
<li><a href="https://qiita.com/kumagi/items/67f9ac0fb4e6f70c056d" target="_blank" rel="noopener">kumagiさんの解説記事</a></li>
<li><a href="https://dl.acm.org/citation.cfm?id=3056101" target="_blank" rel="noopener">ACMライブラリの論文</a></li>
</ul>
<h2><span id="apache-hbase">Apache HBase</span></h2>
<ul>
<li><a href="https://hbase.apache.org/" target="_blank" rel="noopener">HBaseの公式ウェブサイト</a></li>
</ul>
<h2><span id="apache-kudu">Apache Kudu</span></h2>
<ul>
<li><a href="https://kudu.apache.org/" target="_blank" rel="noopener">Apache
Kuduの公式ウェブサイト</a></li>
<li><a href="https://www.slideshare.net/Cloudera_jp/apache-kududb-dbts2017" target="_blank" rel="noopener">DBテックショーでのKudu説明</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="動機">動機</span></h2>
<p>単にPut、Getするだけではなく、例えばデータを内部的に構造化したり、トランザクションに対応したりする機能を持つ
ストレージ機能のことをなんと呼べば良いのかを考えてみる。</p>
<h2><span id="どういうユースケースを想定するか">どういうユースケースを想定するか？</span></h2>
<ul>
<li>データがストリームとして届く
<ul>
<li>できる限り鮮度の高い状態で扱う</li>
</ul></li>
<li>主に分析および分析結果を用いたビジネス</li>
<li>大規模なデータを取り扱う
<ul>
<li>大規模なデータを一度の分析で取り扱う</li>
<li>大規模なデータの中から、一部を一度の分析で取り扱う</li>
</ul></li>
</ul>
<h2><span id="どういう特徴を持つストレージを探るか">どういう特徴を持つストレージを探るか？</span></h2>
<ul>
<li>必須
<ul>
<li>Put、Getなど（あるいは、それ相当）の基本的なAPIを有する
<ul>
<li>補足：POSIXでなくてもよい？</li>
</ul></li>
<li>大規模データを扱える。スケーラビリティを有する。
大規模の種類には、サイズが大きいこと、件数が多いことの両方の特徴がありえる
<ul>
<li>データを高効率で圧縮できることは重要そう</li>
</ul></li>
<li>クエリエンジンと連係する</li>
</ul></li>
<li>あると望ましい
<ul>
<li>トランザクションに対応
<ul>
<li>ストリーム処理の出力先として用いる場合、Exactly Onceセマンティクスを
達成するためには出力先ストレージ側でトランザクション対応していることで
ストリーム処理アプリケーションをシンプルにできる、はず</li>
</ul></li>
<li>ストリームデータを効率的に永続化し、オンデマンドの分析向けにバックエンドで変換。
あるいは分析向けにストア可能
<ul>
<li>高頻度での書き込み + 一括での読み出し</li>
</ul></li>
<li>サービスレス
<ul>
<li>複雑なサービスを運用せずに済むなら越したことはない。</li>
</ul></li>
<li>読み出しについて、プッシュダウンフィルタへの対応
<ul>
<li>更にデータ構造によっては、基本的な集計関数に対応していたら便利ではある</li>
</ul></li>
<li>更新のあるデータについて、過去のデータも取り出せる</li>
</ul></li>
</ul>
<h2><span id="データベースエンジンあるいは-ストレージエンジン">データベースエンジン
あるいは ストレージエンジン</span></h2>
<p><a href="https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3" target="_blank" rel="noopener">jp.wikipediaのデータベースエンジン</a>
によると、</p>
<blockquote>
<p>データベース管理システム (DBMS)がデータベースに対しデータを
挿入、抽出、更新および削除(CRUD参照)するために使用する基礎となる
ソフトウェア部品</p>
</blockquote>
<p>とある。</p>
<p>MySQLの場合には、　<a href="https://dev.mysql.com/doc/refman/5.6/ja/storage-engines.html" target="_blank" rel="noopener">MySQLの代替ストレージエンジン</a>
に載っているようなものが該当する。 なお、デフォルトはInnoDB。 <a href="https://dev.mysql.com/doc/refman/5.6/ja/innodb-introduction.html" target="_blank" rel="noopener">InnoDBイントロダクション</a>
に「表 14.1 InnoDB ストレージエンジンの機能」という項目がある。
インデックス、キャッシュ、トランザクション、バックアップ・リカバリ、レプリケーション、
圧縮、地理空間情報の取扱、暗号化対応などが大まかに挙げられる。</p>
<h2><span id="apache-parquet">Apache Parquet</span></h2>
<p><a href="https://parquet.apache.org/" target="_blank" rel="noopener">Parquetの公式ウェブサイト</a>
によると以下のように定義されている。</p>
<blockquote>
<p>Apache Parquet is a columnar storage format available to any project
in the Hadoop ecosystem, regardless of the choice of data processing
framework, data model or programming language.</p>
</blockquote>
<p>ストレージフォーマット単体でも、バッチ処理向けにはある程度有用であると考えられる。</p>
<h2><span id="delta-lake">Delta Lake</span></h2>
<p><a href="https://delta.io/" target="_blank" rel="noopener">Delta Lake公式ウェブサイト</a>
には、以下のような定義が記載されている。</p>
<blockquote>
<p>Delta Lake is an open-source storage layer that brings ACID
transactions to Apache Spark™ and big data workloads.</p>
</blockquote>
<p>またウェブサイトには、「Key
Features」が挙げられている。2019/05/10時点では以下の通り。</p>
<ul>
<li>ACID Transactions</li>
<li>Scalable Metadata Handling</li>
<li>Time Travel (data versioning)</li>
<li>Open Format</li>
<li>Unified Batch and Streaming Source and Sink</li>
<li>Schema Enforcement</li>
<li>Schema Evolution</li>
<li>100% Compatible with Apache Spark API</li>
</ul>
<p>データベースの「データベースエンジン」、「ストレージエンジン」とは異なる特徴を有することから、
定義上も「Storage Layer」と読んでいるのだろうか。</p>
<h2><span id="apache-hudi">Apache Hudi</span></h2>
<p><a href="https://hudi.incubator.apache.org/" target="_blank" rel="noopener">Apache
Hudiの公式ウェブサイト</a> によると、</p>
<blockquote>
<p>Hudi (pronounced “Hoodie”) ingests &amp; manages storage of large
analytical datasets over DFS (HDFS or cloud stores) and provides three
logical views for query access.</p>
</blockquote>
<p>とのこと。</p>
<p>また特徴としては、2019/05/10現在</p>
<ul>
<li>Read Optimized View</li>
<li>Incremental View</li>
<li>Near-Real time Table</li>
</ul>
<p>が挙げられていた。 なお、機能をイメージするには、 <a href="https://hudi.incubator.apache.org/images/hudi_intro_1.png" target="_blank" rel="noopener">Hudiのイメージを表す図</a>
がちょうどよい。</p>
<p>Apache Hudiでは、「XXなYYである」のような定義は挙げられていない。</p>
<h2><span id="alluxio">Alluxio</span></h2>
<p>ストレージそのものというより、ストレージの抽象化や透過的アクセスの仕組みとして考えられる。</p>
<p><a href="https://www.alluxio.io/" target="_blank" rel="noopener">Alluxio公式ウェブサイト</a>
には以下のように定義されている。</p>
<blockquote>
<p>Data orchestration for analytics and machine learning in the
cloud</p>
</blockquote>
<p><a href="https://www.alluxio.io/" target="_blank" rel="noopener">Alluxio公式ウェブサイト</a>
には、「KEY TECHNICAL FEATURES」というのが記載されている。
気になったものを列挙すると以下の通り。</p>
<ul>
<li>Compute
<ul>
<li>Flexible APIs</li>
<li>Intelligent data caching and tiering</li>
</ul></li>
<li>Storage
<ul>
<li>Built-in data policies</li>
<li>Plug and play under stores</li>
<li>Transparent unified namespace for file system and object stores</li>
</ul></li>
<li>Enterprise
<ul>
<li>Security</li>
<li>Monitoring and management</li>
</ul></li>
</ul>
<h2><span id="awsauroraのバックエンドストレージもしくはストレージエンジン">AWS
Auroraのバックエンドストレージ（もしくはストレージエンジン）</span></h2>
<p><a href="https://qiita.com/kumagi/items/67f9ac0fb4e6f70c056d" target="_blank" rel="noopener">kumagiさんの解説記事</a>
あたりが入り口としてとてもわかり易い。 また、元ネタは、 <a href="https://dl.acm.org/citation.cfm?id=3056101" target="_blank" rel="noopener">ACMライブラリの論文</a>
あたりか。</p>
<p>語弊を恐れずにいえば、上記でも触れられていたとおり、「Redo-logリプレイ機能付き分散ストレージ」という
名前が妥当だろうか。</p>
<h2><span id="apache-hbase">Apache HBase</span></h2>
<p><a href="https://hbase.apache.org/" target="_blank" rel="noopener">HBaseの公式ウェブサイト</a>
によると、</p>
<blockquote>
<p>Apache HBase is the Hadoop database, a distributed, scalable, big
data store.</p>
</blockquote>
<p>のように定義されている。
また、挙げられていた特徴は以下のとおり。（2019/05/12現在）</p>
<ul>
<li>Linear and modular scalability.</li>
<li>Strictly consistent reads and writes.</li>
<li>Automatic and configurable sharding of tables</li>
<li>Automatic failover support between RegionServers.</li>
<li>Convenient base classes for backing Hadoop MapReduce jobs with
Apache HBase tables.</li>
<li>Easy to use Java API for client access.</li>
<li>Block cache and Bloom Filters for real-time queries.</li>
<li>Query predicate push down via server side Filters</li>
<li>Thrift gateway and a REST-ful Web service that supports XML,
Protobuf, and binary data encoding options</li>
<li>Extensible jruby-based (JIRB) shell</li>
<li>Support for exporting metrics via the Hadoop metrics subsystem to
files or Ganglia; or via JMX</li>
</ul>
<p>いわゆるスケールアウト可能なKVSだが、これを分析向けのストレージとして用いる選択肢もありえる。
ただし、いわゆるカラムナフォーマットでデータを扱うわけではない点と、
サービスを運用する必要がある点は懸念すべき点である。</p>
<h2><span id="apache-kudu">Apache Kudu</span></h2>
<p><a href="https://kudu.apache.org/" target="_blank" rel="noopener">Apache Kuduの公式ウェブサイト</a>
を見ると、以下のように定義されている。</p>
<blockquote>
<p>A new addition to the open source Apache Hadoop ecosystem, Apache
Kudu completes Hadoop's storage layer to enable fast analytics on fast
data.</p>
</blockquote>
<p>またKuduの利点は以下のように記載されている。</p>
<ul>
<li>Fast processing of OLAP workloads.</li>
<li>Integration with MapReduce, Spark and other Hadoop ecosystem
components.</li>
<li>Tight integration with Apache Impala, making it a good, mutable
alternative to using HDFS with Apache Parquet.</li>
<li>Strong but flexible consistency model, allowing you to choose
consistency requirements on a per-request basis, including the option
for strict-serializable consistency.</li>
<li>Strong performance for running sequential and random workloads
simultaneously.</li>
<li>Easy to administer and manage.</li>
<li>High availability. Tablet Servers and Masters use the Raft Consensus
Algorithm, which ensures that as long as more than half the total number
of replicas is available, the tablet is available for reads and writes.
For instance, if 2 out of 3 replicas or 3 out of 5 replicas are
available, the tablet is available.</li>
<li>Reads can be serviced by read-only follower tablets, even in the
event of a leader tablet failure.</li>
<li>Structured data model.</li>
</ul>
<p>クエリエンジンImpalaと連係することを想定されている。
また、「ストレージレイヤ」という呼び方が、Delta
Lake同様に用いられている。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/05/10/Storage-Layer-Storage-Engine/" data-id="clmdqacu4017v1vs3i5ardcsc" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Delta-Lake" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/05/05/Delta-Lake/">Delta Lake</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/05/05/Delta-Lake/">
            <time datetime="2019-05-05T14:27:14.000Z" itemprop="datePublished">2019-05-05</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/">Delta Lake</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Delta-Lake/">Delta Lake</a>, <a class="tag-link" href="/memo-blog/tags/Parquet/">Parquet</a>, <a class="tag-link" href="/memo-blog/tags/Spark/">Spark</a>, <a class="tag-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#公式ドキュメントのうち気になったところのメモ20195時点のメモ" id="toc-公式ドキュメントのうち気になったところのメモ20195時点のメモ">公式ドキュメントのうち気になったところのメモ（2019/5時点のメモ）</a>
<ul>
<li><a href="#sparkのユーザから見たときにはデータソースとして使えばよいようになっている" id="toc-sparkのユーザから見たときにはデータソースとして使えばよいようになっている">Sparkのユーザから見たときにはデータソースとして使えばよいようになっている</a></li>
<li><a href="#スキーマの管理" id="toc-スキーマの管理">スキーマの管理</a></li>
<li><a href="#ストリームとバッチの両対応" id="toc-ストリームとバッチの両対応">ストリームとバッチの両対応</a></li>
</ul></li>
<li><a href="#databrciksドキュメントの気になったところのメモ20195時点のメモ" id="toc-databrciksドキュメントの気になったところのメモ20195時点のメモ">Databrciksドキュメントの気になったところのメモ（2019/5時点のメモ）</a>
<ul>
<li><a href="#データリテンション" id="toc-データリテンション">データリテンション</a></li>
<li><a href="#最適化" id="toc-最適化">最適化</a></li>
</ul></li>
<li><a href="#クイックスタートから実装を追ってみる20195時点でのメモ" id="toc-クイックスタートから実装を追ってみる20195時点でのメモ">クイックスタートから実装を追ってみる（2019/5時点でのメモ）</a>
<ul>
<li><a href="#ひとまずrelationを調べて見る" id="toc-ひとまずrelationを調べて見る">ひとまずRelationを調べて見る</a></li>
</ul></li>
<li><a href="#data-source-v1とv2" id="toc-data-source-v1とv2">Data
Source V1とV2</a></li>
<li><a href="#設定の類" id="toc-設定の類">設定の類</a></li>
<li><a href="#チェックポイントを調査してみる" id="toc-チェックポイントを調査してみる">チェックポイントを調査してみる</a>
<ul>
<li><a href="#スナップショットについて" id="toc-スナップショットについて">スナップショットについて</a></li>
</ul></li>
<li><a href="#動作確認しながら実装確認v0.5.0" id="toc-動作確認しながら実装確認v0.5.0">動作確認しながら実装確認（v0.5.0）</a>
<ul>
<li><a href="#クイックスタートの書き込み" id="toc-クイックスタートの書き込み">クイックスタートの書き込み</a></li>
<li><a href="#クイックスタートの読み込み" id="toc-クイックスタートの読み込み">クイックスタートの読み込み</a></li>
<li><a href="#クイックスタートの更新時" id="toc-クイックスタートの更新時">クイックスタートの更新時</a></li>
<li><a href="#クイックスタートのストリームデータ読み書き" id="toc-クイックスタートのストリームデータ読み書き">クイックスタートのストリームデータ読み書き</a></li>
</ul></li>
<li><a href="#スキーマバリデーション" id="toc-スキーマバリデーション">スキーマバリデーション</a>
<ul>
<li><a href="#試しに例外を出してみる" id="toc-試しに例外を出してみる">試しに例外を出してみる</a></li>
<li><a href="#自動でのカラム追加スキーマ変更を試す" id="toc-自動でのカラム追加スキーマ変更を試す">自動でのカラム追加（スキーマ変更）を試す</a></li>
<li><a href="#overwriteモード時のスキーマ上書き" id="toc-overwriteモード時のスキーマ上書き">overwriteモード時のスキーマ上書き</a></li>
</ul></li>
<li><a href="#パーティション" id="toc-パーティション">パーティション</a></li>
<li><a href="#delta-table" id="toc-delta-table">Delta Table</a>
<ul>
<li><a href="#条件でレコード削除" id="toc-条件でレコード削除">条件でレコード削除</a></li>
<li><a href="#更新" id="toc-更新">更新</a></li>
<li><a href="#upsertマージ" id="toc-upsertマージ">upsert（マージ）</a></li>
<li><a href="#マージの例" id="toc-マージの例">マージの例</a></li>
</ul></li>
<li><a href="#タイムトラベル" id="toc-タイムトラベル">タイムトラベル</a></li>
<li><a href="#org.apache.spark.sql.delta.optimistictransactionimplcommit" id="toc-org.apache.spark.sql.delta.optimistictransactionimplcommit">org.apache.spark.sql.delta.OptimisticTransactionImpl#commit</a></li>
<li><a href="#プロトコルバージョン" id="toc-プロトコルバージョン">プロトコルバージョン</a></li>
<li><a href="#アイソレーションレベル" id="toc-アイソレーションレベル">アイソレーションレベル</a>
<ul>
<li><a href="#sbtでテストコードを使って確認" id="toc-sbtでテストコードを使って確認">sbtでテストコードを使って確認</a></li>
</ul></li>
<li><a href="#コンパクション" id="toc-コンパクション">コンパクション</a></li>
<li><a href="#バキューム" id="toc-バキューム">バキューム</a>
<ul>
<li><a href="#読まれているファイルの削除" id="toc-読まれているファイルの削除">読まれているファイルの削除</a></li>
</ul></li>
<li><a href="#コンフィグ" id="toc-コンフィグ">コンフィグ</a></li>
<li><a href="#ログのクリーンアップ" id="toc-ログのクリーンアップ">ログのクリーンアップ</a></li>
<li><a href="#parquetからdeltaテーブルへの変換" id="toc-parquetからdeltaテーブルへの変換">ParquetからDeltaテーブルへの変換</a>
<ul>
<li><a href="#動作確認" id="toc-動作確認">動作確認</a></li>
</ul></li>
<li><a href="#symlink-format-manifest" id="toc-symlink-format-manifest">Symlink Format Manifest</a></li>
<li><a href="#類似技術" id="toc-類似技術">類似技術</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://docs.delta.io/latest/" target="_blank" rel="noopener">公式ドキュメント</a></li>
<li><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式ドキュメント（クイックスタート）</a></li>
<li><a href="https://docs.delta.io/latest/delta-streaming.html" target="_blank" rel="noopener">ストリームへの対応についての説明</a></li>
<li><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式クイックスタート</a></li>
<li><a href="https://docs.databricks.com/delta/index.html" target="_blank" rel="noopener">DatabricksのDelta
Lakeの説明</a></li>
<li><a href="https://docs.databricks.com/delta/delta-batch.html#overwrite-using-dataframes" target="_blank" rel="noopener">DataFrameを使った上書き</a></li>
<li><a href="https://docs.databricks.com/delta/delta-batch.html#data-retention" target="_blank" rel="noopener">データ・リテンション</a></li>
<li><a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/vacuum.html#vacuum" target="_blank" rel="noopener">VACUUM</a></li>
<li><a href="https://docs.databricks.com/delta/optimizations.html" target="_blank" rel="noopener">最適化</a></li>
<li><a href="https://docs.delta.io/latest/concurrency-control.html#optimistic-concurrency-control" target="_blank" rel="noopener">Delta
LakeのOptimistic Concurrency Controlに関する記述</a></li>
<li><a href="https://hudi.apache.org" target="_blank" rel="noopener">Apache Hudi</a></li>
<li><a href="https://docs.delta.io/latest/best-practices.html#compact-files" target="_blank" rel="noopener">公式ドキュメントのCompact
filesの説明</a></li>
<li><a href="https://docs.delta.io/latest/delta-batch.html#partition-data" target="_blank" rel="noopener">公式ドキュメントのパーティション説明</a></li>
<li><a href="https://docs.delta.io/latest/delta-batch.html#schema-validation" target="_blank" rel="noopener">公式ドキュメントのスキーマバリデーション</a></li>
<li><a href="https://docs.delta.io/latest/delta-batch.html#add-columns" target="_blank" rel="noopener">公式ドキュメントのスキーママージ</a></li>
<li><a href="https://docs.delta.io/latest/delta-batch.html#replace-table-schema" target="_blank" rel="noopener">公式ドキュメントのスキーマ上書き</a></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#merge-examples" target="_blank" rel="noopener">公式ドキュメントのマージの例</a></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#data-deduplication-when-writing-into-delta-tables" target="_blank" rel="noopener">公式ドキュメントのマージを使った上書き</a></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#slowly-changing-data-scd-type-2-operation-into-delta-tables" target="_blank" rel="noopener">Slowly
changing data (SCD) Type 2 operation</a></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#write-change-data-into-a-delta-table" target="_blank" rel="noopener">Write
change data into a Delta table</a></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#upsert-from-streaming-queries-using-foreachbatch" target="_blank" rel="noopener">Upsert
from streaming queries using foreachBatch</a></li>
<li><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch" target="_blank" rel="noopener">org.apache.spark.sql.streaming.DataStreamWriter#foreachBatch</a></li>
<li><a href="https://docs.delta.io/latest/delta-utility.html#vacuum" target="_blank" rel="noopener">公式ドキュメントのVacuum</a></li>
<li><a href="https://docs.delta.io/0.5.0/presto-integration.html" target="_blank" rel="noopener">Presto
and Athena to Delta Lake Integration</a></li>
<li><a href="https://docs.delta.io/0.5.0/presto-integration.html#limitations" target="_blank" rel="noopener">Presto
Athena連係の制約</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="公式ドキュメントのうち気になったところのメモ20195時点のメモ">公式ドキュメントのうち気になったところのメモ（2019/5時点のメモ）</span></h2>
<p>以下、ポイントの記載。あくまでドキュメント上の話。</p>
<h3><span id="sparkのユーザから見たときにはデータソースとして使えばよいようになっている">Sparkのユーザから見たときにはデータソースとして使えばよいようになっている</span></h3>
<p>SparkにはData
Sourceの仕組みがあるが、その１種として使えるようになっている。
したがって、DatasetやDataFrameで定義したデータを読み書きするデータソースの１種類として考えて使えば自然に使えるようになっている。</p>
<h3><span id="スキーマの管理">スキーマの管理</span></h3>
<p>通常のSpark
APIでは、DataFrameを出力するときに過去のデータのスキーマを考慮したりしないが、
Delta
Lakeを用いると過去のスキーマを考慮した振る舞いをさせることができる。
例えば、「スキーマを更新させない（意図しない更新が発生しているときにはエラーを吐かせるなど）」、
「既存のスキーマを利用して部分的な更新」などが可能。</p>
<p>またカラムの追加など、言ってみたら、スキーマの自動更新みたいなことも可能。</p>
<h3><span id="ストリームとバッチの両対応">ストリームとバッチの両対応</span></h3>
<p><a href="https://docs.delta.io/latest/delta-streaming.html" target="_blank" rel="noopener">ストリームへの対応についての説明</a>
に記載があるが、Spark Structured Streamingの読み書き宛先として利用可能。
ストリーム処理では「Exactly
Once」セマンティクスの保証が大変だったりするが、 そのあたりをDelta
Lake層で考慮してくれる、などの利点が挙げられる。</p>
<p>Dalta
Lake自身が差分管理の仕組みを持っているので、その仕組みを使って読み書きさせるのだろう、という想像。</p>
<p>なお、入力元としてDelta
Lakeを想定した場合、「レコードの削除」、「レコードの更新」が発生することが
考えうる。それを入力元としてどう扱うか？を設定するパラメータがある。</p>
<ul>
<li>ignoreDeletes: 過去レコードの削除を下流に伝搬しない</li>
<li>ignoreChanges:
上記に加え、更新されたレコードを含むファイルの内容全体を下流に伝搬させる</li>
</ul>
<p>出力先としてDelta
Lakeを想定した場合、トランザクションの仕組みを用いられるところが特徴となる。
つまり複数のストリーム処理アプリケーションが同じテーブルに出力するときにも適切にハンドルできるようになり、
出力先も含めたExactly Onceを実現可能になる。</p>
<h2><span id="databrciksドキュメントの気になったところのメモ20195時点のメモ">Databrciksドキュメントの気になったところのメモ（2019/5時点のメモ）</span></h2>
<p>全体的な注意点として、Databricksのドキュメントなので、Databricksクラウド特有の話が含まれている可能性があることが挙げられる。</p>
<h3><span id="データリテンション">データリテンション</span></h3>
<p><a href="https://docs.databricks.com/delta/delta-batch.html#data-retention" target="_blank" rel="noopener">データ・リテンション</a>
に記述あり。 デフォルトでは30日間のコミットログが保持される、とのこと。
VACUUM句を用いて縮めることができるようだ。</p>
<p>VACUUMについては、<a href="https://docs.databricks.com/spark/latest/spark-sql/language-manual/vacuum.html#vacuum" target="_blank" rel="noopener">VACUUM</a>を参照のこと。</p>
<h3><span id="最適化">最適化</span></h3>
<p><a href="https://docs.databricks.com/delta/optimizations.html" target="_blank" rel="noopener">最適化</a>
に記載がある。コンパクションやZ-Orderingなど。
Databricksクラウド上でSQL文で発行するようだ。</p>
<h2><span id="クイックスタートから実装を追ってみる20195時点でのメモ">クイックスタートから実装を追ってみる（2019/5時点でのメモ）</span></h2>
<p><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式クイックスタート</a>
を参照しながら実装を確認してみる。</p>
<p>上記によると、まずはSBTでは以下の依存関係を追加することになっている。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libraryDependencies += &quot;io.delta&quot; %% &quot;delta-core&quot; % &quot;0.1.0&quot;</span><br></pre></td></tr></table></figure></p>
<p><a href="https://docs.delta.io/latest/quick-start.html#create-a-table" target="_blank" rel="noopener">Create
a tableの章</a>
には、バッチ処理での書き込みについて以下のような例が記載されていた。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Dataset</span>;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">SparkSession</span> spark = ...   <span class="comment">// create SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="type">Dataset</span>&lt;<span class="type">Row</span>&gt; data = data = spark.range(<span class="number">0</span>, <span class="number">5</span>);</span><br><span class="line">data.write().format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>);</span><br></pre></td></tr></table></figure>
<p>SparkのData Sourcesの仕組みとして扱えるようになっている。</p>
<p>テストコードで言えば、
<code>org/apache/spark/sql/delta/DeltaSuiteOSS.scala:24</code>
あたりを眺めるとよいのではないか。</p>
<p>バッチ方式では、以下のようなテストが実装されている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">test(<span class="string">"append then read"</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> tempDir = <span class="type">Utils</span>.createTempDir()</span><br><span class="line">  <span class="type">Seq</span>(<span class="number">1</span>).toDF().write.format(<span class="string">"delta"</span>).save(tempDir.toString)</span><br><span class="line">  <span class="type">Seq</span>(<span class="number">2</span>, <span class="number">3</span>).toDF().write.format(<span class="string">"delta"</span>).mode(<span class="string">"append"</span>).save(tempDir.toString)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">data</span></span>: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"delta"</span>).load(tempDir.toString)</span><br><span class="line">  checkAnswer(data, <span class="type">Row</span>(<span class="number">1</span>) :: <span class="type">Row</span>(<span class="number">2</span>) :: <span class="type">Row</span>(<span class="number">3</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// append more</span></span><br><span class="line">  <span class="type">Seq</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>).toDF().write.format(<span class="string">"delta"</span>).mode(<span class="string">"append"</span>).save(tempDir.toString)</span><br><span class="line">  checkAnswer(data.toDF(), <span class="type">Row</span>(<span class="number">1</span>) :: <span class="type">Row</span>(<span class="number">2</span>) :: <span class="type">Row</span>(<span class="number">3</span>) :: <span class="type">Row</span>(<span class="number">4</span>) :: <span class="type">Row</span>(<span class="number">5</span>) :: <span class="type">Row</span>(<span class="number">6</span>) :: <span class="type">Nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ストリーム方式では以下のようなテストが実装されている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">test(<span class="string">"append mode"</span>) &#123;</span><br><span class="line">  failAfter(streamingTimeout) &#123;</span><br><span class="line">    withTempDirs &#123; (outputDir, checkpointDir) =&gt;</span><br><span class="line">      <span class="keyword">val</span> inputData = <span class="type">MemoryStream</span>[<span class="type">Int</span>]</span><br><span class="line">      <span class="keyword">val</span> df = inputData.toDF()</span><br><span class="line">      <span class="keyword">val</span> query = df.writeStream</span><br><span class="line">        .option(<span class="string">"checkpointLocation"</span>, checkpointDir.getCanonicalPath)</span><br><span class="line">        .format(<span class="string">"delta"</span>)</span><br><span class="line">        .start(outputDir.getCanonicalPath)</span><br><span class="line">      <span class="keyword">val</span> log = <span class="type">DeltaLog</span>.forTable(spark, outputDir.getCanonicalPath)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        inputData.addData(<span class="number">1</span>)</span><br><span class="line">        query.processAllAvailable()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> outputDf = spark.read.format(<span class="string">"delta"</span>).load(outputDir.getCanonicalPath)</span><br><span class="line">        checkDatasetUnorderly(outputDf.as[<span class="type">Int</span>], <span class="number">1</span>)</span><br><span class="line">        assert(log.update().transactions.head == (query.id.toString -&gt; <span class="number">0</span>L))</span><br><span class="line"></span><br><span class="line">        inputData.addData(<span class="number">2</span>)</span><br><span class="line">        query.processAllAvailable()</span><br><span class="line"></span><br><span class="line">        checkDatasetUnorderly(outputDf.as[<span class="type">Int</span>], <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        assert(log.update().transactions.head == (query.id.toString -&gt; <span class="number">1</span>L))</span><br><span class="line"></span><br><span class="line">        inputData.addData(<span class="number">3</span>)</span><br><span class="line">        query.processAllAvailable()</span><br><span class="line"></span><br><span class="line">        checkDatasetUnorderly(outputDf.as[<span class="type">Int</span>], <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        assert(log.update().transactions.head == (query.id.toString -&gt; <span class="number">2</span>L))</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        query.stop()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3><span id="ひとまずrelationを調べて見る">ひとまずRelationを調べて見る</span></h3>
<p>いったんData Source V1だと仮定 <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
して、createRelationメソッドを探したところ、
<code>org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation</code>
あたりを眺めると、 実態としては</p>
<p>org/apache/spark/sql/delta/sources/DeltaDataSource.scala:148
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deltaLog.createRelation()</span><br></pre></td></tr></table></figure></p>
<p>が戻り値を返しているようだ。
このメソッドは、以下のように内部的には<code>org.apache.spark.sql.execution.datasources.HadoopFsRelation</code>クラスを
用いている。というより、うまいことほぼ流用している。
以下のような実装。</p>
<p>org/apache/spark/sql/delta/DeltaLog.scala:600 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">new HadoopFsRelation(</span><br><span class="line">  fileIndex,</span><br><span class="line">  partitionSchema = snapshotToUse.metadata.partitionSchema,</span><br><span class="line">  dataSchema = snapshotToUse.metadata.schema,</span><br><span class="line">  bucketSpec = None,</span><br><span class="line">  snapshotToUse.fileFormat,</span><br><span class="line">  snapshotToUse.metadata.format.options)(spark) with InsertableRelation &#123;</span><br><span class="line">  def insert(data: DataFrame, overwrite: Boolean): Unit = &#123;</span><br><span class="line">    val mode = if (overwrite) SaveMode.Overwrite else SaveMode.Append</span><br><span class="line">    WriteIntoDelta(</span><br><span class="line">      deltaLog = DeltaLog.this,</span><br><span class="line">      mode = mode,</span><br><span class="line">      new DeltaOptions(Map.empty[String, String], spark.sessionState.conf),</span><br><span class="line">      partitionColumns = Seq.empty,</span><br><span class="line">      configuration = Map.empty,</span><br><span class="line">      data = data).run(spark)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>insert</code>メソッドでDelta
Lake独自の書き込み方式を実行する処理を定義している。</p>
<p>なお、runの中で<code>org.apache.spark.sql.delta.DeltaOperations</code>内で定義されたWriteオペレーションを実行するように
なっているのだが、そこでは<code>org.apache.spark.sql.delta.OptimisticTransaction</code>を用いるようになっている。
要は、Optimistic Concurrency
Controlの考え方を応用した実装になっているのではないかと想像。 <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>※ <a href="https://docs.delta.io/latest/concurrency-control.html#optimistic-concurrency-control" target="_blank" rel="noopener">Delta
LakeのOptimistic Concurrency Controlに関する記述</a>
にその旨記載されているようだ。</p>
<p>また、上記の通り、データは<code>DeltaLog</code>クラスを経由して管理される。</p>
<p>実際にデータが書き込まれると思われる
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#write</code>
を眺めてよう。</p>
<p>最初にメタデータ更新？</p>
<p>org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:87
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">updateMetadata(txn, data, partitionColumns, configuration, isOverwriteOperation)</span><br></pre></td></tr></table></figure></p>
<p>updateMetadataメソッドでは、内部的に以下の処理を実施。</p>
<ul>
<li>スキーマをマージ</li>
<li>「パーティションカラム」のチェック（パーティションカラムに指定された名前を持つ「複数のカラム」がないかどうか、など）</li>
<li>replaceWhereオプション（ <a href="https://docs.databricks.com/delta/delta-batch.html#overwrite-using-dataframes" target="_blank" rel="noopener">DataFrameを使った上書き</a>
参照）によるフィルタ構成</li>
</ul>
<p>★2019/5時点では、ここで調査が止まっていた。これ以降に続く実装調査については、
<a href="#動作確認しながら実装確認">動作確認しながら実装確認</a>
を参照</p>
<h2><span id="data-source-v1とv2">Data Source V1とV2</span></h2>
<p><code>org.apache.spark.sql.delta.sources.DeltaDataSource</code>
あたりがData Source V1向けの実装のエントリポイントか。
これを見る限り、V1で実装されているように見える…？</p>
<h2><span id="設定の類">設定の類</span></h2>
<p><code>org.apache.spark.sql.delta.sources.DeltaSQLConf</code>
あたりが設定か。</p>
<p><code>spark.databricks.delta.$key</code>で指定可能なようだ。</p>
<h2><span id="チェックポイントを調査してみる">チェックポイントを調査してみる</span></h2>
<p>チェックポイントは、その名の通り、将来のリプレイ時にショートカットするための機能。
スナップショットからチェックポイントを作成する。</p>
<p>エントリポイントは、
<code>org.apache.spark.sql.delta.Checkpoints</code> だろうか。
ひとまず、
<code>org.apache.spark.sql.delta.Checkpoints#checkpoint</code>
メソッドを見つけた。</p>
<p>org/apache/spark/sql/delta/Checkpoints.scala:118 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def checkpoint(): Unit = recordDeltaOperation(this, &quot;delta.checkpoint&quot;) &#123;</span><br><span class="line">  val checkpointMetaData = checkpoint(snapshot)</span><br><span class="line">  val json = JsonUtils.toJson(checkpointMetaData)</span><br><span class="line">  store.write(LAST_CHECKPOINT, Iterator(json), overwrite = true)</span><br><span class="line"></span><br><span class="line">  doLogCleanup()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>実態は、<code>org.apache.spark.sql.delta.Checkpoints$#writeCheckpoint</code>
メソッドか。</p>
<p>org/apache/spark/sql/delta/Checkpoints.scala:126 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected def checkpoint(snapshotToCheckpoint: Snapshot): CheckpointMetaData = &#123;</span><br><span class="line">  Checkpoints.writeCheckpoint(spark, this, snapshotToCheckpoint)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>ちなみに、
<code>org.apache.spark.sql.delta.Checkpoints$#writeCheckpoint</code>
メソッド内ではややトリッキーな方法でチェックポイントの書き出しを行っている。</p>
<p><code>org.apache.spark.sql.delta.Checkpoints#checkpoint()</code>
メソッドが呼び出されるのは、 以下のようにOptimistic
Transactionのポストコミット処理中である。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:294
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">protected def postCommit(commitVersion: Long, committActions: Seq[Action]): Unit = &#123;</span><br><span class="line">  committed = true</span><br><span class="line">  if (commitVersion != 0 &amp;&amp; commitVersion % deltaLog.checkpointInterval == 0) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      deltaLog.checkpoint()</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: IllegalStateException =&gt;</span><br><span class="line">        logWarning(&quot;Failed to checkpoint table state.&quot;, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>呼び出されるタイミングは予め設定されたインターバルのパラメータに依存する。</p>
<p>ということは、数回に1回はチェックポイント書き出しが行われるため、そのあたりでパフォーマンス上の影響があるのではないか、と想像される。</p>
<h3><span id="スナップショットについて">スナップショットについて</span></h3>
<p>関連事項として、スナップショットも調査。
スナップショットが作られるタイミングの <strong>一例</strong>
としては、<code>org.apache.spark.sql.delta.DeltaLog#updateInternal</code>メソッドが
呼ばれるタイミングが挙げられる。 updateInternalメソッドは
<code>org.apache.spark.sql.delta.DeltaLog#update</code>
メソッド内で呼ばれる。
updateメソッドが呼ばれるタイミングはいくつかあるが、例えばトランザクション開始時に呼ばれる流れも存在する。
つまり、トランザクションを開始する前にはいったんスナップショットが定義されることがわかった。</p>
<p>その他にも、
<code>org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation</code>
メソッドの処理から実行されるケースもある。</p>
<p>このように、いくつかのタイミングでスナップショットが定義（最新化？）されるようになっている。</p>
<h2><span id="動作確認しながら実装確認v050">動作確認しながら実装確認（v0.5.0）</span></h2>
<h3><span id="クイックスタートの書き込み">クイックスタートの書き込み</span></h3>
<p><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式ドキュメント（クイックスタート）</a>
を見る限り、 シェルで利用する分には <code>--package</code> などでDelta
Lakeのパッケージを指定すれば良い。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> &lt;path to spark home&gt;/bin/spark-shell --packages io.delta:delta-core_2.11:0.5.0</span></span><br></pre></td></tr></table></figure>
<p>クイックスタートの例を実行する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = spark.range(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>実際に出力されたParquetファイルは以下の通り。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -R /tmp/delta-table/</span></span><br><span class="line">/tmp/delta-table/:</span><br><span class="line">_delta_log                                                           part-00003-93af5943-2745-42e8-9ac6-c001f257f3a8-c000.snappy.parquet  part-00007-8ed33d7c-5634-4739-afbb-471961bec689-c000.snappy.parquet</span><br><span class="line">part-00000-26d26a0d-ad19-44ac-aa78-046d1709e28b-c000.snappy.parquet  part-00004-11001300-1797-4a69-9155-876319eb2d00-c000.snappy.parquet</span><br><span class="line">part-00001-6e4655ff-555e-441d-bdc9-68176e630936-c000.snappy.parquet  part-00006-94de7a9e-4dbd-4b50-b33c-949ae38dc676-c000.snappy.parquet</span><br><span class="line"></span><br><span class="line">/tmp/delta-table/_delta_log:</span><br><span class="line">00000000000000000000.json</span><br></pre></td></tr></table></figure>
<p>これをデバッガをアタッチして、動作状況を覗いてみることにする。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> &lt;path to spark home&gt;/bin/spark-shell --packages io.delta:delta-core_2.11:0.5.0 --driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,<span class="built_in">suspend</span>=y,address=5005</span></span><br></pre></td></tr></table></figure>
<p>Intellijなどでデバッガをアタッチする。</p>
<p>机上調査があっているか確認するため、
<code>org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation</code>
に ブレイクポイントを設定して動作確認。</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:119</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = parameters.getOrElse(<span class="string">"path"</span>, &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.pathNotSpecifiedException</span><br><span class="line">&#125;)</span><br><span class="line"><span class="keyword">val</span> partitionColumns = parameters.get(<span class="type">DeltaSourceUtils</span>.<span class="type">PARTITIONING_COLUMNS_KEY</span>)</span><br><span class="line">  .map(<span class="type">DeltaDataSource</span>.decodePartitioningColumns)</span><br><span class="line">  .getOrElse(<span class="type">Nil</span>)</span><br></pre></td></tr></table></figure>
<p>パスとパーティション指定するカラムを確認。 上記の例では、
<code>/tmp/delta-table</code> と <code>Nil</code> が戻り値になる。</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:126</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> deltaLog = <span class="type">DeltaLog</span>.forTable(sqlContext.sparkSession, path)</span><br></pre></td></tr></table></figure>
<p>DeltaLogのインスタンスを受け取る。</p>
<p>つづいて、
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#run</code>
メソッドが呼び出される。</p>
<p>src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:63</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">  deltaLog.withNewTransaction &#123; txn =&gt;</span><br><span class="line">    <span class="keyword">val</span> actions = write(txn, sparkSession)</span><br><span class="line">    <span class="keyword">val</span> operation = <span class="type">DeltaOperations</span>.<span class="type">Write</span>(mode, <span class="type">Option</span>(partitionColumns), options.replaceWhere)</span><br><span class="line">    txn.commit(actions, operation)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Seq</span>.empty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>withNewTransaction</code> 内で、
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#write</code>が呼び出される。
つまり、ここでトランザクションが開始され、下記の記載の通り、最終的にコミットされることになる。</p>
<p>いったん <code>withNewTransaction</code>
の中で行われる処理に着目する。
まずはメタデータ（スキーマ？など？）が更新される。</p>
<p>src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:85</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">updateMetadata(txn, data, partitionColumns, configuration, isOverwriteOperation, rearrangeOnly)</span><br></pre></td></tr></table></figure>
<p>つづいて、"reaplaceWhere"
機能（パーティション列に対し、述語に一致するデータのみを置き換える機能）の
フィルタを算出する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> replaceWhere = options.replaceWhere</span><br><span class="line"><span class="keyword">val</span> partitionFilters = <span class="keyword">if</span> (replaceWhere.isDefined) &#123;</span><br><span class="line">  <span class="keyword">val</span> predicates = parsePartitionPredicates(sparkSession, replaceWhere.get)</span><br><span class="line">  <span class="keyword">if</span> (mode == <span class="type">SaveMode</span>.<span class="type">Overwrite</span>) &#123;</span><br><span class="line">    verifyPartitionPredicates(</span><br><span class="line">      sparkSession, txn.metadata.partitionColumns, predicates)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Some</span>(predicates)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="type">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>つづいて、
<code>org.apache.spark.sql.delta.files.TransactionalWrite#writeFiles</code>
メソッドが呼び出される。</p>
<p>src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:105</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newFiles = txn.writeFiles(data, <span class="type">Some</span>(options))</span><br></pre></td></tr></table></figure>
<p>内部的には、
<code>org.apache.spark.sql.execution.datasources.FileFormatWriter#write</code>
を用いて 与えられた <code>data</code> （＝DataFrame つまり
Dataset）を書き出す処理（物理プラン）を実行する。</p>
<p>ここでのポイントは、割と直接的にSparkのDataSourcesの機能を利用しているところだ。
実装が簡素になる代わりに、Sparkに強く依存していることがわかる。</p>
<p>また、newFilesには出力PATHに作られるファイル群の情報（実際にはcase
classのインスタンス）が含まれる。</p>
<p>ここで
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#write</code>
の呼び出し元、
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#run</code>
に戻る。</p>
<p>writeが呼ばれたあとは、オペレーション情報がインスタンス化される。</p>
<p>src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:66</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> operation = <span class="type">DeltaOperations</span>.<span class="type">Write</span>(mode, <span class="type">Option</span>(partitionColumns), options.replaceWhere)</span><br></pre></td></tr></table></figure>
<p>上記のアクションとオペレーション情報を合わせて、以下のようにコミットされる。</p>
<p>src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:67</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">txn.commit(actions, operation)</span><br></pre></td></tr></table></figure>
<h3><span id="クイックスタートの読み込み">クイックスタートの読み込み</span></h3>
<p><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式ドキュメント（クイックスタート）</a>
には以下のような簡単な例が載っている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<p>デバッガをアタッチしながら動作を確認しよう。</p>
<p>まず最初の</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>により、SparkのDataSourceの仕組みに基づいて、リレーションが生成される。
<code>org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation</code>
メソッドあたりを読み解く。</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:149</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> maybeTimeTravel =</span><br><span class="line">  <span class="type">DeltaTableUtils</span>.extractIfPathContainsTimeTravel(sqlContext.sparkSession, maybePath)</span><br></pre></td></tr></table></figure>
<p>最初にタイムトラベル対象かどうかを判定する。タイムトラベル自体は別途。</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:159</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> hadoopPath = <span class="keyword">new</span> <span class="type">Path</span>(path)</span><br><span class="line"><span class="keyword">val</span> rootPath = <span class="type">DeltaTableUtils</span>.findDeltaTableRoot(sqlContext.sparkSession, hadoopPath)</span><br><span class="line">  .getOrElse &#123;</span><br><span class="line">    <span class="keyword">val</span> fs = hadoopPath.getFileSystem(sqlContext.sparkSession.sessionState.newHadoopConf())</span><br><span class="line">    <span class="keyword">if</span> (!fs.exists(hadoopPath)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.pathNotExistsException(path)</span><br><span class="line">    &#125;</span><br><span class="line">    hadoopPath</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>つづいて、Hadoopの機能を利用し、下回りのデータストアのファイルシステムを取得する。
このあたりでHadoopの機能を利用しているあたりが、HadoopやSparkを前提としたシステムであることがわかる。</p>
<p>また、一貫性を考えると、通常のHadoopやSparkを利用するときと同様に、
S3で一貫性を担保する仕組み（例えばs3a、s3ガードなど）を利用したほうが良いだろう。</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:169</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> deltaLog = <span class="type">DeltaLog</span>.forTable(sqlContext.sparkSession, rootPath)</span><br></pre></td></tr></table></figure>
<p>つづいて、DeltaLogインスタンスが生成される。
これにより、ログファイルに対する操作を開始できるようになる。（ここでは読み取りだが、書き込みも対応可能になる）</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:171</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> partitionFilters = <span class="keyword">if</span> (rootPath != hadoopPath) &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (files.count() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.pathNotExistsException(path)</span><br><span class="line">      &#125;</span><br><span class="line">      filters</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Nil</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>パーティションの読み込みかどうかを検知。 ただし、Delta
LakeではパーティションのPATHを直接指定するのは推奨されておらず、where句を使うのが推奨されている。</p>
<p>src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:210</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deltaLog.createRelation(partitionFilters, timeTravelByParams.orElse(timeTravelByPath))</span><br></pre></td></tr></table></figure>
<p>最後に、実際にリレーションを生成し、戻り値とする。</p>
<p>なお、
<code>org.apache.spark.sql.delta.DeltaLog#createDataFrame</code>
メソッドは以下の通り。</p>
<p>src/main/scala/org/apache/spark/sql/delta/DeltaLog.scala:630</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span></span>(</span><br><span class="line">      partitionFilters: <span class="type">Seq</span>[<span class="type">Expression</span>] = <span class="type">Nil</span>,</span><br><span class="line">      timeTravel: <span class="type">Option</span>[<span class="type">DeltaTimeTravelSpec</span>] = <span class="type">None</span>): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">HadoopFsRelation</span>(</span><br><span class="line">      fileIndex,</span><br><span class="line">      partitionSchema = snapshotToUse.metadata.partitionSchema,</span><br><span class="line">      dataSchema = snapshotToUse.metadata.schema,</span><br><span class="line">      bucketSpec = <span class="type">None</span>,</span><br><span class="line">      snapshotToUse.fileFormat,</span><br><span class="line">      snapshotToUse.metadata.format.options)(spark) <span class="keyword">with</span> <span class="type">InsertableRelation</span> &#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">insert</span></span>(data: <span class="type">DataFrame</span>, overwrite: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> mode = <span class="keyword">if</span> (overwrite) <span class="type">SaveMode</span>.<span class="type">Overwrite</span> <span class="keyword">else</span> <span class="type">SaveMode</span>.<span class="type">Append</span></span><br><span class="line">        <span class="type">WriteIntoDelta</span>(</span><br><span class="line">          deltaLog = <span class="type">DeltaLog</span>.<span class="keyword">this</span>,</span><br><span class="line">          mode = mode,</span><br><span class="line">          <span class="keyword">new</span> <span class="type">DeltaOptions</span>(<span class="type">Map</span>.empty[<span class="type">String</span>, <span class="type">String</span>], spark.sessionState.conf),</span><br><span class="line">          partitionColumns = <span class="type">Seq</span>.empty,</span><br><span class="line">          configuration = <span class="type">Map</span>.empty,</span><br><span class="line">          data = data).run(spark)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4><span id="tahoelogfileindex">TahoeLogFileIndex</span></h4>
<p>なお、生成されたDataFrameのプランを確認すると以下のように表示される。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.explain</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(1) FileScan parquet [id#778L] Batched: true, Format: Parquet, Location: TahoeLogFileIndex[file:/tmp/delta-table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt;</span><br></pre></td></tr></table></figure>
<p><code>TahoeLogFileIndex</code> はDelta
Lakeが実装しているFileIndexの一種。</p>
<p>クラス階層は以下の通り。</p>
<ul>
<li>TahoeFileIndex (org.apache.spark.sql.delta.files)
<ul>
<li>TahoeBatchFileIndex (org.apache.spark.sql.delta.files)</li>
<li>TahoeLogFileIndex (org.apache.spark.sql.delta.files)</li>
</ul></li>
</ul>
<p>親クラスの <code>TahoeFileIndex</code> は、以下の通り
<code>FileIndex</code>を継承している。</p>
<p>org/apache/spark/sql/delta/files/TahoeFileIndex.scala:35</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TahoeFileIndex</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val spark: <span class="type">SparkSession</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val deltaLog: <span class="type">DeltaLog</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val path: <span class="type">Path</span></span>) <span class="keyword">extends</span> <span class="title">FileIndex</span> </span>&#123;</span><br></pre></td></tr></table></figure>
<p><code>TahoeFileIndex</code> のJavaDocには、</p>
<blockquote>
<p>A [[FileIndex]] that generates the list of files managed by the Tahoe
protocol.</p>
</blockquote>
<p>とあり、「Tahoeプロトコル」なるものを通じて、SparkのFileIndex機能を実現する。</p>
<p>例えば、showメソッドを呼び出すと、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show()</span><br></pre></td></tr></table></figure>
<p>実行の途中で、
<code>org.apache.spark.sql.delta.files.TahoeFileIndex#listFiles</code>
メソッドが呼び出される。</p>
<p>以下、listFiles内での動作を軽く確認する。</p>
<p>org/apache/spark/sql/delta/files/TahoeFileIndex.scala:56</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matchingFiles(partitionFilters, dataFilters).groupBy(_.partitionValues).map &#123;</span><br></pre></td></tr></table></figure>
<p>まず、
<code>org.apache.spark.sql.delta.files.TahoeFileIndex#matchingFiles</code>
メソッドが呼ばれ、 <code>AddFile</code>
インスタンスのシーケンスが返される。</p>
<p>org/apache/spark/sql/delta/files/TahoeFileIndex.scala:129</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">matchingFiles</span></span>(</span><br><span class="line">    partitionFilters: <span class="type">Seq</span>[<span class="type">Expression</span>],</span><br><span class="line">    dataFilters: <span class="type">Seq</span>[<span class="type">Expression</span>],</span><br><span class="line">    keepStats: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Seq</span>[<span class="type">AddFile</span>] = &#123;</span><br><span class="line">  getSnapshot(stalenessAcceptable = <span class="literal">false</span>).filesForScan(</span><br><span class="line">    projection = <span class="type">Nil</span>, <span class="keyword">this</span>.partitionFilters ++ partitionFilters ++ dataFilters, keepStats).files</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、戻り値は <code>Seq[AddFile]</code> である。</p>
<p>そこで <code>matchingFiles</code>
メソッドの戻り値をグループ化した後は、mapメソッドにより、
以下のような処理が実行される。</p>
<p>org/apache/spark/sql/delta/files/TahoeFileIndex.scala:57</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> (partitionValues, files) =&gt;</span><br><span class="line">  <span class="keyword">val</span> rowValues: <span class="type">Array</span>[<span class="type">Any</span>] = partitionSchema.map &#123; p =&gt;</span><br><span class="line">    <span class="type">Cast</span>(<span class="type">Literal</span>(partitionValues(p.name)), p.dataType, <span class="type">Option</span>(timeZone)).eval()</span><br><span class="line">  &#125;.toArray</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fileStats = files.map &#123; f =&gt;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">FileStatus</span>(</span><br><span class="line">      <span class="comment">/* length */</span> f.size,</span><br><span class="line">      <span class="comment">/* isDir */</span> <span class="literal">false</span>,</span><br><span class="line">      <span class="comment">/* blockReplication */</span> <span class="number">0</span>,</span><br><span class="line">      <span class="comment">/* blockSize */</span> <span class="number">1</span>,</span><br><span class="line">      <span class="comment">/* modificationTime */</span> f.modificationTime,</span><br><span class="line">      absolutePath(f.path))</span><br><span class="line">  &#125;.toArray</span><br><span class="line"></span><br><span class="line">  <span class="type">PartitionDirectory</span>(<span class="keyword">new</span> <span class="type">GenericInternalRow</span>(rowValues), fileStats)</span><br></pre></td></tr></table></figure>
<p>参考までに、このとき、 <code>files</code>
には以下のような値が入っている。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">files = &#123;WrappedArray$ofRef@19711&#125; &quot;WrappedArray$ofRef&quot; size = 6</span><br><span class="line"> 0 = &#123;AddFile@19694&#125; &quot;AddFile(part-00004-11001300-1797-4a69-9155-876319eb2d00-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot;</span><br><span class="line"> 1 = &#123;AddFile@19719&#125; &quot;AddFile(part-00003-93af5943-2745-42e8-9ac6-c001f257f3a8-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot;</span><br><span class="line"> 2 = &#123;AddFile@19720&#125; &quot;AddFile(part-00000-26d26a0d-ad19-44ac-aa78-046d1709e28b-c000.snappy.parquet,Map(),262,1582472443000,false,null,null)&quot;</span><br><span class="line"> 3 = &#123;AddFile@19721&#125; &quot;AddFile(part-00006-94de7a9e-4dbd-4b50-b33c-949ae38dc676-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot;</span><br><span class="line"> 4 = &#123;AddFile@19722&#125; &quot;AddFile(part-00001-6e4655ff-555e-441d-bdc9-68176e630936-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot;</span><br><span class="line"> 5 = &#123;AddFile@19723&#125; &quot;AddFile(part-00007-8ed33d7c-5634-4739-afbb-471961bec689-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot;</span><br></pre></td></tr></table></figure>
<p>ここまでが
<code>org.apache.spark.sql.execution.datasources.FileIndex#listFiles</code>
メソッドの内容である。</p>
<h4><span id="参考tahoefileindexがどこから呼ばれるか">（参考）TahoeFileIndexがどこから呼ばれるか</span></h4>
<p>ここではSpark v2.4.2を確認する。</p>
<p><code>org.apache.spark.sql.Dataset#show</code> メソッドでは、内部的に
<code>org.apache.spark.sql.Dataset#showString</code>
メソッドを呼び出す。</p>
<p>org/apache/spark/sql/Dataset.scala:744</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span></span>(numRows: <span class="type">Int</span>, truncate: <span class="type">Boolean</span>): <span class="type">Unit</span> = <span class="keyword">if</span> (truncate) &#123;</span><br><span class="line">  println(showString(numRows, truncate = <span class="number">20</span>))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  println(showString(numRows, truncate = <span class="number">0</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.Dataset#showString</code> メソッド内で、
<code>org.apache.spark.sql.Dataset#getRows</code>
メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/Dataset.scala:285</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">showString</span></span>(</span><br><span class="line">      _numRows: <span class="type">Int</span>,</span><br><span class="line">      truncate: <span class="type">Int</span> = <span class="number">20</span>,</span><br><span class="line">      vertical: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> numRows = _numRows.max(<span class="number">0</span>).min(<span class="type">ByteArrayMethods</span>.<span class="type">MAX_ROUNDED_ARRAY_LENGTH</span> - <span class="number">1</span>)</span><br><span class="line">    <span class="comment">// Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.</span></span><br><span class="line">    <span class="keyword">val</span> tmpRows = getRows(numRows, truncate)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.Dataset#getRows</code> メソッド内では、
<code>org.apache.spark.sql.Dataset#take</code> メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/Dataset.scala:241</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">getRows</span></span>(</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data = newDf.select(castCols: _*).take(numRows + <span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.Dataset#take</code> メソッドでは
<code>org.apache.spark.sql.Dataset#head(int)</code>
メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/Dataset.scala:2758</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = head(n)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.Dataset#head</code> メソッド内で、
<code>org.apache.spark.sql.Dataset#withAction</code> を用いて、
<code>org.apache.spark.sql.Dataset#collectFromPlan</code>
メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/Dataset.scala:2544</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = withAction(<span class="string">"head"</span>, limit(n).queryExecution)(collectFromPlan)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.Dataset#collectFromPlan</code>
メソッド内で
<code>org.apache.spark.sql.execution.SparkPlan#executeCollect</code>
メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/Dataset.scala:3379</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">collectFromPlan</span></span>(plan: <span class="type">SparkPlan</span>): <span class="type">Array</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="comment">// This projection writes output to a `InternalRow`, which means applying this projection is not</span></span><br><span class="line">  <span class="comment">// thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.</span></span><br><span class="line">  <span class="keyword">val</span> objProj = <span class="type">GenerateSafeProjection</span>.generate(deserializer :: <span class="type">Nil</span>)</span><br><span class="line">  plan.executeCollect().map &#123; row =&gt;</span><br><span class="line">    <span class="comment">// The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type</span></span><br><span class="line">    <span class="comment">// parameter of its `get` method, so it's safe to use null here.</span></span><br><span class="line">    objProj(row).get(<span class="number">0</span>, <span class="literal">null</span>).asInstanceOf[<span class="type">T</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、 <code>org.apache.spark.sql.execution.CollectLimitExec</code>
のcase classインスタンス化の中で、
<code>org.apache.spark.sql.execution.CollectLimitExec#executeCollect</code>
が以下のように定義されている。</p>
<p>org/apache/spark/sql/execution/limit.scala:35</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CollectLimitExec</span>(<span class="params">limit: <span class="type">Int</span>, child: <span class="type">SparkPlan</span></span>) <span class="keyword">extends</span> <span class="title">UnaryExecNode</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">output</span></span>: <span class="type">Seq</span>[<span class="type">Attribute</span>] = child.output</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputPartitioning</span></span>: <span class="type">Partitioning</span> = <span class="type">SinglePartition</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">executeCollect</span></span>(): <span class="type">Array</span>[<span class="type">InternalRow</span>] = child.executeTake(limit)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>そこで、
<code>org.apache.spark.sql.execution.SparkPlan#executeTake</code>
メソッドに着目していく。 当該メソッド内で、
<code>org.apache.spark.sql.execution.SparkPlan#getByteArrayRdd</code>
メソッドが呼ばれ、 <code>childRDD</code> が生成される。</p>
<p>org/apache/spark/sql/execution/SparkPlan.scala:334</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">executeTake</span></span>(n: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (n == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">InternalRow</span>](<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> childRDD = getByteArrayRdd(n).map(_._2)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.execution.SparkPlan#getByteArrayRdd</code>
メソッドの実装は以下の通りで、 内部的に
<code>org.apache.spark.sql.execution.SparkPlan#execute</code>
メソッドが呼ばれる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getByteArrayRdd</span></span>(n: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Array</span>[<span class="type">Byte</span>])] = &#123;</span><br><span class="line">    execute().mapPartitionsInternal &#123; iter =&gt;</span><br><span class="line">      <span class="keyword">var</span> count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.execution.SparkPlan#execute</code>
メソッド内で
<code>org.apache.spark.sql.execution.SparkPlan#executeQuery</code>
を利用し、
その内部で<code>org.apache.spark.sql.execution.SparkPlan#doExecute</code>
メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/execution/SparkPlan.scala:127</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = executeQuery &#123;</span><br><span class="line">  <span class="keyword">if</span> (isCanonicalizedPlan) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"A canonicalized plan is not supposed to be executed."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  doExecute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>参考までに、<code>org.apache.spark.sql.execution.SparkPlan#executeQuery</code>
内では、 <code>org.apache.spark.rdd.RDDOperationScope#withScope</code>
を使ってクエリが実行される。</p>
<p>org/apache/spark/sql/execution/SparkPlan.scala:151</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">executeQuery</span></span>[<span class="type">T</span>](query: =&gt; <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">  <span class="type">RDDOperationScope</span>.withScope(sparkContext, nodeName, <span class="literal">false</span>, <span class="literal">true</span>) &#123;</span><br><span class="line">    prepare()</span><br><span class="line">    waitForSubqueries()</span><br><span class="line">    query</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>つづいて、 <code>doExecute</code> メソッドの確認に戻る。</p>
<p><code>org.apache.spark.sql.execution.WholeStageCodegenExec#doExecute</code>
メソッド内で、
<code>org.apache.spark.sql.execution.CodegenSupport#inputRDDs</code>
メソッドが呼ばれる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> durationMs = longMetric(<span class="string">"pipelineTime"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdds = child.asInstanceOf[<span class="type">CodegenSupport</span>].inputRDDs()</span><br><span class="line">    assert(rdds.size &lt;= <span class="number">2</span>, <span class="string">"Up to two input RDDs can be supported"</span>)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>inputRDDs</code> メソッド内でinputRDDが呼び出される。</p>
<p>org/apache/spark/sql/execution/DataSourceScanExec.scala:326</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputRDDs</span></span>(): <span class="type">Seq</span>[<span class="type">RDD</span>[<span class="type">InternalRow</span>]] = &#123;</span><br><span class="line">  inputRDD :: <span class="type">Nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>inputRDD</code> にRDDインスタンスをバインドする際、その中で
<code>org.apache.spark.sql.execution.FileSourceScanExec#createNonBucketedReadRDD</code>
メソッドが呼ばれ、 そこに渡される <code>readFile</code>
メソッドが実行される。</p>
<p>org/apache/spark/sql/execution/DataSourceScanExec.scala:305</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="comment">// Update metrics for taking effect in both code generation node and normal node.</span></span><br><span class="line">  updateDriverMetrics()</span><br><span class="line">  <span class="keyword">val</span> readFile: (<span class="type">PartitionedFile</span>) =&gt; <span class="type">Iterator</span>[<span class="type">InternalRow</span>] =</span><br><span class="line">    relation.fileFormat.buildReaderWithPartitionValues(</span><br><span class="line">      sparkSession = relation.sparkSession,</span><br><span class="line">      dataSchema = relation.dataSchema,</span><br><span class="line">      partitionSchema = relation.partitionSchema,</span><br><span class="line">      requiredSchema = requiredSchema,</span><br><span class="line">      filters = pushedDownFilters,</span><br><span class="line">      options = relation.options,</span><br><span class="line">      hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options))</span><br><span class="line"></span><br><span class="line">  relation.bucketSpec <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(bucketing) <span class="keyword">if</span> relation.sparkSession.sessionState.conf.bucketingEnabled =&gt;</span><br><span class="line">      createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      createNonBucketedReadRDD(readFile, selectedPartitions, relation)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>readFile</code> メソッドを一緒に
<code>org.apache.spark.sql.execution.FileSourceScanExec#createNonBucketedReadRDD</code>
メソッドに渡されている
<code>org.apache.spark.sql.execution.FileSourceScanExec#selectedPartitions</code>
は以下のように定義される。 <code>Seq[PartitionDirectory]</code>
のインスタンスを <code>selectedPartitions</code> にバインドする際に、
<code>org.apache.spark.sql.execution.datasources.FileIndex#listFiles</code>
メソッドが呼び出される。</p>
<p>org/apache/spark/sql/execution/DataSourceScanExec.scala:190</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@transient private lazy val selectedPartitions: Seq[PartitionDirectory] = &#123;</span><br><span class="line">  val optimizerMetadataTimeNs = relation.location.metadataOpsTimeNs.getOrElse(0L)</span><br><span class="line">  val startTime = System.nanoTime()</span><br><span class="line">  val ret = relation.location.listFiles(partitionFilters, dataFilters)</span><br><span class="line">  val timeTakenMs = ((System.nanoTime() - startTime) + optimizerMetadataTimeNs) / 1000 / 1000</span><br><span class="line">  metadataTime = timeTakenMs</span><br><span class="line">  ret</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>このとき、具象クラス側の <code>listFiles</code>
メソッドが呼ばれることになるが、 <code>TahoeLogFileIndex</code>
クラスの場合は、親クラスのメソッド
<code>org.apache.spark.sql.delta.files.TahoeFileIndex#listFiles</code>
が呼ばれる。 当該メソッドの内容は、上記で示したとおり。</p>
<h3><span id="クイックスタートの更新時">クイックスタートの更新時</span></h3>
<p><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式ドキュメント（クイックスタート）</a>
には 以下のような例が載っている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = spark.range(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">data.write.format(<span class="string">"delta"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>上記を実行した際のデータ保存ディレクトリの構成は以下の通り。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -R /tmp/delta-table/</span></span><br><span class="line">/tmp/delta-table/:</span><br><span class="line">_delta_log                                                           part-00004-11001300-1797-4a69-9155-876319eb2d00-c000.snappy.parquet</span><br><span class="line">part-00000-191c798b-3202-4fdf-9447-891f19953a37-c000.snappy.parquet  part-00004-62735ceb-255f-4349-b043-d14d798f653a-c000.snappy.parquet</span><br><span class="line">part-00000-26d26a0d-ad19-44ac-aa78-046d1709e28b-c000.snappy.parquet  part-00006-94de7a9e-4dbd-4b50-b33c-949ae38dc676-c000.snappy.parquet</span><br><span class="line">part-00001-6e4655ff-555e-441d-bdc9-68176e630936-c000.snappy.parquet  part-00006-f5cb90a2-06bd-46ec-af61-9490bdd4321c-c000.snappy.parquet</span><br><span class="line">part-00001-b3bcd196-dc8b-43b8-ad43-f73ecac35ccb-c000.snappy.parquet  part-00007-6431fca3-bf2c-4ad7-a42c-a2e18feb3ed7-c000.snappy.parquet</span><br><span class="line">part-00003-7fed5d2a-0ba9-4dcf-bc8d-ad9c729884e3-c000.snappy.parquet  part-00007-8ed33d7c-5634-4739-afbb-471961bec689-c000.snappy.parquet</span><br><span class="line">part-00003-93af5943-2745-42e8-9ac6-c001f257f3a8-c000.snappy.parquet</span><br><span class="line"></span><br><span class="line">/tmp/delta-table/_delta_log:</span><br><span class="line">00000000000000000000.json  00000000000000000001.json</span><br></pre></td></tr></table></figure>
<p><code>00000000000000000001.json</code> の内容は以下の通り。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"commitInfo"</span>: &#123;</span><br><span class="line">    <span class="attr">"timestamp"</span>: <span class="number">1582681285873</span>,</span><br><span class="line">    <span class="attr">"operation"</span>: <span class="string">"WRITE"</span>,</span><br><span class="line">    <span class="attr">"operationParameters"</span>: &#123;</span><br><span class="line">      <span class="attr">"mode"</span>: <span class="string">"Overwrite"</span>,</span><br><span class="line">      <span class="attr">"partitionBy"</span>: <span class="string">"[]"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"readVersion"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="attr">"isBlindAppend"</span>: <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"add"</span>: &#123;</span><br><span class="line">    <span class="attr">"path"</span>: <span class="string">"part-00000-191c798b-3202-4fdf-9447-891f19953a37-c000.snappy.parquet"</span>,</span><br><span class="line">    <span class="attr">"partitionValues"</span>: &#123;&#125;,</span><br><span class="line">    <span class="attr">"size"</span>: <span class="number">262</span>,</span><br><span class="line">    <span class="attr">"modificationTime"</span>: <span class="number">1582681285000</span>,</span><br><span class="line">    <span class="attr">"dataChange"</span>: <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>確かに上書きモードで書き込まれたことが確かめられる。</p>
<p>このモードは、例えば
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta</code>
で用いられる。 上記の</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.write.format(<span class="string">"delta"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>が呼ばれるときに、内部的に
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#run</code>
メソッドが呼ばれ、 以下のように、 <code>mode</code> が渡される。</p>
<p>org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:63</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">  deltaLog.withNewTransaction &#123; txn =&gt;</span><br><span class="line">    <span class="keyword">val</span> actions = write(txn, sparkSession)</span><br><span class="line">    <span class="keyword">val</span> operation = <span class="type">DeltaOperations</span>.<span class="type">Write</span>(mode, <span class="type">Option</span>(partitionColumns), options.replaceWhere)</span><br><span class="line">    txn.commit(actions, operation)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Seq</span>.empty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、overwriteモードを指定しないと、 <code>mode</code> には
<code>ErrorIfExists</code> が渡される。 モードの詳細は、enum
<code>org.apache.spark.sql.SaveMode</code> （Spark SQL）を参照。
（例えば、他にも <code>append</code> や <code>Ignore</code>
あたりも指定可能そうではある）</p>
<h3><span id="クイックスタートのストリームデータ読み書き">クイックスタートのストリームデータ読み書き</span></h3>
<p><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式ドキュメント（クイックスタート）</a>
には以下の例が載っていた。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> streamingDf = spark.readStream.format(<span class="string">"rate"</span>).load()</span><br><span class="line">scala&gt; <span class="keyword">val</span> stream = streamingDf.select($<span class="string">"value"</span> as <span class="string">"id"</span>).writeStream.format(<span class="string">"delta"</span>).option(<span class="string">"checkpointLocation"</span>, <span class="string">"/tmp/checkpoint"</span>).start(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>別のターミナルを開き、Sparkシェルを開き、以下でデータを読み込む。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>裏でストリームデータを書き込んでいるので、適当に間をおいてcountを実行するとレコード数が増えていることが確かめられる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.count</span><br><span class="line">res1: <span class="type">Long</span> = <span class="number">1139</span></span><br><span class="line"></span><br><span class="line">scala&gt; df.count</span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">1158</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"id"</span>).count.sort($<span class="string">"count"</span>.desc).show</span><br><span class="line">+----+-----+</span><br><span class="line">|  id|count|</span><br><span class="line">+----+-----+</span><br><span class="line">| <span class="number">964</span>|    <span class="number">1</span>|</span><br><span class="line">|  <span class="number">29</span>|    <span class="number">1</span>|</span><br><span class="line">|  <span class="number">26</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">474</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">831</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="number">1042</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">167</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">112</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">299</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">155</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">385</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">736</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">113</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="number">1055</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">502</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="number">1480</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">656</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">287</span>|    <span class="number">1</span>|</span><br><span class="line">|   <span class="number">0</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">348</span>|    <span class="number">1</span>|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure>
<p>止めるには、ストリーム処理を実行しているターミナルで以下を実行。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; stream.stop()</span><br></pre></td></tr></table></figure>
<p>上記だと面白くないので、次に実行する処理内容を少しだけ修正。10で割った余りとするようにした。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> stream = streamingDf.select($<span class="string">"value"</span> % <span class="number">10</span> as <span class="string">"id"</span>).writeStream.format(<span class="string">"delta"</span>).option(<span class="string">"checkpointLocation"</span>, <span class="string">"/tmp/checkpoint"</span>).start(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>結果は以下の通り。先程書き込んだデータも含まれているので、
件数が1個のデータと、いま書き込んでいる10で割った余りのデータが混在しているように見えるはず。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"id"</span>).count.sort($<span class="string">"count"</span>.desc).show</span><br><span class="line">+----+-----+</span><br><span class="line">|  id|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|   <span class="number">0</span>|   <span class="number">19</span>|</span><br><span class="line">|   <span class="number">9</span>|   <span class="number">19</span>|</span><br><span class="line">|   <span class="number">1</span>|   <span class="number">19</span>|</span><br><span class="line">|   <span class="number">2</span>|   <span class="number">19</span>|</span><br><span class="line">|   <span class="number">3</span>|   <span class="number">18</span>|</span><br><span class="line">|   <span class="number">6</span>|   <span class="number">18</span>|</span><br><span class="line">|   <span class="number">5</span>|   <span class="number">18</span>|</span><br><span class="line">|   <span class="number">8</span>|   <span class="number">18</span>|</span><br><span class="line">|   <span class="number">7</span>|   <span class="number">18</span>|</span><br><span class="line">|   <span class="number">4</span>|   <span class="number">18</span>|</span><br><span class="line">|<span class="number">1055</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="number">1547</span>|    <span class="number">1</span>|</span><br><span class="line">|  <span class="number">26</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="number">1224</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">502</span>|    <span class="number">1</span>|</span><br><span class="line">|<span class="number">1480</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">237</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">588</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">602</span>|    <span class="number">1</span>|</span><br><span class="line">| <span class="number">347</span>|    <span class="number">1</span>|</span><br><span class="line">+----+-----+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br></pre></td></tr></table></figure>
<p>つづいてドキュメントに載っているストリームとしての読み取りを試す。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> stream2 = spark.readStream.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>).writeStream.format(<span class="string">"console"</span>).start()</span><br></pre></td></tr></table></figure>
<p>以下のような表示がコンソールに出るはず。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Batch: 7</span><br><span class="line">-------------------------------------------</span><br><span class="line">+---+</span><br><span class="line">| id|</span><br><span class="line">+---+</span><br><span class="line">|  5|</span><br><span class="line">|  6|</span><br><span class="line">+---+</span><br></pre></td></tr></table></figure>
<p>それぞれ止めておく。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; stream.stop()</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; stream2.stop()</span><br></pre></td></tr></table></figure>
<p>さて、まず書き込み時の動作を確認する。</p>
<p>上記の例で言うと、ストリーム処理を定義し、スタートすると、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> stream = streamingDf.select($<span class="string">"value"</span> as <span class="string">"id"</span>).writeStream.format(<span class="string">"delta"</span>).option(<span class="string">"checkpointLocation"</span>, <span class="string">"/tmp/checkpoint"</span>).start(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>以下の
<code>org.apache.spark.sql.delta.sources.DeltaDataSource#createSink</code>
メソッドが呼ばれる。</p>
<p>org/apache/spark/sql/delta/sources/DeltaDataSource.scala:99</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createSink</span></span>(</span><br><span class="line">    sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">    parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">    partitionColumns: <span class="type">Seq</span>[<span class="type">String</span>],</span><br><span class="line">    outputMode: <span class="type">OutputMode</span>): <span class="type">Sink</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> path = parameters.getOrElse(<span class="string">"path"</span>, &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.pathNotSpecifiedException</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="keyword">if</span> (outputMode != <span class="type">OutputMode</span>.<span class="type">Append</span> &amp;&amp; outputMode != <span class="type">OutputMode</span>.<span class="type">Complete</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.outputModeNotSupportedException(getClass.getName, outputMode)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> deltaOptions = <span class="keyword">new</span> <span class="type">DeltaOptions</span>(parameters, sqlContext.sparkSession.sessionState.conf)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">DeltaSink</span>(sqlContext, <span class="keyword">new</span> <span class="type">Path</span>(path), partitionColumns, outputMode, deltaOptions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、実態は
<code>org.apache.spark.sql.delta.sources.DeltaSink</code> クラスである。
当該クラスはSpark Structured Streamingの
<code>org.apache.spark.sql.execution.streaming.Sink</code>
トレートを継承（ミックスイン）している。</p>
<p><code>org.apache.spark.sql.delta.sources.DeltaSink#addBatch</code>
メソッドが、実際にシンクにバッチを追加する処理を規定している。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:50</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatch</span></span>(batchId: <span class="type">Long</span>, data: <span class="type">DataFrame</span>): <span class="type">Unit</span> = deltaLog.withNewTransaction &#123; txn =&gt;</span><br><span class="line">    <span class="keyword">val</span> queryId = sqlContext.sparkContext.getLocalProperty(<span class="type">StreamExecution</span>.<span class="type">QUERY_ID_KEY</span>)</span><br><span class="line">    assert(queryId != <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SchemaUtils</span>.typeExistsRecursively(data.schema)(_.isInstanceOf[<span class="type">NullType</span>])) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.streamWriteNullTypeException</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>上記のように、
<code>org.apache.spark.sql.delta.DeltaLog#withNewTransaction</code>
メソッドを用いてトランザクションが開始される。
また引数には、バッチのユニークなIDと書き込み対象のDataFrameが含まれる。</p>
<p>このまま、軽く
<code>org.apache.spark.sql.delta.sources.DeltaSink#addBatch</code>
の内容を確認する。 以下、順に記載。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:63</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> selfScan = data.queryExecution.analyzed.collectFirst &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">DeltaTable</span>(index) <span class="keyword">if</span> index.deltaLog.isSameLogAs(txn.deltaLog) =&gt; <span class="literal">true</span></span><br><span class="line">&#125;.nonEmpty</span><br><span class="line"><span class="keyword">if</span> (selfScan) &#123;</span><br><span class="line">  txn.readWholeTable()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>この書き込み時に、同時に読み込みをしているかどうかを確認。
（トランザクション制御の関係で・・・）</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:71</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">updateMetadata(</span><br><span class="line">  txn,</span><br><span class="line">  data,</span><br><span class="line">  partitionColumns,</span><br><span class="line">  configuration = <span class="type">Map</span>.empty,</span><br><span class="line">  outputMode == <span class="type">OutputMode</span>.<span class="type">Complete</span>())</span><br></pre></td></tr></table></figure>
<p>メタデータ更新。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:78</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> currentVersion = txn.txnVersion(queryId)</span><br><span class="line"><span class="keyword">if</span> (currentVersion &gt;= batchId) &#123;</span><br><span class="line">  logInfo(<span class="string">s"Skipping already complete epoch <span class="subst">$batchId</span>, in query <span class="subst">$queryId</span>"</span>)</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>バッチが重なっていないかどうかを確認。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:84</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> deletedFiles = outputMode <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> o <span class="keyword">if</span> o == <span class="type">OutputMode</span>.<span class="type">Complete</span>() =&gt;</span><br><span class="line">    deltaLog.assertRemovable()</span><br><span class="line">    txn.filterFiles().map(_.remove)</span><br><span class="line">  <span class="keyword">case</span> _ =&gt; <span class="type">Nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>削除対象ファイルを確認。
なお、ここで記載している例においてはモードがAppendなので、値がNilになる。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:90</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newFiles = txn.writeFiles(data, <span class="type">Some</span>(options))</span><br></pre></td></tr></table></figure>
<p>つづいて、ファイルを書き込み。
ちなみに、書き込まれたファイル情報が戻り値として得られる。
内容は以下のような感じ。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">newFiles = &#123;ArrayBuffer@20585&#125; &quot;ArrayBuffer&quot; size = 7</span><br><span class="line"> 0 = &#123;AddFile@20592&#125; &quot;AddFile(part-00000-80d99ff7-f0cd-48f7-80a1-30e7f60be763-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br><span class="line"> 1 = &#123;AddFile@20593&#125; &quot;AddFile(part-00001-72553d89-8181-4550-b961-11463562768c-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br><span class="line"> 2 = &#123;AddFile@20594&#125; &quot;AddFile(part-00002-3f95aa5d-03fd-40c7-ae36-c65f20d5c7e0-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br><span class="line"> 3 = &#123;AddFile@20595&#125; &quot;AddFile(part-00003-ad9c1b95-868c-475f-a090-d73127bbff2b-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br><span class="line"> 4 = &#123;AddFile@20596&#125; &quot;AddFile(part-00004-2d1b9266-6325-4f72-aa37-46b065d574a0-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br><span class="line"> 5 = &#123;AddFile@20597&#125; &quot;AddFile(part-00005-aa35596d-3eb6-4046-977f-761d6f3e97f5-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br><span class="line"> 6 = &#123;AddFile@20598&#125; &quot;AddFile(part-00006-5d370239-de8d-45fc-9836-6c154808291a-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot;</span><br></pre></td></tr></table></figure>
<p>実際に更新されたファイルを確認すると以下の通り。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ls -lt /tmp/delta-table/ | head</span><br><span class="line">合計 8180</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00000-80d99ff7-f0cd-48f7-80a1-30e7f60be763-c000.snappy.parquet</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00003-ad9c1b95-868c-475f-a090-d73127bbff2b-c000.snappy.parquet</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00006-5d370239-de8d-45fc-9836-6c154808291a-c000.snappy.parquet</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00004-2d1b9266-6325-4f72-aa37-46b065d574a0-c000.snappy.parquet</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00005-aa35596d-3eb6-4046-977f-761d6f3e97f5-c000.snappy.parquet</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00001-72553d89-8181-4550-b961-11463562768c-c000.snappy.parquet</span><br><span class="line">-rw-r--r-- 1 dobachi dobachi  429  3月  1 00:50 part-00002-3f95aa5d-03fd-40c7-ae36-c65f20d5c7e0-c000.snappy.parquet</span><br></pre></td></tr></table></figure>
<p>では実装確認に戻る。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSink.scala:91</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> setTxn = <span class="type">SetTransaction</span>(queryId, batchId, <span class="type">Some</span>(deltaLog.clock.getTimeMillis())) :: <span class="type">Nil</span></span><br><span class="line"><span class="keyword">val</span> info = <span class="type">DeltaOperations</span>.<span class="type">StreamingUpdate</span>(outputMode, queryId, batchId)</span><br><span class="line">txn.commit(setTxn ++ newFiles ++ deletedFiles, info)</span><br></pre></td></tr></table></figure>
<p>トランザクションをコミットすると、 <code>_delta_log</code>
以下のファイルも更新される。</p>
<p>次は、読み込みの動作を確認する。</p>
<p>読み込み時は、
<code>org.apache.spark.sql.delta.sources.DeltaSource#getBatch</code>
メソッドが呼ばれ、 バッチが取得される。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSource.scala:273</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBatch</span></span>(start: <span class="type">Option</span>[<span class="type">Offset</span>], end: <span class="type">Offset</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>ここでは
<code>org.apache.spark.sql.delta.sources.DeltaSource#getBatch</code>
メソッド内の処理を確認する。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSource.scala:274</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> endOffset = <span class="type">DeltaSourceOffset</span>(tableId, end)</span><br><span class="line">previousOffset = endOffset <span class="comment">// For recovery</span></span><br></pre></td></tr></table></figure>
<p>最初に、当該バッチの終わりの位置を示すオフセットを算出する。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSource.scala:276</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> changes = <span class="keyword">if</span> (start.isEmpty) &#123;</span><br><span class="line">  <span class="keyword">if</span> (endOffset.isStartingVersion) &#123;</span><br><span class="line">    getChanges(endOffset.reservoirVersion, <span class="number">-1</span>L, isStartingVersion = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    assert(endOffset.reservoirVersion &gt; <span class="number">0</span>, <span class="string">s"invalid reservoirVersion in endOffset: <span class="subst">$endOffset</span>"</span>)</span><br><span class="line">    <span class="comment">// Load from snapshot `endOffset.reservoirVersion - 1L` so that `index` in `endOffset`</span></span><br><span class="line">    <span class="comment">// is still valid.</span></span><br><span class="line">    getChanges(endOffset.reservoirVersion - <span class="number">1</span>L, <span class="number">-1</span>L, isStartingVersion = <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> startOffset = <span class="type">DeltaSourceOffset</span>(tableId, start.get)</span><br><span class="line">  <span class="keyword">if</span> (!startOffset.isStartingVersion) &#123;</span><br><span class="line">    <span class="comment">// unpersist `snapshot` because it won't be used any more.</span></span><br><span class="line">    cleanUpSnapshotResources()</span><br><span class="line">  &#125;</span><br><span class="line">  getChanges(startOffset.reservoirVersion, startOffset.index, startOffset.isStartingVersion)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、
<code>org.apache.spark.sql.delta.sources.DeltaSource#getChanges</code>
メソッドを使って
<code>org.apache.spark.sql.delta.sources.IndexedFile</code>
インスタンスのイテレータを受け取る。 特に初回のバッチでは変数
<code>start</code> が <code>None</code>
であり、さらにスナップショットから開始するようになる。</p>
<p>なお、初回では無い場合は、以下が実行される。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSource.scala:285</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> startOffset = <span class="type">DeltaSourceOffset</span>(tableId, start.get)</span><br><span class="line">  <span class="keyword">if</span> (!startOffset.isStartingVersion) &#123;</span><br><span class="line">    <span class="comment">// unpersist `snapshot` because it won't be used any more.</span></span><br><span class="line">    cleanUpSnapshotResources()</span><br><span class="line">  &#125;</span><br><span class="line">  getChanges(startOffset.reservoirVersion, startOffset.index, startOffset.isStartingVersion)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>与えられたオフセット情報を元に
<code>org.apache.spark.sql.delta.sources.DeltaSourceOffset</code>
をインスタンス化し、それを用いてイテレータを生成する。</p>
<p>org/apache/spark/sql/delta/sources/DeltaSource.scala:294</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> addFilesIter = changes.takeWhile &#123; <span class="keyword">case</span> <span class="type">IndexedFile</span>(version, index, _, _) =&gt;</span><br><span class="line">  version &lt; endOffset.reservoirVersion ||</span><br><span class="line">    (version == endOffset.reservoirVersion &amp;&amp; index &lt;= endOffset.index)</span><br><span class="line">&#125;.collect &#123; <span class="keyword">case</span> i: <span class="type">IndexedFile</span> <span class="keyword">if</span> i.add != <span class="literal">null</span> =&gt; i.add &#125;</span><br><span class="line"><span class="keyword">val</span> addFiles =</span><br><span class="line">  addFilesIter.filter(a =&gt; excludeRegex.forall(_.findFirstIn(a.path).isEmpty)).toSeq</span><br></pre></td></tr></table></figure>
<p>上記で得られたイテレータをベースに、 <code>AddFile</code>
インスタンスのシーケンスを得る。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logDebug(<span class="string">s"start: <span class="subst">$start</span> end: <span class="subst">$end</span> <span class="subst">$&#123;addFiles.toList&#125;</span>"</span>)</span><br><span class="line">deltaLog.createDataFrame(deltaLog.snapshot, addFiles, isStreaming = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.delta.DeltaLog#createDataFrame</code>
メソッドを使って、今回のバッチ向けのDataFrameを得る。</p>
<h4><span id="参考deltasinkのaddbatchメソッドがどこから呼ばれるか">（参考）DeltaSinkのaddBatchメソッドがどこから呼ばれるか。</span></h4>
<p>Sparkの
<code>org.apache.spark.sql.execution.streaming.MicroBatchExecution#runBatch</code>
メソッド内で呼ばれる。</p>
<p>org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala:534</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">reportTimeTaken(<span class="string">"addBatch"</span>) &#123;</span><br><span class="line">  <span class="type">SQLExecution</span>.withNewExecutionId(sparkSessionToRunBatch, lastExecution) &#123;</span><br><span class="line">    sink <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">Sink</span> =&gt; s.addBatch(currentBatchId, nextBatch)</span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">StreamWriteSupport</span> =&gt;</span><br><span class="line">        <span class="comment">// This doesn't accumulate any data - it just forces execution of the microbatch writer.</span></span><br><span class="line">        nextBatch.collect()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、ここで渡されている <code>nextBatch</code>
は、その直前で以下のように生成される。</p>
<p>org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala:531</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> nextBatch =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Dataset</span>(sparkSessionToRunBatch, lastExecution, <span class="type">RowEncoder</span>(lastExecution.analyzed.schema))</span><br></pre></td></tr></table></figure>
<p>なお、ほぼ自明であるが、上記例において <code>nextBatch</code>
で渡されるバッチのクエリプランは以下のとおりである。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*(1) Project [value#684L AS id#4L]</span><br><span class="line">+- *(1) Project [timestamp#683, value#684L]</span><br><span class="line">   +- *(1) ScanV2 rate[timestamp#683, value#684L]</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.execution.streaming.MicroBatchExecution#runBatch</code>
メソッド自体は、
<code>org.apache.spark.sql.execution.streaming.MicroBatchExecution#runActivatedStream</code>
メソッド内で呼ばれる。 すなわち、
<code>org.apache.spark.sql.execution.streaming.StreamExecution</code>
クラス内で管理される、ストリーム処理を実行する仕組みの中で、
繰り返し呼ばれることになる。</p>
<h2><span id="スキーマバリデーション">スキーマバリデーション</span></h2>
<p><a href="https://docs.delta.io/latest/delta-batch.html#schema-validation" target="_blank" rel="noopener">公式ドキュメントのスキーマバリデーション</a>
には書き込みの際のスキーマ確認のアルゴリズムが載っている。</p>
<p>原則はカラムや型が一致することを求める。違っていたら例外が上がる。</p>
<p>また大文字小文字だけが異なるカラムを用ってはいけない。</p>
<h3><span id="試しに例外を出してみる">試しに例外を出してみる</span></h3>
<p>試しにスキーマの異なるレコードを追加しようと試みてみた。
ここでは、保存されているDelta
Lakeテーブルには存在しない列を追加したデータを書き込もうとしてみる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// テーブルの準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// いったん別のDFとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// 新しいカラム付きのレコードを定義し、書き込み</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRecords = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"tmp"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; newRecords.write.format(<span class="string">"delta"</span>).mode(<span class="string">"append"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>以下のようなエラーが生じた。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; newRecords.write.format(&quot;delta&quot;).mode(&quot;append&quot;).save(&quot;/tmp/delta-table&quot;)</span><br><span class="line">org.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table.</span><br><span class="line">To enable schema migration, please set:</span><br><span class="line">&apos;.option(&quot;mergeSchema&quot;, &quot;true&quot;)&apos;.</span><br><span class="line"></span><br><span class="line">Table schema:</span><br><span class="line">root</span><br><span class="line">-- id: long (nullable = true)</span><br><span class="line">-- rand: integer (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Data schema:</span><br><span class="line">root</span><br><span class="line">-- id: long (nullable = true)</span><br><span class="line">-- rand: integer (nullable = true)</span><br><span class="line">-- tmp: integer (nullable = true)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">If Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLE</span><br><span class="line">command for changing the schema.</span><br><span class="line">        ;</span><br><span class="line">  at org.apache.spark.sql.delta.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:851)</span><br><span class="line">  at org.apache.spark.sql.delta.schema.ImplicitMetadataOperation$class.updateMetadata(ImplicitMetadataOperation.scala:122)         at org.apache.spark.sql.delta.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:45)</span><br><span class="line">  at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:85)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>上記エラーは、
<code>org.apache.spark.sql.delta.MetadataMismatchErrorBuilder#addSchemaMismatch</code>
メソッド内で 生成されたものである。</p>
<p>org/apache/spark/sql/delta/DeltaErrors.scala:810</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addSchemaMismatch</span></span>(original: <span class="type">StructType</span>, data: <span class="type">StructType</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  bits ++=</span><br><span class="line">    <span class="string">s""</span><span class="string">"A schema mismatch detected when writing to the Delta table.</span></span><br><span class="line"><span class="string">       |To enable schema migration, please set:</span></span><br><span class="line"><span class="string">       |'.option("</span>$&#123;<span class="type">DeltaOptions</span>.<span class="type">MERGE_SCHEMA_OPTION</span>&#125;<span class="string">", "</span><span class="string">true")'.</span></span><br><span class="line"><span class="string">       |</span></span><br><span class="line"><span class="string">       |Table schema:</span></span><br><span class="line"><span class="string">       |<span class="subst">$&#123;DeltaErrors.formatSchema(original)&#125;</span></span></span><br><span class="line"><span class="string">       |</span></span><br><span class="line"><span class="string">       |Data schema:</span></span><br><span class="line"><span class="string">       |<span class="subst">$&#123;DeltaErrors.formatSchema(data)&#125;</span></span></span><br><span class="line"><span class="string">       "</span><span class="string">""</span>.stripMargin :: <span class="type">Nil</span></span><br><span class="line">  mentionedOption = <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、Before /
Afterでスキーマを表示してくれるようになっている。 このメソッドは、
<code>org.apache.spark.sql.delta.schema.ImplicitMetadataOperation#updateMetadata</code>
メソッド内で 呼ばれる。</p>
<p>org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:113</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (isNewSchema) &#123;</span><br><span class="line">  errorBuilder.addSchemaMismatch(txn.metadata.schema, dataSchema)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ここで <code>isNewSchema</code>
は現在のスキーマと新たに渡されたデータの
スキーマが一致しているかどうかを示す変数。</p>
<p>org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:57</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataSchema = data.schema.asNullable</span><br><span class="line"><span class="keyword">val</span> mergedSchema = <span class="keyword">if</span> (isOverwriteMode &amp;&amp; canOverwriteSchema) &#123;</span><br><span class="line">  dataSchema</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="type">SchemaUtils</span>.mergeSchemas(txn.metadata.schema, dataSchema)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> normalizedPartitionCols =</span><br><span class="line">  normalizePartitionColumns(data.sparkSession, partitionColumns, dataSchema)</span><br><span class="line"><span class="comment">// Merged schema will contain additional columns at the end</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isNewSchema</span></span>: <span class="type">Boolean</span> = txn.metadata.schema != mergedSchema</span><br></pre></td></tr></table></figure>
<p>上記の通り、overwriteモードでスキーマのオーバライトが可能なときは、新しいデータのスキーマが有効。
そうでない場合は、
<code>org.apache.spark.sql.delta.schema.SchemaUtils$#mergeSchemas</code>
メソッドを使って 改めてスキーマが確定する。</p>
<h3><span id="自動でのカラム追加スキーマ変更を試す">自動でのカラム追加（スキーマ変更）を試す</span></h3>
<p><a href="https://docs.delta.io/latest/delta-batch.html#add-columns" target="_blank" rel="noopener">公式ドキュメントのスキーママージ</a>
に記載があるとおり試す。</p>
<p>上記の例外が出た例に対し、以下のexpressionは成功する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// テーブルの準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// いったん別のDFとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// 新しいカラム付きのレコードを定義し、書き込み</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRecords = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"tmp"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; newRecords.write.format(<span class="string">"delta"</span>).mode(<span class="string">"append"</span>).option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.delta.schema.ImplicitMetadataOperation#updateMetadata</code>
メソッドの内容を確認する。</p>
<p>org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:57</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataSchema = data.schema.asNullable</span><br><span class="line"><span class="keyword">val</span> mergedSchema = <span class="keyword">if</span> (isOverwriteMode &amp;&amp; canOverwriteSchema) &#123;</span><br><span class="line">  dataSchema</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="type">SchemaUtils</span>.mergeSchemas(txn.metadata.schema, dataSchema)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記expressionを実行した際、まず <code>dataSchema</code>
には、以下のような値が含まれている。
つまり、新しく渡されたレコードのスキーマである。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataSchema = &#123;StructType@16480&#125; &quot;StructType&quot; size = 3</span><br><span class="line"> 0 = &#123;StructField@19860&#125; &quot;StructField(id,LongType,true)&quot;</span><br><span class="line"> 1 = &#123;StructField@19861&#125; &quot;StructField(rand,IntegerType,true)&quot;</span><br><span class="line"> 2 = &#123;StructField@19855&#125; &quot;StructField(tmp,IntegerType,true)&quot;</span><br></pre></td></tr></table></figure>
<p>一方、 <code>txn.metadata.schema</code>
には以下のような値が含まれている。
つまり、書き込み先のテーブルのスキーマである。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;StructType@19866&#125; &quot;StructType&quot; size = 2</span><br><span class="line"> 0 = &#123;StructField@19871&#125; &quot;StructField(id,LongType,true)&quot;</span><br><span class="line"> 1 = &#123;StructField@19872&#125; &quot;StructField(rand,IntegerType,true)&quot;</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.delta.schema.SchemaUtils$#mergeSchemas</code>
メソッドによりマージされたスキーマ <code>mergedSchema</code> は、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;StructType@16487&#125; &quot;StructType&quot; size = 3</span><br><span class="line"> 0 = &#123;StructField@19853&#125; &quot;StructField(id,LongType,true)&quot;</span><br><span class="line"> 1 = &#123;StructField@19854&#125; &quot;StructField(rand,IntegerType,true)&quot;</span><br><span class="line"> 2 = &#123;StructField@19855&#125; &quot;StructField(tmp,IntegerType,true)&quot;</span><br></pre></td></tr></table></figure>
<p>となる。</p>
<p>したがって、 <code>isNewSchema</code> メソッドの戻り値は
<code>true</code> となる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isNewSchema</span></span>: <span class="type">Boolean</span> = txn.metadata.schema != mergedSchema</span><br></pre></td></tr></table></figure>
<p>実際には</p>
<ul>
<li>overwriteモードかどうか</li>
<li>新しいパーティショニングが必要かどうか</li>
<li>スキーマが変わるかどうか</li>
</ul>
<p>に依存して処理が分かれるが、今回のケースでは以下のようにメタデータが更新される。</p>
<p>org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:103</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (isNewSchema &amp;&amp; canMergeSchema &amp;&amp; !isNewPartitioning) &#123;</span><br><span class="line">  logInfo(<span class="string">s"New merged schema: <span class="subst">$&#123;mergedSchema.treeString&#125;</span>"</span>)</span><br><span class="line">  recordDeltaEvent(txn.deltaLog, <span class="string">"delta.ddl.mergeSchema"</span>)</span><br><span class="line">  <span class="keyword">if</span> (rearrangeOnly) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.unexpectedDataChangeException(<span class="string">"Change the Delta table schema"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  txn.updateMetadata(txn.metadata.copy(schemaString = mergedSchema.json))</span><br></pre></td></tr></table></figure>
<h3><span id="overwriteモード時のスキーマ上書き">overwriteモード時のスキーマ上書き</span></h3>
<p><a href="https://docs.delta.io/latest/delta-batch.html#replace-table-schema" target="_blank" rel="noopener">公式ドキュメントのスキーマ上書き</a>
に記載の通り、overwriteモードのとき、デフォルトではスキーマを更新しない。
したがって以下のexpressionは例外を生じる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// テーブルの準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// いったん別のDFとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// 新しいカラム付きのレコードを定義し、書き込み</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRecords = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"tmp"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; newRecords.write.format(<span class="string">"delta"</span>).mode(<span class="string">"overwrite"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p><code>option("overwriteSchema", "true")</code>
をつけると、overwriteモード時にスキーマの異なるデータで上書きするとき、例外を生じなくなる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// テーブルの準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// いったん別のDFとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df = spark.read.format(<span class="string">"delta"</span>).load(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// 新しいカラム付きのレコードを定義し、書き込み</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRecords = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"tmp"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; newRecords.write.format(<span class="string">"delta"</span>).mode(<span class="string">"overwrite"</span>).option(<span class="string">"overwriteSchema"</span>, <span class="string">"true"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<p>つまり、</p>
<ul>
<li>overwriteモード</li>
<li>スキーマ上書き可能</li>
<li>新しいスキーマ</li>
</ul>
<p>という条件の下で、以下のようにメタデータが更新される。</p>
<p>org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:91</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (isOverwriteMode &amp;&amp; canOverwriteSchema &amp;&amp; (isNewSchema || isNewPartitioning)) &#123;</span><br><span class="line">  <span class="comment">// Can define new partitioning in overwrite mode</span></span><br><span class="line">  <span class="keyword">val</span> newMetadata = txn.metadata.copy(</span><br><span class="line">    schemaString = dataSchema.json,</span><br><span class="line">    partitionColumns = normalizedPartitionCols</span><br><span class="line">  )</span><br><span class="line">  recordDeltaEvent(txn.deltaLog, <span class="string">"delta.ddl.overwriteSchema"</span>)</span><br><span class="line">  <span class="keyword">if</span> (rearrangeOnly) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.unexpectedDataChangeException(<span class="string">"Overwrite the Delta table schema or "</span> +</span><br><span class="line">      <span class="string">"change the partition schema"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  txn.updateMetadata(newMetadata)</span><br></pre></td></tr></table></figure>
<h2><span id="パーティション">パーティション</span></h2>
<p><a href="https://docs.delta.io/latest/delta-batch.html#partition-data" target="_blank" rel="noopener">公式ドキュメントのパーティション説明</a>
には、以下のような例が載っている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.write.format(<span class="string">"delta"</span>).partitionBy(<span class="string">"date"</span>).save(<span class="string">"/delta/events"</span>)</span><br></pre></td></tr></table></figure>
<p>特にパーティションを指定せずに実行すると以下のようになる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">id: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Long</span>] = [id: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">data: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: bigint, rand: double]</span><br><span class="line"></span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -1 /tmp/delta-table/</span></span><br><span class="line">_delta_log</span><br><span class="line">part-00000-6861eaa0-a0dc-4365-872b-b0c110fe1462-c000.snappy.parquet</span><br><span class="line">part-00001-11725618-2e8e-4a6b-ad6c-5e32f668cb90-c000.snappy.parquet</span><br><span class="line">part-00002-8d69aea0-a2f0-46c0-b5a3-dd892a182307-c000.snappy.parquet</span><br><span class="line">part-00003-ff70d70b-0252-497e-93db-2cd715db8ab6-c000.snappy.parquet</span><br><span class="line">part-00004-46e681ef-2521-4642-ae8c-63fc3c7c9817-c000.snappy.parquet</span><br><span class="line">part-00005-10aec3fc-9538-408c-b520-894ccf529663-c000.snappy.parquet</span><br><span class="line">part-00006-62bf3268-79f7-47ea-8400-b3f7566914cb-c000.snappy.parquet</span><br><span class="line">part-00007-8683ce7e-9fe6-4b64-be97-bd158cda551f-c000.snappy.parquet</span><br></pre></td></tr></table></figure>
<p>Parquetファイルが出力ディレクトリに直接置かれる。</p>
<p>つづいてパーティションカラムを指定して書き込み。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).partitionBy(<span class="string">"rand"</span>).save(<span class="string">"/tmp/delta-table-partitioned"</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -1 -R /tmp/delta-table-partitioned/</span></span><br><span class="line">/tmp/delta-table-partitioned/:</span><br><span class="line">_delta_log</span><br><span class="line">'rand=0'</span><br><span class="line">'rand=1'</span><br><span class="line">'rand=2'</span><br><span class="line">'rand=3'</span><br><span class="line">'rand=4'</span><br><span class="line">'rand=5'</span><br><span class="line">'rand=6'</span><br><span class="line">'rand=7'</span><br><span class="line">'rand=8'</span><br><span class="line">'rand=9'</span><br><span class="line"></span><br><span class="line">/tmp/delta-table-partitioned/_delta_log:</span><br><span class="line">00000000000000000000.json</span><br><span class="line"></span><br><span class="line">'/tmp/delta-table-partitioned/rand=0':</span><br><span class="line">part-00000-336b194c-8e44-4e37-ac8f-114fb0e813c7.c000.snappy.parquet</span><br><span class="line">part-00001-d49f42cd-f0ca-4a5c-87a6-4f7647aecd52.c000.snappy.parquet</span><br><span class="line">part-00002-2a5e96ed-c56b-4b6b-9471-6ca5830d512e.c000.snappy.parquet</span><br><span class="line">part-00003-4e8d330d-9da7-40f5-91d2-cecf27347769.c000.snappy.parquet</span><br><span class="line">part-00004-8f6b1d40-ea8c-4533-840d-ae688f729b50.c000.snappy.parquet</span><br><span class="line">part-00005-f65d497c-2dc2-48b3-9252-e18dc077c5aa.c000.snappy.parquet</span><br><span class="line">part-00006-eed688bb-104e-4940-a87a-e239294d4975.c000.snappy.parquet</span><br><span class="line">part-00007-b6366ae1-fbbb-4789-910e-d896f62cee68.c000.snappy.parquet</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>先程指定したカラムの値に基づき、パーティション化されていることがわかる。
なお、メタデータ（
<code>/tmp/delta-table-partitioned/_delta_log/00000000000000000000.json</code>
内でも以下のようにパーティションカラムが指定されたことが示されてる。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"commitInfo"</span>:&#123;<span class="attr">"timestamp"</span>:<span class="number">1583070456680</span>,<span class="attr">"operation"</span>:<span class="string">"WRITE"</span>,<span class="attr">"operationParameters"</span>:&#123;<span class="attr">"mode"</span>:<span class="string">"ErrorIfExists"</span>,<span class="attr">"partitionBy"</span>:<span class="string">"[\"rand\"]"</span>&#125;,<span class="attr">"isBlindAppend"</span>:<span class="literal">true</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<h2><span id="delta-table">Delta Table</span></h2>
<p><a href="https://docs.delta.io/latest/quick-start.html" target="_blank" rel="noopener">公式ドキュメント（クイックスタート）</a>
にも記載あるが、データをテーブル形式で操作できる。
上記で掲載されている例は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io.delta.tables._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Update every even value by adding 100 to it</span></span><br><span class="line">deltaTable.update(</span><br><span class="line">  condition = expr(<span class="string">"id % 2 == 0"</span>),</span><br><span class="line">  set = <span class="type">Map</span>(<span class="string">"id"</span> -&gt; expr(<span class="string">"id + 100"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Delete every even value</span></span><br><span class="line">deltaTable.delete(condition = expr(<span class="string">"id % 2 == 0"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Upsert (merge) new data</span></span><br><span class="line"><span class="keyword">val</span> newData = spark.range(<span class="number">0</span>, <span class="number">20</span>).toDF</span><br><span class="line"></span><br><span class="line">deltaTable.as(<span class="string">"oldData"</span>)</span><br><span class="line">  .merge(</span><br><span class="line">    newData.as(<span class="string">"newData"</span>),</span><br><span class="line">    <span class="string">"oldData.id = newData.id"</span>)</span><br><span class="line">  .whenMatched</span><br><span class="line">  .update(<span class="type">Map</span>(<span class="string">"id"</span> -&gt; col(<span class="string">"newData.id"</span>)))</span><br><span class="line">  .whenNotMatched</span><br><span class="line">  .insert(<span class="type">Map</span>(<span class="string">"id"</span> -&gt; col(<span class="string">"newData.id"</span>)))</span><br><span class="line">  .execute()</span><br><span class="line"></span><br><span class="line">deltaTable.toDF.show()</span><br></pre></td></tr></table></figure>
<p>なお、 <code>io.delta.tables.DeltaTable</code>
クラスの実態は、Dataset（DataFrame）とそれを操作するためのAPIの塊である。</p>
<p>例えば上記の</p>
<p><code>io.delta.tables.DeltaTable#forPath</code>
メソッドは以下のように定義されている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forPath</span></span>(sparkSession: <span class="type">SparkSession</span>, path: <span class="type">String</span>): <span class="type">DeltaTable</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="type">DeltaTableUtils</span>.isDeltaTable(sparkSession, <span class="keyword">new</span> <span class="type">Path</span>(path))) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DeltaTable</span>(sparkSession.read.format(<span class="string">"delta"</span>).load(path),</span><br><span class="line">      <span class="type">DeltaLog</span>.forTable(sparkSession, path))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.notADeltaTableException(<span class="type">DeltaTableIdentifier</span>(path = <span class="type">Some</span>(path)))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3><span id="条件でレコード削除">条件でレコード削除</span></h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// データ準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// テーブルとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(spark, <span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// 条件に基づきレコードを削除</span></span><br><span class="line">scala&gt; deltaTable.delete(<span class="string">"rand &gt; 5"</span>)</span><br><span class="line">scala&gt; deltaTable.delete($<span class="string">"rand"</span> &gt; <span class="number">6</span>)</span><br><span class="line">scala&gt; deltaTable.toDF.show</span><br><span class="line">+-----+----+</span><br><span class="line">|   id|rand|</span><br><span class="line">+-----+----+</span><br><span class="line">|<span class="number">62500</span>|   <span class="number">0</span>|</span><br><span class="line">|<span class="number">62501</span>|   <span class="number">5</span>|</span><br><span class="line">|<span class="number">62503</span>|   <span class="number">4</span>|</span><br><span class="line">|<span class="number">62504</span>|   <span class="number">3</span>|</span><br><span class="line">|<span class="number">62505</span>|   <span class="number">3</span>|</span><br><span class="line">|<span class="number">62506</span>|   <span class="number">3</span>|</span><br><span class="line">|<span class="number">62507</span>|   <span class="number">2</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>なお、 <code>io.delta.tables.DeltaTable#delete</code>
メソッドの実態は、
<code>io.delta.tables.execution.DeltaTableOperations#executeDelete</code>
メソッドである。</p>
<p>コメントで、</p>
<blockquote>
<p>// current DELETE does not support subquery, // and the reason why
perform checking here is that // we want to have more meaningful
exception messages, // instead of having some random msg generated by
executePlan().</p>
</blockquote>
<p>と記載されており、バージョン0.5.0の段階ではサブクエリを使った削除には対応していないようだ。</p>
<p>またドキュメントによると、</p>
<blockquote>
<p>delete removes the data from the latest version of the Delta table
but does not remove it from the physical storage until the old versions
are explicitly vacuumed. See vacuum for more details.</p>
</blockquote>
<p>とあり、不要になったファイルはバキュームで削除されるとのこと。</p>
<h3><span id="更新">更新</span></h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// データ準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// テーブルとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(spark, <span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// 条件に基づきレコードを更新</span></span><br><span class="line">scala&gt; deltaTable.updateExpr(<span class="string">"rand = 0"</span>, <span class="type">Map</span>(<span class="string">"rand"</span> -&gt; <span class="string">"-1"</span>))</span><br></pre></td></tr></table></figure>
<p>元のデータが</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; deltaTable.toDF.show</span><br><span class="line">+---+----+</span><br><span class="line">| id|rand|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">0</span>|   <span class="number">4</span>|</span><br><span class="line">|  <span class="number">1</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">2</span>|   <span class="number">0</span>|</span><br><span class="line">|  <span class="number">3</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">4</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">5</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">6</span>|   <span class="number">7</span>|</span><br><span class="line">|  <span class="number">7</span>|   <span class="number">0</span>|</span><br><span class="line">|  <span class="number">8</span>|   <span class="number">0</span>|</span><br><span class="line">|  <span class="number">9</span>|   <span class="number">5</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>だとすると、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; deltaTable.toDF.show</span><br><span class="line">+---+----+</span><br><span class="line">| id|rand|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">0</span>|   <span class="number">4</span>|</span><br><span class="line">|  <span class="number">1</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">2</span>|  <span class="number">-1</span>|</span><br><span class="line">|  <span class="number">3</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">4</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">5</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">6</span>|   <span class="number">7</span>|</span><br><span class="line">|  <span class="number">7</span>|  <span class="number">-1</span>|</span><br><span class="line">|  <span class="number">8</span>|  <span class="number">-1</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>のようになる。</p>
<p><code>io.delta.tables.DeltaTable#updateExpr</code> メソッドの実態は、
<code>io.delta.tables.execution.DeltaTableOperations#executeUpdate</code>
メソッドである。</p>
<h3><span id="upsertマージ">upsert（マージ）</span></h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// データ準備</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id = spark.range(<span class="number">0</span>, <span class="number">100000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data = id.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; data.write.format(<span class="string">"delta"</span>).save(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// テーブルとして読み込み定義</span></span><br><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables._</span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(spark, <span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; <span class="comment">// upsert用のデータ作成</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> id2 = spark.range(<span class="number">0</span>, <span class="number">200000</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> data2 = id2.select($<span class="string">"id"</span>, rand() * <span class="number">10</span> % <span class="number">10</span> as <span class="string">"rand"</span> cast <span class="string">"int"</span>)</span><br><span class="line">scala&gt; <span class="keyword">val</span> dataForUpsert = data2.sample(<span class="literal">false</span>, <span class="number">0.5</span>)</span><br><span class="line">scala&gt; <span class="comment">// データをマージ</span></span><br><span class="line">scala&gt; :paste</span><br><span class="line">deltaTable</span><br><span class="line">  .as(<span class="string">"before"</span>)</span><br><span class="line">  .merge(</span><br><span class="line">    dataForUpsert.as(<span class="string">"updates"</span>),</span><br><span class="line">    <span class="string">"before.id = updates.id"</span>)</span><br><span class="line">  .whenMatched</span><br><span class="line">  .updateExpr(</span><br><span class="line">    <span class="type">Map</span>(<span class="string">"rand"</span> -&gt; <span class="string">"updates.rand"</span>))</span><br><span class="line">  .whenNotMatched</span><br><span class="line">  .insertExpr(</span><br><span class="line">    <span class="type">Map</span>(</span><br><span class="line">      <span class="string">"id"</span> -&gt; <span class="string">"updates.id"</span>,</span><br><span class="line">      <span class="string">"rand"</span> -&gt; <span class="string">"updates.rand"</span>))</span><br><span class="line">  .execute()</span><br></pre></td></tr></table></figure>
<p>まず元データには、99999より大きな値はない。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; deltaTable.toDF.filter($<span class="string">"id"</span> &gt; <span class="number">99997</span>).show</span><br><span class="line">+-----+----+</span><br><span class="line">|   id|rand|</span><br><span class="line">+-----+----+</span><br><span class="line">|<span class="number">99998</span>|   <span class="number">4</span>|</span><br><span class="line">|<span class="number">99999</span>|   <span class="number">0</span>|</span><br><span class="line">+-----+----+</span><br></pre></td></tr></table></figure>
<p>元データの先頭は以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; deltaTable.toDF.show</span><br><span class="line">+---+----+</span><br><span class="line">| id|rand|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">0</span>|   <span class="number">1</span>|</span><br><span class="line">|  <span class="number">1</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">3</span>|   <span class="number">8</span>|</span><br><span class="line">|  <span class="number">4</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">5</span>|   <span class="number">0</span>|</span><br><span class="line">|  <span class="number">6</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">7</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">8</span>|   <span class="number">4</span>|</span><br><span class="line">|  <span class="number">9</span>|   <span class="number">9</span>|</span><br><span class="line">| <span class="number">10</span>|   <span class="number">4</span>|</span><br><span class="line">| <span class="number">11</span>|   <span class="number">4</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>upsert用のデータは以下の通り。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; dataForUpsert.show</span><br><span class="line">+---+----+</span><br><span class="line">| id|rand|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">0</span>|   <span class="number">7</span>|</span><br><span class="line">|  <span class="number">2</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">5</span>|   <span class="number">2</span>|</span><br><span class="line">|  <span class="number">8</span>|   <span class="number">9</span>|</span><br><span class="line">| <span class="number">10</span>|   <span class="number">9</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>99999よりも大きな <code>id</code> も存在する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; dataForUpsert.filter($<span class="string">"id"</span> &gt; <span class="number">99997</span>).show</span><br><span class="line">+------+----+</span><br><span class="line">|    id|rand|</span><br><span class="line">+------+----+</span><br><span class="line">| <span class="number">99999</span>|   <span class="number">0</span>|</span><br><span class="line">|<span class="number">100005</span>|   <span class="number">3</span>|</span><br><span class="line">|<span class="number">100006</span>|   <span class="number">9</span>|</span><br><span class="line">|<span class="number">100008</span>|   <span class="number">7</span>|</span><br><span class="line">|<span class="number">100009</span>|   <span class="number">1</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>upseart（マージ）を実行すると、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; deltaTable.toDF.orderBy($<span class="string">"id"</span>).show</span><br><span class="line">+---+----+</span><br><span class="line">| id|rand|</span><br><span class="line">+---+----+</span><br><span class="line">|  <span class="number">0</span>|   <span class="number">7</span>|</span><br><span class="line">|  <span class="number">1</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">2</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">3</span>|   <span class="number">8</span>|</span><br><span class="line">|  <span class="number">4</span>|   <span class="number">3</span>|</span><br><span class="line">|  <span class="number">5</span>|   <span class="number">2</span>|</span><br><span class="line">|  <span class="number">6</span>|   <span class="number">5</span>|</span><br><span class="line">|  <span class="number">7</span>|   <span class="number">6</span>|</span><br><span class="line">|  <span class="number">8</span>|   <span class="number">9</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>上記のように、既存のレコードについては値が更新された。 <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>また、</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; deltaTable.toDF.filter($<span class="string">"id"</span> &gt; <span class="number">99997</span>).show</span><br><span class="line">+------+----+</span><br><span class="line">|    id|rand|</span><br><span class="line">+------+----+</span><br><span class="line">|<span class="number">100544</span>|   <span class="number">5</span>|</span><br><span class="line">|<span class="number">100795</span>|   <span class="number">1</span>|</span><br><span class="line">|<span class="number">101090</span>|   <span class="number">5</span>|</span><br><span class="line">|<span class="number">101499</span>|   <span class="number">1</span>|</span><br><span class="line">|<span class="number">101963</span>|   <span class="number">7</span>|</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>のように、99997よりも大きな <code>id</code>
のレコードも存在することがわかる。</p>
<h4><span id="iodeltatablesdeltatablemerge">io.delta.tables.DeltaTable#merge</span></h4>
<p>上記の例に載っていた <code>io.delta.tables.DeltaTable#merge</code>
を確認する。</p>
<p>io/delta/tables/DeltaTable.scala:501</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(source: <span class="type">DataFrame</span>, condition: <span class="type">Column</span>): <span class="type">DeltaMergeBuilder</span> = &#123;</span><br><span class="line">  <span class="type">DeltaMergeBuilder</span>(<span class="keyword">this</span>, source, condition)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>io.delta.tables.DeltaMergeBuilder</code>
はマージのロジックを構成するためのクラス。</p>
<ul>
<li>whenMatched</li>
<li>whenNotMatched</li>
</ul>
<p>メソッドが提供されており、それぞれマージの条件が合致した場合、合致しなかった場合に実行する処理を指定可能。
なお、それぞれメソッドの引数にString型の変数を渡すことができる。
その場合、マージの条件に <strong>加えて</strong>
さらに条件を加えられる。</p>
<p>なお、 <code>whenNotMatched</code> に引数を与えた場合は、</p>
<ul>
<li>マージ条件に合致しなかった</li>
<li>引数で与えられた条件に合致した</li>
</ul>
<p>が成り立つ場合に有効である。</p>
<p><code>whenMatched</code> の場合は戻り値は
<code>io.delta.tables.DeltaMergeMatchedActionBuilder</code> である。
<code>io.delta.tables.DeltaMergeMatchedActionBuilder</code> クラスには
<code>update</code> メソッド、 <code>udpateExpr</code>、
<code>updateAll</code>、 <code>delete</code> メソッドがあり、
条件に合致したときに実行する処理を定義できる。</p>
<p>例えば、 <code>update</code> メソッドは以下の通り。</p>
<p>io/delta/tables/DeltaMergeBuilder.scala:298</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(set: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Column</span>]): <span class="type">DeltaMergeBuilder</span> = &#123;</span><br><span class="line">  addUpdateClause(set)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>io/delta/tables/DeltaMergeBuilder.scala:365</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addUpdateClause</span></span>(set: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Column</span>]): <span class="type">DeltaMergeBuilder</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (set.isEmpty &amp;&amp; matchCondition.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// Nothing to update = no need to add an update clause</span></span><br><span class="line">    mergeBuilder</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> setActions = set.toSeq</span><br><span class="line">    <span class="keyword">val</span> updateActions = <span class="type">MergeIntoClause</span>.toActions(</span><br><span class="line">      colNames = setActions.map(x =&gt; <span class="type">UnresolvedAttribute</span>.quotedString(x._1)),</span><br><span class="line">      exprs = setActions.map(x =&gt; x._2.expr),</span><br><span class="line">      isEmptySeqEqualToStar = <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">val</span> updateClause = <span class="type">MergeIntoUpdateClause</span>(matchCondition.map(_.expr), updateActions)</span><br><span class="line">    mergeBuilder.withClause(updateClause)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>戻り値は上記の通り、 <code>io.delta.tables.DeltaMergeBuilder</code>
になる。 当該クラスは、メンバに <code>whenClauses</code>
を持ち、指定された更新用の式（のセット）を保持する。</p>
<p>io/delta/tables/DeltaMergeBuilder.scala:244</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[delta] <span class="function"><span class="keyword">def</span> <span class="title">withClause</span></span>(clause: <span class="type">MergeIntoClause</span>): <span class="type">DeltaMergeBuilder</span> = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">DeltaMergeBuilder</span>(</span><br><span class="line">    <span class="keyword">this</span>.targetTable, <span class="keyword">this</span>.source, <span class="keyword">this</span>.onCondition, <span class="keyword">this</span>.whenClauses :+ clause)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、最後に <code>execute</code> メソッドを確認する。</p>
<p>io/delta/tables/DeltaMergeBuilder.scala:225</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkSession = targetTable.toDF.sparkSession</span><br><span class="line">  <span class="keyword">val</span> resolvedMergeInto =</span><br><span class="line">    <span class="type">MergeInto</span>.resolveReferences(mergePlan)(tryResolveReferences(sparkSession) _)</span><br><span class="line">  <span class="keyword">if</span> (!resolvedMergeInto.resolved) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.analysisException(<span class="string">"Failed to resolve\n"</span>, plan = <span class="type">Some</span>(resolvedMergeInto))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Preprocess the actions and verify</span></span><br><span class="line">  <span class="keyword">val</span> mergeIntoCommand = <span class="type">PreprocessTableMerge</span>(sparkSession.sessionState.conf)(resolvedMergeInto)</span><br><span class="line">  sparkSession.sessionState.analyzer.checkAnalysis(mergeIntoCommand)</span><br><span class="line">  mergeIntoCommand.run(sparkSession)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最初に、DataFrameの変換前後の実行プラン、マージの際の条件等から
マージの実行プランを作成する。 ★要確認</p>
<h4><span id="参考値がユニークではないカラムを使ってのマージ">参考）値がユニークではないカラムを使ってのマージ</span></h4>
<p>値がユニークではないカラムを使ってマージしようとすると、以下のようなエラーを生じる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">java.lang.UnsupportedOperationException: Cannot perform MERGE as multiple source rows matched and attempted to update the same</span><br><span class="line">target row in the Delta table. By SQL semantics of merge, when multiple source rows match</span><br><span class="line">on the same target row, the update operation is ambiguous as it is unclear which source</span><br><span class="line">should be used to update the matching target row.</span><br><span class="line">You can preprocess the source table to eliminate the possibility of multiple matches.</span><br><span class="line">Please refer to</span><br><span class="line">https://docs.delta.io/latest/delta/delta-update.html#upsert-into-a-table-using-merge</span><br><span class="line"></span><br><span class="line">  at org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:444)</span><br><span class="line">  at org.apache.spark.sql.delta.commands.MergeIntoCommand.org$apache$spark$sql$delta$commands$MergeIntoCommand$$findTouchedFiles(MergeIntoCommand.scala:225)</span><br><span class="line">  at org.apache.spark.sql.delta.commands.MergeIntoCommand$$anonfun$run$1$$anonfun$apply$1$$anonfun$1.apply(MergeIntoCommand.scala:132)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>つまり、上記の例では <code>dataForUpsert</code>
の中に、万が一重複する <code>id</code>
をもつレコードが含まれていると、例外を生じることになる。
これは、重複する <code>id</code>
について、どの値を採用したらよいか断定できなくなるため。</p>
<p>実装上は、
<code>org.apache.spark.sql.delta.commands.MergeIntoCommand#findTouchedFiles</code>
メソッド内で重複の確認が行われる。 以下の通り。</p>
<p>org/apache/spark/sql/delta/commands/MergeIntoCommand.scala:223</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> matchedRowCounts = collectTouchedFiles.groupBy(<span class="type">ROW_ID_COL</span>).agg(sum(<span class="string">"one"</span>).as(<span class="string">"count"</span>))</span><br><span class="line"><span class="keyword">if</span> (matchedRowCounts.filter(<span class="string">"count &gt; 1"</span>).count() != <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.multipleSourceRowMatchingTargetRowInMergeException(spark)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>対象のカラムでgroupByして件数を数えていることがわかる。
最も容易に再現する方法は、
<code>val dataForUpsert = data2.sample(false, 0.5)</code>
の第1引数をtrueにすれば、おそらく再現する。</p>
<h3><span id="マージの例">マージの例</span></h3>
<p><a href="https://docs.delta.io/latest/delta-update.html#merge-examples" target="_blank" rel="noopener">公式ドキュメントのマージの例</a>
に有用な例が載っている。</p>
<ul>
<li><a href="https://docs.delta.io/latest/delta-update.html#data-deduplication-when-writing-into-delta-tables" target="_blank" rel="noopener">公式ドキュメントのマージを使った上書き</a>
<ul>
<li>ログなどを収集する際、データソース側で重複させることがある。Delta
Lakeのテーブルにマージしながら入力することで、
レコード重複を排除しながらテーブルの内容を更新できる</li>
<li>さらに、新しいデータをマージする際、マージすべきデータの範囲がわかっていれば（例：最近7日間のログなど）、
それを使ってスキャン・書き換えの範囲を絞りこめる。</li>
</ul></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#slowly-changing-data-scd-type-2-operation-into-delta-tables" target="_blank" rel="noopener">Slowly
changing data (SCD) Type 2 operation</a>
<ul>
<li>Dimensionalなデータを断続的にゆっくりと更新するワークロード</li>
<li>新しい値で更新するときに、古い値を残しておく</li>
</ul></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#write-change-data-into-a-delta-table" target="_blank" rel="noopener">Write
change data into a Delta table</a>
<ul>
<li>外部テーブルの変更をDelta Lakeに取り込む</li>
<li>DataFrame内にキー、タイムスタンプ、新しい値、削除フラグを持つ「変更内容」を表すレコードを保持。</li>
<li>上記DataFrameには、あるキーに関する変更を複数含む可能性があるため、キーごとに一度アグリゲートし、
グループごとに最も新しい変更内容を採用する。</li>
<li>DeltaTableのmerge機能を利用してupsert（や削除）する。</li>
</ul></li>
<li><a href="https://docs.delta.io/latest/delta-update.html#upsert-from-streaming-queries-using-foreachbatch" target="_blank" rel="noopener">Upsert
from streaming queries using foreachBatch</a>
<ul>
<li><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch" target="_blank" rel="noopener">org.apache.spark.sql.streaming.DataStreamWriter#foreachBatch</a>
を用いて、ストリームの各マイクロバッチに対する、 Delta
Lakeのマージ処理を行う。</li>
<li>CDCの方法と組み合わせ、重複排除（ユニークID利用、マイクロバッチのIDが使える？）との組み合わせも可能</li>
</ul></li>
</ul>
<h2><span id="タイムトラベル">タイムトラベル</span></h2>
<p>Deltaの持つヒストリは、上記の <code>DeltaTable</code>
を利用して見られる。（その他にも、メタデータを直接確認する、という手もあるはある）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; deltaTable.history.show</span><br><span class="line">+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+</span><br><span class="line">|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|</span><br><span class="line">+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+</span><br><span class="line">|     <span class="number">13</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-26</span> <span class="number">23</span>:<span class="number">19</span>:<span class="number">17</span>|  <span class="literal">null</span>|    <span class="literal">null</span>|    <span class="type">MERGE</span>|[predicate -&gt; (ol...|<span class="literal">null</span>|    <span class="literal">null</span>|     <span class="literal">null</span>|         <span class="number">12</span>|          <span class="literal">null</span>|        <span class="literal">false</span>|</span><br><span class="line">|     <span class="number">12</span>|<span class="number">2020</span><span class="number">-02</span><span class="number">-26</span> <span class="number">23</span>:<span class="number">16</span>:<span class="number">13</span>|  <span class="literal">null</span>|    <span class="literal">null</span>|   <span class="type">DELETE</span>|[predicate -&gt; [<span class="string">"(...|null|    null|     null|         11|          null|        false|</span></span><br><span class="line"><span class="string">|     11|2020-02-26 23:16:00|  null|    null|   UPDATE|[predicate -&gt; ((i...|null|    null|     null|         10|          null|        false|</span></span><br><span class="line"><span class="string">|     10|2020-02-26 23:12:48|  null|    null|    WRITE|[mode -&gt; Append, ...|null|    null|     null|          9|          null|         true|</span></span><br><span class="line"><span class="string">|      9|2020-02-26 23:12:33|  null|    null|    WRITE|[mode -&gt; Overwrit...|null|    null|     null|          8|          null|        false|</span></span><br><span class="line"><span class="string">|      8|2020-02-26 23:11:48|  null|    null|    WRITE|[mode -&gt; Append, ...|null|    null|     null|          7|          null|         true|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">(snip)</span></span><br></pre></td></tr></table></figure>
<p>このうち、readVersionを指定し、ロードすることもできる。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.read.format(<span class="string">"delta"</span>).option(<span class="string">"versionAsOf"</span>, <span class="number">8</span>).load(<span class="string">"/tmp/delta-table"</span>).show</span><br></pre></td></tr></table></figure>
<p>ここで指定した <code>versionAsOf</code> の値は、
<code>org.apache.spark.sql.delta.sources.DeltaDataSource</code>
内で用いられる。
<code>org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation</code>
メソッド内に以下のような実装があり、
ここでタイムトラベルの値が読み込まれる。</p>
<p>org/apache/spark/sql/delta/sources/DeltaDataSource.scala:153</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> timeTravelByParams = getTimeTravelVersion(parameters)</span><br></pre></td></tr></table></figure>
<p>その他にもタイムスタンプで指定する方法もある。</p>
<p><a href="https://docs.delta.io/latest/" target="_blank" rel="noopener">公式ドキュメント</a>
の例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1 = spark.read.format(<span class="string">"delta"</span>).option(<span class="string">"timestampAsOf"</span>, timestamp_string).load(<span class="string">"/delta/events"</span>)</span><br></pre></td></tr></table></figure>
<p>なお、 <a href="https://docs.delta.io/latest/" target="_blank" rel="noopener">公式ドキュメント</a>
には以下のような記載が見られた。</p>
<blockquote>
<p>This sample code writes out the data in df, validates that it all
falls within the specified partitions, and performs an atomic
replacement.</p>
</blockquote>
<p>これは、データ本体を書き出してから、メタデータを新規作成することでアトミックに更新することを示していると想像される。
<code>org.apache.spark.sql.delta.OptimisticTransactionImpl#commit</code>
メソッドあたりを確認したら良さそう。</p>
<h2><span id="orgapachesparksqldeltaoptimistictransactionimplcommit">org.apache.spark.sql.delta.OptimisticTransactionImpl#commit</span></h2>
<p><code>org.apache.spark.sql.delta.OptimisticTransactionImpl#commit</code>
メソッドは、 例えば
<code>org.apache.spark.sql.delta.commands.WriteIntoDelta#run</code>
メソッド内で呼ばれる。</p>
<p>org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:63</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">  deltaLog.withNewTransaction &#123; txn =&gt;</span><br><span class="line">    <span class="keyword">val</span> actions = write(txn, sparkSession)</span><br><span class="line">    <span class="keyword">val</span> operation = <span class="type">DeltaOperations</span>.<span class="type">Write</span>(mode, <span class="type">Option</span>(partitionColumns), options.replaceWhere)</span><br><span class="line">    txn.commit(actions, operation)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">Seq</span>.empty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Delta Logを書き出した後に、メタデータを更新し書き出したDelta
Logを有効化する。 書き込み衝突検知なども行われる。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:250</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commit</span></span>(actions: <span class="type">Seq</span>[<span class="type">Action</span>], op: <span class="type">DeltaOperations</span>.<span class="type">Operation</span>): <span class="type">Long</span> = recordDeltaOperation(</span><br><span class="line">      deltaLog,</span><br><span class="line">      <span class="string">"delta.commit"</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> version = <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>commitメソッドは上記の通り、 <code>Action</code> と
<code>Operation</code> を引数に取る。</p>
<ul>
<li>Action: Delta
Tableに対する変更内容を表す。Actionのシーケンスがテーブル更新の内容を表す。</li>
<li>Operation: Delta
Tableに対する操作を表す。必ずしもテーブル内容の更新を示すわけではない。</li>
</ul>
<p>なお、Operationの子クラスは以下の通り。</p>
<ul>
<li>ManualUpdate$ in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>UnsetTableProperties in DeltaOperations$
(org.apache.spark.sql.delta)</li>
<li>Write in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>Convert in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>UpgradeProtocol in DeltaOperations$
(org.apache.spark.sql.delta)</li>
<li>ComputeStats in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>SetTableProperties in DeltaOperations$
(org.apache.spark.sql.delta)</li>
<li>FileNotificationRetention$ in DeltaOperations$
(org.apache.spark.sql.delta)</li>
<li>Update in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>ReplaceTable in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>Truncate in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>Fsck in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>CreateTable in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>Merge in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>Optimize in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>ReplaceColumns in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>UpdateColumnMetadata in DeltaOperations$
(org.apache.spark.sql.delta)</li>
<li>StreamingUpdate in DeltaOperations$
(org.apache.spark.sql.delta)</li>
<li>Delete in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>ChangeColumn in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>ResetZCubeInfo in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>UpdateSchema in DeltaOperations$ (org.apache.spark.sql.delta)</li>
<li>AddColumns in DeltaOperations$ (org.apache.spark.sql.delta)</li>
</ul>
<p>では <code>commit</code> メソッドの内容確認に戻る。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:253</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> version = <span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// Try to commit at the next version.</span></span><br><span class="line">  <span class="keyword">var</span> finalActions = prepareCommit(actions, op)</span><br></pre></td></tr></table></figure>
<p>最初に、
<code>org.apache.spark.sql.delta.OptimisticTransactionImpl#prepareCommit</code>
メソッドを利用し準備する。 例えば、</p>
<ul>
<li>メタデータ更新が一度に複数予定されていないか</li>
<li>最初の書き込みか？必要に応じて出力先ディレクトリを作り、初期のプロトコルバージョンを決める、など。</li>
<li>不要なファイルの削除</li>
</ul>
<p>ここで「プロトコルバージョン」と言っているのは、クライアントが書き込みする際に使用するプロトコルのバージョンである。
つまり、テーブルに後方互換性を崩すような変更があった際、最小プロトコルバージョンを規定することで、
古すぎるクライアントのアクセスを許さないような仕組みが、Delta
Lakeには備わっている。</p>
<p>では <code>commit</code> メソッドの内容確認に戻る。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:257</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Find the isolation level to use for this commit</span></span><br><span class="line"><span class="keyword">val</span> noDataChanged = actions.collect &#123; <span class="keyword">case</span> f: <span class="type">FileAction</span> =&gt; f.dataChange &#125;.forall(_ == <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> isolationLevelToUse = <span class="keyword">if</span> (noDataChanged) &#123;</span><br><span class="line">  <span class="comment">// If no data has changed (i.e. its is only being rearranged), then SnapshotIsolation</span></span><br><span class="line">  <span class="comment">// provides Serializable guarantee. Hence, allow reduced conflict detection by using</span></span><br><span class="line">  <span class="comment">// SnapshotIsolation of what the table isolation level is.</span></span><br><span class="line">  <span class="type">SnapshotIsolation</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="type">Serializable</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>書き込みファイル衝突時のアイソレーションレベルの確認。
なお、上記では実際には</p>
<ul>
<li>データ変更なし： <code>SnapshotIsolation</code></li>
<li>データ変更あり： <code>Serializable</code></li>
</ul>
<p>とされている。詳しくは <a href="#アイソレーションレベル">#アイソレーションレベル</a> を参照。</p>
<p>(WIP)</p>
<h2><span id="プロトコルバージョン">プロトコルバージョン</span></h2>
<p>あとで書く。</p>
<h2><span id="アイソレーションレベル">アイソレーションレベル</span></h2>
<p>Delta
Lakeは楽観ロックの仕組みであるが、3種類のアイソレーションレベルが用いられることになっている。（使い分けられることになっている）
DeltaTableのコンストラクタに、 <code>delta</code>
ソースとして読み込んだDataFrameが渡されていることがわかる。 *
org.apache.spark.sql.delta.Serializable *
org.apache.spark.sql.delta.WriteSerializable *
org.apache.spark.sql.delta.SnapshotIsolation</p>
<p><code>org.apache.spark.sql.delta.OptimisticTransactionImpl#commit</code>
メソッド内では
以下のようにデータ変更があるかどうかでアイソレーションレベルを使い分けるようになっている。</p>
<p>src/main/scala/org/apache/spark/sql/delta/OptimisticTransaction.scala:259</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> isolationLevelToUse = <span class="keyword">if</span> (noDataChanged) &#123;</span><br><span class="line">  <span class="comment">// If no data has changed (i.e. its is only being rearranged), then SnapshotIsolation</span></span><br><span class="line">  <span class="comment">// provides Serializable guarantee. Hence, allow reduced conflict detection by using</span></span><br><span class="line">  <span class="comment">// SnapshotIsolation of what the table isolation level is.</span></span><br><span class="line">  <span class="type">SnapshotIsolation</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="type">Serializable</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>また、実際に上記アイソレーションレベルの設定が用いられるのは、
<code>org.apache.spark.sql.delta.OptimisticTransactionImpl#doCommit</code>
メソッドである。</p>
<p>src/main/scala/org/apache/spark/sql/delta/OptimisticTransaction.scala:293</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> commitVersion = doCommit(snapshot.version + <span class="number">1</span>, finalActions, <span class="number">0</span>, isolationLevelToUse)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.delta.OptimisticTransactionImpl#doCommit</code>
メソッドでは、 tryを用いてデルタログのストアファイルに書き込む。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doCommit</span></span>(</span><br><span class="line">    attemptVersion: <span class="type">Long</span>,</span><br><span class="line">    actions: <span class="type">Seq</span>[<span class="type">Action</span>],</span><br><span class="line">    attemptNumber: <span class="type">Int</span>,</span><br><span class="line">    isolationLevel: <span class="type">IsolationLevel</span>): <span class="type">Long</span> = deltaLog.lockInterruptibly &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    logDebug(</span><br><span class="line">      <span class="string">s"Attempting to commit version <span class="subst">$attemptVersion</span> with <span class="subst">$&#123;actions.size&#125;</span> actions with "</span> +</span><br><span class="line">        <span class="string">s"<span class="subst">$isolationLevel</span> isolation level"</span>)</span><br><span class="line"></span><br><span class="line">    deltaLog.store.write(</span><br><span class="line">      deltaFile(deltaLog.logPath, attemptVersion),</span><br><span class="line">      actions.map(_.json).toIterator)</span><br></pre></td></tr></table></figure>
<p>ここで <code>java.nio.file.FileAlreadyExistsException</code>
が発生すると、先程のアイソレーションレベルに
基づいた処理が行われることになる。
これは、楽観ロックであるため、いざ書き込もうとしたら「あ、デルタログのストアファイルが、誰かに書き込まれている…」ということが
ありえるからある。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> e: java.nio.file.<span class="type">FileAlreadyExistsException</span> =&gt;</span><br><span class="line">    checkAndRetry(attemptVersion, actions, attemptNumber, isolationLevel)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の
<code>org.apache.spark.sql.delta.OptimisticTransactionImpl#checkAndRetry</code>
メソッドの内容を確認する。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:442</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">checkAndRetry</span></span>(</span><br><span class="line">    checkVersion: <span class="type">Long</span>,</span><br><span class="line">    actions: <span class="type">Seq</span>[<span class="type">Action</span>],</span><br><span class="line">    attemptNumber: <span class="type">Int</span>,</span><br><span class="line">    commitIsolationLevel: <span class="type">IsolationLevel</span>): <span class="type">Long</span> = recordDeltaOperation(</span><br><span class="line">      deltaLog,</span><br><span class="line">      <span class="string">"delta.commit.retry"</span>,</span><br><span class="line">      tags = <span class="type">Map</span>(<span class="type">TAG_LOG_STORE_CLASS</span> -&gt; deltaLog.store.getClass.getName)) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">import</span> _spark.implicits._</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> nextAttemptVersion = getNextAttemptVersion(checkVersion)</span><br></pre></td></tr></table></figure>
<p>最初に現在の最新のDeltaログのバージョン確認。
これをもとに、チェックするべきバージョン、つまりトランザクションを始めてから、
更新された内容をトラックする。</p>
<p>以下の通り。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:453</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    (checkVersion until nextAttemptVersion).foreach &#123; version =&gt;</span><br><span class="line">      <span class="comment">// Actions of a commit which went in before ours</span></span><br><span class="line">      <span class="keyword">val</span> winningCommitActions =</span><br><span class="line">        deltaLog.store.read(deltaFile(deltaLog.logPath, version)).map(<span class="type">Action</span>.fromJson)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>上記で、トランザクション開始後のアクションを確認できる。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:460</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> metadataUpdates = winningCommitActions.collect &#123; <span class="keyword">case</span> a: <span class="type">Metadata</span> =&gt; a &#125;</span><br><span class="line"><span class="keyword">val</span> removedFiles = winningCommitActions.collect &#123; <span class="keyword">case</span> a: <span class="type">RemoveFile</span> =&gt; a &#125;</span><br><span class="line"><span class="keyword">val</span> txns = winningCommitActions.collect &#123; <span class="keyword">case</span> a: <span class="type">SetTransaction</span> =&gt; a &#125;</span><br><span class="line"><span class="keyword">val</span> protocol = winningCommitActions.collect &#123; <span class="keyword">case</span> a: <span class="type">Protocol</span> =&gt; a &#125;</span><br><span class="line"><span class="keyword">val</span> commitInfo = winningCommitActions.collectFirst &#123; <span class="keyword">case</span> a: <span class="type">CommitInfo</span> =&gt; a &#125;.map(</span><br><span class="line">  ci =&gt; ci.copy(version = <span class="type">Some</span>(version)))</span><br></pre></td></tr></table></figure>
<p>この後の処理のため、アクションから各種情報を取り出す。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:467</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> blindAppendAddedFiles = mutable.<span class="type">ArrayBuffer</span>[<span class="type">AddFile</span>]()</span><br><span class="line"><span class="keyword">val</span> changedDataAddedFiles = mutable.<span class="type">ArrayBuffer</span>[<span class="type">AddFile</span>]()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> isBlindAppendOption = commitInfo.flatMap(_.isBlindAppend)</span><br><span class="line"><span class="keyword">if</span> (isBlindAppendOption.getOrElse(<span class="literal">false</span>)) &#123;</span><br><span class="line">  blindAppendAddedFiles ++= winningCommitActions.collect &#123; <span class="keyword">case</span> a: <span class="type">AddFile</span> =&gt; a &#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  changedDataAddedFiles ++= winningCommitActions.collect &#123; <span class="keyword">case</span> a: <span class="type">AddFile</span> =&gt; a &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>まず、変更のあったファイルを確認する。
このとき、既存ファイルに関係なく書き込まれた（blind append）ファイルか、
そうでないかを仕分けながら確認する。
一応、後々アイソレーションレベルに基づいてファイルを処理する際に、
ここで得られた情報が用いられる。 <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:479</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (protocol.nonEmpty) &#123;</span><br><span class="line">  protocol.foreach &#123; p =&gt;</span><br><span class="line">    deltaLog.protocolRead(p)</span><br><span class="line">    deltaLog.protocolWrite(p)</span><br><span class="line">  &#125;</span><br><span class="line">  actions.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Protocol</span>(_, _) =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ProtocolChangedException</span>(commitInfo)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>プロトコル変更がないかどうかを確認する。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:491</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (metadataUpdates.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">MetadataChangedException</span>(commitInfo)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>メタデータに変更があったら例外。
つまり、トランザクション開始後、例えばスキーマ変更があったら例外になる、ということ。
スキーマ上書きを有効化した上でカラム追加を伴うような書き込みを行った際にメタデータが変わるため、
他のトランザクションからそのような書き込みがあった場合は、例外になることになる。</p>
<p>src/main/scala/org/apache/spark/sql/delta/OptimisticTransaction.scala:496</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> addedFilesToCheckForConflicts = commitIsolationLevel <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Serializable</span> =&gt; changedDataAddedFiles ++ blindAppendAddedFiles</span><br><span class="line">  <span class="keyword">case</span> <span class="type">WriteSerializable</span> =&gt; changedDataAddedFiles <span class="comment">// don't conflict with blind appends</span></span><br><span class="line">  <span class="keyword">case</span> <span class="type">SnapshotIsolation</span> =&gt; <span class="type">Seq</span>.empty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>アイソレーションレベルに基づいて、競合確認する対象ファイルを決める。</p>
<p>なお、前述の通り、実際にはデータの変更のありなしでアイソレーションレベルが変わるようになっている。
具体的には、</p>
<ul>
<li>データ変更あり：Serializable</li>
<li>データ変更なし：SnapshotIsolation</li>
</ul>
<p>となっており、いまの実装ではWriteSerializableは用いられないようにみえる。
<code>org.apache.spark.sql.delta.IsolationLevel</code>
にもそのような旨の記載がある。</p>
<p>org/apache/spark/sql/delta/isolationLevels.scala:83</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** All the valid isolation levels that can be specified as the table isolation level */</span></span><br><span class="line"><span class="keyword">val</span> validTableIsolationLevels = <span class="type">Set</span>[<span class="type">IsolationLevel</span>](<span class="type">Serializable</span>, <span class="type">WriteSerializable</span>)</span><br></pre></td></tr></table></figure>
<p>では、データの変更あり・なしは、どうやって設定されるのかというと、
<code>dataChange</code> というオプションで指定することになる。</p>
<p>データ変更なしの書き込みはいつ使われるのか、というと、
例えば「たくさん作られたDelta
Lakeのファイルをまとめる（Compactする）とき」である。</p>
<p>説明が <a href="https://docs.delta.io/latest/best-practices.html#compact-files" target="_blank" rel="noopener">公式ドキュメントのCompact
filesの説明</a> にある。
ドキュメントの例では以下のようにオプションが指定されている。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> path = <span class="string">"..."</span></span><br><span class="line"><span class="keyword">val</span> numFiles = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">spark.read</span><br><span class="line">  .format(<span class="string">"delta"</span>)</span><br><span class="line">  .load(path)</span><br><span class="line">  .repartition(numFiles)</span><br><span class="line">  .write</span><br><span class="line">  .option(<span class="string">"dataChange"</span>, <span class="string">"false"</span>)</span><br><span class="line">  .format(<span class="string">"delta"</span>)</span><br><span class="line">  .mode(<span class="string">"overwrite"</span>)</span><br><span class="line">  .save(path)</span><br></pre></td></tr></table></figure>
<p>例外処理の内容確認に戻る。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:496</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> addedFilesToCheckForConflicts = commitIsolationLevel <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">Serializable</span> =&gt; changedDataAddedFiles ++ blindAppendAddedFiles</span><br><span class="line">  <span class="keyword">case</span> <span class="type">WriteSerializable</span> =&gt; changedDataAddedFiles <span class="comment">// don't conflict with blind appends</span></span><br><span class="line">  <span class="keyword">case</span> <span class="type">SnapshotIsolation</span> =&gt; <span class="type">Seq</span>.empty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Delta
Lakeバージョン0.5.0では、今回のトランザクションで書き込みがある場合は既存ファイルに影響ない書き込みを含め、すべて確認対象とする。
そうでない場合は確認対象は空である。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:501</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> predicatesMatchingAddedFiles = <span class="type">ExpressionSet</span>(readPredicates).iterator.flatMap &#123; p =&gt;</span><br><span class="line">  <span class="keyword">val</span> conflictingFile = <span class="type">DeltaLog</span>.filterFileList(</span><br><span class="line">    metadata.partitionColumns,</span><br><span class="line">    addedFilesToCheckForConflicts.toDF(), p :: <span class="type">Nil</span>).as[<span class="type">AddFile</span>].take(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  conflictingFile.headOption.map(f =&gt; getPrettyPartitionMessage(f.partitionValues))</span><br><span class="line">&#125;.take(<span class="number">1</span>).toArray</span><br></pre></td></tr></table></figure>
<p>トランザクション中に、別のトランザクションによりファイルが追加された場合、関係するパーティション情報を取得。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:509</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (predicatesMatchingAddedFiles.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">val</span> isWriteSerializable = commitIsolationLevel == <span class="type">WriteSerializable</span></span><br><span class="line">  <span class="keyword">val</span> onlyAddFiles =</span><br><span class="line">    winningCommitActions.collect &#123; <span class="keyword">case</span> f: <span class="type">FileAction</span> =&gt; f &#125;.forall(_.isInstanceOf[<span class="type">AddFile</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> retryMsg =</span><br><span class="line">    <span class="keyword">if</span> (isWriteSerializable &amp;&amp; onlyAddFiles &amp;&amp; isBlindAppendOption.isEmpty) &#123;</span><br><span class="line">      <span class="comment">// This transaction was made by an older version which did not set `isBlindAppend` flag.</span></span><br><span class="line">      <span class="comment">// So even if it looks like an append, we don't know for sure if it was a blind append</span></span><br><span class="line">      <span class="comment">// or not. So we suggest them to upgrade all there workloads to latest version.</span></span><br><span class="line">      <span class="type">Some</span>(</span><br><span class="line">        <span class="string">"Upgrading all your concurrent writers to use the latest Delta Lake may "</span> +</span><br><span class="line">          <span class="string">"avoid this error. Please upgrade and then retry this operation again."</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ConcurrentAppendException</span>(commitInfo, predicatesMatchingAddedFiles.head, retryMsg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当該パーティション情報がから出ない場合は、例外
<code>org.apache.spark.sql.delta.ConcurrentAppendException#ConcurrentAppendException</code>
が生じる。</p>
<p>つづいて、上記でないケースのうち、削除が行われたケース。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:527</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> readFilePaths = readFiles.map(f =&gt; f.path -&gt; f.partitionValues).toMap</span><br><span class="line"><span class="keyword">val</span> deleteReadOverlap = removedFiles.find(r =&gt; readFilePaths.contains(r.path))</span><br><span class="line"><span class="keyword">if</span> (deleteReadOverlap.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">val</span> filePath = deleteReadOverlap.get.path</span><br><span class="line">  <span class="keyword">val</span> partition = getPrettyPartitionMessage(readFilePaths(filePath))</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ConcurrentDeleteReadException</span>(commitInfo, <span class="string">s"<span class="subst">$filePath</span> in <span class="subst">$partition</span>"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>読もうとしたファイルが他のトランザクションにより削除された倍は、 例外
<code>org.apache.spark.sql.delta.ConcurrentDeleteReadException#ConcurrentDeleteReadException</code>
を生じる。</p>
<p>もしくは、同じファイルを複数回消そうとしている場合、</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:536</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> txnDeletes = actions.collect &#123; <span class="keyword">case</span> r: <span class="type">RemoveFile</span> =&gt; r &#125;.map(_.path).toSet</span><br><span class="line"><span class="keyword">val</span> deleteOverlap = removedFiles.map(_.path).toSet intersect txnDeletes</span><br><span class="line"><span class="keyword">if</span> (deleteOverlap.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ConcurrentDeleteDeleteException</span>(commitInfo, deleteOverlap.head)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例外
<code>org.apache.spark.sql.delta.ConcurrentDeleteDeleteException#ConcurrentDeleteDeleteException</code>
が生じる。</p>
<p>その他、何らかトランザクション上重複した場合…</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> txnOverlap = txns.map(_.appId).toSet intersect readTxn.toSet</span><br><span class="line"><span class="keyword">if</span> (txnOverlap.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ConcurrentTransactionException</span>(commitInfo)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例外
<code>org.apache.spark.sql.delta.ConcurrentTransactionException#ConcurrentTransactionException</code>
が生じる。</p>
<p>以上に引っかからなかった場合は、例外を生じない。
トランザクション開始後、すべてのバージョンについて競合チェックが行われた後、
問題ない場合は、</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:549</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logInfo(<span class="string">s"No logical conflicts with deltas [<span class="subst">$checkVersion</span>, <span class="subst">$nextAttemptVersion</span>), retrying."</span>)</span><br><span class="line">doCommit(nextAttemptVersion, actions, attemptNumber + <span class="number">1</span>, commitIsolationLevel)</span><br></pre></td></tr></table></figure>
<p><code>org.apache.spark.sql.delta.OptimisticTransactionImpl#doCommit</code>
メソッドが再実行される。 つまり、リトライである。</p>
<p>こうなるケースは何かというと、コンパクションを行った際、他のトランザクションで削除などが行われなかったケースなどだと想像。</p>
<h3><span id="sbtでテストコードを使って確認">sbtでテストコードを使って確認</span></h3>
<p>上記実装の内容を確認するため、SBTのコンフィグを以下のように修正し、デバッガをアタッチして確認する。</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">diff --git a/build.sbt b/build.sbt</span><br><span class="line">index af4ab71..444fe55 100644</span><br><span class="line"><span class="comment">--- a/build.sbt</span></span><br><span class="line"><span class="comment">+++ b/build.sbt</span></span><br><span class="line">@@ -63,6 +63,7 @@ scalacOptions ++= Seq(</span><br><span class="line"> )</span><br><span class="line"></span><br><span class="line"> javaOptions += "-Xmx1024m"</span><br><span class="line"><span class="addition">+javaOptions += "-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005"</span></span><br><span class="line"></span><br><span class="line"> fork in Test := true</span><br></pre></td></tr></table></figure>
<p>sbtを実行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./build/sbt</span></span><br></pre></td></tr></table></figure>
<p>sbtで以下のテストを実行する。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; testOnly org.apache.spark.sql.delta.OptimisticTransactionSuite -- -z &quot;block concurrent commit on full table scan&quot;</span><br></pre></td></tr></table></figure>
<p>テストの内容は以下の通り。
tx1でスキャンしようとしたとき、別のtx2で先に削除が行われたケース。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">test(<span class="string">"block concurrent commit on full table scan"</span>) &#123;</span><br><span class="line">  withLog(addA_P1 :: addD_P2 :: <span class="type">Nil</span>) &#123; log =&gt;</span><br><span class="line">    <span class="keyword">val</span> tx1 = log.startTransaction()</span><br><span class="line">    <span class="comment">// TX1 full table scan</span></span><br><span class="line">    tx1.filterFiles()</span><br><span class="line">    tx1.filterFiles((<span class="symbol">'part</span> === <span class="number">1</span>).expr :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tx2 = log.startTransaction()</span><br><span class="line">    tx2.filterFiles()</span><br><span class="line">    tx2.commit(addC_P2 :: addD_P2.remove :: <span class="type">Nil</span>, <span class="type">ManualUpdate</span>)</span><br><span class="line"></span><br><span class="line">    intercept[<span class="type">ConcurrentAppendException</span>] &#123;</span><br><span class="line">      tx1.commit(addE_P3 :: addF_P3 :: <span class="type">Nil</span>, <span class="type">ManualUpdate</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>したがって、ここでは <code>removedFiles</code>
に値が含まれることになる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">removedFiles = &#123;ArrayBuffer@12893&#125; &quot;ArrayBuffer&quot; size = 1</span><br><span class="line"> 0 = &#123;RemoveFile@15705&#125; &quot;RemoveFile(part=2/d,Some(1583466515581),true)&quot;</span><br></pre></td></tr></table></figure>
<p>またtx2のアクションは正確には追加と削除であるから、
<code>winningCommitActions</code> の内容は以下の通りとなる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">winningCommitActions = &#123;ArrayBuffer@12887&#125; &quot;ArrayBuffer&quot; size = 3</span><br><span class="line"> 0 = &#123;CommitInfo@16006&#125; &quot;CommitInfo(None,2020-03-05 19:48:35.582,None,None,Manual Update,Map(),None,None,None,Some(0),None,Some(false))&quot;</span><br><span class="line"> 1 = &#123;AddFile@15816&#125; &quot;AddFile(part=2/c,Map(part -&gt; 2),1,1,true,null,null)&quot;</span><br><span class="line"> 2 = &#123;RemoveFile@15705&#125; &quot;RemoveFile(part=2/d,Some(1583466515581),true)&quot;</span><br></pre></td></tr></table></figure>
<p>なお、今回は既存ファイルへの変更になるため、Blind Appendではない。
結果として、 <code>changedDataAddedFiles</code>
には以下のような値が含まれることになる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">changedDataAddedFiles = &#123;ArrayBuffer@15736&#125; &quot;ArrayBuffer&quot; size = 1</span><br><span class="line"> 0 = &#123;AddFile@15816&#125; &quot;AddFile(part=2/c,Map(part -&gt; 2),1,1,true,null,null)&quot;</span><br></pre></td></tr></table></figure>
<p><code>addedFilesToCheckForConflicts</code> も同様。</p>
<p><code>addedFilesToCheckForConflicts</code> には以下の通り、
<code>AddFile</code> インスンタスが含まれる。
つまりtx2で追加しているファイルの情報が含まれる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">addedFilesToCheckForConflicts = &#123;ArrayBuffer@15866&#125; &quot;ArrayBuffer&quot; size = 1</span><br><span class="line"> 0 = &#123;AddFile@15816&#125; &quot;AddFile(part=2/c,Map(part -&gt; 2),1,1,true,null,null)&quot;</span><br></pre></td></tr></table></figure>
<p><code>predicatesMatchingAddedFiles</code>
は以下の通り、ファイル追加に関係するパーティション情報が含まれる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predicatesMatchingAddedFiles = &#123;String[1]@15911&#125; </span><br><span class="line"> 0 = &quot;partition [part=2]&quot;</span><br></pre></td></tr></table></figure>
<p>このことから、</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:509</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (predicatesMatchingAddedFiles.nonEmpty) &#123;</span><br></pre></td></tr></table></figure>
<p>がfalseになり、例外
<code>org.apache.spark.sql.delta.ConcurrentAppendException#ConcurrentAppendException</code>
が生じることになる。 つまり、以下の箇所。</p>
<p>org/apache/spark/sql/delta/OptimisticTransaction.scala:509</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (predicatesMatchingAddedFiles.nonEmpty) &#123;</span><br><span class="line">  <span class="keyword">val</span> isWriteSerializable = commitIsolationLevel == <span class="type">WriteSerializable</span></span><br><span class="line">  <span class="keyword">val</span> onlyAddFiles =</span><br><span class="line">    winningCommitActions.collect &#123; <span class="keyword">case</span> f: <span class="type">FileAction</span> =&gt; f &#125;.forall(_.isInstanceOf[<span class="type">AddFile</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> retryMsg =</span><br><span class="line">    <span class="keyword">if</span> (isWriteSerializable &amp;&amp; onlyAddFiles &amp;&amp; isBlindAppendOption.isEmpty) &#123;</span><br><span class="line">      <span class="comment">// This transaction was made by an older version which did not set `isBlindAppend` flag.</span></span><br><span class="line">      <span class="comment">// So even if it looks like an append, we don't know for sure if it was a blind append</span></span><br><span class="line">      <span class="comment">// or not. So we suggest them to upgrade all there workloads to latest version.</span></span><br><span class="line">      <span class="type">Some</span>(</span><br><span class="line">        <span class="string">"Upgrading all your concurrent writers to use the latest Delta Lake may "</span> +</span><br><span class="line">          <span class="string">"avoid this error. Please upgrade and then retry this operation again."</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ConcurrentAppendException</span>(commitInfo, predicatesMatchingAddedFiles.head, retryMsg)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2><span id="コンパクション">コンパクション</span></h2>
<p><a href="https://docs.delta.io/latest/best-practices.html#compact-files" target="_blank" rel="noopener">公式ドキュメントのCompact
filesの説明</a> にある通り、Delta
Lakeで細かくデータを書き込むとファイルがたくさんできる。
これは性能に悪影響を及ぼす。
そこで、コンパクション（まとめこみ）を行うことがベストプラクティスとして紹介されている…。</p>
<p>なお、コンパクションでは古いファイルは消されないので、バキュームすることってことも書かれている。</p>
<h2><span id="バキューム">バキューム</span></h2>
<p><a href="https://docs.delta.io/latest/delta-utility.html#vacuum" target="_blank" rel="noopener">公式ドキュメントのVacuum</a>
に記載の通り、Deltaテーブルから参照されていないファイルを削除する。
リテンション時間はデフォルト7日間。</p>
<p><code>io.delta.tables.DeltaTable#vacuum</code> メソッドの実態は、</p>
<p>io/delta/tables/DeltaTable.scala:90</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vacuum</span></span>(retentionHours: <span class="type">Double</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  executeVacuum(deltaLog, <span class="type">Some</span>(retentionHours))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>の通り、
<code>io.delta.tables.execution.DeltaTableOperations#executeVacuum</code>
メソッドである。</p>
<p>io/delta/tables/execution/DeltaTableOperations.scala:106</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">executeVacuum</span></span>(</span><br><span class="line">    deltaLog: <span class="type">DeltaLog</span>,</span><br><span class="line">    retentionHours: <span class="type">Option</span>[<span class="type">Double</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="type">VacuumCommand</span>.gc(sparkSession, deltaLog, <span class="literal">false</span>, retentionHours)</span><br><span class="line">  sparkSession.emptyDataFrame</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>なお、
<code>org.apache.spark.sql.delta.commands.VacuumCommand#gc</code>
メソッドの内容は意外と複雑。
というのも、バキューム対象となるのは、「Deltaテーブルで参照されて
<strong>いない</strong> ファイル」となるので、
すべてのファイル（やディレクトリ、そしてディレクトリ内のファイルも）をリストアップし、
その後消してはいけないファイルを除外できるようにして消すようになっている。</p>
<h3><span id="読まれているファイルの削除">読まれているファイルの削除</span></h3>
<p><a href="https://docs.delta.io/latest/delta-utility.html#vacuum" target="_blank" rel="noopener">公式ドキュメントのVacuum</a>
の中で、警告が書かれている。</p>
<blockquote>
<p>We do not recommend that you set a retention interval shorter than 7
days, because old snapshots and uncommitted files can still be in use by
concurrent readers or writers to the table. If VACUUM cleans up active
files, concurrent readers can fail or, worse, tables can be corrupted
when VACUUM deletes files that have not yet been committed.</p>
</blockquote>
<p>これは、バキューム対象から外す期間（リテンション時間）を不用意に短くしすぎると、
バキューム対象となったファイルを何かしらのトランザクションが読み込んでいる可能性があるからだ、という主旨の内容である。</p>
<p>実装から見ても、
<code>org.apache.spark.sql.delta.actions.FileAction</code>
トレートに基づくフィルタリングはあるが、 当該トレートの子クラスは</p>
<ul>
<li>AddFile</li>
<li>RemoveFile</li>
</ul>
<p>であり、読み込みは含まれない。
そのため、仕様上、何らかのトランザクションが読み込んでいるファイルを消すことがあり得る、ということと考えられる。
★要確認</p>
<p>なお、うっかりミスを防止するためのチェック機能はあり、
<code>org.apache.spark.sql.delta.commands.VacuumCommand#checkRetentionPeriodSafety</code>
メソッドがその実態である。
このメソッド内では、DeltaLogごとに定義されている
<code>org.apache.spark.sql.delta.DeltaLog#tombstoneRetentionMillis</code>
で指定されるよりも、短い期間をリテンション時間として
指定しているかどうかを確認し、警告を出すようになっている。</p>
<p>つまり、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deltaTable.vacuum(100)     // vacuum files not required by versions more than 100 hours old</span><br></pre></td></tr></table></figure>
<p>のようにリテンション時間を指定しながらバキュームを実行する際に、DeltaLog自身が保持している
<code>tombstoneRetentionMillis</code> よりも短い期間を閾値として
バキュームを実行しようとすると警告が生じる。なお、このチェックをオフにすることもできる。</p>
<h2><span id="コンフィグ">コンフィグ</span></h2>
<p><code>org.apache.spark.sql.delta.DeltaConfigs</code> あたりにDelta
Lakeのコンフィグと説明がある。</p>
<p>例えば、
<code>org.apache.spark.sql.delta.DeltaConfigs$#LOG_RETENTION</code>
であれば、 <code>org.apache.spark.sql.delta.MetadataCleanup</code>
トレート内で利用されている。</p>
<h2><span id="ログのクリーンアップ">ログのクリーンアップ</span></h2>
<p><a href="#コンフィグ">コンフィグ</a>
にてリテンションを決めるパラメータを例として上げた。
具体的には、以下のように、ログのリテンション時間を決めるパラメータとして利用されている。</p>
<p>org/apache/spark/sql/delta/MetadataCleanup.scala:38</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deltaRetentionMillis</span></span>: <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> interval = <span class="type">DeltaConfigs</span>.<span class="type">LOG_RETENTION</span>.fromMetaData(metadata)</span><br><span class="line">  <span class="type">DeltaConfigs</span>.getMilliSeconds(interval)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の
<code>org.apache.spark.sql.delta.MetadataCleanup#deltaRetentionMillis</code>
メソッドは、
<code>org.apache.spark.sql.delta.MetadataCleanup#cleanUpExpiredLogs</code>
メソッド内で利用される。
このメソッドは古くなったデルタログやチェックポイントを削除する。</p>
<p>このメソッド自体は単純で、以下のような実装である。</p>
<p>org/apache/spark/sql/delta/MetadataCleanup.scala:50</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[delta] <span class="function"><span class="keyword">def</span> <span class="title">cleanUpExpiredLogs</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  recordDeltaOperation(<span class="keyword">this</span>, <span class="string">"delta.log.cleanup"</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> fileCutOffTime = truncateDay(clock.getTimeMillis() - deltaRetentionMillis).getTime</span><br><span class="line">    <span class="keyword">val</span> formattedDate = fileCutOffTime.toGMTString</span><br><span class="line">    logInfo(<span class="string">s"Starting the deletion of log files older than <span class="subst">$formattedDate</span>"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> numDeleted = <span class="number">0</span></span><br><span class="line">    listExpiredDeltaLogs(fileCutOffTime.getTime).map(_.getPath).foreach &#123; path =&gt;</span><br><span class="line">      <span class="comment">// recursive = false</span></span><br><span class="line">      <span class="keyword">if</span> (fs.delete(path, <span class="literal">false</span>)) numDeleted += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    logInfo(<span class="string">s"Deleted <span class="subst">$numDeleted</span> log files older than <span class="subst">$formattedDate</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ポイントはいくつかある。</p>
<ul>
<li><code>org.apache.spark.sql.delta.MetadataCleanup#listExpiredDeltaLogs</code>
メソッドは、チェックポイントファイル、デルタファイルの両方について、
期限切れになっているファイルを返すイテレータを戻す。</li>
<li>上記イテレータに対し、mapとforeachでループさせ、ファイルを消す（<code>fs.delete(path, false)</code>）</li>
<li>最終的に消された件数をログに書き出す</li>
</ul>
<p>なお、このクリーンアップは、チェックポイントのタイミングで実施される。</p>
<p>org/apache/spark/sql/delta/Checkpoints.scala:119</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Unit</span> = recordDeltaOperation(<span class="keyword">this</span>, <span class="string">"delta.checkpoint"</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> checkpointMetaData = checkpoint(snapshot)</span><br><span class="line">  <span class="keyword">val</span> json = <span class="type">JsonUtils</span>.toJson(checkpointMetaData)</span><br><span class="line">  store.write(<span class="type">LAST_CHECKPOINT</span>, <span class="type">Iterator</span>(json), overwrite = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  doLogCleanup()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>チェックポイントが実行されるのは、別途 <a href="#チェックポイントを調査してみる">チェックポイントを調査してみる</a>
で説明したとおり、
トランザクションがコミットされるタイミングなどである。（正確にはコミット後の処理postCommit処理の中で行われる）</p>
<h2><span id="parquetからdeltaテーブルへの変換">ParquetからDeltaテーブルへの変換</span></h2>
<p>スキーマ指定する方法としない方法がある。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io.delta.tables._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert unpartitioned parquet table at path '/path/to/table'</span></span><br><span class="line"><span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.convertToDelta(spark, <span class="string">"parquet.`/path/to/table`"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Convert partitioned parquet table at path '/path/to/table' and partitioned by integer column named 'part'</span></span><br><span class="line"><span class="keyword">val</span> partitionedDeltaTable = <span class="type">DeltaTable</span>.convertToDelta(spark, <span class="string">"parquet.`/path/to/table`"</span>, <span class="string">"part int"</span>)</span><br></pre></td></tr></table></figure>
<p>スキーマを指定しないAPIは以下の通り。</p>
<p>io/delta/tables/DeltaTable.scala:599</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convertToDelta</span></span>(</span><br><span class="line">    spark: <span class="type">SparkSession</span>,</span><br><span class="line">    identifier: <span class="type">String</span>): <span class="type">DeltaTable</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> tableId: <span class="type">TableIdentifier</span> = spark.sessionState.sqlParser.parseTableIdentifier(identifier)</span><br><span class="line">  <span class="type">DeltaConvert</span>.executeConvert(spark, tableId, <span class="type">None</span>, <span class="type">None</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、実態は
<code>io.delta.tables.execution.DeltaConvertBase#executeConvert</code>
メソッドであり、 その中で用いられている
<code>org.apache.spark.sql.delta.commands.ConvertToDeltaCommandBase#run</code>
メソッドである。</p>
<p>io/delta/tables/execution/DeltaConvert.scala:26</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">DeltaConvertBase</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executeConvert</span></span>(</span><br><span class="line">      spark: <span class="type">SparkSession</span>,</span><br><span class="line">      tableIdentifier: <span class="type">TableIdentifier</span>,</span><br><span class="line">      partitionSchema: <span class="type">Option</span>[<span class="type">StructType</span>],</span><br><span class="line">      deltaPath: <span class="type">Option</span>[<span class="type">String</span>]): <span class="type">DeltaTable</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> cvt = <span class="type">ConvertToDeltaCommand</span>(tableIdentifier, partitionSchema, deltaPath)</span><br><span class="line">    cvt.run(spark)</span><br><span class="line">    <span class="type">DeltaTable</span>.forPath(spark, tableIdentifier.table)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>また、上記 <code>run</code> メソッド内の実態は
<code>org.apache.spark.sql.delta.commands.ConvertToDeltaCommandBase#performConvert</code>
メソッドである。</p>
<p>org/apache/spark/sql/delta/commands/ConvertToDeltaCommand.scala:76</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(spark: <span class="type">SparkSession</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> convertProperties = getConvertProperties(spark, tableIdentifier)</span><br><span class="line"></span><br><span class="line">  (snip)</span><br><span class="line"></span><br><span class="line">  performConvert(spark, txn, convertProperties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>run</code> メソッド内の <code>performConvert</code>
メソッドを利用し、実際に元データからDelta Lakeのデータ構造を作りつつ、
その後の
<code>io.delta.tables.DeltaTable$#forPath(org.apache.spark.sql.SparkSession, java.lang.String)</code>
メソッドを呼び出すことで、
データが正しく出力されたことを確認している。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DeltaTable.forPath(spark, tableIdentifier.table)</span><br></pre></td></tr></table></figure>
<p>もし出力された内容に何か問題あるようであれば、 <code>forPath</code>
メソッドの途中で例外が生じるはず。
ただ、この仕組みだと変換自体は、たとえ失敗したとしても動いてしまう（アトミックな動作ではない）ところが気になった。
★気になった点</p>
<h3><span id="動作確認">動作確認</span></h3>
<p>まずは、サンプルとしてSparkに含まれているParquetファイルを読み込む。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> originDFPath = <span class="string">"/opt/spark/default/examples/src/main/resources/users.parquet"</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> originDF = spark.read.format(<span class="string">"parquet"</span>).load(originDFPath)</span><br><span class="line">scala&gt; originDF.show</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|<span class="type">Alyssa</span>|          <span class="literal">null</span>|  [<span class="number">3</span>, <span class="number">9</span>, <span class="number">15</span>, <span class="number">20</span>]|</span><br><span class="line">|   <span class="type">Ben</span>|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br></pre></td></tr></table></figure>
<p>では、このParquetファイルを変換する。
<code>io.delta.tables.DeltaTable#convertToDelta</code>
メソッドの引数で与えるPATHは、
Parqeutテーブルを含む「ディレクトリ」を期待するので、最初にParquetファイルを
適当なディレクトリにコピーしておく。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir /tmp/origin</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp /opt/spark/default/examples/src/main/resources/users.parquet /tmp/origin</span></span><br></pre></td></tr></table></figure>
<p>上記ディレクトリを指定して、変換する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables._</span><br><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.convertToDelta(spark, <span class="string">s"parquet.`/tmp/origin`"</span>)</span><br></pre></td></tr></table></figure>
<p>ディレクトリ以下は以下のようになった。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dobachi@thk:/mnt/c/Users/dobachi/Sources.win/memo-blog-text$ ls -R /tmp/origin/</span><br><span class="line">/tmp/origin/:</span><br><span class="line">_delta_log  users.parquet</span><br><span class="line"></span><br><span class="line">/tmp/origin/_delta_log:</span><br><span class="line">00000000000000000000.checkpoint.parquet  00000000000000000000.json  _last_checkpoint</span><br></pre></td></tr></table></figure>
<p>なお、メタデータは以下の通り。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">$ cat /tmp/origin/_delta_log/00000000000000000000.json | jq</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"commitInfo"</span>: &#123;</span><br><span class="line">    <span class="attr">"timestamp"</span>: <span class="number">1584541495383</span>,</span><br><span class="line">    <span class="attr">"operation"</span>: <span class="string">"CONVERT"</span>,</span><br><span class="line">    <span class="attr">"operationParameters"</span>: &#123;</span><br><span class="line">      <span class="attr">"numFiles"</span>: <span class="number">1</span>,</span><br><span class="line">      <span class="attr">"partitionedBy"</span>: <span class="string">"[]"</span>,</span><br><span class="line">      <span class="attr">"collectStats"</span>: <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"protocol"</span>: &#123;</span><br><span class="line">    <span class="attr">"minReaderVersion"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"minWriterVersion"</span>: <span class="number">2</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"metaData"</span>: &#123;</span><br><span class="line">    <span class="attr">"id"</span>: <span class="string">"40bd74eb-8005-4aaa-a455-fbbb37b22bb7"</span>,</span><br><span class="line">    <span class="attr">"format"</span>: &#123;</span><br><span class="line">      <span class="attr">"provider"</span>: <span class="string">"parquet"</span>,</span><br><span class="line">      <span class="attr">"options"</span>: &#123;&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"schemaString"</span>: <span class="string">"&#123;\"type\":\"struct\",\"fields\":[&#123;\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":&#123;&#125;&#125;,&#123;\"name\":\"favorite_color\",\"type\":\"string\",\"nullable\":true,\"metadata\":&#123;&#125;&#125;,&#123;\"name\":\"favorite_numbers\",\"type\":&#123;\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true&#125;,\"nullable\":true,\"metadata\":&#123;&#125;&#125;]&#125;"</span>,</span><br><span class="line">    <span class="attr">"partitionColumns"</span>: [],</span><br><span class="line">    <span class="attr">"configuration"</span>: &#123;&#125;,</span><br><span class="line">    <span class="attr">"createdTime"</span>: <span class="number">1584541495356</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"add"</span>: &#123;</span><br><span class="line">    <span class="attr">"path"</span>: <span class="string">"users.parquet"</span>,</span><br><span class="line">    <span class="attr">"partitionValues"</span>: &#123;&#125;,</span><br><span class="line">    <span class="attr">"size"</span>: <span class="number">615</span>,</span><br><span class="line">    <span class="attr">"modificationTime"</span>: <span class="number">1584541479000</span>,</span><br><span class="line">    <span class="attr">"dataChange"</span>: <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2><span id="symlink-format-manifest">Symlink Format Manifest</span></h2>
<p><a href="https://docs.delta.io/0.5.0/presto-integration.html" target="_blank" rel="noopener">Presto
and Athena to Delta Lake Integration</a>
によると、Presto等で利用可能なマニフェストを出力できる。</p>
<p>ここでは予め <code>/tmp/delta-table</code> に作成しておいたDelta
Tableを読み込み、マニフェストを出力する。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> io.delta.tables._</span><br><span class="line">scala&gt; <span class="keyword">val</span> deltaTable = <span class="type">DeltaTable</span>.forPath(<span class="string">"/tmp/delta-table"</span>)</span><br><span class="line">scala&gt; deltaTable.generate(<span class="string">"symlink_format_manifest"</span>)</span><br></pre></td></tr></table></figure>
<p>以下のようなディレクトリ、ファイルが出力される。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -1</span></span><br><span class="line">_delta_log</span><br><span class="line">_symlink_format_manifest</span><br><span class="line">derby.log</span><br><span class="line">metastore_db</span><br><span class="line">part-00000-e4518073-8ff1-4c2e-b765-922114a06c08-c000.snappy.parquet</span><br><span class="line">part-00000-f692adf2-c015-4b0b-8db9-8004a69ac80b-c000.snappy.parquet</span><br><span class="line">part-00001-7c3dd52d-f763-4835-9e97-9c6805ceff36-c000.snappy.parquet</span><br><span class="line">part-00001-d13867d8-c685-4a56-b0cd-6541009222a5-c000.snappy.parquet</span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p><code>_delta_log</code> および <code>part-000...</code>
というディレクトリ、ファイルはもともとDelta Lakeとして
存在していたものである。 これに対し、</p>
<ul>
<li><code>_symlink_format_manifest</code>
<ul>
<li>Deltaテーブルが含むファイル群を示すマニフェストを含むディレクトリ</li>
</ul></li>
<li><code>derby.log</code>
<ul>
<li>HiveメタストアDBのログ（Derbyのログ）</li>
</ul></li>
<li><code>metastore_db</code>
<ul>
<li>Hiveメタストア</li>
</ul></li>
</ul>
<p>が出力されたと言える。 マニフェストの内容は以下の通り。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat _symlink_format_manifest/manifest</span></span><br><span class="line">file:/tmp/delta-table/part-00002-5f9ef6f0-da56-442d-8232-13937e00a54e-c000.snappy.parquet</span><br><span class="line">file:/tmp/delta-table/part-00007-5522221f-20c2-4d70-aec8-3a990933b50e-c000.snappy.parquet</span><br><span class="line">file:/tmp/delta-table/part-00003-572f44fd-9c36-409a-bbc8-8f23f869e3f1-c000.snappy.parquet</span><br><span class="line">file:/tmp/delta-table/part-00000-f692adf2-c015-4b0b-8db9-8004a69ac80b-c000.snappy.parquet</span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>上記の例では、パーティション化されていないDeltaテーブルを扱った。
この場合、マニフェストは１個である。 アトミックな書き込みが可能。
Snapshot consistency が実現できる。</p>
<p>一方、 <a href="https://docs.delta.io/0.5.0/presto-integration.html#limitations" target="_blank" rel="noopener">Presto
Athena連係の制約</a>
によるとパーティション化されている場合、マニフェストもパーティション化される。
そのため、全パーティションを通じて一貫性を保つことができない。（部分的な更新が生じうる）</p>
<p>それでは、実装を確認する。</p>
<p>エントリポイントは、公式ドキュメントの例にも載っている
<code>io.delta.tables.DeltaTable#generate</code> メソッド。</p>
<p>io/delta/tables/DeltaTable.scala:151</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span></span>(mode: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> path = deltaLog.dataPath.toString</span><br><span class="line">  executeGenerate(<span class="string">s"delta.`<span class="subst">$path</span>`"</span>, mode)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>これの定義を辿っていくと、
<code>org.apache.spark.sql.delta.commands.DeltaGenerateCommand#run</code>
メソッドにたどり着く。</p>
<p>org/apache/spark/sql/delta/commands/DeltaGenerateCommand.scala:48</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sparkSession: <span class="type">SparkSession</span>): <span class="type">Seq</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!modeNameToGenerationFunc.contains(modeName)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="type">DeltaErrors</span>.unsupportedGenerateModeException(modeName)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> tablePath = getPath(sparkSession, tableId)</span><br><span class="line">    <span class="keyword">val</span> deltaLog = <span class="type">DeltaLog</span>.forTable(sparkSession, tablePath)</span><br><span class="line">    <span class="keyword">if</span> (deltaLog.snapshot.version &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AnalysisException</span>(<span class="string">s"Delta table not found at <span class="subst">$tablePath</span>."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> generationFunc = modeNameToGenerationFunc(modeName)</span><br><span class="line">    generationFunc(sparkSession, deltaLog)</span><br><span class="line">    <span class="type">Seq</span>.empty</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上記の通り、Delta LakeのディレクトリからDeltaLogを再現し、
それを引数としてマニフェストを生成するメソッドを呼び出す。
変数にバインドするなどしているが、実態は
<code>org.apache.spark.sql.delta.hooks.GenerateSymlinkManifestImpl#generateFullManifest</code>
メソッドである。</p>
<p>org/apache/spark/sql/delta/commands/DeltaGenerateCommand.scala:63</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DeltaGenerateCommand</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> modeNameToGenerationFunc = <span class="type">CaseInsensitiveMap</span>(</span><br><span class="line">    <span class="type">Map</span>[<span class="type">String</span>, (<span class="type">SparkSession</span>, <span class="type">DeltaLog</span>) =&gt; <span class="type">Unit</span>](</span><br><span class="line">    <span class="string">"symlink_format_manifest"</span> -&gt; <span class="type">GenerateSymlinkManifest</span>.generateFullManifest</span><br><span class="line">  ))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>そこで当該メソッドの内容を軽く確認する。</p>
<p>org/apache/spark/sql/delta/hooks/GenerateSymlinkManifest.scala:154</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">generateFullManifest</span></span>(</span><br><span class="line">      spark: <span class="type">SparkSession</span>,</span><br><span class="line">      deltaLog: <span class="type">DeltaLog</span>): <span class="type">Unit</span> = recordManifestGeneration(deltaLog, full = <span class="literal">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> snapshot = deltaLog.update(stalenessAcceptable = <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionCols = snapshot.metadata.partitionColumns</span><br><span class="line">    <span class="keyword">val</span> manifestRootDirPath = <span class="keyword">new</span> <span class="type">Path</span>(deltaLog.dataPath, <span class="type">MANIFEST_LOCATION</span>).toString</span><br><span class="line">    <span class="keyword">val</span> hadoopConf = <span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(spark.sessionState.newHadoopConf())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Update manifest files of the current partitions</span></span><br><span class="line">    <span class="keyword">val</span> newManifestPartitionRelativePaths = writeManifestFiles(</span><br><span class="line">      deltaLog.dataPath,</span><br><span class="line">      manifestRootDirPath,</span><br><span class="line">      snapshot.allFiles,</span><br><span class="line">      partitionCols,</span><br><span class="line">      hadoopConf)</span><br><span class="line"></span><br><span class="line">(snip)</span><br></pre></td></tr></table></figure>
<p>ポイントは、
<code>org.apache.spark.sql.delta.hooks.GenerateSymlinkManifestImpl#writeManifestFiles</code>
メソッドである。 これが実際にマニフェストを書き出すメソッド。</p>
<h2><span id="類似技術">類似技術</span></h2>
<ul>
<li>Apache Hudi
<ul>
<li>https://hudi.apache.org/</li>
</ul></li>
<li>Ice
<ul>
<li>https://iceberg.apache.org/</li>
</ul></li>
</ul>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>その後の確認でやはりv1利用のように見えた。（2020/2時点）<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>その後の調査（2020/2時点）で改めて楽観ロックの仕組みで実現されていることを確認<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>マージ後は <code>id</code>
順に並んでいない。明示的なソートが必要のようだ。<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>ただし、バージョン0.5.0では、実際のところ使われているアイソレーションレベルが限られているので、ここでの仕分けはあまり意味がないかもしれない。<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/05/05/Delta-Lake/" data-id="clmdqacyk01fo1vs3in17e26h" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Dask-of-Python" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/23/Dask-of-Python/">Dask of Python</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/23/Dask-of-Python/">
            <time datetime="2019-04-23T14:12:05.000Z" itemprop="datePublished">2019-04-23</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/">Data Processing Engine</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Dask/">Dask</a>, <a class="tag-link" href="/memo-blog/tags/Data-Analysis/">Data Analysis</a>, <a class="tag-link" href="/memo-blog/tags/Data-Processing-Engine/">Data Processing Engine</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#ダミー変数化での支障について" id="toc-ダミー変数化での支障について">ダミー変数化での支障について</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://qiita.com/art_526/items/ca003a78535ab4546a01" target="_blank" rel="noopener">Dask使ってみた</a></li>
<li><a href="http://sinhrks.hatenablog.com/entry/2015/09/24/222735" target="_blank" rel="noopener">Python Dask
で 並列 DataFrame 処理</a></li>
<li><a href="https://qiita.com/katsuki104/items/222a9d5e85e4f92f4c43" target="_blank" rel="noopener">【初めての大規模データ②】Daskでの並列分散処理</a></li>
<li><a href="http://cocodrips.hateblo.jp/entry/2018/12/18/201752" target="_blank" rel="noopener">時間のかかる前処理をDaskで高速化</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="ダミー変数化での支障について">ダミー変数化での支障について</span></h2>
<p><a href="https://qiita.com/katsuki104/items/222a9d5e85e4f92f4c43" target="_blank" rel="noopener">【初めての大規模データ②】Daskでの並列分散処理</a>
に記載あった点が気になった。</p>
<p>以下、引用。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dask.dataframeは行方向にしかデータを分割できないため、ダミー変数を作成する際には1列分のデータを取得するために、すべてのデータを読み込まなければならず、メモリエラーを起こす危険性があります。</span><br><span class="line">そこで、大規模データの処理を行う際には、dask.dataframeを一度dask.arrayに1列ずつに分割した形で変換し、</span><br><span class="line">それから1列分のデータのみを再度dask.dataframeに変換し、get_dummiesしてやるのが良いと思います。</span><br><span class="line">※私はこの縛りに気づき、daskを使うのを諦めました...</span><br></pre></td></tr></table></figure></p>
<p>上記ブログでは、少なくともダミー変数化の部分は素のPythonで実装したようだ。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/23/Dask-of-Python/" data-id="clmdqaceh002z1vs3qbc9jn8d" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-Spark-Summit-NA-2019" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/22/Spark-Summit-NA-2019/">Spark Summit NA 2019</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/22/Spark-Summit-NA-2019/">
            <time datetime="2019-04-22T12:20:43.000Z" itemprop="datePublished">2019-04-22</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Spark/">Spark</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Spark/Spark-Summit/">Spark Summit</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Spark-Summit/">Spark Summit</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://databricks.com/sparkaisummit/north-america/schedule-static" target="_blank" rel="noopener">Spark
Summit NA 2019のスケジュール</a></li>
<li><a href="https://docs.google.com/document/d/1Kbu0f0vY2IALqDwFAms89AJL1Q0-pTcqO9Epr44odKs/edit?usp=sharing" target="_blank" rel="noopener">個人的に気になったセッション</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p>気になるセッションを <a href="https://docs.google.com/document/d/1Kbu0f0vY2IALqDwFAms89AJL1Q0-pTcqO9Epr44odKs/edit?usp=sharing" target="_blank" rel="noopener">個人的に気になったセッション</a>
に示す。 ただし、個別のDeep Diveネタは除く。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/22/Spark-Summit-NA-2019/" data-id="clmdqacie00a11vs31cs14bvb" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-RAPIDS" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/16/RAPIDS/">RAPIDS</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/16/RAPIDS/">
            <time datetime="2019-04-16T13:40:07.000Z" itemprop="datePublished">2019-04-16</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/">Data Processing Engine</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Dask/">Dask</a>, <a class="tag-link" href="/memo-blog/tags/Data-Analysis/">Data Analysis</a>, <a class="tag-link" href="/memo-blog/tags/Data-Processing-Engine/">Data Processing Engine</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/PAPIDS/">PAPIDS</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#概要" id="toc-概要">概要</a>
<ul>
<li><a href="#一言で言うと" id="toc-一言で言うと">一言で言うと…？</a></li>
<li><a href="#コンポーネント群" id="toc-コンポーネント群">コンポーネント群</a></li>
<li><a href="#処理性能に関する雰囲気" id="toc-処理性能に関する雰囲気">処理性能に関する雰囲気</a></li>
<li><a href="#関係者" id="toc-関係者">関係者</a></li>
<li><a href="#分散の仕組みはdaskベース" id="toc-分散の仕組みはdaskベース">分散の仕組みはDaskベース？</a></li>
</ul></li>
<li><a href="#cudf" id="toc-cudf">cuDF</a>
<ul>
<li><a href="#バージョン0.7での予定" id="toc-バージョン0.7での予定">バージョン0.7での予定</a></li>
</ul></li>
<li><a href="#rapids-on-databricks-cloud" id="toc-rapids-on-databricks-cloud">RAPIDS on Databricks Cloud</a></li>
<li><a href="#condaでの動作確認" id="toc-condaでの動作確認">Condaでの動作確認</a>
<ul>
<li><a href="#前提" id="toc-前提">前提</a></li>
<li><a href="#動作確認" id="toc-動作確認">動作確認</a></li>
</ul></li>
<li><a href="#dockerで動作確認" id="toc-dockerで動作確認">Dockerで動作確認</a></li>
<li><a href="#参考dockerで動作確認する際の試行錯誤" id="toc-参考dockerで動作確認する際の試行錯誤">参考）Dockerで動作確認する際の試行錯誤</a>
<ul>
<li><a href="#前提-1" id="toc-前提-1">前提</a></li>
<li><a href="#動作確認-1" id="toc-動作確認-1">動作確認</a></li>
<li><a href="#サンプルノートブックの確認" id="toc-サンプルノートブックの確認">サンプルノートブックの確認</a></li>
</ul></li>
<li><a href="#参考condaで動作確認の試行錯誤メモ" id="toc-参考condaで動作確認の試行錯誤メモ">参考）Condaで動作確認の試行錯誤メモ</a>
<ul>
<li><a href="#インスタンス種類を変えてみる" id="toc-インスタンス種類を変えてみる">インスタンス種類を変えてみる</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://rapids.ai/" target="_blank" rel="noopener">RAPIDSの公式ウェブサイト</a></li>
<li><a href="https://docs.rapids.ai/" target="_blank" rel="noopener">RAPIDSの公式ドキュメント</a></li>
<li><a href="https://docs.rapids.ai/start" target="_blank" rel="noopener">RAPIDSの公式Getting
Started</a></li>
<li><a href="https://rapids.ai/assets/images/Pipeline-FPO-Diagram.png" target="_blank" rel="noopener">コンポーネントの関係図</a></li>
<li><a href="https://rapids.ai/assets/images/rapids-end-to-end-performance-chart-oss-page-r4.svg" target="_blank" rel="noopener">性能比較のグラフ</a></li>
<li><a href="https://medium.com/rapids-ai/the-road-to-1-0-building-for-the-long-haul-657ae1afdfd6" target="_blank" rel="noopener">RAPIDS0.6に関する公式ブログ記事</a></li>
<li><a href="https://medium.com/rapids-ai/rapids-can-now-be-accessed-on-databricks-unified-analytics-platform-666e42284bd1" target="_blank" rel="noopener">RAPIDS
on Databricks Cloudのブログ</a></li>
<li><a href="https://github.com/rapidsai/notebooks-extended/blob/master/tutorials/examples/RAPIDS_PCA_demo_avro_read.ipynb" target="_blank" rel="noopener">RAPIDS_PCA_demo_avro_read.ipynb</a></li>
<li><a href="https://github.com/rapidsai/notebooks-extended/blob/master/tutorials/examples/Dask_with_cuDF_and_XGBoost.ipynb" target="_blank" rel="noopener">Dask_with_cuDF_and_XGBoost.ipynb</a></li>
<li><a href="https://docs.rapids.ai/api/rmm/stable/" target="_blank" rel="noopener">RMMの定義</a></li>
<li><a href="https://docs.rapids.ai/api/cudf/stable/" target="_blank" rel="noopener">cuDFのAPI定義</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="概要">概要</span></h2>
<h3><span id="一言で言うと">一言で言うと…？</span></h3>
<p>PandasライクなAPI、scikit
learnライクなAPIなどを提供するライブラリ群。
CUDAをレバレッジし、GPUを活用しやくしている。</p>
<h3><span id="コンポーネント群">コンポーネント群</span></h3>
<p>公式の <a href="https://rapids.ai/assets/images/Pipeline-FPO-Diagram.png" target="_blank" rel="noopener">コンポーネントの関係図</a>
がわかりやすい。
前処理から、データサイエンス（と、公式で言われている）まで一貫して手助けするものである、というのが思想のようだ。</p>
<h3><span id="処理性能に関する雰囲気">処理性能に関する雰囲気</span></h3>
<p>公式の <a href="https://rapids.ai/assets/images/rapids-end-to-end-performance-chart-oss-page-r4.svg" target="_blank" rel="noopener">性能比較のグラフ</a>
がわかりやすい。 2桁台数の「CPUノード」との比較。 DGX-2
1台、もしくはDGX-1 5台。
計算（ロード、特徴エンジニアリング、変換、学習）時間の比較。</p>
<h3><span id="関係者">関係者</span></h3>
<p>公式ウェブサイトでは、「Contributors」、「Adopters」、「Open
Source」というカテゴリで整理されていた。
Contributorsでは、AnacondaやNVIDIAなどのイメージ通りの企業から、Walmartまであった。
Adoptersでは、Databricksが入っている。</p>
<h3><span id="分散の仕組みはdaskベース">分散の仕組みはDaskベース？</span></h3>
<p>Sparkと組み合わせての動作はまだ対応していないようだが、Daskで分散（なのか、現時点でのシングルマシンでのマルチGPU対応だけなのか）処理できるようだ。
<a href="https://medium.com/rapids-ai/the-road-to-1-0-building-for-the-long-haul-657ae1afdfd6" target="_blank" rel="noopener">RAPIDS0.6に関する公式ブログ記事</a>
によると、Dask-CUDA、Dask-cuDFに対応の様子。</p>
<p>また、Dask-cuMLでは、k-NNと線形回帰に関し、マルチGPU動作を可能にしたようだ。</p>
<p><a href="https://github.com/rapidsai/notebooks-extended/blob/master/tutorials/examples/Dask_with_cuDF_and_XGBoost.ipynb" target="_blank" rel="noopener">Dask_with_cuDF_and_XGBoost.ipynb</a>
を見ると、DaskでcuDFを使う方法が例示されていた。</p>
<p>Daskのクラスタを定義し、 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from dask_cuda import LocalCUDACluster</span><br><span class="line"></span><br><span class="line">cluster = LocalCUDACluster()</span><br><span class="line">client = Client(cluster)</span><br></pre></td></tr></table></figure></p>
<p>RMM（RAPIDS Memory Manager）を初期化する。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from librmm_cffi import librmm_config as rmm_cfg</span><br><span class="line"></span><br><span class="line">rmm_cfg.use_pool_allocator = True</span><br><span class="line">#rmm_cfg.initial_pool_size = 2&lt;&lt;30 # set to 2GiB. Default is 1/2 total GPU memory</span><br><span class="line">import cudf</span><br><span class="line">return cudf._gdf.rmm_initialize()</span><br></pre></td></tr></table></figure></p>
<p>また、 <a href="https://docs.rapids.ai/api/rmm/stable/" target="_blank" rel="noopener">RMMの定義</a>
にRAPIDS Memory ManagerのAPI定義が記載されている。</p>
<p><a href="https://docs.rapids.ai/api/cudf/stable/" target="_blank" rel="noopener">cuDFのAPI定義</a>
にもDaskでの動作について書かれている。 「Multi-GPU with
Dask-cuDF」の項目。</p>
<h2><span id="cudf">cuDF</span></h2>
<p><a href="https://docs.rapids.ai/api/cudf/stable/" target="_blank" rel="noopener">cuDFのAPI定義</a>
に仕様が記載されている。 現状できることなどの記載がある。</p>
<p>線形なオペレーション、集約処理、グルーピング、結合あたりの基本的な操作対応している。</p>
<h3><span id="バージョン07での予定">バージョン0.7での予定</span></h3>
<p><a href="https://medium.com/rapids-ai/the-road-to-1-0-building-for-the-long-haul-657ae1afdfd6" target="_blank" rel="noopener">RAPIDS0.6に関する公式ブログ記事</a>
には一部0.7に関する記述もあった。
個人的に気になったのは、Parquetリーダが改善されてGPUを使うようになること、
デバッグメッセージが改善されること、など。</p>
<h2><span id="rapids-on-databricks-cloud">RAPIDS on Databricks Cloud</span></h2>
<p><a href="https://medium.com/rapids-ai/rapids-can-now-be-accessed-on-databricks-unified-analytics-platform-666e42284bd1" target="_blank" rel="noopener">RAPIDS
on Databricks Cloudのブログ</a> を見ると、Databricks
CloudでRAPIDSが動くことが書かれている。 ただ、 <a href="https://github.com/rapidsai/notebooks-extended/blob/master/tutorials/examples/RAPIDS_PCA_demo_avro_read.ipynb" target="_blank" rel="noopener">RAPIDS_PCA_demo_avro_read.ipynb</a>
を見たら、SparkでロードしたデータをそのままPandasのDataFrameに変換しているようだった。</p>
<h2><span id="condaでの動作確認">Condaでの動作確認</span></h2>
<h3><span id="前提">前提</span></h3>
<p>今回AWS環境を使った。 p3.2xlargeインスタンスを利用。</p>
<p>予めCUDA9.2をインストールしておいた。
またDockerで試す場合には、<code>nvidia-docker</code>をインストールしておく。</p>
<h3><span id="動作確認">動作確認</span></h3>
<p>以下の通り仮想環境を構築し、パッケージをインストールする。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ conda create -n rapids python=3.6 python</span><br><span class="line">$ conda install -c nvidia -c rapidsai -c pytorch -c numba -c conda-forge \</span><br><span class="line">    cudf=0.6 cuml=0.6 python=3.6</span><br></pre></td></tr></table></figure></p>
<p>その後、GitHubからノートブックをクローン。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/rapidsai/notebooks.git</span><br></pre></td></tr></table></figure></p>
<p>ノートブックのディレクトリでJupyterを起動。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jupyter notebook --ip=0.0.0.0 --browser=&quot;&quot;</span><br></pre></td></tr></table></figure></p>
<p>つづいて、<code>notebooks/cuml/linear_regression_demo.ipynb</code>を試した。
なお、<code>%%time</code>マジックを使っている箇所の変数をバインドできなかったので、
適当にセルをコピーして実行した。</p>
<p>確かに学習が早くなったことは実感できた。</p>
<p>scikit-learn <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">skols = skLinearRegression(fit_intercept=True,</span><br><span class="line">                  normalize=True)</span><br><span class="line">skols.fit(X_train, y_train)</span><br><span class="line">CPU times: user 21.5 s, sys: 5.33 s, total: 26.9 s</span><br><span class="line">Wall time: 7.51 s</span><br></pre></td></tr></table></figure></p>
<p>cudf + cuml <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">cuols = cuLinearRegression(fit_intercept=True,</span><br><span class="line">                  normalize=True,</span><br><span class="line">                  algorithm=&apos;eig&apos;)</span><br><span class="line">cuols.fit(X_cudf, y_cudf)</span><br><span class="line">CPU times: user 1.18 s, sys: 350 ms, total: 1.53 s</span><br><span class="line">Wall time: 2.72 s</span><br></pre></td></tr></table></figure></p>
<h2><span id="dockerで動作確認">Dockerで動作確認</span></h2>
<p>つづいてDockerで試す。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker pull rapidsai/rapidsai:cuda9.2-runtime-centos7</span><br><span class="line">$ sudo docker run --runtime=nvidia --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 \</span><br><span class="line">    rapidsai/rapidsai:cuda9.2-runtime-centos7</span><br></pre></td></tr></table></figure>
<p>Dockerコンテナが起動したら、以下の通り、Jupyter Labを起動する。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bash utils/start-jupyter.sh</span><br></pre></td></tr></table></figure></p>
<p>線形回帰のサンプルノートブックを試したが、大丈夫だった。</p>
<h2><span id="参考dockerで動作確認する際の試行錯誤">参考）Dockerで動作確認する際の試行錯誤</span></h2>
<p><a href="https://docs.rapids.ai/start" target="_blank" rel="noopener">RAPIDSの公式Getting
Started</a> を参考に動かしてみる。</p>
<h3><span id="前提">前提</span></h3>
<p>今回は、AWS環境を使った。
予め、CUDAとnvidia-dockerをインストールしておいた。
今回は、CUDA10系を使ったので、以下のようにDockerイメージを取得。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker pull rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04</span><br></pre></td></tr></table></figure>
<p>動作確認。このとき、runtimeにnvidiaを指定する。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --runtime=nvidia --rm nvidia/cuda:10.1-base nvidia-smi</span><br><span class="line">Wed Apr 17 13:28:02 2019</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  Tesla M60           Off  | 00000000:00:1E.0 Off |                    0 |</span><br><span class="line">| N/A   31C    P0    42W / 150W |      0MiB /  7618MiB |     81%      Default |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                       GPU Memory |</span><br><span class="line">|  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>以下のDockerfileを作り、動作確認する。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># FROM defines the base image</span><br><span class="line"># FROM nvidia/cuda:10.0</span><br><span class="line">FROM rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04</span><br><span class="line"></span><br><span class="line"># RUN executes a shell command</span><br><span class="line"># You can chain multiple commands together with &amp;&amp;</span><br><span class="line"># A \ is used to split long lines to help with readability</span><br><span class="line"># This particular instruction installs the source files</span><br><span class="line"># for deviceQuery by installing the CUDA samples via apt</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">        cuda-samples-$CUDA_PKG_VERSION &amp;&amp; \    rm -rf /var/lib/apt/lists/*</span><br><span class="line"># set the working directory</span><br><span class="line">WORKDIR /usr/local/cuda/samples/1_Utilities/deviceQuery</span><br><span class="line"></span><br><span class="line">RUN make</span><br><span class="line"># CMD defines the default command to be run in the container</span><br><span class="line"># CMD is overridden by supplying a command + arguments to</span><br><span class="line"># `docker run`, e.g. `nvcc --version` or `bash`CMD ./deviceQuery</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nvidia-docker build -t device-query .</span><br><span class="line">$ sudo nvidia-docker run --rm -ti device-query</span><br><span class="line">$ sudo nvidia-docker run --rm -ti device-query</span><br><span class="line">./deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;Tesla M60&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version          10.1 / 10.0</span><br><span class="line">  CUDA Capability Major/Minor version number:    5.2</span><br><span class="line">  Total amount of global memory:                 7619 MBytes (7988903936 bytes)</span><br><span class="line">  (16) Multiprocessors, (128) CUDA Cores/MP:     2048 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1178 MHz (1.18 GHz)</span><br><span class="line">  Memory Clock rate:                             2505 Mhz</span><br><span class="line">  Memory Bus Width:                              256-bit</span><br><span class="line">  L2 Cache Size:                                 2097152 bytes</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     No  Integrated GPU sharing Host Memory:            No  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Enabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device supports Compute Preemption:            No  Supports Cooperative Kernel Launch:            No</span><br><span class="line">  Supports MultiDevice Co-op Kernel Launch:      No</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 30</span><br><span class="line">  Compute Mode:     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.0, NumDevs = 1</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure>
<h3><span id="動作確認">動作確認</span></h3>
<p>コンテナを起動。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --runtime=nvidia --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 \</span><br><span class="line">    rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(rapids) root@5d41281699e3:/rapids/notebooks# nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2018 NVIDIA Corporation</span><br><span class="line">Built on Sat_Aug_25_21:08:01_CDT_2018</span><br><span class="line">Cuda compilation tools, release 10.0, V10.0.130</span><br></pre></td></tr></table></figure>
<p>ノートブックも起動してみる。（JupyterLabだった） <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bash utils/start-jupyter.sh</span><br></pre></td></tr></table></figure></p>
<h3><span id="サンプルノートブックの確認">サンプルノートブックの確認</span></h3>
<p>回帰分析の内容を確認してみる。</p>
<p><code>cuml/linear_regression_demo.ipynb</code></p>
<p>冒頭部分でライブラリをロードしている。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import cudf</span><br><span class="line">import os</span><br><span class="line">from cuml import LinearRegression as cuLinearRegression</span><br><span class="line">from sklearn.linear_model import LinearRegression as skLinearRegression</span><br><span class="line">from sklearn.datasets import make_regression</span><br><span class="line"></span><br><span class="line"># Select a particular GPU to run the notebook  </span><br><span class="line">os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;2&quot;</span><br></pre></td></tr></table></figure>
<p><code>cudf</code>、<code>cuml</code>あたりがRAPIDSのライブラリか。</p>
<p>途中、cudfを使うあたりで以下のエラーが発生。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">terminate called after throwing an instance of &apos;cudf::cuda_error&apos;  what():  CUDA error encountered at: /rapids/cudf/cpp/src/bitmask/valid_ops.cu:170: 48 cudaErrorNoKernelImageForDevice no kernel image is available for execution on the device</span><br></pre></td></tr></table></figure></p>
<h2><span id="参考condaで動作確認の試行錯誤メモ">参考）Condaで動作確認の試行錯誤メモ</span></h2>
<p>切り分けも兼ねて、Condaで環境構築し、試してみる。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ conda create -n rapids python=3.6 python</span><br><span class="line">$ conda install -c nvidia/label/cuda10.0 -c rapidsai/label/cuda10.0 -c numba -c conda-forge -c defaults cudf</span><br></pre></td></tr></table></figure>
<p>エラー。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PackageNotFoundError: Dependencies missing in current linux-64 channels:</span><br></pre></td></tr></table></figure></p>
<p>Condaのバージョンが古かったようなので、<code>conda update conda</code>してから再度実行。→成功</p>
<p>cumlを同様にインストールしておく。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ conda install -c nvidia -c rapidsai -c conda-forge -c pytorch -c defaults cuml</span><br></pre></td></tr></table></figure></p>
<p>Jupyterノートブックを起動し、<code>cuml/linear_regression_demo</code>を試す。</p>
<p>cudaをインポートするところで以下のようなエラー。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: cannot load library &apos;/home/centos/.conda/envs/rapids/lib/librmm.so&apos;: libcudart.so.10.0: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<p>そこで、 https://github.com/rapidsai/cudf/issues/496
を参照し、cudatoolkitのインストールを試す。
状況が変化し、今度は、<code>libcublas.so.9.2</code>が必要と言われた。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libcublas.so.9.2: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<p>CUDA9系を指定されているように見える。
しかし実際にインストールしたのはCUDA10系。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/centos/.conda/pkgs/cudatoolkit-10.0.130-0/lib/libcublas.so.10.0</span><br></pre></td></tr></table></figure></p>
<p>ここでCUDA9系で試すことにする。
改めてCUDA10系をアンインストールし、CUDA9系をインストール後に以下を実行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ conda install -c nvidia -c rapidsai -c pytorch -c numba -c conda-forge \</span><br><span class="line">    cudf=0.6 cuml=0.6 python=3.6</span><br></pre></td></tr></table></figure>
<p>その後、少なくともライブラリインポートはうまくいった。</p>
<p>余談だが、手元のJupyterノートブック環境では、<code>%%time</code>マジックを使ったときに、
そのとき定義した変数がバインドされなかった。
（Dockerで試したときはうまく行ったような気がするが…）</p>
<p>cudfをつかうところで以下のエラー発生。改めてcudatoolkitをインストールする。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NvvmSupportError: libNVVM cannot be found. Do `conda install cudatoolkit`:</span><br><span class="line">library nvvm not found</span><br></pre></td></tr></table></figure></p>
<p>その後再度実行。 改めて以下のエラーを発生。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">what():  CUDA error encountered at: /conda/envs/gdf/conda-bld/libcudf_1553535868363/work/cpp/src/bitmask/valid_ops.cu:170: 48 cudaErrorNoKernelImageForDevice no kernel image is available for execution on the device</span><br></pre></td></tr></table></figure>
<h3><span id="インスタンス種類を変えてみる">インスタンス種類を変えてみる</span></h3>
<p>基本的なことに気がついた。
g3.4xlargeではなく、p3.2xlargeに変更してみた。</p>
<p>うまくいった。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/16/RAPIDS/" data-id="clmdqactv017j1vs3599wyhat" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-alexisbcook-hello-seaborn" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/14/alexisbcook-hello-seaborn/">alexisbcook/hello-seaborn</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/14/alexisbcook-hello-seaborn/">
            <time datetime="2019-04-14T13:53:56.000Z" itemprop="datePublished">2019-04-14</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/">Visualization</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/Seaborn/">Seaborn</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Kaggle/">Kaggle</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/Seaborn/">Seaborn</a>, <a class="tag-link" href="/memo-blog/tags/Visualization/">Visualization</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#基本的な使い方" id="toc-基本的な使い方">基本的な使い方</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://www.kaggle.com/alexisbcook/hello-seaborn" target="_blank" rel="noopener">Kaggleのalexisbcook
hello-seaborn</a></li>
<li><a href="https://www.kaggle.com/learn/data-visualization" target="_blank" rel="noopener">Kaggleのlearn
data-visualization</a></li>
<li><a href="https://seaborn.pydata.org/" target="_blank" rel="noopener">seabornの公式</a></li>
<li><a href="https://seaborn.pydata.org/examples/index.html" target="_blank" rel="noopener">seabornのギャラリー</a></li>
<li><a href="https://seaborn.pydata.org/api.html" target="_blank" rel="noopener">seabornのAPI</a></li>
<li><a href="https://www.kaggle.com/kernels/fork/3303713" target="_blank" rel="noopener">Hello,
Seaborn</a></li>
<li><a href="https://www.kaggle.com/learn/data-visualization-from-non-coder-to-coder" target="_blank" rel="noopener">Data
Visualization: from Non-Coder to Coder</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<p><a href="https://seaborn.pydata.org/" target="_blank" rel="noopener">seabornの公式</a> から、 <a href="https://seaborn.pydata.org/examples/index.html" target="_blank" rel="noopener">seabornのギャラリー</a>
の公式ウェブサイトを見ると、 さまざまな例が載っている。 <a href="https://seaborn.pydata.org/api.html" target="_blank" rel="noopener">seabornのAPI</a>
を見ると、各種APIが載っている。</p>
<p>例えば、<a href="https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot" target="_blank" rel="noopener">seaborn.barplot</a>を見ると、
棒グラフの使い方が記載されている。</p>
<h2><span id="基本的な使い方">基本的な使い方</span></h2>
<p>基本的には、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br></pre></td></tr></table></figure>
<p>して、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(data=fifa_data)</span><br></pre></td></tr></table></figure>
<p>のように、プロットの種類ごとに定義されたAPIを呼び出すだけ。
もし、グラフの見た目などを変えたいのであれば、matplotlibをインポートし、
設定すれば良い。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(16,6))</span><br></pre></td></tr></table></figure>
<p>や、</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.xticks(rotation=90)</span><br></pre></td></tr></table></figure>
<p>など。</p>
<p>その他、<a href="https://www.kaggle.com/learn/data-visualization-from-non-coder-to-coder" target="_blank" rel="noopener">Data
Visualization: from Non-Coder to Coder</a>を見ると、
基本的な使い方を理解できるようになっている。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/14/alexisbcook-hello-seaborn/" data-id="clmdqacjx00d31vs3m4iroxws" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-dansbecker-data-leakage" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/14/dansbecker-data-leakage/">dansbecker/data-leakage</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/14/dansbecker-data-leakage/">
            <time datetime="2019-04-14T13:03:51.000Z" itemprop="datePublished">2019-04-14</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/">Model</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Data-Leakage/">Data Leakage</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Data-Leakage/">Data Leakage</a>, <a class="tag-link" href="/memo-blog/tags/Kaggle/">Kaggle</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/Model/">Model</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#leaky-predictor" id="toc-leaky-predictor">Leaky
Predictor</a>
<ul>
<li><a href="#どう対処するのか" id="toc-どう対処するのか">どう対処するのか？</a></li>
</ul></li>
<li><a href="#leaky-validation-strategy" id="toc-leaky-validation-strategy">Leaky Validation Strategy</a>
<ul>
<li><a href="#どう対処するのか-1" id="toc-どう対処するのか-1">どう対処するのか？</a></li>
</ul></li>
<li><a href="#クレジットカードのデータの例" id="toc-クレジットカードのデータの例">クレジットカードのデータの例</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://www.kaggle.com/dansbecker/data-leakage" target="_blank" rel="noopener">Kaggleのdansbecker
data-leakage</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="leaky-predictor">Leaky Predictor</span></h2>
<p>例では、肺炎発症と抗生物質摂取のケースが挙げられていた。</p>
<p>肺炎が発症したあとで、抗生物質を摂取する。
抗生物質を摂取したかどうかは、肺炎発症の前後で値が変わる。
値が変わることを考慮しないと、抗生物質を摂取しない人は肺炎にならない、というモデルが出来上がる可能性がある。</p>
<h3><span id="どう対処するのか">どう対処するのか？</span></h3>
<p>汎用的な対処方法はなく、データや要件に強く依存する。 Leaky
Predictorを発見する <strong>コツ</strong>
は、強い相関のある特徴同士に着目する、
とても高いaccuracyを得られたときに気をつける、など。</p>
<h2><span id="leaky-validation-strategy">Leaky Validation Strategy</span></h2>
<p>例では、train-testスプリットの前に前処理を行おうケースが挙げられていた。</p>
<p>バリデーション対象には、前処理も含めないといけない。
そうでないと、バリデーションで高いモデル性能が得られたとしても、
実際の判定処理で期待したモデル性能が得られない可能性がある。</p>
<h3><span id="どう対処するのか">どう対処するのか？</span></h3>
<p>パイプラインを組むときに、例えばクロスバリデーションの処理内に、
前処理を入れるようにする、など。</p>
<h2><span id="クレジットカードのデータの例">クレジットカードのデータの例</span></h2>
<p>クレジットカードの使用がアプリケーションで認められたかどうか、を判定する例。
ここでは、クレジットカードの使用量の特徴が、Leaky
Predictorとして挙げられていた。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/14/dansbecker-data-leakage/" data-id="clmdqacka00do1vs3jniitfan" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-dansbecker-cross-validation" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/13/dansbecker-cross-validation/">dansbecker/cross-validation</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/13/dansbecker-cross-validation/">
            <time datetime="2019-04-13T14:57:21.000Z" itemprop="datePublished">2019-04-13</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/">Model</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Cross-Validation/">Cross Validation</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Cross-Validation/">Cross Validation</a>, <a class="tag-link" href="/memo-blog/tags/Kaggle/">Kaggle</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/Model/">Model</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#いつ使うか" id="toc-いつ使うか">いつ使うか？</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://www.kaggle.com/dansbecker/cross-validation" target="_blank" rel="noopener">Kaggleのdansbecker
cross-validation</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="いつ使うか">いつ使うか？</span></h2>
<p>端的には、データ量が少ないときの効果が大きい。
逆に、データ量が大きいときは、用いなくてもよいかもしれない。</p>
<p>結果論で言えば、クロスバリデーションして結果が書く回で変わらなかったとき、
それはtrain-testスプリットで十分とも言える。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/13/dansbecker-cross-validation/" data-id="clmdqack700dk1vs3o89db384" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <article id="post-dansbecker-partial-dependence-plots" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
    
        <h1 itemprop="name">
            <a class="article-title" href="/memo-blog/2019/04/13/dansbecker-partial-dependence-plots/">dansbecker/partial-dependence-plots</a>
        </h1>
    

                
                    <div class="article-meta">
                        
    <div class="article-date">
        <i class="fas fa-calendar-alt"></i>
        <a href="/memo-blog/2019/04/13/dansbecker-partial-dependence-plots/">
            <time datetime="2019-04-13T13:33:24.000Z" itemprop="datePublished">2019-04-13</time>
        </a>
    </div>


                        
    <div class="article-category">
    	<i class="fas fa-folder"></i>
        <a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/">Model</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Partial-Dependency-Plot/">Partial Dependency Plot</a>
    </div>

                        
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/memo-blog/tags/Kaggle/">Kaggle</a>, <a class="tag-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a>, <a class="tag-link" href="/memo-blog/tags/Model/">Model</a>, <a class="tag-link" href="/memo-blog/tags/Partial-Dependency-Plot/">Partial Dependency Plot</a>
    </div>

                    </div>
                
            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
            
            <div id="TOC">
	<ul>
<li><a href="#参考" id="toc-参考">参考</a></li>
<li><a href="#メモ" id="toc-メモ">メモ</a>
<ul>
<li><a href="#pdpはモデルが学習されたあとに計算可能" id="toc-pdpはモデルが学習されたあとに計算可能">PDPはモデルが学習されたあとに計算可能</a></li>
<li><a href="#pdpの計算方法" id="toc-pdpの計算方法">PDPの計算方法</a></li>
<li><a href="#タイタニックの例" id="toc-タイタニックの例">タイタニックの例</a></li>
<li><a href="#考察に関する議論" id="toc-考察に関する議論">「考察」に関する議論</a></li>
</ul></li>
</ul>
</div>
<h1><span id="参考">参考</span></h1>
<ul>
<li><a href="https://www.kaggle.com/dansbecker/partial-dependence-plots" target="_blank" rel="noopener">Kaggleのdansbecker
partial-dependence-plots</a></li>
<li><a href="https://linus-mk.hatenablog.com/entry/2018/10/07/222909" target="_blank" rel="noopener">機械学習モデルを解釈する方法
Permutation Importance / Partial Dependence Plot</a></li>
</ul>
<h1><span id="メモ">メモ</span></h1>
<h2><span id="pdpはモデルが学習されたあとに計算可能">PDPはモデルが学習されたあとに計算可能</span></h2>
<p>ただし、様々なモデルに適用可能。</p>
<h2><span id="pdpの計算方法">PDPの計算方法</span></h2>
<p>以下のような感じ。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_plots = plot_partial_dependence(my_model,       </span><br><span class="line">                                   features=[0, 1, 2], # column numbers of plots we want to show</span><br><span class="line">                                   X=X,            # raw predictors data.</span><br><span class="line">                                   feature_names=[&apos;Distance&apos;, &apos;Landsize&apos;, &apos;BuildingArea&apos;], # labels on graphs</span><br><span class="line">                                   grid_resolution=10) # number of values to plot on x axis</span><br></pre></td></tr></table></figure></p>
<p>注意点として挙げられていたのは、<code>grid_resolution</code>を細かくしたときに、
乱れたグラフが見られたとしても、その細かな挙動に対して文脈を考えすぎること。
どうしてもランダム性があるので、細かな挙動にいちいち気にしているとミスリードになる。</p>
<p>なお、<code>partial_dependence</code>という関数を用いると、グラフを出力するのではなく、数値データそのものを得られる。
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_plots2 = partial_dependence(my_model,       </span><br><span class="line">                                   target_variables=[0, 1, 2], # column numbers of plots we want to show</span><br><span class="line">                                   X=X,            # raw predictors data.</span><br><span class="line">                                   grid_resolution=10) # number of values to plot on x axis</span><br></pre></td></tr></table></figure></p>
<p>なお、微妙にオプションが異なることに注意…。</p>
<h2><span id="タイタニックの例">タイタニックの例</span></h2>
<p>PDPを見ることで、年齢や支払い料金と生存結果の関係を解釈する例が記載されていた。</p>
<h2><span id="考察に関する議論">「考察」に関する議論</span></h2>
<p>PDPで得られた結果を考察すること自体について、議論があるようだ。
意味のある・なし、という点において。</p>

        
        </div>
        <footer class="article-footer">
            <div class="share-container">



</div>

    <a data-url="https://dobachi.github.io/memo-blog/2019/04/13/dansbecker-partial-dependence-plots/" data-id="clmdqackd00dw1vs3l4dxib9u" class="article-share-link"><i class="fas fa-share"></i>共有</a>
<script>
    (function ($) {
        // Prevent duplicate binding
        if (typeof(__SHARE_BUTTON_BINDED__) === 'undefined' || !__SHARE_BUTTON_BINDED__) {
            __SHARE_BUTTON_BINDED__ = true;
        } else {
            return;
        }
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="fab fa-twitter article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="fab fa-facebook article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="fab fa-pinterest article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="fab fa-google article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

            
    

        </footer>
    </div>
    
</article>



    <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/memo-blog/page/12/">&laquo; 前</a><a class="page-number" href="/memo-blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/memo-blog/page/11/">11</a><a class="page-number" href="/memo-blog/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/memo-blog/page/14/">14</a><a class="page-number" href="/memo-blog/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/memo-blog/page/21/">21</a><a class="extend next" rel="next" href="/memo-blog/page/14/">次 &raquo;</a>
    </nav>
</section>
            
                
<aside id="sidebar">
   
        
    <div class="widget-wrap">
        <h3 class="widget-title">最近の記事</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/09/09/Generate-OpenAPI-Spec-of-EDC-Connector/" class="title">Generate OpenAPI Spec of EDC Connector</a></p>
                            <p class="item-date"><time datetime="2023-09-09T13:26:58.000Z" itemprop="datePublished">2023-09-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Open-API/">Open API</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/09/07/OpenAPI-Generator-for-Flask/" class="title">OpenAPI Generator for Flask</a></p>
                            <p class="item-date"><time datetime="2023-09-07T13:10:58.000Z" itemprop="datePublished">2023-09-07</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/08/31/IDS-Dataspace-Protocol/" class="title">IDS Dataspace Protocol</a></p>
                            <p class="item-date"><time datetime="2023-08-31T06:56:27.000Z" itemprop="datePublished">2023-08-31</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/SDK/">SDK</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/08/27/How-to-create-SDK/" class="title">How to create SDK (WIP)</a></p>
                            <p class="item-date"><time datetime="2023-08-27T05:11:45.000Z" itemprop="datePublished">2023-08-27</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><i class="fas fa-angle-right"></i><a class="article-category-link" href="/memo-blog/categories/Knowledge-Management/Windows/">Windows</a></p>
                            <p class="item-title"><a href="/memo-blog/2023/08/15/check-windows-cpu-resources/" class="title">check windows cpu resources</a></p>
                            <p class="item-date"><time datetime="2023-08-14T15:32:07.000Z" itemprop="datePublished">2023-08-15</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">カテゴリ</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/">Clipping</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/AI/">AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Camera/">Camera</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Camera/Lighting/">Lighting</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Cloud/">Cloud</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Database/">Database</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Kafka/">Kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/List/">List</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/List/Research/">Research</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Management/">Management</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/PostgreSQL/">PostgreSQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Stream-Processing/">Stream Processing</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Uber/">Uber</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Vim/">Vim</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Clipping/Windows-Tools/">Windows Tools</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/">Home server</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/File-server/">File server</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Hardware/">Hardware</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Nature-Remo/">Nature Remo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Network/">Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Remote-desktop/">Remote desktop</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/">Ubuntu</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/Adobe-Reader/">Adobe Reader</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/Gnome/">Gnome</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/KVM/">KVM</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/OneDrive/">OneDrive</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Ubuntu/vim/">vim</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Home-server/Video-processing/">Video processing</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/">Knowledge Management</a><span class="category-list-count">158</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Alluxio/">Alluxio</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/BaaS/">BaaS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Configuration-Management/">Configuration Management</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Configuration-Management/Ansible/">Ansible</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Catalog/">Data Catalog</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Catalog/CKAN/">CKAN</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/">Data Collaboration</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/">Delta Sharing</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Collaboration/X-Road/">X-Road</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/">Data Engineering</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Lineage/">Data Lineage</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Transformation/">Data Transformation</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Mesh/">Data Mesh</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/">Data Processing Engine</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/">Data Spaces</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/EDC/">EDC</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Data-Spaces/IDS/">IDS</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Documentation/">Documentation</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Flask/">Flask</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/">GPD Pocket</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/">Device</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/Bluetooth/">Bluetooth</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/HBase/">HBase</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/">Hadoop</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/Ambari/">Ambari</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/BigTop/">BigTop</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hadoop/HDP/">HDP</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hexo/">Hexo</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Home-Network/">Home Network</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper/">Hyper</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper/Plugin/">Plugin</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Hyper-V/">Hyper-V</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/">Keyboard</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/Corne-Chocolate/">Corne Chocolate</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Keyboard/QMK/">QMK</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Kubernetes/">Kubernetes</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/">Machine Learning</a><span class="category-list-count">25</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Analytics-Zoo/">Analytics Zoo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/AutoML/">AutoML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Data-Platform/">Data Platform</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Flow-Engine/">Flow Engine</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/MLflow/">MLflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/">Model</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Cross-Validation/">Cross Validation</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Data-Leakage/">Data Leakage</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Partial-Dependency-Plot/">Partial Dependency Plot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/XGBoost/">XGBoost</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/">Model Management</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/Clipper/">Clipper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/OpML/">OpML</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Preparation/">Preparation</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Software-Engineering-Patterns/">Software Engineering Patterns</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Stream-Processing/">Stream Processing</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/">Visualization</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/Seaborn/">Seaborn</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Machine-Learning/Word2Vec/">Word2Vec</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/">Messaging System</a><span class="category-list-count">15</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/">Kafka</a><span class="category-list-count">14</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Metadata-Management/">Metadata Management</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Monitering/">Monitering</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-API/">Open API</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/">Open Data</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/Scraping/">Scraping</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Open-Data/Tellus/">Tellus</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Pinot/">Pinot</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Power-Grid-Data/">Power Grid Data</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/">Python</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/Jupyter/">Jupyter</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/Pipenv/">Pipenv</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Python/pyenv/">pyenv</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/SDK/">SDK</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/SQLAlchemy/">SQLAlchemy</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Scala/">Scala</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Scala/SBT/">SBT</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Spark/">Spark</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Spark/Spark-Summit/">Spark Summit</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/">Storage Layer</a><span class="category-list-count">13</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/">Delta Lake</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Hudi/">Hudi</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Storage-Layer/Minio/">Minio</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/">Stream Processing</a><span class="category-list-count">9</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Apache-Edgent/">Apache Edgent</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Kappa-Architecture/">Kappa Architecture</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/MillWheel/">MillWheel</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Stream-Processing/Twitter-Heron/">Twitter Heron</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/">Tools</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Git/">Git</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Intellij/">Intellij</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/Selenium/">Selenium</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Tools/tmux/">tmux</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/">WSL</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/CentOS/">CentOS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Docker/">Docker</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Terminal-tool/">Terminal tool</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/Vagrant/">Vagrant</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/WSL/X-Window/">X Window</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/">Windows</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Ansible/">Ansible</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Docker/">Docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Windows/Hyper-V/">Hyper-V</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/Zeppelin/">Zeppelin</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/ZooKeeper/">ZooKeeper</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Knowledge-Management/vim/">vim</a><span class="category-list-count">5</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/">Research</a><span class="category-list-count">14</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/AWS/">AWS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Conference/">Conference</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Conference/DevSumi/">DevSumi</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Data-Analytics/">Data Analytics</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Data-Analytics/Tools/">Tools</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/">Machine Learning</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/BigDL/">BigDL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Machine-Learning/TensorFlow/">TensorFlow</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/NVM/">NVM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/SX-Aurora-Frovedis/">SX-Aurora/Frovedis</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Trends/">Trends</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Video-processing/">Video processing</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Video-processing/BlazeIt/">BlazeIt</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Visualization/">Visualization</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/memo-blog/categories/Research/Visualization/Superset/">Superset</a><span class="category-list-count">1</span></li></ul></li></ul></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">アーカイブ</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2023/09/">9月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2023/08/">8月 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/05/">5月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/02/">2月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2022/01/">1月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/10/">10月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/09/">9月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/08/">8月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/07/">7月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/06/">6月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/05/">5月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/04/">4月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/02/">2月 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2021/01/">1月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/12/">12月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/11/">11月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/10/">10月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/09/">9月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/08/">8月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/07/">7月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/06/">6月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/05/">5月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/04/">4月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/03/">3月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/02/">2月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2020/01/">1月 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/12/">12月 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/11/">11月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/10/">10月 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/09/">9月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/08/">8月 2019</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/07/">7月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/06/">6月 2019</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/05/">5月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/04/">4月 2019</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/03/">3月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/02/">2月 2019</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2019/01/">1月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/12/">12月 2018</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/11/">11月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/10/">10月 2018</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/memo-blog/archives/2018/09/">9月 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">タグ</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AI/">AI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AWS/">AWS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Academia/">Academia</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Adobe-Reader/">Adobe Reader</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Alluxio/">Alluxio</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ambari/">Ambari</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Analytics-Zoo/">Analytics Zoo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ansible/">Ansible</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Edgent/">Apache Edgent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Hudi/">Apache Hudi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Kafka/">Apache Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apache-Spark/">Apache Spark</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Apple/">Apple</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/AutoML/">AutoML</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Automagica/">Automagica</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Autonomous-Database/">Autonomous Database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BaaS/">BaaS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Behavioral-Economics/">Behavioral Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Big-Data/">Big Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BigDL/">BigDL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BigTop/">BigTop</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/BlazeIt/">BlazeIt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Blog/">Blog</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Bluetooth/">Bluetooth</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CDC/">CDC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Camera/">Camera</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CentOS/">CentOS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CentOS7/">CentOS7</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/CircleCI/">CircleCI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Clipper/">Clipper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Clipping/">Clipping</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Cloud/">Cloud</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Comcast/">Comcast</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Computing-resource/">Computing resource</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Conference/">Conference</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Configuration-Management/">Configuration Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Corne-Chocolate/">Corne Chocolate</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Cross-Validation/">Cross Validation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/DB/">DB</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dask/">Dask</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Analysis/">Data Analysis</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Analytics/">Data Analytics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Lake/">Data Lake</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Leakage/">Data Leakage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Lineage/">Data Lineage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Mesh/">Data Mesh</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Platform/">Data Platform</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Processing-Engine/">Data Processing Engine</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Spaces/">Data Spaces</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Data-Transformation/">Data Transformation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Delta-Lake/">Delta Lake</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Delta-Sharing/">Delta Sharing</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/DevSumi/">DevSumi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Docker/">Docker</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Dockerfile/">Dockerfile</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Druid/">Druid</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/EDC/">EDC</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Flask/">Flask</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Flow-Engine/">Flow Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Frovedis/">Frovedis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GIS/">GIS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GNOME/">GNOME</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GPD-Pocket/">GPD Pocket</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Git/">Git</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/GitHub-Actions/">GitHub Actions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Google/">Google</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Goverment/">Goverment</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Graceful-Shutdown/">Graceful Shutdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Gradle/">Gradle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HBase/">HBase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HDP/">HDP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/HPC/">HPC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hadoop/">Hadoop</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hexo/">Hexo</a><span class="tag-list-count">10</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hexo-Plugin/">Hexo Plugin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hyper/">Hyper</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Hyper-V/">Hyper-V</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IDS/">IDS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IEEE/">IEEE</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/IPv6/">IPv6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Icarus/">Icarus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Incident/">Incident</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Intel/">Intel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Intellij/">Intellij</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/JSON/">JSON</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Java/">Java</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Jupyter/">Jupyter</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/KVM/">KVM</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafak-Connect/">Kafak Connect</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka/">Kafka</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Connect/">Kafka Connect</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Streams/">Kafka Streams</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kafka-Summit/">Kafka Summit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kaggle/">Kaggle</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kappa-Architecture/">Kappa Architecture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Keyboard/">Keyboard</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kinesis/">Kinesis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Kubernetes/">Kubernetes</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Lambda-Architecture/">Lambda Architecture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Lighthing/">Lighthing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/LinkedIn/">LinkedIn</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MATE-Desktop/">MATE Desktop</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ML-Model-Management/">ML Model Management</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ML-Ops/">ML Ops</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MLflow/">MLflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">29</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning-Lifecycle/">Machine Learning Lifecycle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Machine-Learning-Lifecycle/">Machine Learning Lifecycle/</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Management/">Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Map/">Map</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Markdown/">Markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Messaging-System/">Messaging System</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Metadata-Management/">Metadata Management</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/MillWheel/">MillWheel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Minikube/">Minikube</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Minio/">Minio</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Model/">Model</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Model-Management/">Model Management</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Monitering/">Monitering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Mouse/">Mouse</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/NERDTree/">NERDTree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/NVM/">NVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Nature-Remo/">Nature Remo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Network/">Network</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OLAP/">OLAP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OneDrive/">OneDrive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpML/">OpML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-API/">Open API</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-Data/">Open Data</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Open-Messaging-Benchmark/">Open Messaging Benchmark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpenAPI/">OpenAPI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/OpenML/">OpenML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Oracle/">Oracle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PAPIDS/">PAPIDS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PDF/">PDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pandoc/">Pandoc</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Paper/">Paper</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Parquet/">Parquet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Partial-Dependency-Plot/">Partial Dependency Plot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pinot/">Pinot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pipenv/">Pipenv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PostgreSQL/">PostgreSQL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Power-Grid-Data/">Power Grid Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PowerShell/">PowerShell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Preparation/">Preparation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Profiler/">Profiler</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Pulsar/">Pulsar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/PySpark/">PySpark</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Python/">Python</a><span class="tag-list-count">12</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Python3/">Python3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/QMK/">QMK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Query-Engine/">Query Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/RDBMS/">RDBMS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/RPA/">RPA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Redshift/">Redshift</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Research-later/">Research later</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/S3/">S3</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SBT/">SBT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SDK/">SDK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SQLAlchemy/">SQLAlchemy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SQLite/">SQLite</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/SX-Aurora/">SX-Aurora</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Samba/">Samba</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Scala/">Scala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Scraping/">Scraping</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Security/">Security</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Session/">Session</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Slenium/">Slenium</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Smart-Home/">Smart Home</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Snowflake/">Snowflake</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Software-Engineering-Patterns/">Software Engineering Patterns</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Spark/">Spark</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Spark-Summit/">Spark Summit</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Sphinx/">Sphinx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Statistic/">Statistic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Stonebraker/">Stonebraker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage/">Storage</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Engine/">Storage Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Layer/">Storage Layer</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Storage-Layer-Software/">Storage Layer Software</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Stream-Processing/">Stream Processing</a><span class="tag-list-count">13</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Superset/">Superset</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Supervision/">Supervision</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Tellus/">Tellus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/TensorFlow/">TensorFlow</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/TensorFlowOnSpark/">TensorFlowOnSpark</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Tools/">Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Trends/">Trends</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Troubleshoot/">Troubleshoot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Twitter/">Twitter</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Twitter-Heron/">Twitter Heron</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Uber/">Uber</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/VMWare/">VMWare</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vagrant/">Vagrant</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vault/">Vault</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vector-Engine/">Vector Engine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Video-Processing/">Video Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Vim/">Vim</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Visualization/">Visualization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WSL/">WSL</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Web/">Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WhereHows/">WhereHows</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WiFi/">WiFi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/WiFi6/">WiFi6</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Windows/">Windows</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Windows-Tools/">Windows Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Word/">Word</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Word2Vec/">Word2Vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/X-Window/">X Window</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/X-Road/">X-Road</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/XGBoost/">XGBoost</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/Zeppelin/">Zeppelin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/ZooKeeper/">ZooKeeper</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/dein/">dein</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/dstat/">dstat</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/fsync/">fsync</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/git/">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/keyboard/">keyboard</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/libvirt/">libvirt</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/markdown/">markdown</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/nltk/">nltk</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/pandoc/">pandoc</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/pyenv/">pyenv</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/tmux/">tmux</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/vim/">vim</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/memo-blog/tags/windows/">windows</a><span class="tag-list-count">1</span></li></ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title">タグクラウド</h3>
        <div class="widget tagcloud">
            <a href="/memo-blog/tags/AI/" style="font-size: 10px;">AI</a> <a href="/memo-blog/tags/AWS/" style="font-size: 10px;">AWS</a> <a href="/memo-blog/tags/Academia/" style="font-size: 10px;">Academia</a> <a href="/memo-blog/tags/Adobe-Reader/" style="font-size: 10px;">Adobe Reader</a> <a href="/memo-blog/tags/Alluxio/" style="font-size: 10.77px;">Alluxio</a> <a href="/memo-blog/tags/Ambari/" style="font-size: 10px;">Ambari</a> <a href="/memo-blog/tags/Analytics-Zoo/" style="font-size: 10px;">Analytics Zoo</a> <a href="/memo-blog/tags/Ansible/" style="font-size: 11.54px;">Ansible</a> <a href="/memo-blog/tags/Apache-Edgent/" style="font-size: 10px;">Apache Edgent</a> <a href="/memo-blog/tags/Apache-Hudi/" style="font-size: 10px;">Apache Hudi</a> <a href="/memo-blog/tags/Apache-Kafka/" style="font-size: 10px;">Apache Kafka</a> <a href="/memo-blog/tags/Apache-Spark/" style="font-size: 11.54px;">Apache Spark</a> <a href="/memo-blog/tags/Apple/" style="font-size: 10px;">Apple</a> <a href="/memo-blog/tags/AutoML/" style="font-size: 10.77px;">AutoML</a> <a href="/memo-blog/tags/Automagica/" style="font-size: 10px;">Automagica</a> <a href="/memo-blog/tags/Autonomous-Database/" style="font-size: 10px;">Autonomous Database</a> <a href="/memo-blog/tags/BaaS/" style="font-size: 10px;">BaaS</a> <a href="/memo-blog/tags/Behavioral-Economics/" style="font-size: 10px;">Behavioral Economics</a> <a href="/memo-blog/tags/Big-Data/" style="font-size: 10px;">Big Data</a> <a href="/memo-blog/tags/BigDL/" style="font-size: 10.77px;">BigDL</a> <a href="/memo-blog/tags/BigTop/" style="font-size: 10.77px;">BigTop</a> <a href="/memo-blog/tags/BlazeIt/" style="font-size: 10px;">BlazeIt</a> <a href="/memo-blog/tags/Blog/" style="font-size: 10px;">Blog</a> <a href="/memo-blog/tags/Bluetooth/" style="font-size: 10px;">Bluetooth</a> <a href="/memo-blog/tags/CDC/" style="font-size: 10px;">CDC</a> <a href="/memo-blog/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/memo-blog/tags/CentOS/" style="font-size: 10px;">CentOS</a> <a href="/memo-blog/tags/CentOS7/" style="font-size: 11.54px;">CentOS7</a> <a href="/memo-blog/tags/CircleCI/" style="font-size: 10px;">CircleCI</a> <a href="/memo-blog/tags/Clipper/" style="font-size: 10px;">Clipper</a> <a href="/memo-blog/tags/Clipping/" style="font-size: 10px;">Clipping</a> <a href="/memo-blog/tags/Cloud/" style="font-size: 10px;">Cloud</a> <a href="/memo-blog/tags/Comcast/" style="font-size: 10px;">Comcast</a> <a href="/memo-blog/tags/Computing-resource/" style="font-size: 10px;">Computing resource</a> <a href="/memo-blog/tags/Conference/" style="font-size: 10px;">Conference</a> <a href="/memo-blog/tags/Configuration-Management/" style="font-size: 10px;">Configuration Management</a> <a href="/memo-blog/tags/Corne-Chocolate/" style="font-size: 10px;">Corne Chocolate</a> <a href="/memo-blog/tags/Cross-Validation/" style="font-size: 10px;">Cross Validation</a> <a href="/memo-blog/tags/DB/" style="font-size: 10px;">DB</a> <a href="/memo-blog/tags/Dask/" style="font-size: 10.77px;">Dask</a> <a href="/memo-blog/tags/Data-Analysis/" style="font-size: 10.77px;">Data Analysis</a> <a href="/memo-blog/tags/Data-Analytics/" style="font-size: 10px;">Data Analytics</a> <a href="/memo-blog/tags/Data-Lake/" style="font-size: 10px;">Data Lake</a> <a href="/memo-blog/tags/Data-Leakage/" style="font-size: 10px;">Data Leakage</a> <a href="/memo-blog/tags/Data-Lineage/" style="font-size: 10px;">Data Lineage</a> <a href="/memo-blog/tags/Data-Mesh/" style="font-size: 10px;">Data Mesh</a> <a href="/memo-blog/tags/Data-Platform/" style="font-size: 10px;">Data Platform</a> <a href="/memo-blog/tags/Data-Processing-Engine/" style="font-size: 10.77px;">Data Processing Engine</a> <a href="/memo-blog/tags/Data-Spaces/" style="font-size: 11.54px;">Data Spaces</a> <a href="/memo-blog/tags/Data-Transformation/" style="font-size: 10.77px;">Data Transformation</a> <a href="/memo-blog/tags/Delta-Lake/" style="font-size: 16.92px;">Delta Lake</a> <a href="/memo-blog/tags/Delta-Sharing/" style="font-size: 12.31px;">Delta Sharing</a> <a href="/memo-blog/tags/DevSumi/" style="font-size: 10px;">DevSumi</a> <a href="/memo-blog/tags/Docker/" style="font-size: 14.62px;">Docker</a> <a href="/memo-blog/tags/Dockerfile/" style="font-size: 10.77px;">Dockerfile</a> <a href="/memo-blog/tags/Druid/" style="font-size: 10px;">Druid</a> <a href="/memo-blog/tags/EDC/" style="font-size: 10.77px;">EDC</a> <a href="/memo-blog/tags/Flask/" style="font-size: 12.31px;">Flask</a> <a href="/memo-blog/tags/Flow-Engine/" style="font-size: 10px;">Flow Engine</a> <a href="/memo-blog/tags/Frovedis/" style="font-size: 10px;">Frovedis</a> <a href="/memo-blog/tags/GIS/" style="font-size: 10px;">GIS</a> <a href="/memo-blog/tags/GNOME/" style="font-size: 10px;">GNOME</a> <a href="/memo-blog/tags/GPD-Pocket/" style="font-size: 10px;">GPD Pocket</a> <a href="/memo-blog/tags/Git/" style="font-size: 10.77px;">Git</a> <a href="/memo-blog/tags/GitHub-Actions/" style="font-size: 10px;">GitHub Actions</a> <a href="/memo-blog/tags/Google/" style="font-size: 10px;">Google</a> <a href="/memo-blog/tags/Goverment/" style="font-size: 10px;">Goverment</a> <a href="/memo-blog/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/memo-blog/tags/Gradle/" style="font-size: 10px;">Gradle</a> <a href="/memo-blog/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/memo-blog/tags/HDP/" style="font-size: 10px;">HDP</a> <a href="/memo-blog/tags/HPC/" style="font-size: 10px;">HPC</a> <a href="/memo-blog/tags/Hadoop/" style="font-size: 11.54px;">Hadoop</a> <a href="/memo-blog/tags/Hexo/" style="font-size: 16.15px;">Hexo</a> <a href="/memo-blog/tags/Hexo-Plugin/" style="font-size: 10px;">Hexo Plugin</a> <a href="/memo-blog/tags/Hyper/" style="font-size: 10.77px;">Hyper</a> <a href="/memo-blog/tags/Hyper-V/" style="font-size: 10.77px;">Hyper-V</a> <a href="/memo-blog/tags/IDS/" style="font-size: 10px;">IDS</a> <a href="/memo-blog/tags/IEEE/" style="font-size: 10.77px;">IEEE</a> <a href="/memo-blog/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/memo-blog/tags/Icarus/" style="font-size: 10px;">Icarus</a> <a href="/memo-blog/tags/Incident/" style="font-size: 10px;">Incident</a> <a href="/memo-blog/tags/Intel/" style="font-size: 10px;">Intel</a> <a href="/memo-blog/tags/Intellij/" style="font-size: 10.77px;">Intellij</a> <a href="/memo-blog/tags/JSON/" style="font-size: 10px;">JSON</a> <a href="/memo-blog/tags/Java/" style="font-size: 10px;">Java</a> <a href="/memo-blog/tags/Jupyter/" style="font-size: 10.77px;">Jupyter</a> <a href="/memo-blog/tags/KVM/" style="font-size: 11.54px;">KVM</a> <a href="/memo-blog/tags/Kafak-Connect/" style="font-size: 10px;">Kafak Connect</a> <a href="/memo-blog/tags/Kafka/" style="font-size: 19.23px;">Kafka</a> <a href="/memo-blog/tags/Kafka-Connect/" style="font-size: 10px;">Kafka Connect</a> <a href="/memo-blog/tags/Kafka-Streams/" style="font-size: 10.77px;">Kafka Streams</a> <a href="/memo-blog/tags/Kafka-Summit/" style="font-size: 10px;">Kafka Summit</a> <a href="/memo-blog/tags/Kaggle/" style="font-size: 14.62px;">Kaggle</a> <a href="/memo-blog/tags/Kappa-Architecture/" style="font-size: 10px;">Kappa Architecture</a> <a href="/memo-blog/tags/Keyboard/" style="font-size: 10.77px;">Keyboard</a> <a href="/memo-blog/tags/Kinesis/" style="font-size: 10px;">Kinesis</a> <a href="/memo-blog/tags/Kubernetes/" style="font-size: 11.54px;">Kubernetes</a> <a href="/memo-blog/tags/Lambda-Architecture/" style="font-size: 10px;">Lambda Architecture</a> <a href="/memo-blog/tags/Lighthing/" style="font-size: 10px;">Lighthing</a> <a href="/memo-blog/tags/LinkedIn/" style="font-size: 12.31px;">LinkedIn</a> <a href="/memo-blog/tags/MATE-Desktop/" style="font-size: 10px;">MATE Desktop</a> <a href="/memo-blog/tags/ML-Model-Management/" style="font-size: 10.77px;">ML Model Management</a> <a href="/memo-blog/tags/ML-Ops/" style="font-size: 10px;">ML Ops</a> <a href="/memo-blog/tags/MLflow/" style="font-size: 10px;">MLflow</a> <a href="/memo-blog/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/memo-blog/tags/Machine-Learning-Lifecycle/" style="font-size: 10px;">Machine Learning Lifecycle</a> <a href="/memo-blog/tags/Machine-Learning-Lifecycle/" style="font-size: 10px;">Machine Learning Lifecycle/</a> <a href="/memo-blog/tags/Management/" style="font-size: 10px;">Management</a> <a href="/memo-blog/tags/Map/" style="font-size: 10px;">Map</a> <a href="/memo-blog/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/memo-blog/tags/Messaging-System/" style="font-size: 10px;">Messaging System</a> <a href="/memo-blog/tags/Metadata-Management/" style="font-size: 10px;">Metadata Management</a> <a href="/memo-blog/tags/MillWheel/" style="font-size: 10px;">MillWheel</a> <a href="/memo-blog/tags/Minikube/" style="font-size: 10px;">Minikube</a> <a href="/memo-blog/tags/Minio/" style="font-size: 10.77px;">Minio</a> <a href="/memo-blog/tags/Model/" style="font-size: 12.31px;">Model</a> <a href="/memo-blog/tags/Model-Management/" style="font-size: 12.31px;">Model Management</a> <a href="/memo-blog/tags/Monitering/" style="font-size: 10px;">Monitering</a> <a href="/memo-blog/tags/Mouse/" style="font-size: 10px;">Mouse</a> <a href="/memo-blog/tags/NERDTree/" style="font-size: 10px;">NERDTree</a> <a href="/memo-blog/tags/NVM/" style="font-size: 10px;">NVM</a> <a href="/memo-blog/tags/Nature-Remo/" style="font-size: 10px;">Nature Remo</a> <a href="/memo-blog/tags/Network/" style="font-size: 10px;">Network</a> <a href="/memo-blog/tags/OLAP/" style="font-size: 10px;">OLAP</a> <a href="/memo-blog/tags/OneDrive/" style="font-size: 10px;">OneDrive</a> <a href="/memo-blog/tags/OpML/" style="font-size: 10px;">OpML</a> <a href="/memo-blog/tags/Open-API/" style="font-size: 10px;">Open API</a> <a href="/memo-blog/tags/Open-Data/" style="font-size: 10.77px;">Open Data</a> <a href="/memo-blog/tags/Open-Messaging-Benchmark/" style="font-size: 10px;">Open Messaging Benchmark</a> <a href="/memo-blog/tags/OpenAPI/" style="font-size: 10px;">OpenAPI</a> <a href="/memo-blog/tags/OpenML/" style="font-size: 10px;">OpenML</a> <a href="/memo-blog/tags/Oracle/" style="font-size: 10px;">Oracle</a> <a href="/memo-blog/tags/PAPIDS/" style="font-size: 10px;">PAPIDS</a> <a href="/memo-blog/tags/PDF/" style="font-size: 10px;">PDF</a> <a href="/memo-blog/tags/Pandoc/" style="font-size: 10px;">Pandoc</a> <a href="/memo-blog/tags/Paper/" style="font-size: 18.46px;">Paper</a> <a href="/memo-blog/tags/Parquet/" style="font-size: 10px;">Parquet</a> <a href="/memo-blog/tags/Partial-Dependency-Plot/" style="font-size: 10px;">Partial Dependency Plot</a> <a href="/memo-blog/tags/Pinot/" style="font-size: 10px;">Pinot</a> <a href="/memo-blog/tags/Pipenv/" style="font-size: 10px;">Pipenv</a> <a href="/memo-blog/tags/PostgreSQL/" style="font-size: 10px;">PostgreSQL</a> <a href="/memo-blog/tags/Power-Grid-Data/" style="font-size: 10px;">Power Grid Data</a> <a href="/memo-blog/tags/PowerShell/" style="font-size: 10px;">PowerShell</a> <a href="/memo-blog/tags/Preparation/" style="font-size: 10.77px;">Preparation</a> <a href="/memo-blog/tags/Profiler/" style="font-size: 10px;">Profiler</a> <a href="/memo-blog/tags/Pulsar/" style="font-size: 10px;">Pulsar</a> <a href="/memo-blog/tags/PySpark/" style="font-size: 12.31px;">PySpark</a> <a href="/memo-blog/tags/Python/" style="font-size: 17.69px;">Python</a> <a href="/memo-blog/tags/Python3/" style="font-size: 10px;">Python3</a> <a href="/memo-blog/tags/QMK/" style="font-size: 10px;">QMK</a> <a href="/memo-blog/tags/Query-Engine/" style="font-size: 10px;">Query Engine</a> <a href="/memo-blog/tags/RDBMS/" style="font-size: 10.77px;">RDBMS</a> <a href="/memo-blog/tags/RPA/" style="font-size: 10px;">RPA</a> <a href="/memo-blog/tags/Redshift/" style="font-size: 10px;">Redshift</a> <a href="/memo-blog/tags/Research-later/" style="font-size: 10px;">Research later</a> <a href="/memo-blog/tags/S3/" style="font-size: 10px;">S3</a> <a href="/memo-blog/tags/SBT/" style="font-size: 10px;">SBT</a> <a href="/memo-blog/tags/SDK/" style="font-size: 10px;">SDK</a> <a href="/memo-blog/tags/SQLAlchemy/" style="font-size: 10.77px;">SQLAlchemy</a> <a href="/memo-blog/tags/SQLite/" style="font-size: 10px;">SQLite</a> <a href="/memo-blog/tags/SX-Aurora/" style="font-size: 10px;">SX-Aurora</a> <a href="/memo-blog/tags/Samba/" style="font-size: 10px;">Samba</a> <a href="/memo-blog/tags/Scala/" style="font-size: 10px;">Scala</a> <a href="/memo-blog/tags/Scraping/" style="font-size: 10px;">Scraping</a> <a href="/memo-blog/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/memo-blog/tags/Security/" style="font-size: 10px;">Security</a> <a href="/memo-blog/tags/Session/" style="font-size: 10px;">Session</a> <a href="/memo-blog/tags/Slenium/" style="font-size: 10px;">Slenium</a> <a href="/memo-blog/tags/Smart-Home/" style="font-size: 10px;">Smart Home</a> <a href="/memo-blog/tags/Snowflake/" style="font-size: 10px;">Snowflake</a> <a href="/memo-blog/tags/Software-Engineering-Patterns/" style="font-size: 10px;">Software Engineering Patterns</a> <a href="/memo-blog/tags/Spark/" style="font-size: 15.38px;">Spark</a> <a href="/memo-blog/tags/Spark-Summit/" style="font-size: 10px;">Spark Summit</a> <a href="/memo-blog/tags/Sphinx/" style="font-size: 10px;">Sphinx</a> <a href="/memo-blog/tags/Statistic/" style="font-size: 10px;">Statistic</a> <a href="/memo-blog/tags/Stonebraker/" style="font-size: 10px;">Stonebraker</a> <a href="/memo-blog/tags/Storage/" style="font-size: 10px;">Storage</a> <a href="/memo-blog/tags/Storage-Engine/" style="font-size: 10px;">Storage Engine</a> <a href="/memo-blog/tags/Storage-Layer/" style="font-size: 11.54px;">Storage Layer</a> <a href="/memo-blog/tags/Storage-Layer-Software/" style="font-size: 10px;">Storage Layer Software</a> <a href="/memo-blog/tags/Stream-Processing/" style="font-size: 18.46px;">Stream Processing</a> <a href="/memo-blog/tags/Superset/" style="font-size: 10px;">Superset</a> <a href="/memo-blog/tags/Supervision/" style="font-size: 10px;">Supervision</a> <a href="/memo-blog/tags/Tellus/" style="font-size: 10px;">Tellus</a> <a href="/memo-blog/tags/TensorFlow/" style="font-size: 11.54px;">TensorFlow</a> <a href="/memo-blog/tags/TensorFlowOnSpark/" style="font-size: 10.77px;">TensorFlowOnSpark</a> <a href="/memo-blog/tags/Tools/" style="font-size: 10px;">Tools</a> <a href="/memo-blog/tags/Trends/" style="font-size: 10px;">Trends</a> <a href="/memo-blog/tags/Troubleshoot/" style="font-size: 10px;">Troubleshoot</a> <a href="/memo-blog/tags/Twitter/" style="font-size: 10px;">Twitter</a> <a href="/memo-blog/tags/Twitter-Heron/" style="font-size: 10px;">Twitter Heron</a> <a href="/memo-blog/tags/Uber/" style="font-size: 10.77px;">Uber</a> <a href="/memo-blog/tags/Ubuntu/" style="font-size: 14.62px;">Ubuntu</a> <a href="/memo-blog/tags/VMWare/" style="font-size: 10px;">VMWare</a> <a href="/memo-blog/tags/Vagrant/" style="font-size: 10.77px;">Vagrant</a> <a href="/memo-blog/tags/Vault/" style="font-size: 10px;">Vault</a> <a href="/memo-blog/tags/Vector-Engine/" style="font-size: 10px;">Vector Engine</a> <a href="/memo-blog/tags/Video-Processing/" style="font-size: 10px;">Video Processing</a> <a href="/memo-blog/tags/Vim/" style="font-size: 10.77px;">Vim</a> <a href="/memo-blog/tags/Visualization/" style="font-size: 10.77px;">Visualization</a> <a href="/memo-blog/tags/WSL/" style="font-size: 13.85px;">WSL</a> <a href="/memo-blog/tags/Web/" style="font-size: 10px;">Web</a> <a href="/memo-blog/tags/WhereHows/" style="font-size: 10px;">WhereHows</a> <a href="/memo-blog/tags/WiFi/" style="font-size: 10px;">WiFi</a> <a href="/memo-blog/tags/WiFi6/" style="font-size: 10px;">WiFi6</a> <a href="/memo-blog/tags/Windows/" style="font-size: 13.08px;">Windows</a> <a href="/memo-blog/tags/Windows-Tools/" style="font-size: 10px;">Windows Tools</a> <a href="/memo-blog/tags/Word/" style="font-size: 10px;">Word</a> <a href="/memo-blog/tags/Word2Vec/" style="font-size: 10px;">Word2Vec</a> <a href="/memo-blog/tags/X-Window/" style="font-size: 10px;">X Window</a> <a href="/memo-blog/tags/X-Road/" style="font-size: 10px;">X-Road</a> <a href="/memo-blog/tags/XGBoost/" style="font-size: 10px;">XGBoost</a> <a href="/memo-blog/tags/Zeppelin/" style="font-size: 10px;">Zeppelin</a> <a href="/memo-blog/tags/ZooKeeper/" style="font-size: 12.31px;">ZooKeeper</a> <a href="/memo-blog/tags/bug/" style="font-size: 10px;">bug</a> <a href="/memo-blog/tags/dein/" style="font-size: 10px;">dein</a> <a href="/memo-blog/tags/dstat/" style="font-size: 10px;">dstat</a> <a href="/memo-blog/tags/fsync/" style="font-size: 10px;">fsync</a> <a href="/memo-blog/tags/git/" style="font-size: 10px;">git</a> <a href="/memo-blog/tags/keyboard/" style="font-size: 10px;">keyboard</a> <a href="/memo-blog/tags/libvirt/" style="font-size: 10px;">libvirt</a> <a href="/memo-blog/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/memo-blog/tags/nltk/" style="font-size: 10px;">nltk</a> <a href="/memo-blog/tags/pandoc/" style="font-size: 11.54px;">pandoc</a> <a href="/memo-blog/tags/pyenv/" style="font-size: 10px;">pyenv</a> <a href="/memo-blog/tags/tmux/" style="font-size: 10.77px;">tmux</a> <a href="/memo-blog/tags/vim/" style="font-size: 13.85px;">vim</a> <a href="/memo-blog/tags/windows/" style="font-size: 10px;">windows</a>
        </div>
    </div>

    
        
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">リンク</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


    
    <div id="toTop" class="fas fa-angle-up"></div>
</aside>

            
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            &copy; 2023 dobachi<br>
            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="http://github.com/ppoffice">PPOffice</a>
        </div>
    </div>
</footer>
        


    
        <script src="/memo-blog/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/memo-blog/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/memo-blog/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    



<!-- Custom Scripts -->
<script src="/memo-blog/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>