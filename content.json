{"meta":{"title":"memo-blog","subtitle":null,"description":"This is just a memo","author":"dobachi","url":"https://dobachi.github.io/memo-blog"},"pages":[{"title":"Categories","date":"2024-08-27T15:34:26.031Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"categories/index.html","permalink":"https://dobachi.github.io/memo-blog/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2024-08-27T15:34:26.071Z","updated":"2024-08-27T15:34:26.071Z","comments":true,"path":"tags/index.html","permalink":"https://dobachi.github.io/memo-blog/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Use HOME in case of using sudo [ansible]","slug":"Use-HOME-in-case-of-using-sudo-ansible","date":"2024-08-25T12:30:22.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2024/08/25/Use-HOME-in-case-of-using-sudo-ansible/","link":"","permalink":"https://dobachi.github.io/memo-blog/2024/08/25/Use-HOME-in-case-of-using-sudo-ansible/","excerpt":"","text":"メモ 参考 メモ ansible で sudo 時に実行ユーザの HOME 環境変数を取得する を参考にした。 以下のようなロールを作っておき、プレイブックの最初の方に含めておくと良い。 以降のロール実行時に、変数 ansible_home に格納された値を利用できる。 roles/regsiter_home/tasks/main.yml 1234567891011- block: - name: Get ansible_user home directory shell: 'getent passwd \"&#123;&#123;ansible_env.SUDO_USER&#125;&#125;\" | cut -d: -f6' register: ansible_home_result - name: Set the fact for the other scripts to use set_fact: ansible_home: '&#123;&#123;ansible_home_result.stdout&#125;&#125;' cacheable: truetags: - register_home 元記事と変えているのは、SUDO実行ユーザ名を取るところ。環境変数から取るようにしている。 プレイブックでは以下のようにする。 playbooks/something.yml 1234- hosts: \"&#123;&#123; server | default('mypc') &#125;&#125;\" roles: - register_home - something 参考 ansible で sudo 時に実行ユーザの HOME 環境変数を取得する","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Ansible","slug":"Knowledge-Management/Ansible","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Ansible/"}],"tags":[{"name":"ansible","slug":"ansible","permalink":"https://dobachi.github.io/memo-blog/tags/ansible/"}]},{"title":"Management Domain of EDC","slug":"Management-Domain","date":"2024-07-14T03:09:34.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2024/07/14/Management-Domain/","link":"","permalink":"https://dobachi.github.io/memo-blog/2024/07/14/Management-Domain/","excerpt":"","text":"メモ 概要 Topology Architecture 実装 参考 メモ 概要 2024/6あたりから、 EDC/management-domains のドキュメントが追加された。 EDC/management-domains/Introduction の通り、Management Domainを利用することで、EDCコンポーネント群を組織的に管理することができる。 例えば、管理組織と下部組織にわけ、下部組織は管理組織に管理を委譲できる。 ここで言うEDCコンポーネントとは、例えば以下のようなものが挙げられている。 Catalog Server Control Plane Data Plane Identity Hub 2024/7/14現在では、対象は以下のようになっている。 1The current document will outline how management domains operate for three EDC components: the Catalog Server, Control Plane, and Data Plane. Topology デプロイメトには種類がある。 大まかに分けて、単一構成（Single Management Domain）と分散型構成（Distributed Management Domains）である。 1: Single Mangement Domain Catalog Server、Control Plane、Data Planeがすべて同一のプロセスもしくは同居プロセスに存在する。k8sで構成されるが、ひとつのドメインで管理される場合も含まれる。 Single Management Domainの最も単純な例 Single Management Domainのクラスタの例 2: Distributed Management Domains 下部組織を持つ他国籍、複合企業体のような分散型のドメイン管理。この複合企業体は、ひとつのWeb DIDを用いる。当該複合企業体のデータを利用する企業は、ある下部組織のアクセス権しか持たないケースもある。 2a: Catalog Serverを持つ下部組織をRoot Catalogでまとめる Distributed Management DomainでCatalog Serverを持つ下部組織をまとめる例 2b: 親のCatalog Serverが異なるManagement Domainを管理する Distributed Management DomainでCatalog Serverが下部組織の異なるManagement Domainを管理する例 2c: 親のManagement DomainがCatalog ServerとControl Planeを持ち、下部組織のData Planeをまとめる Distributed Management Domainで親Management DomainがCatalog ServerとControl Planeをもつ例 分散型の構成は、親Management Domainがどこまでを担うかによります。 Architecture EDCはDCAT3のCatalog型を利用する。Catalog型の親はDataset型である。 またCatalog型はほかの複数のCatalogを含むことができる。その際、Service型で定義する。 したがって、以下のような例になる。 12345678910111213141516171819202122232425&#123; \"@context\": \"https://w3id.org/dspace/v0.8/context.json\", \"@id\": \"urn:uuid:3afeadd8-ed2d-569e-d634-8394a8836d57\", \"@type\": \"dcat:Catalog\", \"dct:title\": \"Data Provider Root Catalog\", \"dct:description\": [ \"A catalog of catalogs\" ], \"dct:publisher\": \"Data Provider A\", \"dcat:catalog\": &#123; \"@type\": \"dcat:Catalog\", \"dct:publisher\": \"Data Provider A\", \"dcat:distribution\": &#123; \"@type\": \"dcat:Distribution\", \"dcat:accessService\": \"urn:uuid:4aa2dcc8-4d2d-569e-d634-8394a8834d77\" &#125;, \"dcat:service\": [ &#123; \"@id\": \"urn:uuid:4aa2dcc8-4d2d-569e-d634-8394a8834d77\", \"@type\": \"dcat:DataService\", \"dcat:endpointURL\": \"https://provder-a.com/subcatalog\" &#125; ] &#125;&#125; Sub CatalogはDCATのService型で関連付けされる。 この例は、Distributed Management Domainの2aに相当する。 アクセス管理についても考える。 2bのパターンのでは、親のCatalog Serverが下部組織のManagement Domainを管理する。そのため、ContractDefinitionにアクセス権を定義することで下部組織へのアクセス管理を実現する。 また、さらに集中型のControl Planeを設ける場合は、親のControl Planeが子のData Planeを管理する。 また、2024/7時点での設計では、複製を避けるようになっている。これは性能と単純さのためである。複合Catalogは上記の通り、ハイパーリンクを使用するようにしており、非同期のクローラで遅延ナビゲートされることも可能。 また、Catalogもそうだが、Contract NegotiationやTransfer Processのメタデータも複製されないことにも重きが置かれている。各Management Domainはそれぞれ責任を持って管理するべき、としている。 もし組織間で連携が必要な場合は読み取り専用の複製として渡すEDC拡張機能として実現可能である。 なお、個人的な所感だが、これは2aや2bであれば実現しうるが、2cだとControl Planeが親側に存在するため、Contract Negotiationの管理主体が親になってしまうのではないか、と思うがどうだろうか。Data Plane側はもしかしたら、下部組織側に残せるかもしれない。 実装 以上の内容を実現するため、いくつかEDCに変更が行われる。 Asset にCatalogを示す真理値を追加。もし @type が edc:CatalogAsset の場合は真になる。 Management APIが更新される。 Asset に @type を持てるように。 Dataset の拡張として Catalog を追加 DatasetResolverImpl を更新。 Asset のCatalogサブタイプを扱えるように。 JsonObjectFromDatasetTransformer をリファンクたリング。 Catalog サブタイプを扱えるように。 合わせて、Fedrated Catalog Crawler (FCC) をリファクタリング。複合カタログをナビゲートし、キャッシュできるように。 参加者ごとにCatalogの更新をアトミックに実行できるようにする必要がある。 Management APIはCatalogを要求する際、ローカルのFCCキャッシュを参照するようにリファクタリングされる。 Catalog Serverもリファクタリングされる。 参考 EDC/management-domains EDC/management-domains/Introduction Single Management Domainの最も単純な例 Single Management Domainのクラスタの例 Distributed Management DomainでCatalog Serverが下部組織の異なるManagement Domainを管理する例 Distributed Management DomainでCatalog Serverを持つ下部組織をまとめる例 Distributed Management Domainで親Management DomainがCatalog ServerとControl Planeをもつ例","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Dataspace Connector","slug":"Knowledge-Management/Dataspace-Connector","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Dataspace-Connector/"},{"name":"Eclipse Dataspace Components","slug":"Knowledge-Management/Dataspace-Connector/Eclipse-Dataspace-Components","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Dataspace-Connector/Eclipse-Dataspace-Components/"}],"tags":[{"name":"Dataspace Connector","slug":"Dataspace-Connector","permalink":"https://dobachi.github.io/memo-blog/tags/Dataspace-Connector/"},{"name":"IDS","slug":"IDS","permalink":"https://dobachi.github.io/memo-blog/tags/IDS/"},{"name":"EDC","slug":"EDC","permalink":"https://dobachi.github.io/memo-blog/tags/EDC/"}]},{"title":"MVD_of_EDC","slug":"MVD-of-EDC","date":"2024-07-06T16:16:32.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2024/07/07/MVD-of-EDC/","link":"","permalink":"https://dobachi.github.io/memo-blog/2024/07/07/MVD-of-EDC/","excerpt":"","text":"メモ Identity &amp; Trustについての説明 目的 シナリオ Data setup 参考 メモ 2024/7/12時点でプロジェクトのドキュメントが更新され、旧来Developer Documentとされていたものがなくなった。 その代わり、README に色々な記述が追加されている。今回はこれを使って試す。 Identity &amp; Trustについての説明 README#Introduction にある通り、Eclipse Dataspace Working Groupの下で、IdentityやTrustの仕様検討が行われている。 ただ、当該章に記載のリンクをたどると、Tractus-Xのサイトにたどり着く。 -&gt; Eclipse Tractus-X/identity-trust 当該レポジトリのREADMEでも記載されているが、この状態は暫定のようだ。 1Until the first version of the specification from the working group is released and implemented, this repository contains the current specification implemented in the Catena-X data space. Maintenance is limited to urgent issues concerning the operation of this data space. 何はともあれ、2024/7現在は、Verifiable Credentialを用いたDecentralized Claims Protocolは絶賛仕様決定・実装中である。 目的 README#Purpose には、このデモの目的が記載されている。 このデモでは、2者のデータスペース参加者が、クレデンシャルを交換した後、DSPメッセージをやり取りする例を示す。 もちろん、旧来からの通り、このデモは商用化品質のものではなく、いくつかの shortcut が存在する。 シナリオ management-domain を用いたFederated Catalogsを用いている。 シナリオの概要をいかに示す。 MVDシナリオの概要 図の通り、Provider CorpとConsumer Corpの2種類の企業があり、 Provdier CorpにはQ&amp;AとManufacturingという組織が存在する。Q&amp;AとManufacturingにはそれぞれEDCが起動している。Provdier Copの親組織にはCatalogとIdentityHubが起動している。このIdentityHubはCatalog、EDC（provider-qna）、EDC（provider-manufacuturing）に共通である。また、participantIdも共通のものを使う。Consumer CorpにはEDCとIdentityHubがある。 Data setup 図の通り、実際のデータ（assets）は各下部組織にある。そのカタログは直接外部に晒されない。その代わり、親組織であるProvider CorpのCatalog Server内のroot catalogにそのポインタ（catalog assets）が保持される。これにより、Consumer Corpはroot catalogを用いて、実際のassetの情報を解決できる。 (wip) 参考 Developer document ... これはなくなった。 README README#Introduction Eclipse Tractus-X/identity-trust README#Purpose shortcut README/scenario management-domain","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Dataspace Connector","slug":"Knowledge-Management/Dataspace-Connector","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Dataspace-Connector/"},{"name":"Eclipse Dataspace Components","slug":"Knowledge-Management/Dataspace-Connector/Eclipse-Dataspace-Components","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Dataspace-Connector/Eclipse-Dataspace-Components/"}],"tags":[{"name":"Dataspace Connector","slug":"Dataspace-Connector","permalink":"https://dobachi.github.io/memo-blog/tags/Dataspace-Connector/"},{"name":"IDS","slug":"IDS","permalink":"https://dobachi.github.io/memo-blog/tags/IDS/"},{"name":"EDC","slug":"EDC","permalink":"https://dobachi.github.io/memo-blog/tags/EDC/"}]},{"title":"Dataspace Protocol of EDC","slug":"Dataspace-Protocol-of-EDC","date":"2023-09-20T14:07:49.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2023/09/20/Dataspace-Protocol-of-EDC/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/09/20/Dataspace-Protocol-of-EDC/","excerpt":"","text":"メモ DSP Data Planeの実装を確認する おまけ）古い（？）Data Planeの実装を確認する（HTTPの例） 参考 ドキュメント ソースコード メモ 注意：このメモはかなり初期のものなので、今となっては怪しい内容が含まれてる・・・。 EDCは現在IDSが提唱する、Dataspace Protocolにしたがって、コネクタ間でやりとりする。 DSP Data Planeの実装を確認する data-protocols/dsp （dsp）以下に、Dataspace Protocolに対応したモジュールが含まれている。 例えば、org.eclipse.edc.protocol.dsp.dispatcher.PostDspHttpRequestFactory、org.eclipse.edc.protocol.dsp.dispatcher.GetDspHttpRequestFactoryなどのファクトリが定義されている。 これは、前述のPOST、GETオペレーションに対応するリクエストを生成するためのファクトリである。 以下は、カタログのリクエストを送るための実装である。 org/eclipse/edc/protocol/dsp/catalog/dispatcher/DspCatalogHttpDispatcherExtension.java:54 123456789101112public void initialize(ServiceExtensionContext context) &#123; messageDispatcher.registerMessage( CatalogRequestMessage.class, new PostDspHttpRequestFactory&lt;&gt;(remoteMessageSerializer, m -&gt; BASE_PATH + CATALOG_REQUEST), new CatalogRequestHttpRawDelegate() ); messageDispatcher.registerMessage( DatasetRequestMessage.class, new GetDspHttpRequestFactory&lt;&gt;(m -&gt; BASE_PATH + DATASET_REQUEST + \"/\" + m.getDatasetId()), new DatasetRequestHttpRawDelegate() );&#125; 他にも、org.eclipse.edc.protocol.dsp.transferprocess.dispatcher.DspTransferProcessDispatcherExtensionなどが挙げられる。 これは以下のように、org.eclipse.edc.connector.transfer.spi.types.protocol.TransferRequestMessageが含まれており、ConsumerがProviderにデータ転送プロセスをリクエストする際のメッセージのディスパッチャが登録されていることがわかる。 org/eclipse/edc/protocol/dsp/transferprocess/dispatcher/DspTransferProcessDispatcherExtension.java:60 12345678910111213141516171819202122public void initialize(ServiceExtensionContext context) &#123; messageDispatcher.registerMessage( TransferRequestMessage.class, new PostDspHttpRequestFactory&lt;&gt;(remoteMessageSerializer, m -&gt; BASE_PATH + TRANSFER_INITIAL_REQUEST), new TransferRequestDelegate(remoteMessageSerializer) ); messageDispatcher.registerMessage( TransferCompletionMessage.class, new PostDspHttpRequestFactory&lt;&gt;(remoteMessageSerializer, m -&gt; BASE_PATH + m.getProcessId() + TRANSFER_COMPLETION), new TransferCompletionDelegate(remoteMessageSerializer) ); messageDispatcher.registerMessage( TransferStartMessage.class, new PostDspHttpRequestFactory&lt;&gt;(remoteMessageSerializer, m -&gt; BASE_PATH + m.getProcessId() + TRANSFER_START), new TransferStartDelegate(remoteMessageSerializer) ); messageDispatcher.registerMessage( TransferTerminationMessage.class, new PostDspHttpRequestFactory&lt;&gt;(remoteMessageSerializer, m -&gt; BASE_PATH + m.getProcessId() + TRANSFER_TERMINATION), new TransferTerminationDelegate(remoteMessageSerializer) );&#125; ◆参考情報はじめ このファクトリは、ディスパッチャの org.eclipse.edc.protocol.dsp.dispatcher.DspHttpRemoteMessageDispatcherImpl#dispatch メソッドから、間接的に呼び出されて利用される。 このメソッドはorg.eclipse.edc.spi.message.RemoteMessageDispatcher#dispatchメソッドを実装したものである。ディスパッチャとして、リモートへ送信するメッセージ生成をディスパッチするための。メソッドである。 さらに、これは org.eclipse.edc.connector.core.base.RemoteMessageDispatcherRegistryImpl 内で使われている。ディスパッチャのレジストリ内で、ディスパッチ処理が起動、管理されるようだ。 なお、これはorg.eclipse.edc.spi.message.RemoteMessageDispatcherRegistry#dispatch を実装したものである。このメソッドは、色々なところから呼び出される。 例えば、TransferCoreExtensionクラスではサービス起動時に、転送プロセスを管理するorg.eclipse.edc.connector.transfer.process.TransferProcessManagerImplを起動する。 org/eclipse/edc/connector/transfer/TransferCoreExtension.java:205 1234@Overridepublic void start() &#123; processManager.start();&#125; これにより、以下のようにステートマシンがビルド、起動され、各プロセッサが登録される。 org/eclipse/edc/connector/transfer/process/TransferProcessManagerImpl.java:143 123456789101112stateMachineManager = StateMachineManager.Builder.newInstance(\"transfer-process\", monitor, executorInstrumentation, waitStrategy) .processor(processTransfersInState(INITIAL, this::processInitial)) .processor(processTransfersInState(PROVISIONING, this::processProvisioning)) .processor(processTransfersInState(PROVISIONED, this::processProvisioned)) .processor(processTransfersInState(REQUESTING, this::processRequesting)) .processor(processTransfersInState(STARTING, this::processStarting)) .processor(processTransfersInState(STARTED, this::processStarted)) .processor(processTransfersInState(COMPLETING, this::processCompleting)) .processor(processTransfersInState(TERMINATING, this::processTerminating)) .processor(processTransfersInState(DEPROVISIONING, this::processDeprovisioning)) .build();stateMachineManager.start(); 上記のプロセッサとして登録されているorg.eclipse.edc.connector.transfer.process.TransferProcessManagerImpl#processStartingの中では org.eclipse.edc.connector.transfer.process.TransferProcessManagerImpl#sendTransferStartMessage が呼び出されている。 org/eclipse/edc/connector/transfer/process/TransferProcessManagerImpl.java:376 123456return entityRetryProcessFactory.doSyncProcess(process, () -&gt; dataFlowManager.initiate(process.getDataRequest(), contentAddress, policy)) .onSuccess((p, dataFlowResponse) -&gt; sendTransferStartMessage(p, dataFlowResponse, policy)) .onFatalError((p, failure) -&gt; transitionToTerminating(p, failure.getFailureDetail())) .onFailure((t, failure) -&gt; transitionToStarting(t)) .onRetryExhausted((p, failure) -&gt; transitionToTerminating(p, failure.getFailureDetail())) .execute(description); org.eclipse.edc.connector.transfer.process.TransferProcessManagerImpl#sendTransferStartMessage メソッド内では、 org.eclipse.edc.connector.transfer.spi.types.protocol.TransferStartMessageのメッセージがビルドされ、 ディスパッチャにメッセージとして渡される。 org/eclipse/edc/connector/transfer/process/TransferProcessManagerImpl.java:386 1234567891011121314151617var message = TransferStartMessage.Builder.newInstance() .processId(process.getCorrelationId()) .protocol(process.getProtocol()) .dataAddress(dataFlowResponse.getDataAddress()) .counterPartyAddress(process.getConnectorAddress()) .policy(policy) .build();var description = format(\"Send %s to %s\", message.getClass().getSimpleName(), process.getConnectorAddress());entityRetryProcessFactory.doAsyncStatusResultProcess(process, () -&gt; dispatcherRegistry.dispatch(Object.class, message)) .entityRetrieve(id -&gt; transferProcessStore.findById(id)) .onSuccess((t, content) -&gt; transitionToStarted(t)) .onFailure((t, throwable) -&gt; transitionToStarting(t)) .onFatalError((n, failure) -&gt; transitionToTerminated(n, failure.getFailureDetail())) .onRetryExhausted((t, throwable) -&gt; transitionToTerminating(t, throwable.getMessage(), throwable)) .execute(description); ◆参考情報おわり ということで、org.eclipse.edc.protocol.dsp.spi.dispatcher.DspHttpRemoteMessageDispatcherというディスパッチャは、Dataspace Protocolに基づくリモートメッセージを生成する際に用いられるディスパッチャである。 おまけ）古い（？）Data Planeの実装を確認する（HTTPの例） Dataspace Protocol以前の実装か？ extensions/data-plane 以下にData Planeの実装が拡張として含まれている。 例えば、 extensions/data-plane/data-plane-http には、HTTPを用いてデータ共有するための拡張の実装が含まれている。 当該拡張のREADMEの通り、 （transfer APIの）DataFlowRequest がHttpDataだった場合に、 HttpDataSourceFactory HttpDataSinkFactory HttpDataSource HttpDataSink の実装が用いられる。パラメータもREADMEに（data-plane-httpのデザイン指針）記載されている。 基本的には、バックエンドがHTTPなのでそれにアクセスするためのパラメータが定義されている。 当該ファクトリは、 org.eclipse.edc.connector.dataplane.http.DataPlaneHttpExtension#initialize 内で用いられている。 org/eclipse/edc/connector/dataplane/http/DataPlaneHttpExtension.java:75 1234567var httpRequestFactory = new HttpRequestFactory();var sourceFactory = new HttpDataSourceFactory(httpClient, paramsProvider, monitor, httpRequestFactory);pipelineService.registerFactory(sourceFactory);var sinkFactory = new HttpDataSinkFactory(httpClient, executorContainer.getExecutorService(), sinkPartitionSize, monitor, paramsProvider, httpRequestFactory);pipelineService.registerFactory(sinkFactory); ここでは、試しにData Source側を確認してみる。 org/eclipse/edc/connector/dataplane/http/pipeline/HttpDataSourceFactory.java:63 1234567891011121314@Overridepublic DataSource createSource(DataFlowRequest request) &#123; var dataAddress = HttpDataAddress.Builder.newInstance() .copyFrom(request.getSourceDataAddress()) .build(); return HttpDataSource.Builder.newInstance() .httpClient(httpClient) .monitor(monitor) .requestId(request.getId()) .name(dataAddress.getName()) .params(requestParamsProvider.provideSourceParams(request)) .requestFactory(requestFactory) .build();&#125; 上記の通り、まずデータのアドレスを格納するインスタンスが生成され、 つづいて、HTTPのデータソースがビルドされる。 HTTPのData Sourceの実体は org.eclipse.edc.connector.dataplane.http.pipeline.HttpDataSource である。 このクラスはSPIの org.eclipse.edc.connector.dataplane.spi.pipeline.DataSourceインタフェースを実装したものである。 org.eclipse.edc.connector.dataplane.http.pipeline.HttpDataSource#openPartStream がオーバライドされて実装されている。 詳しくは、openPartStream参照。 参考 ドキュメント data-plane-httpのデザイン指針 ソースコード openPartStream dsp","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Spaces","slug":"Knowledge-Management/Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/"},{"name":"EDC","slug":"Knowledge-Management/Data-Spaces/EDC","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/EDC/"}],"tags":[{"name":"EDC","slug":"EDC","permalink":"https://dobachi.github.io/memo-blog/tags/EDC/"},{"name":"Data Spaces","slug":"Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Spaces/"},{"name":"Dataspace Protocol","slug":"Dataspace-Protocol","permalink":"https://dobachi.github.io/memo-blog/tags/Dataspace-Protocol/"}]},{"title":"Generate OpenAPI Spec of EDC Connector","slug":"Generate-OpenAPI-Spec-of-EDC-Connector","date":"2023-09-09T13:26:58.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2023/09/09/Generate-OpenAPI-Spec-of-EDC-Connector/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/09/09/Generate-OpenAPI-Spec-of-EDC-Connector/","excerpt":"","text":"メモ 準備 生成 Data Planeの中身を軽く確認 概要 paths Transfer Data Plane control-plane-api /transferprocess/{processId}/complete /transferprocess/{processId}/fail マネージメントAPIの類 Dataspace Protocol Architecture 後方互換性 ゴール アプローチ JSON-LD Processing Architecture Dataspace Protocol Endpoints and Services Architecture Dataspace Protocol Contract Negotiation Architecture 参考 プロジェクト ドキュメント ソースコード メモ EDCのConnectorのOpenAPIスペックを出力するための手順がGenerating the OpenApi Spec (*.yaml)に記載されている。 これに従い、試しに出力してみることにする。 ただ、このSpecはいわゆる現在EDCが採用している、Dataspace Protocol仕様ではないものが含まれている可能性が高い。 pathが/v2となっているのは、Dataspace Protocol準拠か？ → 実際に調べてみると、v2が必ずしも、Dataspace Protocol向けというわけではなさそうである。 ちなみに、参考までに、IDSA Dataspace ConnectorのOpenAPI Specは Dataspace ConnectorのOpenAPI Spec にある。 このコネクタは昨年からあまり更新されていないので注意。 準備 もしまだソースコードを取得していなければ取得しておく。 12git pull git@github.com:eclipse-edc/Connector.gitcd Connector 生成 ビルド環境にはJDK17を利用したいので、今回はDockerで簡単に用意する。 そのまま実行する場合： 1docker run --rm -v $&#123;PWD&#125;:/local --workdir /local openjdk:17-alpine ./gradlew clean resolve いったんシェル立ち上げる場合： 12docker run -it --rm -v $&#123;PWD&#125;:/local --workdir /local openjdk:17-alpine sh./gradlew clean resolve BUILD SUCCESSFULとなったらOK。 ちなみに、このYAMLファイル生成は自前のビルドツールを用いているようだ。参考：SwaggerGeneratorExtension Data Planeの中身を軽く確認 resources/openapi/yaml/control-api/data-plane-api.yaml にある、Data Planeを試しに見てみる。 概要 description部分を機械翻訳したのが以下である。 12Data PlaneのパブリックAPIはデータプロキシであり、データコンシューマがData Planeインスタンスを通じて、プロバイダのデータソース（バックエンドのRest APIや内部データベースなど）から能動的にデータを問い合わせることを可能にします。Data PlaneのパブリックAPIはプロキシであるため、すべての動詞（GET、POST、PUT、PATCH、DELETEなど）をサポートしており、データソースが必要になるまでデータを転送することができます。これは、実際のデータソースがRest APIそのものである場合に特に便利です。同じように、任意のクエリパラメータ、パスパラメータ、リクエストボディのセットも（HTTPサーバによって固定された範囲内で）サポートされ、実際のデータソースに伝えることができます。 企業が持つデータストアをデータソースとしてデータ連携する際、そのプロキシとして働く。 paths APIのパスを確認する。 transfer データ転送をリクエストする。 リクエストボディには、データ転送のリクエスト情報が含まれる。 1234567891011121314151617/transfer: post: description: Initiates a data transfer for the given request. The transfer will be performed asynchronously. operationId: initiateTransfer requestBody: content: application/json: schema: $ref: '#/components/schemas/DataFlowRequest' responses: \"200\": description: Data transfer initiated \"400\": description: Failed to validate request tags: - Data Plane control API transfer/{processId} パラメータprocessIdで与えられたIDのデータ転送処理の状態を確認する。 123456789101112131415/transfer/&#123;processId&#125;: get: description: Get the current state of a data transfer. operationId: getTransferState parameters: - in: path name: processId required: true schema: type: string responses: \"200\": description: Missing access token tags: - Data Plane control API /{any} /{any}以下にはDELETE、GET、PATCH、POST、PUTのOperationが定義されている。 12345678910/&#123;any&#125;: delete: (snip) get: (snip) patch: (snip) post: (snip) put: 単純にデータを取得するだけではない。 Transfer Data Plane resources/openapi/yaml/control-api/transfer-data-plane.yaml に含まれるのは以下のSpecだった。 トークンを受け取り検証するAPIのようだ。 12345678910111213141516171819202122232425262728293031openapi: 3.0.1paths: /token: get: description: \"Checks that the provided token has been signed by the present\\ \\ entity and asserts its validity. If token is valid, then the data address\\ \\ contained in its claims is decrypted and returned back to the caller.\" operationId: validate parameters: - in: header name: Authorization schema: type: string responses: \"200\": description: Token is valid \"400\": description: Request was malformed \"403\": description: Token is invalid tags: - Consumer Pull Token Validationcomponents: schemas: DataAddress: type: object properties: properties: type: object additionalProperties: type: object control-plane-api resources/openapi/yaml/control-api/control-plane-api.yaml にコントロールプレーンのSpecが含まれている。 /transferprocess/{processId}/complete 転送プロセスの完了をリクエストする。 転送が非同期、処理なので、受付成功が返る。 1234567891011121314151617181920212223/transferprocess/&#123;processId&#125;/complete: post: description: \"Requests completion of the transfer process. Due to the asynchronous\\ \\ nature of transfers, a successful response only indicates that the request\\ \\ was successfully received\" operationId: complete parameters: - in: path name: processId required: true schema: type: string responses: \"400\": content: application/json: schema: type: array items: $ref: '#/components/schemas/ApiErrorDetail' description: \"Request was malformed, e.g. id was null\" tags: - Transfer Process Control Api /transferprocess/{processId}/fail 転送プロセスを失敗で完了させるリクエストを送る。 123456789101112131415161718192021222324252627post: description: \"Requests completion of the transfer process. Due to the asynchronous\\ \\ nature of transfers, a successful response only indicates that the request\\ \\ was successfully received\" operationId: fail parameters: - in: path name: processId required: true schema: type: string requestBody: content: application/json: schema: $ref: '#/components/schemas/TransferProcessFailStateDto' responses: \"400\": content: application/json: schema: type: array items: $ref: '#/components/schemas/ApiErrorDetail' description: \"Request was malformed, e.g. id was null\" tags: - Transfer Process Control Api マネージメントAPIの類 resources/openapi/yaml/management-api 以下には、マネージメント系のAPIのSpecが。含まれている。 例えば、 カタログ: おそらくDataspace Protocolに対応している。DCATカタログのやり取り。 /v2/catalog/dataset/request /v2/catalog/request データアセット: データアドレスの情報と合わせて、データアセットを登録する /v2/assets post: 登録 put: 更新 /v2/assets/request: クエリに従ってアセット群を取得する /v2/assets/{assetId}/dataaddress: データアドレスの更新 /v2/assets/{id} delete: 消す get: アセット取得 /v2/assets/{id}/dataaddress: アドレス取得 /v3/assets ... v3とは？ v2とおおよそ同じ /v3/assets/request v2とおおよそ同じ など。ただ、/v2としていながら、DSPではなかったりするものがある（例：/v2/contractnegotiations）など注意が必要。 Dataspace Protocol Architecture IDS Dataspace Protocolのドキュメント にIDSプロトコル対応の概要が記載されている。 後方互換性 当該ドキュメントに記載の通り、後方互換性を保証するものではない。 新しいプロトコルに対応次第、古い実装は破棄される。 ゴール （将来リリースされる？）IDS-TCK（IDS Test Compatibility Kit)の必須項目をパスすること Dataspace Protocol仕様を満たす他のコネクタと相互運用可能であること Dataspace Protocolよりも前のバージョンのIDSには対応しない。 Usage Policyは実装しない。他のプロジェクトで実装される。 アプローチ Dataspace ProtocolはJSON-LD、DCAT、ODRLで実現されている。 このプロトコルの対応で、Contract NegotiationとTransfer Processステートが新たに実装されることになる。 ただし、新しいプロトコルの対応が完了するまで、テストが通るようにする。 JSON-LD Processing Architecture に基きJSON-LD対応する。 Dataspace Protocol Endpoints and Services Architecture に基きエンドポイントとサービスの拡張を実装する。 Dataspace Protocol Contract Negotiation Architecture に基きContract Negotiationマネージャのステートマシンを更新する。 The Dataspace Protocol Transfer Process Architecture に基きTransfer Processのステートマシンを更新する。 この1から4項目が安定すると、古いモジュールとサービスが削除される。 Management APIを更新する。 JSON-LD Processing Architecture JSON-LD Processing Architecture にJSON-LDを処理するアーキテクチャに関するコンセプトとアプローチが記載されている。 冒頭に記載あるとおり、結果として、JDS InfoModel Java Libraryを用いるのをやめ、JSON-LDメッセージをやり取りすることになる。 既存の TypeManagerに機能付加する。 JSONP対応する。 文書上は、以下のようなコンセプトが例として載っていた。 12345678910111213141516var mapper = new ObjectMapper();mapper.registerModule(new JSONPModule());var module = new SimpleModule() &#123; @Override public void setupModule(SetupContext context)&#123; super.setupModule(context); &#125; &#125;;mapper.registerModule(module);typeManager.registerContext(\"json-ld\",mapper) 実際に、2023/9/24時点での実装においても、以下のようにTypeManagerに登録されたJSON_JDのマッパーを利用していることが見られます。 org/eclipse/edc/protocol/dsp/api/configuration/DspApiConfigurationExtension.java:128 123var jsonLdMapper = typeManager.getMapper(JSON_LD);webService.registerResource(config.getContextAlias(), new ObjectMapperProvider(jsonLdMapper));webService.registerResource(config.getContextAlias(), new JerseyJsonLdInterceptor(jsonLd, jsonLdMapper)); org/eclipse/edc/protocol/dsp/api/configuration/DspApiConfigurationExtension.java:135 123456789101112131415161718192021222324252627282930313233private void registerTransformers() &#123; var mapper = typeManager.getMapper(JSON_LD); mapper.registerSubtypes(AtomicConstraint.class, LiteralExpression.class); var jsonBuilderFactory = Json.createBuilderFactory(Map.of()); // EDC model to JSON-LD transformers transformerRegistry.register(new JsonObjectFromCatalogTransformer(jsonBuilderFactory, mapper)); transformerRegistry.register(new JsonObjectFromDatasetTransformer(jsonBuilderFactory, mapper)); transformerRegistry.register(new JsonObjectFromPolicyTransformer(jsonBuilderFactory)); transformerRegistry.register(new JsonObjectFromDistributionTransformer(jsonBuilderFactory)); transformerRegistry.register(new JsonObjectFromDataServiceTransformer(jsonBuilderFactory)); transformerRegistry.register(new JsonObjectFromAssetTransformer(jsonBuilderFactory, mapper)); transformerRegistry.register(new JsonObjectFromDataAddressTransformer(jsonBuilderFactory)); transformerRegistry.register(new JsonObjectFromQuerySpecTransformer(jsonBuilderFactory)); transformerRegistry.register(new JsonObjectFromCriterionTransformer(jsonBuilderFactory, mapper)); // JSON-LD to EDC model transformers // DCAT transformers transformerRegistry.register(new JsonObjectToCatalogTransformer()); transformerRegistry.register(new JsonObjectToDataServiceTransformer()); transformerRegistry.register(new JsonObjectToDatasetTransformer()); transformerRegistry.register(new JsonObjectToDistributionTransformer()); // ODRL Transformers OdrlTransformersFactory.jsonObjectToOdrlTransformers().forEach(transformerRegistry::register); transformerRegistry.register(new JsonValueToGenericTypeTransformer(mapper)); transformerRegistry.register(new JsonObjectToAssetTransformer()); transformerRegistry.register(new JsonObjectToQuerySpecTransformer()); transformerRegistry.register(new JsonObjectToCriterionTransformer()); transformerRegistry.register(new JsonObjectToDataAddressTransformer());&#125; 特に後者の実装は、各種情報をJSONのオブジェクトに変換するトランスフォーマー（や、その逆）を登録している。 JSON-LDにてメッセージをやりとりしている様子の一端をみられる。 また、ドキュメントの方ではコンセプトとして、以下のような変換の流れが例として挙げられていた。 12345678910111213// message is de-serialized as Map&lt;String, Object&gt; by Jersey var document = JsonDocument.of(mapper.convertValue(message, JsonObject.class));try &#123; var compacted = JsonLd.compact(document,EMPTY_CONTEXT).get(); var convertedDocument = mapper.convertValue(compacted,Map.class); // process converted document&#125; catch(JsonLdError e) &#123; throw new RuntimeException(e);&#125; もし実際の実装をみるのであれば、 org.eclipse.edc.core.transform.transformer.from.JsonObjectFromCatalogTransformer#transform メソッドのようなものを確認すると良い。 なお、ドキュメントローダとしては、titanium-json-ldが使われているようだ。 参考→ org.eclipse.edc.jsonld.TitaniumJsonLd Dataspace Protocol Endpoints and Services Architecture にもその旨記載されている。 Dataspace Protocol Endpoints and Services Architecture Dataspace Protocol Endpoints and Services Architecture にIDS Controller Endpoint実装のアプローチが記載されている。 また当該ドキュメントには、以下のように拡張との対応関係が示されている。 Description Repository Extension Contract Negotiation Connector control-plane-ids Transfer Process Connector control-plane-ids Catalog requests Connector catalog-ids また前述の通り、Dataspace ProtocolではJSON-LDにてメッセージがやりとりされる。 それらを「（デ）マーシャル」（シリアライズ、デシリアライズ）する必要がある。 デシリアライズは以下のように行われると例示されている。 12var document = JsonDocument.of(jsonObject);var expanded = JsonLd.expand(document).get(); シリアライズは以下のように行われると例示されている。 123var document = JsonDocument.of(jsonObject);var compacted = JsonLd.compact(document,EMPTY_CONTEXT).get();var compacted = mapper.convertValue(compacted,Map.class); マイグレーションのポイント 大きなポイントの例は、 アセット（DCATにおけるデータセット）にODRLポリシーであるofferを含むようになること データセットはカタログに含まれる。 （もともとEDCが採用していたIDS Infomodelでは、offerにアセットが含まれる） Contract Definition、Asset、Dataset、ODRL Offerの関係は以下のように表現されていた。 123456789101112131415CD = Contract DefinitionA = AssetDS = DatasetO = ODRL OfferIf the Contract Definitions are:CD 1 --selects--&gt; [A1, A2]CD 2 --selects--&gt; [A1, A3]the resulting Catalog containing Datasets is:DS 1 -&gt; A1 [O:CD1, O:CD2] DS 2 -&gt; A2 [O:CD1]DS 3 -&gt; A3 [O:CD2] 上記は包含関係を表している。 また、ProviderにContract Negotiationyや転送タイプをリクエストするためのエンドポイントは、DCAT Distributionである。 Distributionは、コネクタエンドポイントのメタデータとDataAdress属性で示される転送タイプの組み合わせで示される。 なお、現状のEDCではまだ未実装の部分があり、フューチャーワークとされていた。 また、DCAT CatalogやDatasetは名前空間プロパティを使用して拡張可能である必要がある。CatalogDecoratorが必要。 型変換 もともとあったIdsTypeTransfomerを実装し直す必要がある。 これは先に上げていたJsonObjectFromCatalogTransformerのようなTransformerである。 本ドキュメントには、その実装コンセプト/アプローチが記載されている。 その他 Identificationの取り扱い方についても変更あり。 RemoteMessageDispatcherも変更あり。 以下のようなクラス設計になっている。 12345RemoteMessageDispatcher (org.eclipse.edc.spi.message) GenericHttpRemoteDispatcher (org.eclipse.edc.connector.callback.dispatcher.http) GenericHttpRemoteDispatcherImpl (org.eclipse.edc.connector.callback.dispatcher.http) DspHttpRemoteMessageDispatcher (org.eclipse.edc.protocol.dsp.spi.dispatcher) DspHttpRemoteMessageDispatcherImpl (org.eclipse.edc.protocol.dsp.dispatcher) Dataspace Protocol対応は、org.eclipse.edc.protocol.dsp.dispatcher.DspHttpRemoteMessageDispatcherImplと考えておくとよい。 Dataspace Protocol Contract Negotiation Architecture Dataspace Protocol Contract Negotiation Architecture にContract Negotiationの変更アプローチが記載されている。 ステートマシンの変化内容を一覧化した表が載っていた。 表の通り、Dataspace Protocolに対応したのちも、IDSにはもともと無いステートが一部残っている。 Contract Negotiationでは、その状態が重要であるから、より厳密に扱っている印象がある。 EDC Existing EDC New IDS Transition Function Notes UNSAVED (remove) N/A This state is not needed INITIAL INITIAL N/A REQUESTING REQUESTING N/A REQUESTED REQUESTED REQUESTED Provider (new &amp; counter) PROVIDER_OFFERING OFFERING N/A PROVIDER_OFFERED OFFERED OFFERED Consumer CONSUMER_OFFERING (REQUESTING) CONSUMER_OFFERED (REQUESTED) CONSUMER_APPROVING ACCEPTING N/A CONSUMER_APPROVED ACCEPTED ACCEPTED Provider DECLINING (TERMINATING) DECLINED (TERMINATED) CONFIRMING AGREEING N/A CONFIRMED AGREED AGREED Consumer VERIFYING N/A VERIFIED VERIFIED Provider FINALIZING N/A FINALIZED FINALIZED Consumer TERMINATING N/A TERMINATED TERMINATED P &amp; C ERROR (TERMINATED) 参考 プロジェクト titanium-json-ld ドキュメント Generating the OpenApi Spec (*.yaml) IDS Dataspace Protocolのドキュメント JSON-LD Processing Architecture Dataspace Protocol Endpoints and Services Architecture Dataspace Protocol Contract Negotiation Architecture The Dataspace Protocol Transfer Process Architecture ソースコード openPartStream dsp Dataspace ConnectorのOpenAPI Spec SwaggerGeneratorExtension","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Spaces","slug":"Knowledge-Management/Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/"},{"name":"EDC","slug":"Knowledge-Management/Data-Spaces/EDC","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/EDC/"}],"tags":[{"name":"EDC","slug":"EDC","permalink":"https://dobachi.github.io/memo-blog/tags/EDC/"},{"name":"Data Spaces","slug":"Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Spaces/"},{"name":"OpenAPI","slug":"OpenAPI","permalink":"https://dobachi.github.io/memo-blog/tags/OpenAPI/"}]},{"title":"OpenAPI Generator for Flask","slug":"OpenAPI-Generator-for-Flask","date":"2023-09-07T13:10:58.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2023/09/07/OpenAPI-Generator-for-Flask/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/09/07/OpenAPI-Generator-for-Flask/","excerpt":"","text":"メモ 簡単な動作確認 参考 記事 メモ 簡単な動作確認 ひとまず分かりやすかった OpenAPI GeneratorでPython Web API構築 をそのまま試す。 1234567$ mkdir -p ~/Sources/OpenAPIGenFlaskSample/original$ cd ~/Sources/OpenAPIGenFlaskSample/original$ cat &lt;&lt; EOF &gt; openapi.yaml(snip)EOF openapi.yamlの中身は OpenAPI GeneratorでPython Web API構築 に記載されている。 123$ mkdir -p server client$ docker run --rm -v $&#123;PWD&#125;:/local openapitools/openapi-generator-cli generate -i /local/openapi.yaml -g python-flask -o /local/server$ docker run --rm -v $&#123;PWD&#125;:/local openapitools/openapi-generator-cli generate -i /local/openapi.yaml -g go -o /local/client 今回用があるのはサーバ側のPython実装（Flask）の方なので、そちらを確認する。 記事にもあるが以下のようなファイルが生成されているはず。 123456789101112131415161718192021222324252627282930$ tree.├── Dockerfile├── README.md├── git_push.sh├── openapi_server│ ├── __init__.py│ ├── __main__.py│ ├── controllers│ │ ├── __init__.py│ │ ├── security_controller.py│ │ └── stock_price_controller.py│ ├── encoder.py│ ├── models│ │ ├── __init__.py│ │ ├── base_model.py│ │ ├── error.py│ │ ├── ok.py│ │ └── stock_price.py│ ├── openapi│ │ └── openapi.yaml│ ├── test│ │ ├── __init__.py│ │ └── test_stock_price_controller.py│ ├── typing_utils.py│ └── util.py├── requirements.txt├── setup.py├── test-requirements.txt└── tox.ini まずは、 __main__.py を見てみる。 12345678910111213141516171819#!/usr/bin/env python3import connexionfrom openapi_server import encoderdef main(): app = connexion.App(__name__, specification_dir='./openapi/') app.app.json_encoder = encoder.JSONEncoder app.add_api('openapi.yaml', arguments=&#123;'title': 'Stock API'&#125;, pythonic_params=True) app.run(port=8080)if __name__ == '__main__': main() 上記の通り、 connexionを用いていることがわかる。 connexionはFlaskで動作する、APIとpython関数をマッピングするためのパッケージである。 connexionについては、 connexionを使ってPython APIサーバのAPI定義と実装を関連付ける のような記事を参考にされたし。 openapi_server/controllers/stock_price_controller.py を確認する。 123456789101112131415161718192021import connexionfrom typing import Dictfrom typing import Tuplefrom typing import Unionfrom openapi_server.models.error import Error # noqa: E501from openapi_server.models.stock_price import StockPrice # noqa: E501from openapi_server import utildef stock_price(security_cd): # noqa: E501 \"\"\"株価取得 現在の株価を取得する # noqa: E501 :param security_cd: 証券コードを指定する :type security_cd: str :rtype: Union[StockPrice, Tuple[StockPrice, int], Tuple[StockPrice, int, Dict[str, str]] \"\"\" return 'do some magic!' operationId にて指定した名称がコントローラのメソッド名に反映されている。 parameters にて指定したパラメータがコントローラの引数になっていることが確認できる。 components にて指定したスキーマに基づき、openapi_server/models 以下に反映されていることが分かる。 今回の例だと、戻り値用の StockPrice やOK、Errorが定義されている。 なお、これはOpenAPIにて生成されたものであり、それを編集して使うことはあまり想定されていないようだ。 openapi_server/util.py にはデシリアライザなどが含まれている。 さて、記事通り、Dockerで動かしてみる。 12$ docker build -t openapi_server .$ docker run -p 8080:8080 openapi_server 試しに、適当な引数を与えて動かすと、実装通り戻り値を得られる。 12$ curl http://localhost:8080/v1/sc/4721/stockPrice\"do some magic!\" なおDockerfileはこんな感じである。 12345678910111213141516FROM python:3-alpineRUN mkdir -p /usr/src/appWORKDIR /usr/src/appCOPY requirements.txt /usr/src/app/RUN pip3 install --no-cache-dir -r requirements.txtCOPY . /usr/src/appEXPOSE 8080ENTRYPOINT [\"python3\"]CMD [\"-m\", \"openapi_server\"] Docker化しなくてもそのままでも動く。 ここでは一応venvを使って仮想環境を構築しておく。 1234$ python -m venv venv$ . venv/bin/activate$ pip install -r requirements.txt$ python -m openapi_server 参考 記事 OpenAPI GeneratorでPython Web API構築 connexionを使ってPython APIサーバのAPI定義と実装を関連付ける","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Open API","slug":"Knowledge-Management/Open-API","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Open-API/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Flask","slug":"Flask","permalink":"https://dobachi.github.io/memo-blog/tags/Flask/"},{"name":"Open API","slug":"Open-API","permalink":"https://dobachi.github.io/memo-blog/tags/Open-API/"}]},{"title":"IDS Dataspace Protocol","slug":"IDS-Dataspace-Protocol","date":"2023-08-31T06:56:27.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2023/08/31/IDS-Dataspace-Protocol/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/08/31/IDS-Dataspace-Protocol/","excerpt":"","text":"メモ v0.8の位置づけ 概要 インターオペラビリティの確保 Terminology Information Model Dataspace Authority Participant、Participant Agent Identity Provider Credential Issuer ParticipantAgent周りのクラス設計 実態的なクラス Catalog Protocol仕様 Message Type DCATとIDS Informationモデルの対応関係 Technical Consideration Catalog HTTPS Binding Contract Negotiation仕様 ステート変化 メッセージタイプ Contract Negotiation HTTPS Bindings Transfer Process仕様 Control and Data Plane アセット転送のタイプ ステート変化 Message Type 参考 メモ 最近のEDCでは、IDSが提唱しているDataspace Protocolが使用されている。 International-Data-Spaces-Association/ids-specification を眺めてコンテンツをざっくり書き下す。 v0.8の位置づけ 以下のような記載あり。あくまでドラフト扱い。 Working Draft 1 February 2023 概要 スキーマとプロトコルの仕様を示すものである。 データのPublish 利用合意形成 データアクセス 自律エンティティ（要はコネクタか）間でデータ共有するにはメタデータの提供が必要である。 以下、メタデータ管理として挙げられていた項目。 data assetをDCAT Catalogに入れる方法 Usage ControlをODRLポリシーとして表現する方法 契約合意を構文的に電子記録する方法 データ転送プロトコルを用いてデータアセットにアクセスする方法 仕様として挙げられていたのは以下。 Dataspace ModelとDataspace Terminologyドキュメント Catalog ProtocolとCatalog HTTPS Bindings（DCATカタログの公開、アクセス方法） Contract Negotiation ProtocolとContract Negotiation HTTPS Bindingドキュメント Transfer Process ProtocolとTransfer Process HTTPS BIndingsドキュメント 概ね、モデルを明らかにし、そのうえでカタログ、契約合意、データ転送の一連の流れに沿って仕様が示されている、と言える。 注意点として、データ転送プロセス自体は言及せずプロトコルのみ示されていることである。 Protocol Overviewに各種情報へのリンクが載っている。 インターオペラビリティの確保 Dataspace Protocolはインターオペラビリティを確保するために用いられる、とされている。 ただし、本プロトコルによりテクニカルな側面は担保されるが、セマンティックな側面はData Space参加者により担保されるべきとしている。 なお、異なるData Spaceを跨ぐインターオペラビリティは本ドキュメントの対象外である。 全体概要は、Protocol Overviewの図を参照されたし。 以下は本ドキュメントではスコープ外だが、Data Space Protocolにとって必要。 Identity Provider Trust Frameworkを実現するための情報を提供する 参加者（のエージェント、コネクタ）のバリデーション、請求内容のバリデーションが基本的な機能であるが、 その請求の構造・内容はData Spaceごと、もっといえばIdentity Provider毎に異なる。 モニタリング Policy Engine Terminology Terminologyに記載されているが、量は多くない。 特にポリシー周りの用語には注意したい。 Assest: Participantによって共有されるデータやテクニカルサービス Policy: Asset利用のためのルール、義務、制限 Offer: とあるAssetに結びつけられたPolicy Agreement: とあるAssetに結びつけられた具体的なPolicy。Provier、Consumerの両Participantにより署名されている なお、関係性という意味では、Information ModelやParticipantAgent周りのクラス設計の方がわかりやすい。 Information Model Information Modelに記載されている。 Information Modelの関係図 に関係図が載っている。 以下、ポイントのみ記載。 Dataspace Authority ひとつ or 複数のDataspaceを管理する。 Participantの登録も含む。Participantにビジネスサーティフィケーションの取得（提出？）を求める、など。 Participant、Participant Agent ParticipantがDataspaceへの参加者であり、Participant Agentが実際のタスクを担う。 Participant Agentはcredentialから生成されたverifiable presentationを用いる。credentialは第三者のissuerから発行されたものを用いる。 また第三者のIdentity providerか提供されたID tokenも用いる。 Identity Provider トラストアンカー。 ID tokenを払い出す。ID tokenはParticipant Agentのアイデンティティの検証を行う。 複数のIdentity ProviderがひとつのDataspaceに存在することも許容される。 ID tokenのセマンティクスは本仕様書の対象外。 Identity Providerは外部でもよいし、Participant自身（e.g. DID）でもよい。 Credential Issuer Verifiable Credentialを発行する。 ParticipantAgent周りのクラス設計 ParticipantAgent周りのクラス設計の図に大まかな設計が書かれている。 これによると、CatalogServiceとConnectorは同じParticipantAgentの一種である。 DCAT CatalogにはAsest EntryとDCAT DataServiceが含まれる。 ちなみに、DCAT DataServiceはAssetの提供元となるConnectorへのReferenceである。 Asset EntryはODRL Offerを保持する。当該Assetに紐づけられたUsage Control Policyである。 ConnectorもParticipant Agentの一種である。 Contract NegotiationとData Transferを担う。 Contract Negotiation結果、ODRL Agreementが生成される。ODRL Agreementは、合意された当該Assetに関するUsage Control Policyと言える。 実態的なクラス Dataspace AuthorityやParticipant Agentのように、実際のフローには登場しないエンティティもあるようだ。 Classesドキュメント に実際のフローに登場するクラスの説明が記載されている。 Catalog Protocol仕様 Catalog Protocolの仕様に以下のような要素の仕様が記載されている。 message type: メッセージの構造 message: message typeのインスタンス catalog: データ提供者によりオファーされたDCAT Catalog catalog service: 提供されたasset（の情報）を公表するParticipant Agent Consumer: 提供されたassetを要望するParticipant Agent Message Type JSON-LDでシリアライズされたメッセージ。なお、将来的にはシリアライズ方式が追加される可能性がるようだ。 CatalogRequestMessage: ConsumerからCatalog Serviceに送られる要望メッセージ。filterプロパティあり。 Catalog: Providerから送られるAsset Entiry CatalogEror: ConsumerもしくはProviderから送られるエラーメッセージ DatasetRequestMessage: ConsumerからCatalog Serviceに送られる要望メッセージ。Catalog ServiceはDataset（DCAT Dataset）を答える。それにはデータセットのIDが含まれる。 DCATとIDS Informationモデルの対応関係 Asset Entity: DCAT Dataset Distributions: DCAT Distributions DataService: ConnectorのようなIDSサービスエンドポイント。なお、dataServiceTypeが定義されている。現状ではdspace:connectorのみか。 Technical Consideration Technical Considerationsとしてはクエリやフィルタ、レプリケーションプロトコル、セキュリティ、ブローカについて言及されている。 Catalog HTTPS Binding Contract Negotiation仕様 ProviderとConsumerの間のContract Negotiation（CN）。 CNは、 https://www.w3.org/International/articles/idn-and-iri/ にてユニークに識別できる。 CNは状態遷移を経る。それらはProviderとConsumerにトラックされる。 ステート変化 Contract Negotiationのステート変化の図 にステート変化の概要が描かれている。 ステート 説明 REQUESTED ConsumerからOfferに基づいてリクエストが送られた状態。ProviderがAckを返した後。 OFFERED ProviderがConsumerにOfferを送った状態。ConsumerがAckを返した後。 ACCEPTED Consumerが最新のContract Offerを承諾した状態。ProviderがAckを返した後。 AGREED Providerが最新のContract Offerに承諾し、Consumerに合意を送った状態。ConsumerがAckを返した後。 VERIFIED ConsumerがProviderに合意検証結果を返した状態。ProviderがAckを返した後。 FINALIZED Providerが自身の合意検証結果を含むファイナライズのメッセージをConsumerに送った状態。ConsumerがAckを返した後。データがConsumerに利用可能な状態になっている。 TERMINATED ProviderかConsumerがCNを終了させた状態。どちらからか終了メッセージが送られ、Ackが返る。 メッセージタイプ Contract NegotiationのMessage Types に上記ステート変化における各種メッセージの説明が記載されている。 ほぼ名称通りの意味。 Contract Negotiation HTTPS Bindings Transfer Process仕様 CNと同じく、Transfer Process (TP) もProviderとConsumerの両方に関係する。 またCNと同じくステート変化が定義されている。 Control and Data Plane TPはConnectorにより管理される。Connectorは2個のロジカルコンポーネントで成り立つ。 Control Plane: 対抗のメッセージを受領し、TPステートを管理する Data Plane: 実際の転送を担う これらはロジカルなものであり、実装では単独プロセスとしてもよい。 アセット転送のタイプ push / pull finite / non-finite Push Transferの流れ と Pull Transferの流れ にそれぞれの流れのイメージが載っている。 基本的な流れは同じだが、実際のデータやり取りの部分だけ異なる。 finiteデータの場合、データの転送が終わったらTPが終わる。 一方、non-finiteデータの場合は明示的に終了させる必要がある。 non-finiteデータはストリームデータ、APIエンドポイントが例として挙げられている。 ステート変化 Trasnfer Processのステート変化 にステート変化の様子が描かれている。 ステートはREQUESTED、STARTED、COMPLETED、SUSPENDED、TERMINATED。 Message Type Transfer ProcessのMessage Type にメッセージタイプが記載されている。 特筆すべきものはない。 参考 International-Data-Spaces-Association/ids-specification Protocol Overview Summary Terminology Information Model Information Modelの関係図 ParticipantAgent周りのクラス設計 Classesドキュメント Catalog Protocolの仕様 Technical Considerations Contract Negotiationのステート変化の図 Contract NegotiationのMessage Types Push Transferの流れ Pull Transferの流れ Trasnfer Processのステート変化 Transfer ProcessのMessage Type","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Spaces","slug":"Knowledge-Management/Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/"},{"name":"IDS","slug":"Knowledge-Management/Data-Spaces/IDS","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/IDS/"}],"tags":[{"name":"IDS","slug":"IDS","permalink":"https://dobachi.github.io/memo-blog/tags/IDS/"},{"name":"Data Spaces","slug":"Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Spaces/"}]},{"title":"How to create SDK (WIP)","slug":"How-to-create-SDK","date":"2023-08-27T05:11:45.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2023/08/27/How-to-create-SDK/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/08/27/How-to-create-SDK/","excerpt":"","text":"メモ 注意 世の中でSDKと呼ばれているものには何が含まれているのか SDKの具体例 工夫点 開発ツールの提供形態 ライセンス 各言語向けのバインディング 先人の知恵 参考 SDKとは？ SDKの具体例 先人の知恵 メモ 注意 まだ書き始めの殴り書きなので、中身の完成度や信ぴょう性がかなり低い文章である。 世の中でSDKと呼ばれているものには何が含まれているのか 開発者の強い味方！「SDK（ソフトウェア開発キット）」とは？ KDDIによるブログ記事 「ソフトウェアを開発する際に必要なプログラムやAPI・文書・サンプルなどをまとめてパッケージ化したもの」 e-WordsのSDK 【Software Development Kit】 ソフトウェア開発キット 「SDKとは、あるシステムに対応したソフトウェアを開発するために必要なプログラムや文書などをひとまとめにしたパッケージのこと。システムの開発元や販売元が希望する開発者に配布あるいは販売する。インターネットを通じて公開されているものもある。」 AWS SDKとは？ 「特定のプラットフォーム、オペレーティングシステム、またはプログラミング言語で実行されるコードを作成するには、デバッガー、コンパイラー、ライブラリなどのコンポーネントが必要です。SDK は、ソフトウェアの開発と実行に必要なすべてを 1 か所にまとめます。」 製品により異なるのが前提ではあるが、おおむね含まれるとされるのは以下。 開発ツール コンパイラ、デバッガ、プロファイラー、デプロイツール、IDE 既成のプログラム クラスファイル、（APIなど）ライブラリ、モジュール、ドライバ 文書ファイル API、通信プロトコル プログラムファイル サンプルプログラム SDKの具体例 Android SDK Android Developer情報より iOS SDK Xcodeより JDK Javaの開発キット Java Downloadsより DDK Windowsのドライバ開発キット Windows ハードウェア開発者向けドキュメントより AWS SDK AWS SDKとは？より 各言語向けのSDKが存在する 工夫点 開発ツールの提供形態 開発に必要な環境ごと丸ごと提供する方法、2. 特定のIDEを前提としたプラグインを提供する方法、などがある。 方法例 メリット デメリット 丸ごと提供 すぐに開発始められる 自由度が低い プラグイン提供 自由度が高い 環境準備が手間大きめ ライセンス SDK自体のライセンスだけではなく、SDKに含まれるライセンスにも注意を払う必要がある。 各言語向けのバインディング サービスを使うためのSDKの場合は、様々な開発言語向けのバインディングを提供することも考慮したい。 先人の知恵 SDK の開発と維持の難しさ にはAndroidやiOS向けのSDKを開発した際の経緯が書かれていた。 参考 SDKとは？ 開発者の強い味方！「SDK（ソフトウェア開発キット）」とは？ e-WordsのSDK 【Software Development Kit】 ソフトウェア開発キット SDKの具体例 Android Developer情報 Xcode Java Downloads Windows ハードウェア開発者向けドキュメント AWS SDKとは？ ばいんでぃば 先人の知恵 SDK の開発と維持の難しさ","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"SDK","slug":"Knowledge-Management/SDK","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/SDK/"}],"tags":[{"name":"SDK","slug":"SDK","permalink":"https://dobachi.github.io/memo-blog/tags/SDK/"}]},{"title":"check windows cpu resources","slug":"check-windows-cpu-resources","date":"2023-08-14T15:32:07.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2023/08/15/check-windows-cpu-resources/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/08/15/check-windows-cpu-resources/","excerpt":"","text":"メモ 参考 メモ マシンが暴走気味になることがあり、原因追跡するためにLinuxなどでいうpsコマンド相当の内容をログ取りたいと思った。 ので、軽く調べたところ、以下のウェブサイトがヒットした。 Windows がなんか重いときにコマンドで調べる（WMIC PROCESS） 上記サイトが丁寧に解説してくれている。 その中から、今回は CPUの利用率でフィルタ あたりを参考にした。 コマンド例 1WMIC PATH Win32_PerfFormattedData_PerfProc_Process WHERE \"PercentUserTime &gt; 1\" GET Name,IDProcess,PercentUserTime,CommandLine /FORMAT:CSV ユーザタイムが1%以上使われたものをリストしている。 もし行数制限したい場合は、 PowerShellでhead,tail相当の処理を行う あたりを参考にする。 コマンド例 1(WMIC PATH Win32_PerfFormattedData_PerfProc_Process WHERE \"PercentUserTime &gt; 1\" GET Name,IDProcess,PercentUserTime /FORMAT:CSV)[0..10] 上記では、コマンドの引数が分からない。 そこで、それを詳細に調べようとすると、 コマンドラインからプロセスを特定 を参考にするとよい。 おいおい、自動的に要素を結合できるようにしよう。 参考 Windows がなんか重いときにコマンドで調べる（WMIC PROCESS） CPUの利用率でフィルタ コマンドラインからプロセスを特定 Windows10でCPUに負荷をかけるコマンドを紹介！ PowerShellでhead,tail相当の処理を行う","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Windows","slug":"Knowledge-Management/Windows","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://dobachi.github.io/memo-blog/tags/Windows/"}]},{"title":"Open project of EDC Connector with Intellij","slug":"Open-project-of-EDC-Connector-with-Intellij","date":"2023-08-01T01:46:35.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2023/08/01/Open-project-of-EDC-Connector-with-Intellij/","link":"","permalink":"https://dobachi.github.io/memo-blog/2023/08/01/Open-project-of-EDC-Connector-with-Intellij/","excerpt":"","text":"1. メモ 1.1. 準備 1.2. Intellijで開く 1.3. トラブルシュート 1.3.1. Intellijのメモリ不足 1.3.2. Gradleのメモリ不足 1.3.3. JDKバージョンの不一致 1.3.4. Gradleで使うJDKの選択 2. 参考 1. メモ EDC Connector 公式GitHub のプロジェクトをIntellijで開くための手順メモ。 いろいろなやり方があるが一例として。 1.1. 準備 gitクローンしておく。 ここでは、gitプロトコルを用いているが環境に合わせて適宜変更してクローンする。 12$ git clone git@github.com:eclipse-edc/Connector.git$ cd Connector なお、必要に応じて特定のタグをチェックアウトしてもよい。 ここでは、v0.2.0をチェックアウトした。 1$ git checkout -b v0.2.0 refs/tags/v0.2.0 当該プロジェクトでは、ビルドツールにGradleを用いている。プロジェクトに gradlew も含まれている。 本環境では、以下のようにGradle8.0を利用した。 ◇参考情報 1234567$ cat gradle/wrapper/gradle-wrapper.propertiesdistributionBase=GRADLE_USER_HOMEdistributionPath=wrapper/distsdistributionUrl=https\\://services.gradle.org/distributions/gradle-8.0-bin.zipnetworkTimeout=10000zipStoreBase=GRADLE_USER_HOMEzipStorePath=wrapper/dists ◇参考情報おわり さて、ビルドに利用するOpenJDKをインストールしておく。 ここでは17系を用いた。（もともと手元の環境にあった8系を用いようとしたら互換性エラーが生じたため、17系を用いることとした） 1$ sudo apt install openjdk-17-jdk なお、 Gradle Compatibility に互換性のあるGradleとJDKのバージョンが記載されているので参考にされたし。 この状態でIntellijを使わずにビルドするには、以下のように実行する。 ここでは念のために、コマンドでビルドできることを確かめるため、あらかじめ以下を実行しておいた。 1$ ./gradlew clean build 特に問題なければ、success表示が出て終わるはず。 1.2. Intellijで開く ひとまず、プロジェクトトップでInteliljを開く。 1$ &lt;Intellijのホームディレクトリ&gt;/bin/idea.sh . &amp; なお、IntellijにGradle拡張機能がインストールされていなければ、インストールしておく。「Files」→「Settings」→「Plugins」→「Gradleで検索」。 また使用するJDKを先にインストールしたJDK17を用いるようにする。「Files」→「Project Structure」→「Project」を開く。 「SDK:」のところで先ほどインストールしたJDKを設定する。 「Language Lavel:」も合わせて変更しておく。 開いたら、右側の「Gradle」ペインを開き、設定ボタンを押して「Gradle settings ...」を選択する。 「Build and run」章のところは、「Gradle」が選択されていることを確認する。 「Gradle」章のところは、「Use Gradle from:」でgradle wrapperの情報を用いるようになっていること、「Gradle JVM」でProject JDKを用いるようになっていることを確認する。 特に問題なければ、 BUILD SUCCESSFUL となるはずである。 1.3. トラブルシュート 1.3.1. Intellijのメモリ不足 ビルド中にヒープが不足することがある。 その場合は、 Intellijのメモリを増やす設定 を参考に、ヒープサイズを増やす。 「Help」→「Memory Setting」を開き、「2024」（MB）あたりにしておく。 1.3.2. Gradleのメモリ不足 ビルド中にヒープが不足することがある。 プロジェクト内にある、 gradle.properties 内に以下を追記する。ここでは最大4GBとした。 ◇diff 1234567891011diff --git a/gradle.properties b/gradle.propertiesindex 376da414a..d8da811a9 100644--- a/gradle.properties+++ b/gradle.properties@@ -7,3 +7,6 @@ edcGradlePluginsVersion=0.1.4-SNAPSHOT metaModelVersion=0.1.4-SNAPSHOT edcScmUrl=https://github.com/eclipse-edc/Connector.git edcScmConnection=scm:git:git@github.com:eclipse-edc/Connector.git++# Increase Gradle JVM Heap size+org.gradle.jvmargs=-Xmx4096M 1.3.3. JDKバージョンの不一致 当初、環境にあったJDK8系を用いていたが、ビルド時に構文エラー（互換性のエラー）が生じたため、JDK17を用いるようにした。 1.3.4. Gradleで使うJDKの選択 環境によっては、独自の証明書などをJDKのcacertsに導入して用いているかもしれない。 その場合は、用いているGradle、JDKに注意が必要。 例えば、/etc/ssl/certs/java/cacertsに独自の証明書を追加し、それを各JDKで用いるようにすると良い。 1234567cd /etc/ssl/certs/javacp cacerts cacerts.orgkeytool -import -alias CA -keystore cacerts -file ~/any.cer# passwordはchnageitcd ~/.jdks/jbr-xxxxxx/lib/securitymv cacerts cacerts.orgln -s /etc/ssl/certs/java/cacerts cacerts 2. 参考 EDC Connector 公式GitHub Gradle Compatibility Intellijのメモリを増やす設定","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Spaces","slug":"Knowledge-Management/Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/"},{"name":"EDC","slug":"Knowledge-Management/Data-Spaces/EDC","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Spaces/EDC/"}],"tags":[{"name":"EDC","slug":"EDC","permalink":"https://dobachi.github.io/memo-blog/tags/EDC/"},{"name":"Data Spaces","slug":"Data-Spaces","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Spaces/"}]},{"title":"GitHub Actions for Hexo with Pandoc","slug":"GitHub-Actions-for-Hexo-with-Pandoc","date":"2022-05-02T00:57:25.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2022/05/02/GitHub-Actions-for-Hexo-with-Pandoc/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/05/02/GitHub-Actions-for-Hexo-with-Pandoc/","excerpt":"","text":"メモ GitHub Actionで用いられるDockerfile等の作成 Dockerfile action.yml entrypoint.yml GitHub Actionの設定 パーソナルトークンの生成 GitHub Actionを動かしたいレポジトリのAction用トークンを設定する。 ワークフローの設定ファイル作成 参考 メモ 遅ればせながら（？）、Circle CIからGitHub ActionsにHexo使ったブログのビルド・公開を移行する、と思い立ち、 このブログ生成ではPandocなどを利用していることから、独自のコンテナ環境があるとよいかと思い、調査。 Pandocインストールを含むDockerfileを準備し利用することにした。 ◆理由 Pandoc環境を含む、GitHub Actionがなさそうだった デプロイまで含めたActionがほとんどだったが、ビルドとデプロイを分けたかった（デプロイ部分は他の取り組みと共通化したかった） Docker コンテナのアクションを作成する を参考にアクションを作成し、登録する。 またHexo環境の作成については、 heowc/action-hexo が参考になった。 GitHub Actionで用いられるDockerfile等の作成 Docker コンテナのアクションを作成する を参考に以下のファイルを作成する。 なお、 dobachi/hexo-pandoc-action に該当するファイルを置いてある。 12345[dobachi@home HexoPandocDocker]$ ls -1DockerfileREADME.mdaction.ymlentrypoint.sh Dockerfile Dockerファイル内では依存するパッケージのインストール、Pandocのインストールを実施。 NPMの必要パッケージインストールは、entrypoint側で実施するため、ここでは不要。 1234567891011121314151617181920[dobachi@home HexoPandocDocker]$ cat DockerfileFROM ubuntu:20.04# You may need to configure the time zoneENV TZ=Asia/TokyoRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone# Install minimal packages to prepare HexoRUN apt-get update &amp;&amp; \\ apt-get install -y git-core npm wget# Install Pandoc using deb packageRUN wget https://github.com/jgm/pandoc/releases/download/2.18/pandoc-2.18-1-amd64.debRUN apt-get install -y ./pandoc-2.18-1-amd64.deb# Copy script for GitHub ActionCOPY entrypoint.sh /entrypoint.sh# Configure entrypointo for GitHub ActionENTRYPOINT [&quot;/entrypoint.sh&quot;] action.yml 最低限の設定のみ使用。 1234567[dobachi@home HexoPandocDocker]$ cat action.yml# action.ymlname: &apos;Build Hexo with Pandoc&apos;description: &apos;build Hexo blog using Pandoc generator&apos;runs: using: &apos;docker&apos; image: &apos;Dockerfile&apos; entrypoint.yml npmパッケージの依存パッケージをインストールし、HTMLを生成する。 123456789[dobachi@home HexoPandocDocker]$ cat entrypoint.sh#!/bin/sh -l# Instlal Hexo and dependencies.npm install -g hexo-clinpm install# Buildhexo g GitHub Actionの設定 パーソナルトークンの生成 予め、GitHubの設定からパーソナルトークンを生成しておく。後ほど使用する。 GitHub Actionを動かしたいレポジトリのAction用トークンを設定する。 ここでは PERSONAL_TOKEN という名前で設定した。 内容は、先に作っておいたもの。 ワークフローの設定ファイル作成 GitHub Actionを使ってビルドするHexoブログのレポジトリにて、 .github/workflows/gh-pages.yml を生成する。 ここではプライベートのレポジトリで作成したHexo原稿をビルドし、 パブリックなGitHub Pages用レポジトリにpushする例を示す。 12345678910111213141516171819202122232425262728293031323334353637[dobachi@home memo-blog-text]$ cat .github/workflows/gh-pages.ymlname: GitHub Pageson: push: branches: - master # Set a branch name to trigger deployment pull_request:jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: $&#123;&#123; github.workflow &#125;&#125;-$&#123;&#123; github.ref &#125;&#125; steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod # Deploy hexo blog website. - name: Generate uses: dobachi/hexo-pandoc-action@v1.0.8 # Copy source to repository for convinience - name: copy sources run: | sudo chown -R runner public cp -r ./source ./public/source - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: $&#123;&#123; github.ref == &apos;refs/heads/master&apos; &#125;&#125; with: external_repository: dobachi/memo-blog personal_token: $&#123;&#123; secrets.PERSONAL_TOKEN &#125;&#125; publish_dir: ./public 参考 GitHub Actionsでmarkdownをpdfで出力する heowc/action-hexo Docker コンテナのアクションを作成する dobachi/hexo-pandoc-action","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"},{"name":"Pandoc","slug":"Pandoc","permalink":"https://dobachi.github.io/memo-blog/tags/Pandoc/"}]},{"title":"Nest VM on Hyper-V","slug":"Nest-VM-on-Hyper-V","date":"2022-02-03T14:47:34.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2022/02/03/Nest-VM-on-Hyper-V/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/02/03/Nest-VM-on-Hyper-V/","excerpt":"","text":"メモ 参考 メモ Get-VM を利用して、VMの名称を確認する。 12345PS C:\\Windows\\system32&gt; Get-VM (snip)ubu18 Running 0 9344 24.00:30:35.9900000 正常稼働中 9.0 入れ子になった仮想化による仮想マシンでの Hyper-V の実行 の通り、VM 名称を指定しながら設定変更する。 1PS C:\\Windows\\system32&gt; Set-VMProcessor -VMName ubu18 -ExposeVirtualizationExtensions $true 参考 Get-VM 入れ子になった仮想化による仮想マシンでの Hyper-V の実行","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hyper-V","slug":"Knowledge-Management/Hyper-V","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hyper-V/"}],"tags":[{"name":"Hyper-V","slug":"Hyper-V","permalink":"https://dobachi.github.io/memo-blog/tags/Hyper-V/"}]},{"title":"Getting_started_CKAN","slug":"Getting-started-CKAN","date":"2022-01-20T09:04:05.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2022/01/20/Getting-started-CKAN/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/01/20/Getting-started-CKAN/","excerpt":"","text":"メモ ドキュメントより 特徴 データのモデル DataStoreとDataPusher DataStoreの拡張 バックグラウンドジョブ Dockerで動作確認 セットアップ データストアとしてMinIO環境を整える AWS CLIでアクセスする CKANのUIで操作 （補足）初期化 （補足）FIWAREのCKANは拡張済？ インストール（パッケージ） 参考 CKAN MinIO AWS CLI メモ CKAN公式サイト によると、CKANはデータマネジメントシステムである。 CKAN公式サイト の内容をもとに進める。 このメモの時点では、CKAN 2.9.5を対象とする。 なお、公式ドキュメントによると CKAN is a tool for making open data websites. とされており、 あくまで「オープンデータ向け」であることが伺える。 ドキュメントより 特徴 CKANの特徴 の通り、以下のような特徴を有している。 CKAN APIを提供 バックエンドのデータストア（アドホックなデータ置き場） データソースかデータがpullされ、ストアされる Data Explorer拡張を利用することでデータのプレビューが可能になる 検索、更新などをデータ全てをダウンロード・アップロードせずにできる ★要確認 DataPusher 機能と一緒に使われることが多い データソースから表形式のデータを取得し、DataStore APIを用いてCKANにデータ登録する仕組み 拡張機能 地理空間機能 メタデータ データ管理のためのUI キーワード検索 テーマ設定 可視化 フェデレーション ファイルストア データのモデル Users, organizations and authorization に記載の通り、基本的にはOrganizationに紐づけてDatasetが登録される。 Datasetは任意のフォーマットでよい。CKANにアップロードしてもよいし、単純にURLを登録してもよい。 デフォルトの挙動では、登録されたDatasetはPrivate扱いになり、所有するOrganizationに所属するユーザのみ見られる。 Adding a new dataset にある通り、Datasetを登録できる。 なお、Licenseに並んでいる項目を見ると、やはりオープンデータとの相性がよさそう、と感じる。 DataStoreとDataPusher DataStore Extension に記載の通り、 DataStore拡張機能を利用すると、Data Explorer拡張機能と併用することで自動的にプレビュー可能になる。 またDataStore APIを利用し、データ本体を丸ごとダウンロードすることなく、検索、フィルタ、更新できる。 自動的にデータを取り込むData Pusher拡張機能と併用されることが多い。 DataPusher の通り、データを取り出し自動的にCKANに登録できる。 ただし、注意書きにある通り、制約があるので注意。 Data Dictionary の通り、DataStoreのスキーマ情報はデータ辞書で管理できる。 Downloading Resources の通り、表形式のデータをCSVやエクセルで読み込める形式でダウンロードできる。 DataStore APIを利用すると、DataStoreリソースを検索したり、インクリメンタルにデータを追加できる。 バリデーションの機能もあるようだ。 APIについては、 API reference に記載されている。 DataStoreの拡張 Extending DataStore に記載の通り、独自のDataStoreを利用可能。 正確には、実装可能。 DatastoreBackend がベースクラスである。 バックグラウンドジョブ Background jobs の通り、メインのアプリケーションをブロックせずに処理を実行できる。 Dockerで動作確認 セットアップ Installing CKAN with Docker Compose を参考に、Docker Composeを利用して簡易的に動作確認する。 事前に、DockerとDocker Composeを導入しておくこと。 また、 オープンソースのデータ管理システム「CKAN」を試してみた が分かりやすかったのでおすすめ。 ソースコードをダウンロード。 ここではステーブルバージョンを利用することにした。 1234$ mkdir -p ~/Sources$ cd ~/Sources$ git clone https://github.com/ckan/ckan.git$ git checkout -b ckan-2.9.5 tags/ckan-2.9.5 設定ファイルのコピー 適宜パスワードを変更するなど。 環境によっては、サイトURLを変更するのを忘れずに。 1$ cp contrib/docker/.env.template contrib/docker/.env ビルドと起動 12$ cd contrib/docker$ docker-compose up -d --build 後の作業のため、ストレージを変数化（しなくても、作業はできる。ボリュームのパスを覚えておけばよい） （公式サイト手順の「Convenience: paths to named volumes」に記載あり） 123456$ export VOL_CKAN_HOME=$(docker volume inspect docker_ckan_home | jq -r -c '.[] | .Mountpoint')$ echo $VOL_CKAN_HOME$ export VOL_CKAN_CONFIG=$(docker volume inspect docker_ckan_config | jq -r -c '.[] | .Mountpoint')$ echo $VOL_CKAN_CONFIG$ export VOL_CKAN_STORAGE=$(docker volume inspect docker_ckan_storage | jq -r -c '.[] | .Mountpoint')$ echo $VOL_CKAN_STORAGE Datastoreとdatapusherを機能させるため、いくつか設定する。 まずはデータベースの設定。 1$ docker exec ckan /usr/local/bin/ckan -c /etc/ckan/production.ini datastore set-permissions | docker exec -i db psql -U ckan プラグインなどの設定 1$ sudo vim $VOL_CKAN_CONFIG/production.ini なお変更点は以下の通り。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960$ sudo diff -u $VOL_CKAN_CONFIG/production.ini&#123;.org,&#125;--- /var/lib/docker/volumes/docker_ckan_config/_data/production.ini.org 2022-01-22 22:22:11.992878002 +0900+++ /var/lib/docker/volumes/docker_ckan_config/_data/production.ini 2022-01-22 22:23:56.367637134 +0900@@ -21,7 +21,7 @@ use = egg:ckan ## Development settings-ckan.devserver.host = localhost+ckan.devserver.host = ubu18 ckan.devserver.port = 5000 @@ -47,10 +47,10 @@ [2/7337] # who.timeout = 86400 ## Database Settings-sqlalchemy.url = postgresql://ckan_default:pass@localhost/ckan_default+sqlalchemy.url = postgresql://ckan_default:pass@ubu18/ckan_default-#ckan.datastore.write_url = postgresql://ckan_default:pass@localhost/datastore_default-#ckan.datastore.read_url = postgresql://datastore_default:pass@localhost/datastore_default+ckan.datastore.write_url = postgresql://ckan_default:pass@ubu18/datastore_default+ckan.datastore.read_url = postgresql://datastore_default:pass@ubu18/datastore_default # PostgreSQL' full-text search parameters ckan.datastore.default_fts_lang = english@@ -101,7 +101,7 @@ ## Redis Settings # URL to your Redis instance, including the database to be used.-#ckan.redis.url = redis://localhost:6379/0+ckan.redis.url = redis://ubu18:6379/0 ## CORS Settings@@ -119,7 +119,7 @@ # Add ``datapusher`` to enable DataPusher # Add ``resource_proxy`` to enable resorce proxying and get around the # same origin policy-ckan.plugins = stats text_view image_view recline_view+ckan.plugins = stats text_view image_view recline_view datastore datapusher # Define which views should be created by default # (plugins must be loaded in ckan.plugins)@@ -181,7 +181,7 @@ # Make sure you have set up the DataStore-#ckan.datapusher.formats = csv xls xlsx tsv application/csv application/vnd.ms-excel application/vnd.openxmlformats-officedocument.spreadsheetml.sheet+ckan.datapusher.formats = csv xls xlsx tsv application/csv application/vnd.ms-excel application/vnd.openxmlformats-officedocument.spreadsheetml.sheet #ckan.datapusher.url = http://127.0.0.1:8800/ #ckan.datapusher.assume_task_stale_after = 3600@@ -206,7 +206,7 @@ #email_to = errors@example.com #error_email_from = ckan-errors@example.com-#smtp.server = localhost+smtp.server = ubu18 #smtp.starttls = False #smtp.user = username@example.com #smtp.password = your_password ↑の変更点を説明する。 まず、今回はVM上のDockerで起動したので、SSHトンネル経由でのアクセスで困らないように開発環境ホスト名を localhost から変更した。 また、Datastore機能、Datapusher機能を利用するためにプラグインを追加。 Datapusherのフォーマットを有効化（コメントアウトを解除） コンテナを再起動。 1$ docker-compose restart 公式サイトに記載の通り、APIに試しにアクセスるとレスポンスがあるはず。 1$ curl 'http://localhost:5000/api/3/action/datastore_search?resource_id=_table_metadata' アドミンユーザ作成。 1$ docker exec -it ckan /usr/local/bin/ckan -c /etc/ckan/production.ini sysadmin add johndoe http:///ckan にアクセスすれば、ウェブUIが確認できる。 先に設定した、アドミンユーザでとりあえずログインする。 データストアとしてMinIO環境を整える MinIOのDocker を参考に、Dockerでサーバを立ち上げる。 1$ docker run -p 9000:9000 -p 9001:9001 minio/minio server /data --console-address \":9001\" http://:9000 でMinIOのウェブコンソールにアクセスできる。 特に設定していなければ、ID：minioadmin、パスワード：minioadminである。 MinIのUI MinIのUI MinIのUI 別のコンソールで mcクライアントを立ち上げる。 1$ docker run --name my-mc --hostname my-mc -it --entrypoint /bin/bash --rm minio/mc エイリアスを設定し、バケットを作成する。 （URLは、MinIO起動時にコンソールに表示されるAPI用のURLを利用すること） 12# mc alias set myminio http://172.17.0.2:9000 minioadmin minioadmin# mc mb myminio/mybucket ファイルを作成し、MinIOにアップロード（コピー）する。 12# echo \"hoge\" &gt; /tmp/hoge.txt# mc cp /tmp/hoge.txt myminio/mybucket/hoge.txt AWS CLIでアクセスする AWS CLI v2 Docker image を利用してAWS CLIを起動し、MinIOにアクセスする。 なお、コンテナで起動するのでそのままではホスト側の ~/.aws にアクセスできない。そこで -v オプションを利用しマウントしてから configure するようにしている。 また、S3プロトコルでアクセスする際には、今回は手元のMinIO環境を利用するため、エンドポイントを指定していることに注意。 123$ docker run --rm -it -v ~/.aws:/root/.aws amazon/aws-cli configure --profile myminio$ docker run --rm -it -v ~/.aws:/root/.aws amazon/aws-cli --profile myminio s3 --endpoint-url http://172.17.0.2:9000 ls s3://mybucket/hoge.txt2022-01-21 16:38:19 5 hoge.txt 無事にS3プロトコルでアクセスできたことが確かめられた。 CKANのUIで操作 組織の作成 組織作成 組織作成 組織作成 データセット作成 データセット作成 データセット作成 データセット作成 先ほどのS3プロトコルURL（ s3://mybucket/hoge.txt ）を登録する。 （補足）初期化 何らかの理由でDockerの環境をまっさらに戻したいことがあるかもしれない。 その場合は以下の通り。 1234567$ docker-compose down$ docker rmi -f docker_ckan docker_db$ docker rmi $(docker images -f dangling=true -q)$ docker volume rm docker_ckan_config docker_ckan_home docker_ckan_storage docker_pg_data docker_solr_data$ docker-compose build$ docker-compose up -d$ docker-compose restart ckan # give the db service time to initialize the db cluster on first run （補足）FIWAREのCKANは拡張済？ FIWAREのCKAN の説明を見ると、Dataset登録時に Searchable のような項目がある。公式2.9.5で試したときにはなかった。 他にも Additional Info という項目があり、そこには OAuth-Token という項目もあった。 このように、特にアクセス管理や認証認可の機能が拡張されているのか、と思った。 インストール（パッケージ） Ubuntu18環境にCKANをインストールする。 基本的には、 Installing CKAN from package に従う。 参考 CKAN CKAN公式サイト CKANの特徴 公式ドキュメント Installing CKAN with Docker Compose オープンソースのデータ管理システム「CKAN」を試してみた FIWAREのCKAN Users, organizations and authorization Adding a new dataset DataPusher Data Dictionary Downloading Resources API reference Extending DataStore DatastoreBackend Background jobs background-job-queues MinIO MinIOのDocker AWS CLI AWS CLI v2 Docker image","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Catalog","slug":"Knowledge-Management/Data-Catalog","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Catalog/"},{"name":"CKAN","slug":"Knowledge-Management/Data-Catalog/CKAN","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Catalog/CKAN/"}],"tags":[]},{"title":"Latency Guarantee of Stream Processing","slug":"Latency-Guarantee-of-Stream-Processing","date":"2022-01-17T15:49:18.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2022/01/18/Latency-Guarantee-of-Stream-Processing/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/01/18/Latency-Guarantee-of-Stream-Processing/","excerpt":"","text":"メモ 特許 論文 ソフトウェア そのほか 参考 特許 論文 メモ ストリーム処理におけるレイテンシ保証の仕組みにどんなものがあるか調査したメモ。 ストリーム処理のレイテンシとして気にするべきは、 正常時のレイテンシ 故障発生時のリカバリ含むレイテンシ が挙げられそうである。 特許 ストリームデータ処理における性能保証方法および装置 日立製作所 予め複数の計算方法を用意しておき、過去の計算結果から ユーザの要求するレイテンシ、精度を実現する計算方式に 切り替えて処理する仕組みに関する提案 切り替え判断の入力とするのは、処理量 先行となる特許が2件あるようだ。 制御装置、情報処理装置、情報処理制御方法及びコンピュータプログラム アプリケーションの遅延要件を考慮して処理する 特開2021-60753(P2021-60753A):情報処理システム、情報処理方法、および情報処理プログラム 待ち時間を考慮したストリーム処理 論文 Elastic Stream Processing with Latency Guarantees アブストラクトを読む限り、パフォーマンスを計測し、 適切なスケーリングを実行時に決定する。 これによりレイテンシ保証する。 この論文の被引用数は大きいので、これを軸に情報を探すとよいか。 Topology-Aware Task Allocation for Distributed Stream Processing with Latency Guarantee レイテンシ保証。転送レイテンシとリソース需要を勘案。 Move Fast and Meet Deadlines: Fine-grained Real-time Stream Processing with Cameo Cameoの提案。ユーザが指定したレイテンシ対象に従い、イベントのプライオリティを伝搬し制御する。 Real-Time Stream Processing in Java Java8でストリーム処理。レイテンシ保証もありそう？ Minimum Backups for Stream Processing With Recovery Latency Guarantees Elastic Stream Processing with Latency Guarantees を引用している論文として見つけた。 Fault Tolerancyにおけるレイテンシのトレードオフを扱っている。 Integrated recovery and task allocation for stream processing にもFTに関する記載あり Task Allocation for Stream Processing with Recovery Latency Guarantee Elastic Stream Processing with Latency Guarantees を引用している 故障発生時のリカバリ遅延を小さくする工夫 A Reactive Batching Strategy of Apache Kafka for Reliable Stream Processing in Real-time Elastic Stream Processing with Latency Guarantees を引用している バッチベースのストリーム処理ではバッチサイズがデータロスの様子に影響を与えることに着目し、理アクティブなバッチの仕組みを提案 InferLine: latency-aware provisioning and scaling for prediction serving pipelines ストリーム処理ではなく、機械学習の推論システムだがレイテンシアウェアな処理の話 Self-Adaptive Data Stream Processing in Geo-Distributed Computing Environments セルフアダプティブな地理分散ストリーム処理。レイテンシ保証の話とは直接関係ないが、エッジコンピューティングとの関連から。 レイテンシ保証の話ではないが、ウォーターマークに関する取り組みもある。 以下は、2021年の論文。 Watermarks in stream processing systems: semantics and comparative analysis of Apache Flink and Google cloud dataflow ソフトウェア いわゆるタイムウィンドウ処理やウォータマークの仕組みは既存の ストリーム処理OSSに採用されている。 そのほか A survey on data stream, big data and real-time ストリーム処理に関するサーベイ A Survey on the Evolution of Stream Processing Systems ストリーム処理に関するサーベイ 参考 特許 ストリームデータ処理における性能保証方法および装置 制御装置、情報処理装置、情報処理制御方法及びコンピュータプログラム 特開2021-60753(P2021-60753A):情報処理システム、情報処理方法、および情報処理プログラム 論文 Elastic Stream Processing with Latency Guarantees Task Allocation for Stream Processing with Recovery Latency Guarantee Topology-Aware Task Allocation for Distributed Stream Processing with Latency Guarantee Move Fast and Meet Deadlines: Fine-grained Real-time Stream Processing with Cameo Real-Time Stream Processing in Java Watermarks in stream processing systems: semantics and comparative analysis of Apache Flink and Google cloud dataflow Minimum Backups for Stream Processing With Recovery Latency Guarantees Integrated recovery and task allocation for stream processing A survey on data stream, big data and real-time A Survey on the Evolution of Stream Processing Systems InferLine: latency-aware provisioning and scaling for prediction serving pipelines Self-Adaptive Data Stream Processing in Geo-Distributed Computing Environments A Reactive Batching Strategy of Apache Kafka for Reliable Stream Processing in Real-time","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"}]},{"title":"Error about the memory configuration of Minikube","slug":"Error-about-the-memory-configuration-of-Minikube","date":"2022-01-10T13:43:27.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2022/01/10/Error-about-the-memory-configuration-of-Minikube/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/01/10/Error-about-the-memory-configuration-of-Minikube/","excerpt":"","text":"メモ 現状の結論（2022/1/10時点） 雑記 参考 メモ 現状の結論（2022/1/10時点） 解決まで至ってはいなく、状況整理したのみ。 雑記 minikube 1.23 Your cgroup does not allow setting memory #12842 にも記載されているエラーが以下の環境で生じた。 環境 12345678910111213141516dobachi@ubu18:~$ minikube versionminikube version: v1.24.0commit: 76b94fb3c4e8ac5062daf70d60cf03ddcc0a741bdobachi@ubu18:~$ cat /etc/lsb-releaseDISTRIB_ID=UbuntuDISTRIB_RELEASE=18.04DISTRIB_CODENAME=bionicDISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"dobachi@ubu18:~$ uname -aLinux ubu18 5.4.0-92-generic #103~18.04.2-Ubuntu SMP Wed Dec 1 16:50:36 UTC 2021 x86_64 x86_64 x86_64 GNU/Linuxdobachi@ubu18:~$ grep cgroup /proc/filesystemsnodev cgroupnodev cgroup2 上記のIssueに記載されている通り、Minikubeでは、cgroupでメモリリミットを設定できるかどうかを確認する際、 以下のような関数を利用する。 oci.go#L111-L124 1234567891011121314func HasMemoryCgroup() bool &#123; memcg := true if runtime.GOOS == \"linux\" &#123; var memory string if cgroup2, err := IsCgroup2UnifiedMode(); err == nil &amp;&amp; cgroup2 &#123; memory = \"/sys/fs/cgroup/memory/memsw.limit_in_bytes\" &#125; if _, err := os.Stat(memory); os.IsNotExist(err) &#123; klog.Warning(\"Your kernel does not support memory limit capabilities or the cgroup is not mounted.\") memcg = false &#125; &#125; return memcg&#125; 以下のファイルをstatしているのだが… 1memory = \"/sys/fs/cgroup/memory/memsw.limit_in_bytes\" これは上記環境では存在しない。 12dobachi@ubu18:~$ sudo stat /sys/fs/cgroup/memory/memsw.limit_in_bytesstat: '/sys/fs/cgroup/memory/memsw.limit_in_bytes' を stat できません: そのようなファイルやディレクトリはありません 一方、 /sys/fs/cgroup/memory/memory.soft_limit_in_bytes なら存在する。 123456789dobachi@ubu18:~$ sudo stat /sys/fs/cgroup/memory/memory.soft_limit_in_bytes File: /sys/fs/cgroup/memory/memory.soft_limit_in_bytes Size: 0 Blocks: 0 IO Block: 4096 通常の空ファイルDevice: 23h/35d Inode: 11 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2022-01-10 21:39:31.409626300 +0900Modify: 2022-01-10 21:39:31.409626300 +0900Change: 2022-01-10 21:39:31.409626300 +0900 Birth: - なお、 エラーメッセージにある通り、 Your kernel does not support cgroup swap limit capabilities にしたがってGRUP設定を変更・反映し、リブートしても状況は変わらない。 PodmanのIssueではあるが、関連する議論が podman unable to limit memory (-m flag) on Ubuntu/Debian distros #6365 に記載されている。 参考 minikube 1.23 Your cgroup does not allow setting memory #12842 oci.go#L111-L124 Your kernel does not support cgroup swap limit capabilities /sys/fs/cgroup/memory/memory.limit_in_bytes not present in Fedora 33 podman unable to limit memory (-m flag) on Ubuntu/Debian distros #6365","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Kubernetes","slug":"Knowledge-Management/Kubernetes","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://dobachi.github.io/memo-blog/tags/Kubernetes/"},{"name":"Minikube","slug":"Minikube","permalink":"https://dobachi.github.io/memo-blog/tags/Minikube/"},{"name":"Troubleshoot","slug":"Troubleshoot","permalink":"https://dobachi.github.io/memo-blog/tags/Troubleshoot/"}]},{"title":"Blogs about the portability of kubernetes ","slug":"Blogs-about-the-portability-of-kubernetes","date":"2022-01-07T01:48:03.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2022/01/07/Blogs-about-the-portability-of-kubernetes/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/01/07/Blogs-about-the-portability-of-kubernetes/","excerpt":"","text":"メモ Kubernetes を本番環境に適用するための Tips Kubernetes 8 Factors - Kubernetes クラスタの移行から学んだクラスタのポータビリティの重要性と条件 Twelve-Factor App Kubernetesを採用するべき１２の理由 Kubernetes、やめました 多分あなたにKubernetesは必要ない 「あなたにKubernetesは必要ですか？」を、Kubernetes Meetup Tokyoのコアメンバーが議論 CNDO2021、Kubernetesがない世界とある世界の違いをインフラエンジニアが解説 参考 メモ このメモは、Kubernetesを利用することで以下のポータビリティを得られるのではないか、という点に関する議論を列挙したものである。 アプリケーションのポータビリティ あるKubernetesクラスタ上にデプロイした or 可能なアプリケーションを異なるKubernetes環境で実行する Kubernetesクラスタのポータビリティ Kubernetesクラスタを様々な環境で実行する Kubernetes を本番環境に適用するための Tips Kubernetes を本番環境に適用するための Tips の中に、アプリケーションのポータビリティに関する議論が含まれている。 具体的には、 Kubernetes環境の違い Kubernetesが同じバージョンでも機能の有効・無効、使える・使えないが異なる Kubernetesのバージョンが異なると設定ファイルの書き方が同一である保証がない 具体例として、LoadBalancerやVolumeが環境によって異なる ことが挙げられた。 Kubernetes 8 Factors - Kubernetes クラスタの移行から学んだクラスタのポータビリティの重要性と条件 Kubernetes 8 Factors - Kubernetes クラスタの移行から学んだクラスタのポータビリティの重要性と条件 では、 KubernetesのBreaking Changeに関して言及されていた。 （アプリケーションのポータビリティの話もあるが、Kubernetesクラスタの運用に関する点が言及された記事） そこそこの頻度（マイナーバージョンアップにおいても、という意味）でも、Breaking Changeが生じるため、アップデートしていくのが大変だ、と。 小さな規模だったらクラスタ丸ごとデプロイしなおす（※）、という対応も可能だったが、様々な業務が載ってくるとそれも大変、と。 ※小さな規模でクラスタ丸ごと都度デプロイしなおす、ということだと、Kubernetesで抽象化する意味が…？ということも考えうるか？ Twelve-Factor App Twelve-Factor App にはアプリケーションのポータビリティの考えを含む、以下のような「Software as a Service 」を実現するにあたってのポイントが提言されている。 セットアップ自動化のために 宣言的な フォーマットを使い、プロジェクトに新しく加わった開発者が要する時間とコストを最小化する。 下層のOSへの 依存関係を明確化 し、実行環境間での 移植性を最大化 する。 モダンな クラウドプラットフォーム 上への デプロイ に適しており、サーバー管理やシステム管理を不要なものにする。 開発環境と本番環境の 差異を最小限 にし、アジリティを最大化する 継続的デプロイ を可能にする。 ツール、アーキテクチャ、開発プラクティスを大幅に変更することなく スケールアップ できる。 Kubernetesを採用するべき１２の理由 Kubernetesを採用するべき１２の理由 にはアプリケーションのポータビリティに関連するKubernetesの利点が記載されている。 一部、Kubernetesクラスタのポータビリティと考えてもよい内容もある。 理由4 仮想サーバーやベアメタルとハイブリッド構成可能な柔軟な運用基盤 理由6 オンプレとクラウドの両方で利用できる運用基盤 Kubernetes、やめました ポータビリティの直接的な議論ではないが、Kubernetes、やめました にはKubernetesを採用しなかったケースの話が記載されている。 Kubernetesの学習コストと効果を天秤にかけた結果、他の技術で目的は達成できるはず、ということ。 多分あなたにKubernetesは必要ない 多分あなたにKubernetesは必要ない では、KubernetesではなくNomadを利用することにした経緯が記載されている。 Nomadは「オーケストレータ」だがそれで用が足りた、ということのようだ。 もしあなたが大規模インフラ上に同一サービス群をデプロイする計画をしているのであれば、Kubernetesという道を選んで良いでしょう。ただ、複雑性や運用コストが増えることを肝に命じてください。そしてこういったコストの一部は、Google Kubernetes EngineやAmazon EKSのようなマネージドKubernetes環境を利用することで省くことができます。 「あなたにKubernetesは必要ですか？」を、Kubernetes Meetup Tokyoのコアメンバーが議論 「あなたにKubernetesは必要ですか？」を、Kubernetes Meetup Tokyoのコアメンバーが議論 では、 多分あなたにKubernetesは必要ない の記事を受けて行われた、 日本でKubernetesを本格運用している企業の著名エンジニアによるパネルディスカッションの様子が記載されている。 なお、個人的に気になったのは以下の下り。 青山氏は、Kubernetesでも、CI/CDツールなどとうまく組み合わせることで、カスタマイズできるPaaSに近い環境を作ることができるとした。また、五十嵐氏は、「開発者と運用支援担当が別チームとして役割分担しやすい」というKubernetesの特徴を生かせれば、開発者が最小限の設定で使えるようにできると話した。 ルイス氏は、Kubernetes上でPaaSやFaaSが稼働するような動きが今後進むことで、一部の懸念は解消に向かうのではないかとしている。 PaaSやFaaSを実現する基盤技術は、単体でクラスタ構成可能なものもあるため、Kubernetes上に載せることで「抽象化の層」が増えることになる。 抽象化の恩恵と複雑性増加の辛さを天秤にかけることになるだろう。 CNDO2021、Kubernetesがない世界とある世界の違いをインフラエンジニアが解説 ポータビリティはあまり関係ない。 CNDO2021、Kubernetesがない世界とある世界の違いをインフラエンジニアが解説 では、 「CI/CDまではできているがKubernetesがない」場合と、Kubernetesがある場合を比較し、 「Kubernetesがなくてもクラウドネイティブ化の目的は達成できうるが、結局はKubernetesらが実現していることを自分でやらないといけない」とした。 なお、クラウドネイティブ化の目的については、CNCFの定義が引用されていた。 素早く継続的にユーザーに価値を届ける 参考 Kubernetes を本番環境に適用するための Tips Kubernetes 8 Factors - Kubernetes クラスタの移行から学んだクラスタのポータビリティの重要性と条件 Twelve-Factor App Kubernetesを採用するべき１２の理由 Kubernetes、やめました 多分あなたにKubernetesは必要ない 「あなたにKubernetesは必要ですか？」を、Kubernetes Meetup Tokyoのコアメンバーが議論 CNDO2021、Kubernetesがない世界とある世界の違いをインフラエンジニアが解説","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Kubernetes","slug":"Knowledge-Management/Kubernetes","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://dobachi.github.io/memo-blog/tags/Kubernetes/"}]},{"title":"Getting started Spark on k8s","slug":"Getting-started-Spark-on-k8s","date":"2022-01-07T01:20:58.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2022/01/07/Getting-started-Spark-on-k8s/","link":"","permalink":"https://dobachi.github.io/memo-blog/2022/01/07/Getting-started-Spark-on-k8s/","excerpt":"","text":"メモ 基本的な流れ ビルド 実行 （補足）ビルドされたイメージを起動して内容確認 （補足）Minikube使う場合のリソース設定についての注意事項 （補足）Kubernetes環境 （補足）ボリュームのマウントについて （補足）上記例ではサービスアカウントを作成しているが・・・ 参考 メモ このメモは、Spark3.2.0をKubernetes上で動かすことを簡単に紹介するものである。 なお、日本語での説明としては、 Apache Spark on Kubernetes入門（Open Source Conference 2021 Online Hiroshima 発表資料） がとても分かりやすいので参考になる。 基本的な流れ 公式の Running Spark on Kubernetes の通りでよい。 ビルド まずは、パッケージに含まれているDockerfileを利用して、Dockerイメージを自分でビルドする。 今回はMinikube環境で動かしているので -m オプションを利用した。 簡易的な例 1$ /opt/spark/default/bin/docker-image-tool.sh -m -t testing build Minikube内のDockerでイメージ一覧を確認すると以下の通り。 12345$ eval $(minikube -p minikube docker-env)$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEspark testing e19cebbf23e7 44 minutes ago 602MB(snip) 実行 サービスアカウントを作る。 123456$ minikube kubectl -- create serviceaccount spark$ minikube kubectl -- create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default$ minikube kubectl -- get serviceaccountNAME SECRETS AGEdefault 1 179mspark 1 2m39s 実行は以下の通り。先ほど作ったサービスアカウントを使用するようにする。 1$ /opt/spark/default/bin/spark-submit --master k8s://https://192.168.49.2:8443 --deploy-mode cluster --name pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=2 --conf spark.kubernetes.container.image=spark:testing --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark local:///opt/spark/examples/jars/spark-examples_2.12-3.2.0.jar なお、今回はMinikube上で実行しており、Jarファイルとして指定するのはMinikube内で起動したドライバのコンテナ内のローカルファイルシステムパスである。試しに、当該近店をアタッチして起動するとわかる。 ドライバのログを確認する。 12345678$ minikube kubectl -- logs pi-43bff77e450bdba3-driver(snip)22/01/10 17:32:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished22/01/10 17:32:02 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.824388 sPi is roughly 3.14247571237856222/01/10 17:32:02 INFO SparkUI: Stopped Spark web UI at http://pi-43bff77e450bdba3-driver-svc.default.svc:404022/01/10 17:32:02 INFO KubernetesClusterSchedulerBackend: Shutting down all executors(snip) pi計算の結果がログに出力されているのがわかる。 （補足）ビルドされたイメージを起動して内容確認 以下は、MinikubeのDockerを利用しビルドしたイメージのコンテナを起動して、シェルをアタッチした例。 12$ eval $(minikube -p minikube docker-env)$ docker run -it --rm spark:testing /bin/bash （補足）Minikube使う場合のリソース設定についての注意事項 Prerequisites の通り、Minikube等を使うようであれば、リソースに対して注意がある。 以下、Minikube立ち上げ例。 1$ minikube start --memory='4g' --cpus=3 （補足）Kubernetes環境 動作確認のためには、Kubernetes環境が必要である。 minikube start あたりを参考に環境構築しておくこと。 （補足）ボリュームのマウントについて volume-mounts の通り、DriverやExecutorのPodにボリュームをマウントできるのだが、 HostPathに関するリスクがあるようだ。 KubernetesのhostPathについて を参照。 （補足）上記例ではサービスアカウントを作成しているが・・・ もし default サービスアカウントを利用すると以下に記載されたのと同様のエラーを生じる。 How to fix \"Forbidden!Configured service account doesn't have access\" with Spark on Kubernetes? そこで、あらかじめサービスアカウントを作成して使うようにした。 参考 Running Spark on Kubernetes volume-mounts KubernetesのhostPathについて minikube start Apache Spark on Kubernetes入門（Open Source Conference 2021 Online Hiroshima 発表資料） How to fix \"Forbidden!Configured service account doesn't have access\" with Spark on Kubernetes?","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Spark","slug":"Knowledge-Management/Spark","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Spark/"}],"tags":[{"name":"Apache Spark","slug":"Apache-Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Apache-Spark/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://dobachi.github.io/memo-blog/tags/Kubernetes/"}]},{"title":"Delta Sharing with MinIO","slug":"Delta-Sharing-with-MinIO","date":"2021-10-22T02:19:39.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2021/10/22/Delta-Sharing-with-MinIO/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/10/22/Delta-Sharing-with-MinIO/","excerpt":"","text":"メモ MinIOの起動 Delta Sharingサーバの起動 リリースされたパッケージを利用する場合 現在のmainブランチを利用する場合 設定ファイルの作成 Delta Sharingサーバの起動 Delta SharingのPythonクライアントを利用してアクセスする。 トラブルシュート パス指定方法の誤り 参考 MinIO Delta Sharing Sample code メモ Delta SharingはAWS S3、Azure Blob Storage、Azure Data Lake Storage Gen2に対応している。 そこで、AWS S3互換のストレージソフトウェアだったら使えるのではないか、ということで、 MinIO を利用してみることにする。 もろもろのサンプルコードは dobachi DeltaSharingMinioExample にある。 MinIOの起動 MinIO Quickstart Guide を参考に、手元でMinIOサーバを構成する。 以下は手順の一例。 バイナリをダウンロードし、データ保存ディレクトリを作って立ち上げる。 123456$ mkdir -p ~/Minio$ cd ~/Minio$ wget https://dl.min.io/server/minio/release/linux-amd64/minio$ mkdir -p data$ chmod +x minio$ ./minio server data コンソールにメッセージが色々出る。 今回はテスト用なので特段指定しなかったが、Rootユーザの名称、パスワードが表示されているはずである。 これは後ほど、AWS S3プロトコルでMinIOサービスにアクセスする際のID、シークレットキーとしても利用する。 MinIOのクライアントをダウンロードし、エイリアスを設定する。 別のターミナルを開く。 1234$ cd ~/Minio$ wget https://dl.min.io/client/mc/release/linux-amd64/mc$ chmod +x mc$ ./mc alias set myminio http://your_host:9000 minioadmin minioadmin your_hostのところは各自の環境に合わせて変更してほしい。 これにより、myminio という名前でエイリアスが作成された。 ちなみに、http://your_host:40915/ からアクセスできる。 なお、試しにAWS S3クライアントで接続してみることにする。 1234567$ ./mc mb data/test_bucket$ echo 'test' &gt; test.txt$ export AWS_ACCESS_KEY_ID=minioadmin$ export AWS_SECRET_ACCESS_KEY=minioadmin$ aws --endpoint-url http://localhost:9000 s3 ls2021-10-22 12:26:13 test_bucket$ aws --endpoint-url http://localhost:9000 s3 cp test.txt s3://test_bucket/test.txt ここで、今回はMinioを利用していることから、エンドポイントURLを指定していることに注意。 テストバケットに入ったデータ Delta Sharingサーバの起動 Delta Sharing Reference Server を参考に、サーバを起動する。 リリースされたパッケージを利用する場合 パッケージを公式リリースからダウンロードして展開する。 12345$ mkdir -p ~/DeltaSharing$ cd ~/DeltaSharing$ wget https://github.com/delta-io/delta-sharing/releases/download/v0.2.0/delta-sharing-server-0.2.0.zip$ unzip delta-sharing-server-0.2.0.zip$ cd delta-sharing-server-0.2.0 現在のmainブランチを利用する場合 12345$ mkdir -p ~/Sources/$ cd ~/Sources$ git clone git@github.com:delta-io/delta-sharing.git$ delta-sharing $ ./build/sbt server/universal:packageBin server/target/universal/delta-sharing-server-x.y.z-SNAPSHOT.zip にパッケージが保存される。 なお、x.y.zにはバージョン情報が入る。ここでは0.3.0とする。 1234$ cp server/target/universal/delta-sharing-server-x.y.z-SNAPSHOT.zip ~/DeltaSharing/$ cd ~/DeltaSharing/$ unzip delta-sharing-server-0.3.0-SNAPSHOT.zip$ cd delta-sharing-server-0.3.0-SNAPSHOT 設定ファイルの作成 Delta Sharingはデータストアのアクセスについて、間接的にHadoopライブラリに依存している。 そこで、AWS S3プロトコルでアクセスするためのHadoop設定ファイル core-site.xml を作成する。 conf/core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.s3a.endpoint&lt;/name&gt; &lt;value&gt;http://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ここではエンドポイントURL、パス指定方法の設定をしている。 なおパス指定方法の設定行わないと、Delta SharingからMinIOサーバにリクエストを送る際に、 ホスト名が bucket_name.host のような指定になってしまい、 400 But Reuqest エラーを生じてしまう。 詳しくは、 トラブルシュートの節を参照。 続いて、Delta Sharingサーバの設定ファイルを作成する。 以下は参考。 conf/delta-sharing-server.yml 12345678910111213141516171819202122232425# The format version of this config fileversion: 1# Config shares/schemas/tables to shareshares:- name: \"share1\" schemas: - name: \"schema1\" tables: - name: \"table1\" location: \"s3a://test_bucket/delta_table\"# Set the host name that the server will usehost: \"localhost\"# Set the port that the server will listen onport: 18080# Set the url prefix for the REST APIsendpoint: \"/delta-sharing\"# Set the timeout of S3 presigned url in secondspreSignedUrlTimeoutSeconds: 900# How many tables to cache in the serverdeltaTableCacheSize: 10# Whether we can accept working with a stale version of the table. This is useful when sharing# static tables that will never be changed.stalenessAcceptable: false# Whether to evaluate user provided `predicateHints`evaluatePredicateHints: false ここで s3a://test_bucket/delta_table に指定しているのがMinIO内のデータである。 ここでは予め Delta Lakeフォーマットのデータを保存してあるものとする。 Delta Sharingサーバの起動 1$ ./bin/delta-sharing-server -- --conf conf/delta-sharing-server.yaml Delta SharingのPythonクライアントを利用してアクセスする。 まずクライアント用のプロファイルを準備する。 minio.share 12345&#123; \"shareCredentialsVersion\": 1, \"endpoint\": \"http://localhost:18080/delta-sharing/\", \"bearerToken\": \"\"&#125; これを利用しながら、テーブルにアクセスする。 Delta Sharingのクライアントでアクセス ここでは試しにPandas DataFrameとして読み取っている。 トラブルシュート パス指定方法の誤り Hadoop設定上で、パス指定方法の設定を行わないと、以下のようなエラーが生じる。 Delta Sharingサーバ側のエラー 123456789101112131415161718Caused by: org.apache.hadoop.fs.s3a.AWSS3IOException: doesBucketExist on test: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: null; S3 Extended Request ID: null), S3 Extended Request ID: null: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: null; S3 Extended Request ID: null) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:194) at org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:335) at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:280) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3247) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3296) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3264) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:475) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) at io.delta.standalone.internal.DeltaSharedTable.$anonfun$deltaLog$1(DeltaSharedTableLoader.scala:76) at io.delta.standalone.internal.DeltaSharedTable.withClassLoader(DeltaSharedTableLoader.scala:95) at io.delta.standalone.internal.DeltaSharedTable.&lt;init&gt;(DeltaSharedTableLoader.scala:74) at io.delta.standalone.internal.DeltaSharedTableLoader.$anonfun$loadTable$1(DeltaSharedTableLoader.scala:53) at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4693) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3445) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2194) ... 60 more Minioサーバ側でのエラー 1$ ./mc admin trace myminio --debug -v 1234567891011121314151617181920212223test.localhost:9000 [REQUEST methodNotAllowedHandler.func1] [2021-10-22T03:57:08:000] [Client IP: 127.0.0.1]test.localhost:9000 HEAD /test.localhost:9000 Proto: HTTP/1.1test.localhost:9000 Host: test.localhost:9000test.localhost:9000 Amz-Sdk-Invocation-Id: 821962e2-e1f8-31d7-4d0d-431529a3725ctest.localhost:9000 Authorization: AWS4-HMAC-SHA256 Credential=minioadmin/20211022/us-east-1/s3/aws4_request, SignedHeaders=amz-sdk-invocation-id;amz-sdk-retry;content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=07977380f3fd92b149e8c60937554fc5ee5287a0863c101431ef51aa3968c37btest.localhost:9000 Connection: Keep-Alivetest.localhost:9000 Content-Type: application/octet-streamtest.localhost:9000 X-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855test.localhost:9000 Amz-Sdk-Retry: 0/0/500test.localhost:9000 Content-Length: 0test.localhost:9000 User-Agent: Hadoop 2.10.1, aws-sdk-java/1.11.271 Linux/4.19.104-microsoft-standard OpenJDK_64-Bit_Server_VM/11.0.12+7-LTS java/11.0.12 scala/2.12.10test.localhost:9000 X-Amz-Date: 20211022T035708Ztest.localhost:9000test.localhost:9000 [RESPONSE] [2021-10-22T03:57:08:000] [ Duration 66µs ↑ 137 B ↓ 375 B ]test.localhost:9000 400 Bad Requesttest.localhost:9000 Content-Type: application/xmltest.localhost:9000 Server: MinIOtest.localhost:9000 Vary: Origintest.localhost:9000 Accept-Ranges: bytestest.localhost:9000 Content-Length: 261test.localhost:9000 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;BadRequest&lt;/Code&gt;&lt;Message&gt;An error occurred when parsing the HTTP request HEAD at &amp;#39;/&amp;#39;&lt;/Message&gt;&lt;Resource&gt;/&lt;/Resource&gt;&lt;RequestId&gt;&lt;/RequestId&gt;&lt;HostId&gt;baec9ee7-bfb0-441b-a70a-493bfd80d745&lt;/HostId&gt;&lt;/Error&gt; 参考 MinIO MinIO MinIO Quickstart Guide Delta Sharing Delta Sharing Reference Server Sample code dobachi DeltaSharingMinioExample","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Collaboration","slug":"Knowledge-Management/Data-Collaboration","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/"},{"name":"Delta Sharing","slug":"Knowledge-Management/Data-Collaboration/Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/"}],"tags":[{"name":"Delta Sharing","slug":"Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Sharing/"},{"name":"Minio","slug":"Minio","permalink":"https://dobachi.github.io/memo-blog/tags/Minio/"}]},{"title":"Getting Started of Delta Sharing","slug":"Getting-Started-of-Delta-Sharing","date":"2021-09-10T02:14:58.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2021/09/10/Getting-Started-of-Delta-Sharing/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/09/10/Getting-Started-of-Delta-Sharing/","excerpt":"","text":"参考 メモ 特徴 リファレンスサーバを動かす 前提 サンプルスクリプト 基本的な動作 プロファイルの場所にURLを指定できるか？ 認可 REST APIでアクセス 参考 公式GitHub Autorization REST API Shareの取得 Tableバージョンの取得 テーブルメタデータの取得 テーブルデータの読み出し Delta Sharing Example CreateDeltaTableS3 pyspark_jupyter PythonConnectorExample TestProfileOnS3 メモ 特徴 特徴については以下を参照。 リファレンスサーバを動かす 公式GitHub のREADMEを参考に、リファレンスサーバを動かす。 前提 OS: CentOS Linux release 7.8.2003 (Core)、CentOS Linux release 7.9.2009 (Core) 必要なライブラリ bzip2-devel、readline-devel、openssl-devel、sqlite-devel、libffi-devel Python: 3.7.10 pipでjupyter labを入れておく。 Spark:3.1.2 w/ Hadoop3.2 pipか公式サイトからダウンロードしたパッケージを利用してインストールしておく。 今回は簡易的な動作確認のため、Spark単体（ローカルモード）で動作させる。Hadoopとの連係はさせない。 サンプルスクリプト Delta Sharing Example にこの記事で取り扱うサンプルスクリプト（Jupyterのノートブック）を置いてある。 なお、このプロジェクトには、Jupyter LabをPySparkと一緒に起動するサンプル補助スクリプト ./bin/pyspark_jupyter.sh が入っている。 適宜編集して利用されたし。 以降の手順では、このノートブック群を利用した例を示す。 利用する場合はサブモジュールごと以下のようにクローンすると便利。 123$ mkdir -p -/Sources$ cd -/Sources$ git clone --recursive https://github.com/dobachi/delta_sharing_example.git .profileについて なお、この補助スクリプトは同一ディレクトリに .profile があれば、それを読み込むようになっている。 特に、環境変数 OPTIONS や S3_TEST_URL に個人的な値を設定するために利用するとよい。 .profileで設定する環境変数について OPTIONS : Jupyter Lab起動時に、Sparkに渡すオプションを設定するために用意した。 S3_TEST_URL : ノートブック内で読み書き動作確認用に用いるS3のURLを設定するために用意した。 基本的な動作 以下の流れで試す。 S3上にサンプルデータを作成する 手元のマシンでDelta Sharingのサーバを立ち上げる githubからクローンしたライブラリをローカル環境にインストール S3に置いたサンプルデータをDelta Sharingのサーバ経由で取得する S3上にサンプルデータを作成する まずはS3上にデータを置く。なんの手段でも良いが、S3へのアクセスロールを持つEC2インスタンス上で CreateDeltaTableS3 を実行する。 なお、PySparkをJupyter Labで起動する補助するスクリプトの例がpyspark_jupyterに載っている。 CreateDeltaTableS3 のレポジトリにおいても当該スクリプトがサブモジュールとして読み込まれる。 手元のマシンでDelta Sharingのサーバを立ち上げる つづいて、Delta Sharingのソースコードをクローンする。 なお、公式でリリースされたパッケージを用いてもよいのだが、 Delta Sharingはまだプロダクトが若く、変更も多いためmainブランチを パッケージ化して用いることにする。 12345$ mkdir -p -/Sources$ cd -/Sources$ git clone https://github.com/delta-io/delta-sharing.git$ cd delta-sharing$ ./build/sbt server/universal:packageBin これで server/target/universal/delta-sharing-server-0.3.0-SNAPSHOT.zip ができたはず。（2021/9現在。これ以降だと、バージョンが上がっている可能性がある） これを適当なディレクトリに展開して用いるようにする。 12345$ mkdir -p ~/DeltaSharing$ cp server/target/universal/delta-sharing-server-0.3.0-SNAPSHOT.zip ~/DeltaSharing$ cd ~/DeltaSharing/$ if [ -d delta-sharing-server-0.3.0-SNAPSHOT ]; then rm -r delta-sharing-server-0.3.0-SNAPSHOT; fi$ unzip delta-sharing-server-0.3.0-SNAPSHOT.zip 展開したパッケージの中に、設定のテンプレートが入っているのでコピーして 自分の環境に合わせて編集する。 123$ cd ~/DeltaSharing/delta-sharing-server-0.2.0-SNAPSHOT$ cp conf/delta-sharing-server.yaml&#123;.template,&#125;$ vim conf/delta-sharing-server.yaml 設定ファイルの例 12345678910111213141516171819202122232425# The format version of this config fileversion: 1# Config shares/schemas/tables to shareshares:- name: \"share1\" schemas: - name: \"schema1\" tables: - name: \"table1\" location: \"s3a://&lt;your configuration&gt;\"# Set the host name that the server will usehost: \"localhost\"# Set the port that the server will listen onport: 80# Set the url prefix for the REST APIsendpoint: \"/delta-sharing\"# Set the timeout of S3 presigned url in secondspreSignedUrlTimeoutSeconds: 900# How many tables to cache in the serverdeltaTableCacheSize: 10# Whether we can accept working with a stale version of the table. This is useful when sharing# static tables that will never be changed.stalenessAcceptable: false# Whether to evaluate user provided `predicateHints`evaluatePredicateHints: false サーバを起動する。 1$ ./bin/delta-sharing-server -- --config conf/delta-sharing-server.yaml githubからクローンしたライブラリをローカル環境にインストール 起動したサーバとは別のターミナルを開き、Pythonクライアントを試す。 先ほどクローンしたDelta Sharingのレポジトリを利用し、 venvなどで構築した環境下にpipでdelta sharingのPythonクライアントライブラリをインストールする。 1$ pip install ~/Sources/delta-sharing/python/ なお、もしすでに一度インストールしたことがあるようであれば、アップデートするようにするなど工夫すること。 つづいて、Spark用のパッケージを作る。 12$ cd ~/Sources/delta-sharing$ ./build/sbt spark/package spark/target/scala-2.12/delta-sharing-spark_2.12-0.3.0-SNAPSHOT.jar にJarファイルができる。 これをコネクタの起動時にロードするようにする。 例えば、PySparkのJupyter Lab起動時に以下のようなオプションを渡す。 1--jars /home/centos/Sources/delta-sharing/spark/target/scala-2.12/delta-sharing-spark_2.12-0.3.0-SNAPSHOT.jar のような S3に置いたサンプルデータをDelta Sharingのサーバ経由で取得する PythonConnectorExample にPythonのクライアントライブラリを用いた例を示す。 Pandas DataFrameで取得する例を掲載している。 上記スクリプト内でも利用している通り、クライアントがアクセスするためには、 以下のようなプロファイルを渡す必要がある。 12345&#123; \"shareCredentialsVersion\": 1, \"endpoint\": \"http://localhost:80/delta-sharing/\", \"bearerToken\": \"\"&#125; このプロファイルは以下の通り、ファイルのPATH等を渡すか、delta_sharing.protocol.DeltaSharingProfileインスタンスを渡すかすれば良さそう。 後者の場合、JSONテキストから生成できる。 delta_sharing/delta_sharing.py:92 1def __init__(self, profile: Union[str, BinaryIO, TextIO, Path, DeltaSharingProfile]): 一方、 delta_sharing.delta_sharing.load_as_pandas メソッドを用いて、 Pandas DataFrameで取得する場合は、その引数に渡すプロファイルはURLのテキストが期待されている。 12345678910111213def load_as_pandas(url: str) -&gt; pd.DataFrame: \"\"\" Load the shared table using the give url as a pandas DataFrame. :param url: a url under the format \"&lt;profile&gt;#&lt;share&gt;.&lt;schema&gt;.&lt;table&gt;\" :return: A pandas DataFrame representing the shared table. \"\"\" profile_json, share, schema, table = _parse_url(url) profile = DeltaSharingProfile.read_from_file(profile_json) return DeltaSharingReader( table=Table(name=table, share=share, schema=schema), rest_client=DataSharingRestClient(profile), ).to_pandas() クライアントと同様に、JSONのテキストでも受け付けられるようにしたら便利か。 そもそもプロファイルとスキーマ等の指定が必ずしもひとつのURLになっていなくてもよいのでは…？と思う節もある。 が、共有は基本的にすべてURLで…という統一性を大事にするのも分かる。 プロファイルの場所にURLを指定できるか？ Spark Connector利用時は、例えばS3に置いたプロファイルを使用できるか？の確認をする。 ◇補足： というのも、Spark Connectorを利用している際、Sparkのコンフィグにて、 HADOOP_HOME を設定し、Hadoopを利用するようにしてみたらどうやらHDFS を探しに行っているようだったため、SparkのAPIを通じてプロファイルを読みに行っているのだとしたら、S3等に置かれたプロファイルを読めるはずだ、と考えたため。 ここでは、 s3://hoge/fuga/deltasharing.json のようなURLを渡すことにする。 結論から言えば、delta sharingはプロファイルの読み出しにfsspecを利用しているため、仕様上はリモートのファイルを読み出せるようになっている。 ここで実行したノートブックは TestProfileOnS3 に置いてある。 delta sharing clientの生成 まず、共有データ一覧を取得するために用いるクライアントだが、 1client = delta_sharing.SharingClient(profile_file) の引数にS3のURLを渡したら、以下のエラーになった。 1ImportError: Install s3fs to access S3 これは、delta sharing内で用いられるfsspecにより出された例外である。 python/delta_sharing/protocol.py:41 123456789101112@staticmethoddef read_from_file(profile: Union[str, IO, Path]) -&gt; \"DeltaSharingProfile\": if isinstance(profile, str): infile = fsspec.open(profile).open() elif isinstance(profile, Path): infile = fsspec.open(profile.as_uri()).open() else: infile = profile try: return DeltaSharingProfile.from_json(infile.read()) finally: infile.close() ということで、Python環境にpipでs3fsをインストールしてからもう一度試したところ、ひとまず動作した。 fsspecを利用しているということは、仕様上はリモートに置いてあるファイルシステムにも対応可能である、ということだった。 Pandas DataFrame、Spark DataFrameそれぞれへの読み出しについて、動作した。 認可 Autorization の通り、Bearer認証を利用できるようだ。 REST APIでアクセス REST API を参考に確認する。 Shareのリスト ひとまず一番簡単な、Share一覧を取得する。 12345678$ curl http://127.0.0.1:80/delta-sharing/shares | jq&#123; \"items\": [ &#123; \"name\": \"share1\" &#125; ]&#125; テーブルバージョンの取得 Tableバージョンの取得 の通り。 なお、なぜか2行出力される。 1234567$ curl -I -D - http://127.0.0.1:80/delta-sharing/shares/share1/schemas/schema1/tables/table1HTTP/1.1 200 OKHTTP/1.1 200 OKdelta-table-version: 4delta-table-version: 4content-length: 0content-length: 0 Shareの取得にあるようにクエリパラメタとして maxResult やページングの情報を渡せる。 メタデータの取得 テーブルメタデータの取得 の通り。 12345678910111213141516171819$ curl http://127.0.0.1:80/delta-sharing/shares/share1/schemas/schema1/tables/table1/metadata | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 271 0 271 0 0 919 0 --:--:-- --:--:-- --:--:-- 921&#123; \"protocol\": &#123; \"minReaderVersion\": 1 &#125;&#125;&#123; \"metaData\": &#123; \"id\": \"cea27a35-d139-4a74-a5f7-5596985784b8\", \"format\": &#123; \"provider\": \"parquet\" &#125;, \"schemaString\": \"&#123;\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[&#123;\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":&#123;&#125;&#125;]&#125;\", \"partitionColumns\": [] &#125;&#125; データの取得 テーブルデータの読み出し に従うと、「ヒント句」を渡しながら、データ（のアクセスURL）を取得できるようだ。 ドキュメントを読む限り、このヒントが働くかどうかはベストエフォートとのこと。 1234567$ curl -X POST -H &quot;Content-Type: application/json; charset=utf-8&quot; http://127.0.0.1:80/delta-sharing/shares/share1/schemas/schema1/tables/table1/query -d @- &lt;&lt; EOL&#123; &quot;predicateHints&quot;: [ &quot;id &lt; 1&quot; ], &quot;limitHint&quot;: 1&#125;EOL このヒントの働きについては、別途調査する。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Collaboration","slug":"Knowledge-Management/Data-Collaboration","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/"},{"name":"Delta Sharing","slug":"Knowledge-Management/Data-Collaboration/Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/"}],"tags":[{"name":"Delta Sharing","slug":"Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Sharing/"}]},{"title":"X-Road","slug":"X-Road","date":"2021-09-03T13:58:43.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2021/09/03/X-Road/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/09/03/X-Road/","excerpt":"","text":"参考 メモ X-Roadとは？ 最初の動作確認 Ansibleプレイブックの確認 起動した環境の動作確認 コンポーネントの確認 Security Server 参考 X-RoadのGitHub X-Roadについての簡易メモ Using LXD-hosts X Road Academy Session 1 Setting up a local X-Road development environment [X-Road: Security Server Architecture] X-Road Security Server Architecture Overview X-Road Security Server Architecture Process View メモ X-Roadとは？ 一言で言えば、エストニアの電子政府や公共と民間の間のデータ連携に用いられている データ流通のためのオープンソースソフトウェアである。 他の簡単な紹介は X-Roadについての簡易メモ あたりを参照。 最初の動作確認 X-RoadのGitHub のREADMEによると、AnsibleのPlaybookがある。 READMEから簡単に読み解いてみる。 Ansibleプレイブックの確認 インベントリ ansible/hosts/example_xroad_hosts.txt によると、 central server（cs）、security server（ss）、configuration proxy、certification authorityの グループが定義されており、csやssは複数台のノードが記載されている。冗長化されている？ ansible/hosts/lxd_hosts.txt はコンテナ向けに見える。 なお、エストニア等の国？エリア？に特化するかどうか、という変数がある。 該当しない場合は、 vanilla でよさそう。 プレイブック（エントリポイント） デプロイするときは xroad_init.yml を利用する。 ロール プレイブックの通り、 xroad-cs や xroad-cp といったロールが各種類のサーバに対応したロールである。 また、このロールは依存関係に 12dependencies: - &#123; role: xroad-base &#125; のようなものを持っており、 xroad-base がレポジトリ等の設定をする役割を担っている。 LXDコンテナを利用した動作確認用のデプロイ Using LXD-hosts によると、LXDコンテナを使用し動作確認用のX-Road環境を構築できる。 ansible/hosts/lxd_hosts.txt がインベントリファイル（の例）である。 中で定義されているコンテナは、 ansible/roles/init-lxd ロールで起動される。 Ubuntu18で試す。 あらかじめLXD環境をセットアップしておく。 123$ sudo apt-get install lxd$ newgrp lxd$ sudo lxd init 特に問題なければLXDコンテナが起動できるはず。詳しくは、 https://linuxcontainers.org/ja/lxd/getting-started-cli あたりを参照。 Ansibleもインストールしておく。 1234$ sudo apt update$ sudo apt install software-properties-common$ sudo apt-add-repository --yes --update ppa:ansible/ansible$ sudo apt install ansible 詳しくは、 https://docs.ansible.com/ansible/2.9_ja/installation_guide/intro_installation.html#ubuntu-ansible あたりを参照。 X-Roadのコンテナを準備する。 1$ ansible-playbook -i hosts/lxd_hosts.txt xroad_init.yml 実際に以下のようなコンテナが立ち上がる。 12345678$ lxc list+---------------+---------+-----------------------+-----------------------------------------------+------------+-----------+| NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS |+---------------+---------+-----------------------+-----------------------------------------------+------------+-----------+| xroad-lxd-cs | RUNNING | xx.xx.xx.xx (eth0) | xxxx:xxxx:xxxx:xxxx:xxx:xxxx:xxxx:xxxx (eth0) | PERSISTENT | 0 |+---------------+---------+-----------------------+-----------------------------------------------+------------+-----------+| xroad-lxd-ss1 | RUNNING | xx.xx.xx.xx (eth0) | xxxx:xxxx:xxxx:xxxx:xxx:xxxx:xxxx:xxxx (eth0) | PERSISTENT | 0 |+---------------+---------+-----------------------+-----------------------------------------------+------------+-----------+ なお、パッケージインストール（内部的にはaptを利用してインストール）している箇所の進みが遅く、一度インタラプトして手動でaptコマンドでインストールした。 開発用のプレイブック 開発用のAnsibleプレイブックもある。 ansible/xroad_dev.yml や ansible/xroad_dev_partial.yml である。 起動した環境の動作確認 X Road Academy Session 1 Setting up a local X-Road development environmentが参考になりそう。 コンポーネントの確認 Security Server X-Road Security Server Architecture にSecurity Serverのアーキテクチャが記載されている。 特に X-Road Security Server Architecture Overview に記載されている図が分かりやすい。 Security Serverはサービスクライアントとプロバイダの仲介を担う。 電子署名と暗号を用いてメッセージのやり取りは保護される。 Proxyがメッセージを受けとるのだが、メッセージは署名と一緒に保存される。 設定情報はPostgreSQL内に保持される。 設定はユーザインターフェースを通じて行われるようになっている。 グローバルコンフィグレーションをダウンロードするConfiguration Client というのもあるようだ。 そのほかのコンポーネントは名称の通りの機能である。 X-Road Security Server Architecture Process View に処理の流れが記載されている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Collaboration","slug":"Knowledge-Management/Data-Collaboration","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/"},{"name":"X-Road","slug":"Knowledge-Management/Data-Collaboration/X-Road","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/X-Road/"}],"tags":[{"name":"X-Road","slug":"X-Road","permalink":"https://dobachi.github.io/memo-blog/tags/X-Road/"}]},{"title":"Softwares for Data Sharing","slug":"Software-for-Data-Sharing","date":"2021-08-30T07:01:26.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/08/30/Software-for-Data-Sharing/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/08/30/Software-for-Data-Sharing/","excerpt":"","text":"参考 共有対象の情報 まとめ ソリューション OSS 組織 メモ そもそもデータ共有対象のデータって何があるのか？ 機密性を保持するべきデータ オープンなデータ まとめ系の情報源 データ共有のためのオープンソースソフトウェア CKAN Eclipse Dataspace Connector X-Road ソリューションに付帯されるデータ共有の仕組み Snowflake Data Sharing S3 Access Points AWS Storage Gateway AWS上で素朴にS3バケットポリシーを使ってアカウント間共有 AWS Redshift Data Sharing Treasure Data Data Exchange PlanetCross （関連）Nortal社 Aktors社 データエスクロー（第三者預託） Crossbeam codekeeper データコラボレーションに関する研究機関 AIDAC セキュリティの情報共有 MISP 参考 共有対象の情報 セキュリティ対策は何故必要か？: 機密情報とは？営業秘密との違いや管理方法、漏洩リスク等を解説！ 第1部 特集 「スマートICT」の戦略的活用でいかに日本に元気と成長をもたらすか まとめ DATA COLLABORATION GUIDE What is data collaboration? ソリューション CROSSBEAM DATA ESCROW 101 Codekeeperのエスクローサービス Planetway PlanetCorss Nortal Aktors MISP Snowflake Secure Data Sharingの紹介 S3 Access Pointsに関するAWSブログ S3アクセスポイントのうれしい点を自分なりの理解で解説してみる Amazon S3 Access Points AWS Storage Gateway Amazon S3のバケットを特定のAWSアカウントに共有する Amazon Redshift Data Sharing が一般提供開始となり、東京リージョンでもご利用可能 Announcing Amazon Redshift data sharing (preview) トレジャーデータ契約企業同士のデータ連携を簡単に実現する「Data Exchange」機能を紹介！ OSS CKAN CKANドキュメント CKANのショーケース Business Data Sharing Based on Open-Source Technology Eclipse Dataspace Connector eclipse-dataspaceconnector / DataSpaceConnector X-Road X-Roadの歴史 X-Roadの組織モデル X-Roadのアーキテクチャ X-Roadのアーキテクチャ詳細 X-Roadエコシステム間のフェデレーション X-Roadの開発ロードマップ X-RoadのGitHub X-Road Implementation Models X-ROAD® WORLD MAP On the Systematic Exploitation of the Estonian Data Exchange Layer X-Road for Strengthening Public-Private Partnerships Setting Up Government 3.0 Solutions Based on Open Source Software: The Case of X-Road Automatization of Cross-Border Customs Declaration: Potential and Challenges Prerequisites for the Adoption of the X - Road Interoperability and Data Exchange Framework: A Comparative Study X-Road - An interoperable Information Technology solution for the national Health Information Platform (nHIP) Federating X-Road System Overview X-Roadのドキュメントライブラリ X-Road Trust Model and Technology Threat Analysis Planetway社のプロフィール 組織 AIDAC 人工知能技術適用によるスマート社会の実現 人工知能技術適用によるスマート社会の実現 プロジェクト紹介 メモ そもそもデータ共有対象のデータって何があるのか？ 機密性を保持するべきデータ セキュリティ対策は何故必要か？ によると企業が保護すべき情報には「個人情報」と「企業内機密情報」があるとのこと。 機密情報とは？営業秘密との違いや管理方法、漏洩リスク等を解説！ には、「機密情報」の定義が記載されている。 企業が保有している情報のうち、外部への開示が予定されていないこと 情報秘密として管理されている情報 開示されれば、企業に損害が生じ得る情報 また、不正競争防止法に「営業秘密」の定義が載っている。（ようだ） 秘密管理性（秘密として管理されていること） 有用性（有用な営業上又は技術上の情報であること） 非公知性（公然と知られていないこと） オープンなデータ 網羅性、という意味ではオープンなデータも共有対象になる。 いわゆる「オープンデータ」の意義・目的が総務省の情報通信白書に記載されている。 第1部 特集 「スマートICT」の戦略的活用でいかに日本に元気と成長をもたらすか 参照。 これに合致しなくとも、企業などで取り扱うデータには機密性を保たなくてもよい類の情報もあるだろう。 まとめ系の情報源 DATA COLLABORATION GUIDE にデータコラボレーションに関するまとめが記載されている。 データコラボレーションの具体例（パートナーとアライアンス、チャネル販売、など）に触れながら、 データエスクローについて紹介している。 What is data collaboration?にデータコラボレーションによってもたらされるメリットなどに少し触れている生地記事。 「データブリッジ」というクエリを移動させる方式（考え方？）が紹介されていた。 データコラボレーションは、線シティブナデータを共有することなく、サイロ化したデータを組み合わせられる仕組みであると紹介されている。 データ共有のためのオープンソースソフトウェア CKAN CKAN の通り、データ共有のためのAPI、データストア管理、機能拡張の仕組みなどを提供する。 ポータルサイトを構築可能。 そのほかにも、地理情報に基づく管理、メタデータ管理、データ管理用ウェブフロントエンド、検索などを提供する。 フェデレーションも可能そうである。 ハーベスト（収穫？）機能を利用し、リモートのメタデータを収集する。 ファイルストアには、ローカルのデータストアのほか、クラウドのデータストアも利用可能。 データカタログソフトウェア「CKAN」の使い方について備忘録 にAPIの特徴についての考察が記載されている。 CKANのショーケース には公式の例が載っている。 Eclipse Dataspace Connector Business Data Sharing Based on Open-Source Technology の通り、IDS企画に準拠したコネクター。 Eclipse Dataspace Connector がプロジェクトディレクトリのようだがメタな情報しかない。 アクティブな開発者としては、BMWなどが挙げられるがMicrosoftも重要な位置付けにあるようだ。 もともとMicrosoftが開発していた（？）Data Appliance GXとIDS Dataspace Connectorを基にしているからだろうか。 eclipse-dataspaceconnector / DataSpaceConnector にソースコードがある。 X-Road X-Road の通り、「distributed data exchage layer」とのこと。 X-Roadのインスタンス同士でデータ交換が可能になる。 X-RoadのGitHub によると、 The following activities, among others, are undertaken by the Nordic Institute for Interoperability Solutions (NIIS) with regard to the X-Road core とのこと。NIISが主にメンテ活動しているようだ。 特徴 特徴： address management message routing access rights management organization-level authentication machine-level authentication transport-level encryption time-stamping digital signature of messages logging error handling. 大まかな構成 X-Roadで作られたエコシステムに接続するにはCAによって証明されないといけない。 X-Roadの組織モデル にある通り、 X-Road Operator ... エコシステムの責任者。規約・ルールの策定・事項、新しいメンバの受け入れ、など。 Member組織 ... サービスの提供者・利用者。オンボードプロセスを経て、エコシステムに参加可能 Trust Service Provider ... Time-stamp Authority（TSA）、Certification Authority（CA）機能を提供。第三者機関でもよいし、X-Road Operatorが兼ねることもある。 という関係者で構成される。 モデルとしては、中央集権的である。 X-Roadのアーキテクチャ にあるように、Central Serivceを提供するためのCentral Serverがあり、 X-Road Operatorが責任をもって運用する。 そこにはMemberの登録情報やポリシーが管理されている。 なお、分散配置されたSecurity Serverを通じて、これらの情報を利用可能になっている。 MemberはSecurity Serverを通じてX-Roadのセキュリティ面の機能を利用可能。 Security ServerはCentral Serverに保持されたコンフィグやTrust Service Provierが持っているAuthorityの情報を取得し、ローカルにキャッシュし動作する。 Information SystemはREST or SOAPプロトコルを用いてMemberが持つデータを提供・利用可能。 Security Serverがエントリポイントになり、Information Systemを利用可能。 データ交換のログが保存される。やり取りされるメッセージにはタイムスタンプが付く。 ログにはタイムスタンプとデジタル署名が用いられ、訴訟手続などにも利用可能。 TSAがタイムスタンプを提供し、Security Serverがログを保存する。 CAがメッセージへのサインを提供する。 詳しくは、 X-Roadのアーキテクチャ詳細 参照。そこの絵がわかりやすい。 フェデレーション X-Roadエコシステム間のフェデレーション にあるように、異なるX-Roadエコシステム間をフェデレーションできる。 複数のエコシステムとフェデレーション可能だが、推移的なつながりは不可能であり、直接つながった同士のみ。 Federating X-Road System Overview にも記載がある。 開発ロードマップ X-Roadの開発ロードマップ によると、 同期的なメッセージングからPub/Subモデルへの対応（Kafkaなど） 大量データノストリームへの対応 アーキテクチャの改善。モジュライズの促進。拡張機能を取り込みやすくする といった改善が見込まれているようだ。次期バージョンはX-Road 7。 歴史 X-Roadの歴史 にあるように、2001年にエストニアにおいてプロジェクトが始まった。 前身から考えるとかなり歴史のあるプロダクトである。 また、歴史的背景から、最初は公的機関での利用が前提となっており、そこから民間に広がりつつあるような印象である。 民間での利用を促進するにはどうしたらよいのか？という主旨と思われる論文も公開されている。 -&gt; On the Systematic Exploitation of the Estonian Data Exchange Layer X-Road for Strengthening Public-Private Partnerships 世界中で採用が進む X-ROAD® WORLD MAP を見ると、世界各地でX-Road採用実績がある。 日本にも採用実績がある。 ニチガスの例。 なお、本事例にPlanetway社が関係している。Planetway社のプロフィール 参照。 実装モデル X-Road Implementation Models に示されている通り、X-Roadの実装モデルは複数挙げられる。 上記サイトに掲載されていた例は以下の通り。 国家で単一のX-Roadモデル 地域ごとのX-Roadモデル ビジネスドメインやセクタごとのX-Roadモデル 組織ごとのX-Roadモデル またX-Roadはフェデレーション機能を持っているので、 異なるX-Roadエコシステム間を繋ぎ利用することができる。 国際間で繋いでいる例もある。 またIstioのようなマイクロサービスアーキテクチャに対し、 X-Roadはあくまで異なるステークホルダ間のデータ連携を対象としているため、 一見似たように見えても異なるモデルである。 開発（ソースコード） X-RoadのGitHub 関連文献 関連する論文がいくつかありそうだった。 On the Systematic Exploitation of the Estonian Data Exchange Layer X-Road for Strengthening Public-Private Partnerships 等。 なお、X-Roadのドキュメントライブラリに一覧が載っている。 On the Systematic Exploitation of the Estonian Data Exchange Layer X-Road for Strengthening Public-Private Partnerships 公的機関と民間部門の双方が使いやすいプラットフォームとは？という議論。X-Roadについて触れる。 なお、本論文時点でX-Roadに接続されているエストニアの民間企業は数百とされていた Setting Up Government 3.0 Solutions Based on Open Source Software: The Case of X-Road Goverment 3.0の視点からX-Roadを説明。他のOSSとの比較もあるかもしれない。 Automatization of Cross-Border Customs Declaration: Potential and Challenges 税関の事例。X-Roadが引用されいてる。 Prerequisites for the Adoption of the X - Road Interoperability and Data Exchange Framework: A Comparative Study X-Roadの適用条件を明らかにする。複数の国を調査。 X-Road Trust Model and Technology Threat Analysis X-Roadの信頼性モデルの説明、驚異の分類 X-Road - An interoperable Information Technology solution for the national Health Information Platform (nHIP) X-Roadをニュージーランドの医療データプラットフォームに活用した事例 特に公的セクターと民間セクターを繋ぐ Federating X-Road System Overview フィンランドが関係7か国との連携のため、X-Roadを国際間でフェデレーションできるようにする研究 X-Roadの広がりに関する課題 On the Systematic Exploitation of the Estonian Data Exchange Layer X-Road for Strengthening Public-Private Partnerships にて X-Roadの民間利用が進まない原因と対策のマトリクスが紹介されていた。 以下に簡単に翻訳したものを示す。 問題/ソリューション 認知を挙げる 契約の簡素化（単一化） セミナー・ハッカソン 導入マテリアル 収益性に関する具体的な例 メンバーシップパッケージの充実（？） セクター別のアプローチ 官僚を減らす（？） 様々なチャンネルでの情報発信 X-Roadのアクセスや利用が複雑すぎる y y y y y プロセスに関係者が多すぎる y y y プラットフォームが高すぎる y y 十分に認知されていない y y y 収益性についての理解が足りない y y y ソリューションに付帯されるデータ共有の仕組み Snowflake Data Sharing Snowflake Secure Data Sharingの紹介 のとおり、 Snowflakeに保存したオブジェクトを他のSnowflakeアカウントに共有できる。 ポイントは「他のSnowflakeアカウントに」というところである。つまり異るサービス間で共有する目的のものではなさそう。 ただ、その後の説明を読むと「リーダーアカウント」（ここではReaderか）を発行でき、リーダーアカウント向けに読み取り専用でデータを共有できるようだ。 共有できるオブジェクトは以下の通り。 テーブル 外部テーブル 安全なビュー 安全なマテリアライズドビュー 安全な UDFs 共有されたデータを利用する際は、「読み取り専用」のデータベースが定義される。 また実際にコピー／転送されるわけではない。（同じ実態をコンピュートリソースが参照するという意味だと理解） S3 Access Points S3 Access Pointsに関するAWSブログ に記載の通り、2019年時点でS3に個別のアクセスポイントを設ける機能がリリースされている。 アクセスポイントを発行し、アクセスポイントポリシーを適用可能。 以前であれば、バケットに対してバケットポリシーを適用していたのだが、S3 Access Pointsを利用して個別のアクセスポイントを払い出すことができるようになった。 つまり、バケットポリシーで個別の制御を実現しようとすると、たくさんのポリシーをひとつのバケットポリシーに記載する必要があったが、 ひとつの接続可否をひとつのポリシーで管理出来るようになった、と言える。 S3アクセスポイントのうれしい点を自分なりの理解で解説してみる あたりにユーザ視点での意義が書かれている。分かり易い。 Amazon S3 Access Points が公式情報。 AWS Storage Gateway AWS Storage Gateway にアーキテクチャの絵がまとまっているが、S3とオンプレのシステムを繋いでデータ共有するにあたり、 ゲートウェイの機能を提供する。 AWS上で素朴にS3バケットポリシーを使ってアカウント間共有 もっと素朴に実現する方法はあるのか？という意味では、Amazon S3のバケットを特定のAWSアカウントに共有する のブログの通り、S3のバケットポリシー上の工夫で対応可能。 バケットポリシーで特定のAWSアカウントを指定して共有できる。 ただし、この場合は、S3 Access Pointsで示したようなバケットポリシーの課題が生じる。 AWS Redshift Data Sharing Amazon Redshift Data Sharing が一般提供開始となり、東京リージョンでもご利用可能 のとおり、 Redshift間でデータを共有する機能がある。 Announcing Amazon Redshift data sharing (preview) の通り、アカウントまたぎも可能そうに見える。 Treasure Data Data Exchange トレジャーデータ契約企業同士のデータ連携を簡単に実現する「Data Exchange」機能を紹介！ の通り、 ユーザが自身で所有しているデータを他のアカウントにデータを共有する事が可能。 また共有する際に、データ抽出量のクエリを渡すことができる。 これにより、特定のデータだけ渡す、などが可能。 PlanetCross Planetway 社が提供するソリューション。 PlanetCorss の通り、エストニアの電子政府で利用されているX-Roadを 民間企業向けにカスタマイズして提供している。 上記ウェブサイトに記載されているが、ユーザ企業ごとにPlanetCross環境を立て、 それを通じて互いにデータ連携できるようになっている。 このモデルだと、GAIA-X/IDSのコネクタでデータスペースをつなげるモデルに近い。 受賞歴を見ると、特に2016、2017年あたりに注目を集めた様子。 （関連）Nortal社 Nortal 社はエストニアの電子政府ソリューションの40％異常を計画・実行した実績を持つ。 eIDに強みを持つようだ。 Aktors社 Aktors 社はX-Roadの先駆者。 Aktors CEOのAleksander Reitsakasは、2001年の最初のX-Roadプロジェクトマネージャ。 データエスクロー（第三者預託） Crossbeam CROSSBEAM、 DATA ESCROW 101あたりに紹介されている。 データを預かり、利害関係者に部分的に共有する機能を提供する。 パートナーマネジメントの一貫。 codekeeper Codekeeperのエスクローサービス に記載の通り、重要なデータを預かるサービス。 データコラボレーションに関する研究機関 AIDAC AIDAC の通り、筑波大学の研究グループ。 データコラボレーション 解析 を中心とした研究。 主旨は以下の通り。 組織や分野を越えたデータ統合には、データサイズが巨大であることや組織ごとのデータ特性の違いなどの課題があります。またさらにデータの秘匿性についても考慮する必要があります。本事業では、組織や分野にまたがって分散したデータに対して、元データの直接的な統合を行うことなく、高度な統合解析を可能とするための技術開発を行います。 また、以下の通りNEDOプロジェクトとして手がけた様だ。 本プロジェクトは、独立行政法人新エネルギー・産業技術総合開発機構(NEDO)の委託を受けて、 「人工知能技術適用によるスマート社会の実現（人工知能技術の社会実装に関する日米共同研究開発）」において実施するものです。\u001c 人工知能技術適用によるスマート社会の実現によると、 実施期間は以下の通り。 事業期間：2018年度～2022年度、2020年度予算：19.5億円 人工知能技術適用によるスマート社会の実現 プロジェクト紹介 に具体的なプロジェクトタイトルが載っている。 気になったプロジェクトは以下の通り。 MyData に基づく人工知能開発運用プラットフォームの構築：国立大学法人東京大学他 データコラボレーション解析による生産性向上を目指した次世代人工知能技術の研究開発：国立大学法人筑波大学他 セキュリティの情報共有 MISP MISP にあるとおり、セキュリティの脅威を共有するプラットフォームとオープンソースソフトウェア。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Collaboration","slug":"Knowledge-Management/Data-Collaboration","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/"}],"tags":[]},{"title":"Source Code Reading of Delta Sharing","slug":"Source-Code-Reading-of-Delta-Sharing","date":"2021-08-30T03:51:21.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/08/30/Source-Code-Reading-of-Delta-Sharing/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/08/30/Source-Code-Reading-of-Delta-Sharing/","excerpt":"","text":"参考 メモ サーバの参考実装 io.delta.sharing.server.DeltaSharingServiceの概略（起動の流れ） Shareのリストを返す箇所の実装 テーブルのバージョンを返す箇所の実装 io.delta.standalone.internal.DeltaSharedTable クラス ファイルリストを返す箇所の実装 クラスローダについて 参考 Delta Sharingの公式GitHub Read Data from a Table Armeria ArmeriaのAnnotated service メモ サーバの参考実装 io.delta.sharing.server.DeltaSharingServiceの概略（起動の流れ） サーバ起動スクリプトを見ると、以下のように io.delta.sharing.server.DeltaSharingService クラスが用いられている事が分かる。 123456789101112131415declare -r lib_dir=\"$(realpath \"$&#123;app_home&#125;/../lib\")\"declare -a app_mainclass=(io.delta.sharing.server.DeltaSharingService)```\u001cエントリポイントは以下の通り、io/delta/sharing/server/DeltaSharingService.scala:276```scala def main(args: Array[String]): Unit = &#123; val ns = parser.parseArgsOrFail(args) val serverConfigPath = ns.getString(\"config\") val serverConf = ServerConfig.load(serverConfigPath) start(serverConf).blockUntilShutdown() &#125; コンフィグのロード 渡された設定ファイルのPATHを用いて、設定を読み込む。 1234567891011121314/** * Load the configurations for the server from the config file. If the file name ends with * `.yaml` or `.yml`, load it using the YAML parser. Otherwise, throw an error. */def load(configFile: String): ServerConfig = &#123; if (configFile.endsWith(\".yaml\") || configFile.endsWith(\".yml\")) &#123; val serverConfig = createYamlObjectMapper.readValue(new File(configFile), classOf[ServerConfig]) serverConfig.checkConfig() serverConfig &#125; else &#123; throw new IOException(\"The server config file must be a yml or yaml file\") &#125;&#125; startメソッド オブジェクトのmainメソッド。その中で用いられている、 start メソッド内を確認していく。 io/delta/sharing/server/DeltaSharingService.scala:230 123456789101112131415161718192021222324252627282930313233343536373839404142434445def start(serverConfig: ServerConfig): Server = &#123; lazy val server = &#123; updateDefaultJsonPrinterForScalaPbConverterUtil() val builder = Server.builder() .defaultHostname(serverConfig.getHost) .disableDateHeader() .disableServerHeader() .annotatedService(serverConfig.endpoint, new DeltaSharingService(serverConfig): Any) if (serverConfig.ssl == null) &#123; builder.http(serverConfig.getPort) &#125; else &#123; builder.https(serverConfig.getPort) if (serverConfig.ssl.selfSigned) &#123; builder.tlsSelfSigned() &#125; else &#123; if (serverConfig.ssl.certificatePasswordFile == null) &#123; builder.tls( new File(serverConfig.ssl.certificateFile), new File(serverConfig.ssl.certificateKeyFile)) &#125; else &#123; builder.tls( new File(serverConfig.ssl.certificateFile), new File(serverConfig.ssl.certificateKeyFile), FileUtils.readFileToString(new File(serverConfig.ssl.certificatePasswordFile), UTF_8) ) &#125; &#125; &#125; if (serverConfig.getAuthorization != null) &#123; // Authorization is set. Set up the authorization using the token in the server config. val authServiceBuilder = AuthService.builder.addOAuth2((_: ServiceRequestContext, token: OAuth2Token) =&gt; &#123; // Use `MessageDigest.isEqual` to do a time-constant comparison to avoid timing attacks val authorized = MessageDigest.isEqual( token.accessToken.getBytes(UTF_8), serverConfig.getAuthorization.getBearerToken.getBytes(UTF_8)) CompletableFuture.completedFuture(authorized) &#125;) builder.decorator(authServiceBuilder.newDecorator) &#125; builder.build() &#125; server.start().get() server&#125; 一番最後の箇所の通り、 12server.start().get()server server は、 Armeria のビルダを用いてインスタンス化されたサーバを起動する。 なお、startメソッド内ではTLS周りの設定、トークンの設定などが行われる。 なお、サーバに渡されるクラスは以下の通り、 12345val builder = Server.builder() .defaultHostname(serverConfig.getHost) .disableDateHeader() .disableServerHeader() .annotatedService(serverConfig.endpoint, new DeltaSharingService(serverConfig): Any) com.linecorp.armeria.server.ServerBuilder#annotatedService(java.lang.String, java.lang.Object) メソッドを用いて渡される。 渡されているのは io.delta.sharing.server.DeltaSharingService クラスである。 annotatedService メソッドについては、 ArmeriaのAnnotated service を参照。 Shareのリストを返す箇所の実装 以下の通り、 io/delta/sharing/server/DeltaSharingService.scala:108 12345678@Get(\"/shares\")@ProducesJsondef listShares( @Param(\"maxResults\") @Default(\"500\") maxResults: Int, @Param(\"pageToken\") @Nullable pageToken: String): ListSharesResponse = processRequest &#123; val (shares, nextPageToken) = sharedTableManager.listShares(Option(pageToken), Some(maxResults)) ListSharesResponse(shares, nextPageToken)&#125; io.delta.sharing.server.SharedTableManager#SharedTableManager クラスのインスタンスを利用し、 io.delta.sharing.server.SharedTableManager#listShares メソッドを用いて、 設定ファイルから読み込んだShareのリストを取得し返す。 つまり、このあたりの値を返す時にはデータ本体にアクセスしていない、ということになる。 仮に...もし設定ファイルに書かれたデータの実体が無かった場合はどうなるのだろうか。 ★要確認 テーブルのリスト取得も同じような感じだった。 テーブルのバージョンを返す箇所の実装 io/delta/sharing/server/DeltaSharingService.scala:144 12345678910@Head(\"/shares/&#123;share&#125;/schemas/&#123;schema&#125;/tables/&#123;table&#125;\")def getTableVersion( @Param(\"share\") share: String, @Param(\"schema\") schema: String, @Param(\"table\") table: String): HttpResponse = processRequest &#123; val tableConfig = sharedTableManager.getTable(share, schema, table) val version = deltaSharedTableLoader.loadTable(tableConfig).tableVersion val headers = createHeadersBuilderForTableVersion(version).build() HttpResponse.of(headers)&#125; io.delta.sharing.server.SharedTableManager#getTable メソッドを利用し、コンフィグから読み込んだ情報を元に テーブル情報を取得する。 つづいて、 io.delta.standalone.internal.DeltaSharedTableLoader#loadTable メソッドを利用し、 io.delta.standalone.internal.DeltaSharedTable#DeltaSharedTable クラスのインスタンスを取得する。 DeltaSharedTable クラスは、 DeltaLog クラスをラップした管理用のクラス。 io.delta.standalone.internal.DeltaSharedTable クラス このクラスは、DeltaLog クラスをラップしたものであり、サーバの管理機能の主要なコンポーネントである。 deltaLogの取得 例えば deltaLog インスタンスを取得できる。 io/delta/standalone/internal/DeltaSharedTableLoader.scala:74 12345678private val deltaLog = withClassLoader &#123; val tablePath = new Path(tableConfig.getLocation) val fs = tablePath.getFileSystem(conf) if (!fs.isInstanceOf[S3AFileSystem]) &#123; throw new IllegalStateException(\"Cannot share tables on non S3 file systems\") &#125; DeltaLog.forTable(conf, tablePath).asInstanceOf[DeltaLogImpl]&#125; 上記の通り、内部では io.delta.standalone.DeltaLog#forTable(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) メソッドが 用いられており、実際にDelta Lake形式で保存されたデータ本体（Delta Lakeのメタデータ）にアクセスする。 テーブルバージョン取得 Delta Logの機能を利用し、テーブルのバージョンを取得できる。 123456/** Return the current table version */def tableVersion: Long = withClassLoader &#123; val snapshot = deltaLog.snapshot validateDeltaTable(snapshot) snapshot.version&#125; この機能は io.delta.sharing.server.DeltaSharingService#getTableVersion の実装に利用されている。 クエリ io.delta.standalone.internal.DeltaSharedTable#query メソッドは、 後述の通り、DeltaSharingSerivce 内でファイルリストを取得したり、メタデータを取得したりするときに用いられる。 ファイルリストを返す箇所の実装 query はファイルリストを返すAPIである。 io/delta/sharing/server/DeltaSharingService.scala:169 1234567891011121314@Post(\"/shares/&#123;share&#125;/schemas/&#123;schema&#125;/tables/&#123;table&#125;/query\")@ConsumesJsondef listFiles( @Param(\"share\") share: String, @Param(\"schema\") schema: String, @Param(\"table\") table: String, queryTableRequest: QueryTableRequest): HttpResponse = processRequest &#123; val tableConfig = sharedTableManager.getTable(share, schema, table) val (version, actions) = deltaSharedTableLoader.loadTable(tableConfig).query( includeFiles = true, queryTableRequest.predicateHints, queryTableRequest.limitHint) streamingOutput(version, actions)&#125; 特徴的なのは、 predicateHints や limitHint が渡されており、フィルタ条件が指定できる点。 ただし、公式ドキュメント上では「ベストエフォート」と書かれている。 このメソッドのポイントは、 io.delta.standalone.internal.DeltaSharedTableLoader#DeltaSharedTableLoader クラスを用いたクエリの部分だと考えられる。 このクラスは、Deltaテーブルの本体にアクセスし、各種情報を読みこむためのもので、コメントを読む限り、キャッシュする機能なども有している。 内部では io.delta.standalone.internal.DeltaSharedTable クラスが利用されている。 DeltaSharedTable クラスは、 DeltaLog をラップしたものであり、Delta Logの各種情報をサーバ内で扱うための機能を提供する。 特に、今回用いられているのは io.delta.standalone.internal.DeltaSharedTable#query メソッドである。 io/delta/standalone/internal/DeltaSharedTableLoader.scala:118 1234567 def query( includeFiles: Boolean, predicateHits: Seq[String], limitHint: Option[Int]): (Long, Seq[model.SingleAction]) = withClassLoader &#123; // TODO Support `limitHint`(snip) メソッドの定義の通り、このクエリは最終的に、 io/delta/standalone/internal/DeltaSharedTableLoader.scala:165 1snapshot.version -&gt; actions Delta Logから情報取得したスナップショットのバージョン、ファイル情報のシーケンスを返す。 なお、 actions に格納されている SingleAction クラスは以下の通り、 ファイルひとつに関する情報、プロトコルの情報、メタデータについての情報を保持している。 io/delta/sharing/server/model.scala:22 1234567891011121314151617case class SingleAction( file: AddFile = null, metaData: Metadata = null, protocol: Protocol = null) &#123; def unwrap: Action = &#123; if (file != null) &#123; file &#125; else if (metaData != null) &#123; metaData &#125; else if (protocol != null) &#123; protocol &#125; else &#123; null &#125; &#125;&#125; アクションの生成される箇所 上記の通り、ここでいう「アクション」とはデータの実体であるファイル1個に関連する情報の塊である。 生成されるのは以下の箇所。 io/delta/standalone/internal/DeltaSharedTableLoader.scala:137 12345678910111213141516171819202122232425262728val actions = Seq(modelProtocol.wrap, modelMetadata.wrap) ++ &#123; if (includeFiles) &#123; val selectedFiles = state.activeFiles.values.toSeq val filteredFilters = if (evaluatePredicateHints &amp;&amp; modelMetadata.partitionColumns.nonEmpty) &#123; PartitionFilterUtils.evaluatePredicate( modelMetadata.schemaString, modelMetadata.partitionColumns, predicateHits, selectedFiles ) &#125; else &#123; selectedFiles &#125; filteredFilters.map &#123; addFile =&gt; val cloudPath = absolutePath(deltaLog.dataPath, addFile.path) val signedUrl = signFile(cloudPath) val modelAddFile = model.AddFile(url = signedUrl, id = Hashing.md5().hashString(addFile.path, UTF_8).toString, partitionValues = addFile.partitionValues, size = addFile.size, stats = addFile.stats) modelAddFile.wrap &#125; &#125; else &#123; Nil &#125;&#125; 最初から主要な部分の実装を確認する。 まず、シーケンスを作る際、先頭にプロトコルとメタデータをラップしたアクションが保持される。 1val actions = Seq(modelProtocol.wrap, modelMetadata.wrap) ++ &#123; その後、適宜ファイルに関する情報が格納される。 一応 includeFiles で空情報を返すかどうかの判定があるが、ここではTrueだとして話を進める。 まず、Delta Lakeテーブルのアクティブなファイルが取得される。 1val selectedFiles = state.activeFiles.values.toSeq つづいて、Predicateヒントがあり、パーティションカラムが設定されている場合は、 それに基づいてフィルタが行われる。 1234567891011val filteredFilters = if (evaluatePredicateHints &amp;&amp; modelMetadata.partitionColumns.nonEmpty) &#123; PartitionFilterUtils.evaluatePredicate( modelMetadata.schemaString, modelMetadata.partitionColumns, predicateHits, selectedFiles ) &#125; else &#123; selectedFiles &#125; ヒントを用いてフィルタしている箇所については後述する。 ここではフィルタされたファイルのリストが返されたとして話を続ける。 次に、AddFile の情報を用いて、より詳細な情報が確認される。 12345678910filteredFilters.map &#123; addFile =&gt; val cloudPath = absolutePath(deltaLog.dataPath, addFile.path) val signedUrl = signFile(cloudPath) val modelAddFile = model.AddFile(url = signedUrl, id = Hashing.md5().hashString(addFile.path, UTF_8).toString, partitionValues = addFile.partitionValues, size = addFile.size, stats = addFile.stats) modelAddFile.wrap&#125; 絶対PATH（というかURL）の取得、下回りのストレージ（現時点ではS3のみ対応）の署名済みURL取得が行われ、 改めて AddFile に格納されたのち、アクションにラップされて返される。 ここで、1点だけ署名済みURL取得する部分だけ確認する。 取得には、 io.delta.standalone.internal.DeltaSharedTable#signFile メソッドが用いられる。 io/delta/standalone/internal/DeltaSharedTableLoader.scala:192 123456private def signFile(path: Path): String = &#123; val absPath = path.toUri val bucketName = absPath.getHost val objectKey = absPath.getPath.stripPrefix(\"/\") signer.sign(bucketName, objectKey).toString&#125; このメソッドでは、上記の通り、内部でバケット名やオブジェクトキーを渡しながら、 io.delta.sharing.server.S3FileSigner#sign メソッドが利用されている。 なお、signer インスタンスは以下のようになっており、 123private val signer = withClassLoader &#123; new S3FileSigner(deltaLog.dataPath.toUri, conf, preSignedUrlTimeoutSeconds)&#125; 将来的にS3以外にも対応できるような余地が残されている。 さて、 sign メソッドに戻る。 io/delta/sharing/server/CloudFileSigner.scala:41 12345678override def sign(bucket: String, objectKey: String): URL = &#123; val expiration = new Date(System.currentTimeMillis() + SECONDS.toMillis(preSignedUrlTimeoutSeconds)) val request = new GeneratePresignedUrlRequest(bucket, objectKey) .withMethod(HttpMethod.GET) .withExpiration(expiration) s3Client.generatePresignedUrl(request)&#125; 上記の通り、 com.amazonaws.services.s3.model.GeneratePresignedUrlRequest#GeneratePresignedUrlRequest(java.lang.String, java.lang.String) メソッドを利用してS3の署名付きURLが取得されている事が分かる。 ヒントが利用されている箇所 2021/9/18現在では、 predicateHits は用いられているが、limitHint は用いていないように見える。 predicateHints が用いられるのは以下の個所。 io/delta/standalone/internal/DeltaSharedTableLoader.scala:142 123456PartitionFilterUtils.evaluatePredicate( modelMetadata.schemaString, modelMetadata.partitionColumns, predicateHits, selectedFiles) io.delta.standalone.internal.PartitionFilterUtils$#evaluatePredicate メソッドは、 引数に predicateHits や対象となるDelta Tableに紐づいているアクティブなファイルリストを引数に渡される。 内部で、有効なpredicateかどうかなどをチェックされたのち、有効なpredicate文言があれば、 それに従いファイルリストがフィルタされる。 なお、内部的にはDelta LakeやSpark（の特にCatalyst）の実装に依存しているような箇所がみられる。 具体的には、 evaluatePredicate メソッド内の以下の個所。 io/delta/standalone/internal/PartitionFilterUtils.scala:61 1234addFiles.filter &#123; addFile =&gt; val converter = CatalystTypeConverters.createToCatalystConverter(addSchema) predicate.eval(converter(addFile).asInstanceOf[InternalRow])&#125; ここで predicate は org.apache.spark.sql.catalyst.expressions.InterpretedPredicate クラスの インスタンスである。（SparkのCatalystで用いられる、Predicate表現のひとつ） このとき、eval メソッドが呼び出されていることがわかる。 /home/dobachi/.cache/coursier/v1/https/repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/2.4.7/spark-catalyst_2.12-2.4.7-sources.jar!/org/apache/spark/sql/catalyst/expressions/predicates.scala:39 12case class InterpretedPredicate(expression: Expression) extends BasePredicate &#123; override def eval(r: InternalRow): Boolean = expression.eval(r).asInstanceOf[Boolean] このように、SparkのCatalystにおいては eval メソッドには、Spark SQLの行の表現である InternalRow のインスタンスが渡され、当該 predicate に合致するかどうかがチェックされる。 クラスローダについて このクラスには、 withClassLoader が定義されており、 Armeriaではなく、DeltaSharedTableのクラスローダの下で関数を実行できるようになっている。 123456789101112131415/** * Run `func` under the classloader of `DeltaSharedTable`. We cannot use the classloader set by * Armeria as Hadoop needs to search the classpath to find its classes. */private def withClassLoader[T](func: =&gt; T): T = &#123; val classLoader = Thread.currentThread().getContextClassLoader if (classLoader == null) &#123; Thread.currentThread().setContextClassLoader(this.getClass.getClassLoader) try func finally &#123; Thread.currentThread().setContextClassLoader(null) &#125; &#125; else &#123; func &#125;&#125;","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Collaboration","slug":"Knowledge-Management/Data-Collaboration","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/"},{"name":"Delta Sharing","slug":"Knowledge-Management/Data-Collaboration/Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"},{"name":"Delta Sharing","slug":"Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Sharing/"}]},{"title":"Recent hot activities of Delta Sharing","slug":"Recent-hot-activities-of-Delta-Sharing","date":"2021-08-27T00:22:38.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/08/27/Recent-hot-activities-of-Delta-Sharing/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/08/27/Recent-hot-activities-of-Delta-Sharing/","excerpt":"","text":"参考 メモ いま何がどのくらいできるようになっているのか？ クライアント、開発言語バインディング サーバの参考実装 プロトコル テーブルメタデータ Support writing to Delta Shares Add pre-signed URL request logging to server Support Azure Storage in Delta Sharing Server Support Google Cloud Storage in Delta Sharing Server 参考 Delta Sharing公式GitHub Delta Sharing Protocal delta-sharing-server.yaml.template Concept Metadata Support writing to Delta Shares #33 Add pre-signed URL request logging to server Support Azure Storage in Delta Sharing Server Delegate access with a shared access signature Support Google Cloud Storage in Delta Sharing Server 署名付きURL メモ いま何がどのくらいできるようになっているのか？ 2021/9時点で何ができるのか？ Delta Sharing公式GitHub のREADMEによると以下の通り。 クライアント、開発言語バインディング SQL（Spark SQL） Python Scala Java R サーバの参考実装 Delta Sharing Protocal に乗っ取ってデータ共有の動作確認が可能な 参考実装が公開されている。 ただし、この実装にはセキュリティ周りの機能が十分に実装されておらず、 もし公開して利用する場合はプロキシの裏側で使うことが推奨されている。 サーバを立ち上げるときに共有するデータの定義を渡せる。 delta-sharing-server.yaml.template にコンフィグのサンプルが載っている。 Concept にデータモデル、主要なコンポーネントが載っている。 Share（シェア）: 共有の論理的なグループ このグループに含まれるものが受信者に共有される。 Schema（スキーマ）: 複数のTableを含む Table（テーブル）: Delta Lakeのテーブルもしくはビュー Recipient（受信者）: シェアにアクセス可能なBearerトークンをもつもの Sharing Server（共有サーバ）: プロトコルを実装したサーバ プロトコル Delta Sharing Protocal に載っている通り、 Delta Sharingのサーバとクライアントの間のプロトコルが規定されている。 シェアをリスト シェア内のスキーマをリスト スキーマ内のテーブルをリスト テーブルのバージョン情報取得 おそらくDelta Lakeとしてのバージョン情報 テーブルのメタデータ取得 データ取得 データ取得時には、PredicateHint、LimitHintを渡せる テーブルメタデータ Metadata にテーブルに付帯できるメタデータの定義が載っている。 スキーマやフォーマットなどの情報に加え、任意の説明を保持できるようだ。 Support writing to Delta Shares Support writing to Delta Shares #33 書き込みへの対応について提案がなされたが、2021/8/27現在は保留となっている。 いったん読み込みに注力、とのこと。 なお、Treasure DataのTaroさんがTreasure Dataにおける対応例を紹介している。 （が、それに対する応答はない） Add pre-signed URL request logging to server Add pre-signed URL request logging to server 今のところ、プリサインドURLの件数を知る術がない。 そこで、DEBUGログにプリサインドURLの発行情報を出力するのはどうか、というチケットが挙がっていた。 Support Azure Storage in Delta Sharing Server Support Azure Storage in Delta Sharing Server Azureでは、 Delegate access with a shared access signature の機能が提供されているので、割と対応しやすいのでは？という議論になっている。 AzureのSASは、定められたパーミッションで定められた期間だけリソースにアクセス可能にする。 なお、SASにはアカウントレベル、サービスレベル、ユーザデリゲーションの3種類がある。 それぞれスコープやできることが異なる。 Delta Sharingがストレージに特化しているのであれば、まずはBlobに対するデリゲーションURLを発行できれば良さそうである。 Support Google Cloud Storage in Delta Sharing Server Support Google Cloud Storage in Delta Sharing Server Azureと同じく、 署名付きURL に対応しているため、そこまで難しくは無いだろう、というコメントがあった。 これはどうだろうか？","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Collaboration","slug":"Knowledge-Management/Data-Collaboration","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/"},{"name":"Delta Sharing","slug":"Knowledge-Management/Data-Collaboration/Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Collaboration/Delta-Sharing/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"},{"name":"Delta Sharing","slug":"Delta-Sharing","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Sharing/"}]},{"title":"Spark Summit 2021の前にきになってみたセッションのメモ","slug":"SparkSummit2021","date":"2021-08-24T05:19:25.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2021/08/24/SparkSummit2021/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/08/24/SparkSummit2021/","excerpt":"","text":"参考 メモ Lakehouse with Delta Lake Deep Dive Training Manufacturing and Distribution Industry Forum Data Distribution and Ordering for Efficient Data Source V2 Delta Lake Streaming Under the Hood Introducing Delta Live Tables Make Reliable ETL Easy on Delta Lake Magnet Shuffle Service Push-based Shuffle at LinkedIn 参考 メモ Spark Summit 2021の気になったセッションのメモ Lakehouse with Delta Lake Deep Dive Training Manufacturing and Distribution Industry Forum Data Distribution and Ordering for Efficient Data Source V2 Delta Lake Streaming Under the Hood Introducing Delta Live Tables Make Reliable ETL Easy on Delta Lake Magnet Shuffle Service Push-based Shuffle at LinkedIn","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"Socks proxy for Jupyter Lab","slug":"Socks-proxy-for-Jupyter-Lab","date":"2021-07-12T08:53:27.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/07/12/Socks-proxy-for-Jupyter-Lab/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/07/12/Socks-proxy-for-Jupyter-Lab/","excerpt":"","text":"参考 メモ 参考 メモ 以下の条件に当てはまる場合に設定する項目。 Jupyter Labをリモート環境で実行している Socksプロキシを使っている ChromeやFirefoxのエクステンションでFoxyProxyのようなURLマッチングでプロキシを差し替えている このとき、https?に加え、wsも指定する必要があります。 具体的には以下の通り。 Foxy Proxyでの設定例1 Foxy Proxyでの設定例2 つまりは、ウェブソケットのことを忘れることなかれ、ということである。 （いつも忘れるから、備忘録として記載しておく）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Python","slug":"Knowledge-Management/Python","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Python/"},{"name":"Jupyter","slug":"Knowledge-Management/Python/Jupyter","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Python/Jupyter/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://dobachi.github.io/memo-blog/tags/Jupyter/"}]},{"title":"Minio getting started","slug":"Minio-getting-started","date":"2021-06-13T13:01:12.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/06/13/Minio-getting-started/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/06/13/Minio-getting-started/","excerpt":"","text":"参考 公式 ブログ メモ 動作確認 前提 サーバ起動 コンフィグ mcコマンドの利用 s3コマンドで利用 アップデート 署名済みURL発行 S3互換サービスのためのエイリアス 作業用バケット作成 ポリシーファイルの作成 ポリシーを適用 ユーザ作成 ユーザにポリシーを設定 共有の動作確認用のファイルの準備 Python botoを利用して一時アカウント発行＆署名済みURL発行 参考 公式 Minioの公式ウェブサイト Minioのクライアントガイド Minioのアドミンガイド Minioのコンフィグガイド boto3 を用いて MinIO でユーザの追加と Security Token Service(STS) による一時認証情報での署名済み URL を発行する mc alias mc admin policy mc admin userを利用したユーザ作成 ブログ MinIO オブジェクトストレージの構築 開発のためにローカルにもS3が欲しいというわがまま、MINIOが叶えてくれました メモ S3互換の仕組みを導入したく、検索してヒットしたので試した。 動作確認 前提 12$ cat /etc/redhat-releaseCentOS Linux release 7.9.2009 (Core) サーバ起動 Minioの公式ウェブサイト を確認し、 123456$ mkdir ~/Minio$ cd ~/Minio$ wget https://dl.min.io/server/minio/release/linux-amd64/minio$ chmod +x minio$ mkdir data$ ./minio server data http://localhost:9000/minio/ にアクセスすると、ログイン画面が出るので、 デフォルトのID、パスワードを入力する。 コンフィグ Minioのコンフィグガイド を見ると、アドミンユーザとパスワードの設定方法が載っていた。 123$ export MINIO_ROOT_USER=minio$ export MINIO_ROOT_PASSWORD=miniominio$ ./minio server data 上記のようにすればよさそうである。なお、USERは3文字以上、PASSWORDは8文字以上が必要。 ここでは、ローカル環境用にお試し用のID、パスワードとした。 mcコマンドの利用 MinioのアドミンガイドとMinioのクライアントガイド を参考に、いくつか動作確認、設定してみる。 12$ cd ~/Minio$ wget https://dl.min.io/client/mc/release/linux-amd64/mc エイリアスを設定。 1$ ./mc alias set myminio http://172.24.88.24:9000 minio miniominio 情報を確認。 12345$ ./mc admin info myminio● 172.24.88.24:9000 Uptime: 6 minutes Version: 2021-06-09T18:51:39Z Network: 1/1 OK バケットを作成 12$ ./mc mb myminio/testBucket created successfully `myminio/test`. ファイルを書き込み 123$ echo test &gt; /tmp/test.txt$ ./mc cp /tmp/test.txt myminio/test//tmp/test.txt: 5 B / 5 B ┃┃ 224 B/s 0s s3コマンドで利用 開発のためにローカルにもS3が欲しいというわがまま、MINIOが叶えてくれました を参照し、 S3としてアクセスしてみる。 123456$ aws --profile myminio configureAWS Access Key ID [None]: minioAWS Secret Access Key [None]: miniominioDefault region name [None]:Default output format [None]:$ aws --endpoint-url http://127.0.0.1:9000 --profile myminio s3 mb s3://test2 上記のように、エンドポイントを指定して利用する。 12$ aws --endpoint-url http://127.0.0.1:9000 --profile myminio s3 cp /tmp/test.txt s3://test2upload: ../../../tmp/test.txt to s3://test2/test.txt アップデート 以下のメッセージが出たので、試しにアップデートしてみた。 12You are running an older version of MinIO released 2 months agoUpdate: Run `mc admin update` まず、Minioのサーバを起動しておく。 1$ ./minio server data この後別のターミナルを開き、エイリアすを設定しておく。 1$ ./mc alias set myminio http://172.24.32.250:9000 minioadmin minioadmin アップデート実行 12$ ./mc admin update myminioServer `myminio` updated successfully from 2021-06-09T18:51:39Z to 2021-08-25T00-41-18Z 署名済みURL発行 boto3 を用いて MinIO でユーザの追加と Security Token Service(STS) による一時認証情報での署名済み URL を発行する を参考に、 上記環境で署名済みURLを発行してみる。 これにより、あるオブジェクトへのアクセス権をデリゲート可能になるはずである。 S3互換サービスのためのエイリアス 前述の通り、 mc alias を利用して、S3互換サービスのためのエイリアスを作成済みである。 ここでは、その作成した myminio を利用する。 作業用バケット作成 今回の動作確認で使用するバケットを作成する。 1$ ./mc mb myminio/share ポリシーファイルの作成 以下のファイルを作成する。 share.json 123456789101112131415161718$ cat &lt;&lt; EOF &gt; share.json&gt; &#123;&gt; \"Version\": \"2012-10-17\",&gt; \"Statement\": [&gt; &#123;&gt; \"Action\": [&gt; \"s3:GetObject\",&gt; \"s3:PutObject\"&gt; ],&gt; \"Effect\": \"Allow\",&gt; \"Resource\": [&gt; \"arn:aws:s3:::share/*\"&gt; ],&gt; \"Sid\": \"\"&gt; &#125;&gt; ]&gt; &#125;&gt; EOF ポリシーを適用 mc admin policy の通り、AWSのIAM互換のポリシーを利用し、Minioのポリシーを設定する。 先程作成したファイルを用いる。 1$ ./mc admin policy add myminio share share.json 適用されたことを確かめる 12345678910111213141516$ ./mc admin policy info myminio share&#123; \"Version\": \"2012-10-17\", \"Statement\": [ &#123; \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\" ], \"Resource\": [ \"arn:aws:s3:::share/*\" ] &#125; ]&#125; ユーザ作成 mc admin userを利用したユーザ作成 を利用して、 動作確認のためのユーザを作成する。 1$ ./mc admin user add myminio bob bob123456 なお、上記ドキュメントでは mc admin user add ALIAS ACCESSKEY SECRETKEY と記載されているが、ACCESSKEY にしていた内容がユーザ名になるようだ。 ユーザにポリシーを設定 1$ ./mc admin policy set myminio share user=bob ユーザ作成され、ポリシーが設定されたことを確かめる。 12$ ./mc admin user list myminioenabled bob share 共有の動作確認用のファイルの準備 手元でファイルを作り、Minio上にアップロードしておく。 12$ echo \"hoge\" &gt; test.txt$ ./mc cp test.txt myminio/share/test.txt Python botoを利用して一時アカウント発行＆署名済みURL発行 ウェブUIからの発行 実は、ウェブUIからも発行できるため、先にそちらを試した。 ウェブUIからの署名済みURL発行の例 オブジェクトブラウザから辿りオブジェクトのページを開き、共有マークを押下すると発行できた。 実際にアクセスしたところ、ファイル本体にアクセスできた。 前提 Pythonバージョン: 3.7.10 利用パッケージ: boto3、jupyter ソースコードの例 https://github.com/dobachi/MinioPresignedURLExample にサンプルを載せておくことにする。 こちらの手順で特に問題なく、発行できた。 （トラブルシュート）署名済みURLにアクセスしてみたらエラー 署名済みURLを発行してアクセスしてみたら、以下のようなエラーが生じた。 123456789&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access Denied.&lt;/Message&gt;&lt;Key&gt;test.txt&lt;/Key&gt;&lt;BucketName&gt;share&lt;/BucketName&gt;&lt;Resource&gt;/share/test.txt&lt;/Resource&gt;&lt;RequestId&gt;16A34CDD784CBCCC&lt;/RequestId&gt;&lt;HostId&gt;baec9ee7-bfb0-441b-a70a-493bfd80d745&lt;/HostId&gt;&lt;/Error&gt; ひとまず、./mc admin traceを利用してデバッグメッセージを確認しながらアクセスしてみる。 生じたエラーメッセージは以下の通り。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061172.24.32.250:9000 [REQUEST s3.GetObject] [2021-09-10T00:34:26:000] [Client IP: 172.24.32.1]172.24.32.250:9000 GET /share/test.txt?xxxxxxxx172.24.32.250:9000 Proto: HTTP/1.1172.24.32.250:9000 Host: 172.24.32.250:9000172.24.32.250:9000 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9172.24.32.250:9000 Accept-Encoding: gzip, deflate172.24.32.250:9000 Cache-Control: max-age=0172.24.32.250:9000 Content-Length: 0172.24.32.250:9000 Dnt: 1172.24.32.250:9000 Upgrade-Insecure-Requests: 1172.24.32.250:9000 Accept-Language: ja,en;q=0.9,en-GB;q=0.8,en-US;q=0.7172.24.32.250:9000 Connection: keep-alive172.24.32.250:9000 Cookie: token=xxxxxxxxx172.24.32.250:9000 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36 Edg/92.0.902.84172.24.32.250:9000 &lt;BODY&gt;172.24.32.250:9000 [RESPONSE] [2021-09-10T00:34:26:000] [ Duration 387µs ↑ 132 B ↓ 617 B ]172.24.32.250:9000 403 Forbidden172.24.32.250:9000 Accept-Ranges: bytes172.24.32.250:9000 Content-Length: 294172.24.32.250:9000 Server: MinIO172.24.32.250:9000 X-Content-Type-Options: nosniff172.24.32.250:9000 X-Amz-Request-Id: 16A34EBDA67B7B88172.24.32.250:9000 X-Xss-Protection: 1; mode=block172.24.32.250:9000 Content-Security-Policy: block-all-mixed-content172.24.32.250:9000 Content-Type: application/xml172.24.32.250:9000 Strict-Transport-Security: max-age=31536000; includeSubDomains172.24.32.250:9000 Vary: Origin172.24.32.250:9000 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Request has expired&lt;/Message&gt;&lt;Key&gt;test.txt&lt;/Key&gt;&lt;BucketName&gt;share&lt;/BucketName&gt;&lt;Resource&gt;/share/test.txt&lt;/Resource&gt;&lt;RequestId&gt;16A34EBDA67B7B88&lt;/RequestId&gt;&lt;HostId&gt;baec9ee7-bfb0-441b-a70a-493bfd80d745&lt;/HostId&gt;&lt;/Error&gt;172.24.32.250:9000172.24.32.250:9000 [REQUEST s3.ListObjectsV1] [2021-09-10T00:34:26:000] [Client IP: 172.24.32.1]172.24.32.250:9000 GET /favicon.ico172.24.32.250:9000 Proto: HTTP/1.1172.24.32.250:9000 Host: 172.24.32.250:9000172.24.32.250:9000 Accept: image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8172.24.32.250:9000 Cache-Control: no-cache172.24.32.250:9000 Connection: keep-alive172.24.32.250:9000 Cookie: token=xxxxxxxxxxxxxxxxxxx172.24.32.250:9000 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36 Edg/92.0.902.84172.24.32.250:9000 Accept-Encoding: gzip, deflate172.24.32.250:9000 Accept-Language: ja,en;q=0.9,en-GB;q=0.8,en-US;q=0.7172.24.32.250:9000 Content-Length: 0172.24.32.250:9000 Dnt: 1172.24.32.250:9000 Pragma: no-cache172.24.32.250:9000 Referer: http://172.24.32.250:9000/share/test.txt?xxxxxxxxxxxxxx172.24.32.250:9000172.24.32.250:9000 [RESPONSE] [2021-09-10T00:34:26:000] [ Duration 184µs ↑ 121 B ↓ 596 B ]172.24.32.250:9000 403 Forbidden172.24.32.250:9000 X-Xss-Protection: 1; mode=block172.24.32.250:9000 Content-Length: 273172.24.32.250:9000 Content-Type: application/xml172.24.32.250:9000 Strict-Transport-Security: max-age=31536000; includeSubDomains172.24.32.250:9000 X-Amz-Request-Id: 16A34EBDAA2A2630172.24.32.250:9000 X-Content-Type-Options: nosniff172.24.32.250:9000 Accept-Ranges: bytes172.24.32.250:9000 Content-Security-Policy: block-all-mixed-content172.24.32.250:9000 Server: MinIO172.24.32.250:9000 Vary: OriginAccept-Encoding172.24.32.250:9000 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access Denied.&lt;/Message&gt;&lt;BucketName&gt;favicon.ico&lt;/BucketName&gt;&lt;Resource&gt;/favicon.ico&lt;/Resource&gt;&lt;RequestId&gt;16A34EBDAA2A2630&lt;/RequestId&gt;&lt;HostId&gt;baec9ee7-bfb0-441b-a70a-493bfd80d745&lt;/HostId&gt;&lt;/Error&gt;172.24.32.250:9000 あまり追加情報はなさそう。ひとまず、いずれにせよ403 Forbidenエラーであることがわかる。 で、結論としては、PythonでURLを発行する際に指定したポリシが間違っていた。 以下のようにすべきところを、 123\"Resource\": [ \"arn:aws:s3:::share/*\"], 以下のように * が抜けていた。 123\"Resource\": [ \"arn:aws:s3:::share/\"], 結果として指定するスコープが小さすぎたということか。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Minio","slug":"Knowledge-Management/Storage-Layer/Minio","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Minio/"}],"tags":[{"name":"Minio","slug":"Minio","permalink":"https://dobachi.github.io/memo-blog/tags/Minio/"},{"name":"Storage","slug":"Storage","permalink":"https://dobachi.github.io/memo-blog/tags/Storage/"},{"name":"S3","slug":"S3","permalink":"https://dobachi.github.io/memo-blog/tags/S3/"}]},{"title":"PubNub","slug":"PubNub","date":"2021-05-06T04:28:14.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/05/06/PubNub/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/05/06/PubNub/","excerpt":"","text":"参考 公式ドキュメント blog サンプル メモ 概要 動作確認 アカウント作成 （補足）公式チュートリアルのソースコード プロジェクトディレクトリ作成 ファイル （補足）ERROR: Could not find a version that satisfies the requirement pubnub&gt;=4.5.1 参考 aa 公式ドキュメント The PubNub Platform Overview SDK PubNubのGitHub Pythonチュートリアル Python API サインアップ プライシング Files API for PubNub Python SDK download-file send-file blog PubNubで5分でリアルタイムWebこと初め サンプル PubNubExample メモ 概要 The PubNub Platform Overview によると、 クイックスタートの一覧 とのこと。 概要にIoTと記載されている通り、小型のデバイス利用も想定されているようだ。 主な用途は「メッセージング」と「シグナリング」。 特徴は以下の通り。 特徴の一覧 サイズの小さなメッセージに限らず、画像、動画、ファイルを対象に含めているのがポイントか。 ユースケースは以下の通り。 ユースケースの一覧 これ自体に特筆すべきポイントはない。 SDK に記載のある通り、非常に幅広いSDKを提供している。 PubNubのGitHub を見る限り、多様なクライアントライブラリがOSSとして公開、開発されているようだ。 動作確認 プライシング によるとアカウントを作ってお試しする限りでは無料のようだ。 ということで、Pythonチュートリアル の通り実行してみる。 このチュートリアルでは、PubSubモデルのメッセージングを試せる。 アカウント作成 サインアップ からアカウント作成。 もろもろ情報入れて登録したら、Demo Keysetが得られた。 （補足）公式チュートリアルのソースコード 以下のようにクローンできる。 1[dobachi@home Sources]$ git clone git@github.com:pubnub/python-quickstart-platform.git 今回はゼロから作ってみるので、これは参考とする。 プロジェクトディレクトリ作成 適当なところに空ディレクトリを作り、プロジェクトのルートとする。 12[dobachi@home Sources]$ mkdir pubnub_example[dobachi@home Sources]$ cd pubnub_example/ 今回は、pyenvを利用してインストールしたPython3.8.10を利用し、venvで環境構築する。 1234[dobachi@home pubnub_example]$ pyenv local 3.8.10[dobachi@home pubnub_example]$ python -m venv venv[dobachi@home pubnub_example]$ . ./venv/bin/activate(venv) [dobachi@home pubnub_example]$ pip install 'pubnub&gt;=4.5.1' ここで、Pythonチュートリアル に従い、2個の実装pub.py、sub.pyを作成する。 このとき、ソースコードのキーのところに先ほどアカウント作成した際に得られたキーを記載するようにする。 ここから端末を2個立ち上げる。 1個目でサブスクライバーを起動 12345678(venv) [dobachi@home pubnub_example]$ python sub.py**************************************************** Waiting for updates to The Guide about Earth... ****************************************************[STATUS: PNConnectedCategory]connected to channels: ['the_guide', 'the_guide-pnpres'][PRESENCE: join]uuid: serverUUID-SUB, channel: the_guide 2個目でパブリッシャーを起動 12345678(venv) [dobachi@home pubnub_example]$ python pub.py****************************************** Submit updates to The Guide for Earth ** Enter 42 to exit this process ******************************************Enter an update for Earth: Harmless[PUBLISH: sent]timetoken: 16202911568484264 プロンプトが表示されるので、「Harmless」と入力してエンターキーを押下。 サブスクライバーに以下のように表示される。 12[MESSAGE received]Earth: Harmless Pythonチュートリアル に実装の説明があるが、 基本的には サブスクライバーはハンドラの実装のみ パブリッシャーは標準入力からメッセージを受け取り、それを送信するのみ という単純な仕組み。 パブリッシャー側のポイントは、 1envelope = pubnub.publish().channel(CHANNEL).message(the_message).sync() まだ詳しくは分からないが、チャンネルやエントリという概念がありそう。 チャンネルを使うと同報できるのだろうか。 Python API あたりを読むと記載ありそう。 ファイル Files API for PubNub Python SDK を読むと、ファイルを取り扱うAPIがあった。 5MBまでのファイルをアップロードできるようだ。 アプリケーションで画像を取り扱う場合などを想定している様子。 これを簡単に動作確認してみる。 Pythonチュートリアル のサンプルコードpub.pyとsub.pyを参考に実装する。 参考実装を PubNubExample にまとめておいた。 READMEを参照。 メッセージングのAPIとは異なるが、簡単に小さなファイルを登録できるのは便利ではある。 またファイルを登録したことを通知するAPIもあるようだ。 これは使い方としては ファイルを送信し、合わせてファイル送信したことをチャンネルにメッセージ送信 受信者は、メッセージを受領したら、その情報を使ってファイル受信 ということだと思われる。 ファイルの送受信自体は、メッセージングのようにSubscribeできず、 単発実行のAPIであり、かつ受信についてはIDを引数に渡さないといけないから、と考えられる。 （補足）ERROR: Could not find a version that satisfies the requirement pubnub&gt;=4.5.1 以下のようなエラーが出たら、 1ERROR: Could not find a version that satisfies the requirement pubnub&gt;=4.5.1 libffi-devel をインストールしたうえで、Pythonをビルドするようにするとよい。 123(venv) [dobachi@home pubnub_example]$ pyenv uninstall 3.8.10(venv) [dobachi@home pubnub_example]$ sudo yum install -y libffi-devel(venv) [dobachi@home pubnub_example]$ pyenv install 3.8.10","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"BaaS","slug":"Knowledge-Management/BaaS","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/BaaS/"}],"tags":[{"name":"Messaging System","slug":"Messaging-System","permalink":"https://dobachi.github.io/memo-blog/tags/Messaging-System/"},{"name":"BaaS","slug":"BaaS","permalink":"https://dobachi.github.io/memo-blog/tags/BaaS/"}]},{"title":"Use any fonts in CentOS7","slug":"Use-any-fonts-in-CentOS7","date":"2021-04-30T01:55:16.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2021/04/30/Use-any-fonts-in-CentOS7/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/04/30/Use-any-fonts-in-CentOS7/","excerpt":"","text":"参考 メモ 元ネタ 参考 Microsoft Windows用TrueTypeフォントを使用する メモ /usr/share/fonts以下に適当なディレクトリを作成。 1$ sudo mkdir /usr/share/fonts/original フォントを格納 1$ sudo cp ＜何かしらのフォント＞ /usr/share/fonts/original 更新。 123$ cd /usr/share/fonts/original$ sudo mkfontdir$ sudo mkfontscale 元ネタ Microsoft Windows用TrueTypeフォントを使用する が参考になった。 （が、MS社フォントは今回は用いていない）","categories":[],"tags":[]},{"title":"CentOS on WSL","slug":"CentOS-on-WSL","date":"2021-04-25T04:30:49.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2021/04/25/CentOS-on-WSL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/04/25/CentOS-on-WSL/","excerpt":"","text":"参考 メモ CentOS CentOS7 CentOS8 Stream Fedora 参考 wsl2上で無料でCentOS8を動かそう CentOS-WSL CentOS7.zip メモ Redhat系LinuxをWSL（ないしWSL2）で動かしたい、という話。 wsl2上で無料でCentOS8を動かそう で挙げられているレポジトリはアーカイブされているため、 代替手段で対応した。 CentOS CentOS7 CentOS-WSL を参考に、まずは手堅くCentOS7。 wsl2上で無料でCentOS8を動かそう を参考にすすめる。 CentOS7.zip をダウンロードし解凍。 中に含まれている rootfs.tar.gz を解凍しておきます。 ここでは、 C:\\Users\\dobachi\\Downloads\\CentOS7\\rootfs.tar\\rootfs.tar として解凍されたものとします。 インポート先のディレクトリがなければ、予め作成。 ここでは以下にしました。（他のイメージと同じ感じで） 1C:\\Users\\dobachi\\AppData\\Local\\Packages\\CentOS7 1wsl --import CentOS7 \"C:\\Users\\dobachi\\AppData\\Local\\Packages\\CentOS7\" \"C:\\Users\\dobachi\\Downloads\\CentOS7\\rootfs.tar\\rootfs.tar\" 必要に応じてWSL2用に変換。 1wsl --set-version CentOS7 2 その他、必要に応じてWindows Terminalの設定を行う。 また日本語とGUI利用のための設定は、UbuntuとCentOSは少し違うので注意。 日本語でのハマりポイントは以下。 localectlが使えない /etc/profileに直接ロケール情報を記載すると良い Ubuntuとは日本語対応方法が異なる CentOSではibus-kkcを利用すると良い CentOS8 Stream ＜あとで書く＞ Fedora ＜あとで書く＞","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"CentOS","slug":"Knowledge-Management/WSL/CentOS","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/CentOS/"}],"tags":[{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"CentOS","slug":"CentOS","permalink":"https://dobachi.github.io/memo-blog/tags/CentOS/"}]},{"title":"readPredicates in inserting records","slug":"readPredicates-in-inserting-records","date":"2021-04-23T00:28:13.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2021/04/23/readPredicates-in-inserting-records/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/04/23/readPredicates-in-inserting-records/","excerpt":"","text":"参考 メモ 前提 動機 準備 簡単な動作確認 readPredicatesに書き込みが生じるケース 参考 メモ 前提 Delta Lake 0.7.0 動機 Delta Lakeのトランザクション管理について、 org.apache.spark.sql.delta.OptimisticTransactionImpl#readPredicates がどのように利用されるか、を確認した。 特にInsert時に、競合確認するかどうかを決めるにあたって重要な変数である。 org/apache/spark/sql/delta/OptimisticTransaction.scala:600 1234567val predicatesMatchingAddedFiles = ExpressionSet(readPredicates).iterator.flatMap &#123; p =&gt; val conflictingFile = DeltaLog.filterFileList( metadata.partitionSchema, addedFilesToCheckForConflicts.toDF(), p :: Nil).as[AddFile].take(1) conflictingFile.headOption.map(f =&gt; getPrettyPartitionMessage(f.partitionValues))&#125;.take(1).toArray 準備 SparkをDelta Lakeと一緒に起動する。 （ここではついでにデバッガをアタッチするコンフィグを付与している。不要なら、--driver-java-optionsを除外する） 1dobachi@home:~/tmp$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" --driver-java-options \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=localhost:5005\" ファイルはSparkに含まれているサンプルファイルを使うことにする。 12345scala&gt; val filePath = \"/home/dobachi/Sources/spark/examples/src/main/resources/users.parquet\"filePath: String = /home/dobachi/Sources/spark/examples/src/main/resources/users.parquetscala&gt; val df = spark.read.parquet(filePath)df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field] こんな感じのデータ 1234567scala&gt; df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+ テーブル作成 1df.write.format(\"delta\").save(\"users1\") ここまでで後でデータを追記するためのテーブル作成が完了。 簡単な動作確認 ここで、競合状況を再現するため、もうひとつターミナルを開き、Sparkシェルを起動した。 1/opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" ターミナル1でデバッガを有効化しながら、以下を実行。 12val tblPath = \"users1\"df.write.format(\"delta\").mode(\"append\").save(tblPath) ちなみにブレイクポイントは以下の箇所に設定した。 org/apache/spark/sql/delta/OptimisticTransaction.scala:482 123deltaLog.store.write( deltaFile(deltaLog.logPath, attemptVersion), actions.map(_.json).toIterator) ターミナル2で以下を実行し、コンパクション実施。 12345678910val tblPath = \"users1\"spark.read .format(\"delta\") .load(tblPath) .repartition(2) .write .option(\"dataChange\", \"false\") .format(\"delta\") .mode(\"overwrite\") .save(tblPath) ターミナル1の処理で使用しているデバッガで、以下のあたりにブレイクポイントをおいて確認した。 org/apache/spark/sql/delta/OptimisticTransaction.scala:595 12345val addedFilesToCheckForConflicts = commitIsolationLevel match &#123; case Serializable =&gt; changedDataAddedFiles ++ blindAppendAddedFiles case WriteSerializable =&gt; changedDataAddedFiles // don't conflict with blind appends case SnapshotIsolation =&gt; Seq.empty&#125; この場合、org.apache.spark.sql.delta.OptimisticTransactionImpl#readPredicates はこのタイミングでは空のArrayBufferだった。 readPredicatesに書き込みが生じるケース 以下の通り。 org/apache/spark/sql/delta/OptimisticTransaction.scala:239 1234567891011121314def filterFiles(filters: Seq[Expression]): Seq[AddFile] = &#123; val scan = snapshot.filesForScan(Nil, filters) val partitionFilters = filters.filter &#123; f =&gt; DeltaTableUtils.isPredicatePartitionColumnsOnly(f, metadata.partitionColumns, spark) &#125; readPredicates += partitionFilters.reduceLeftOption(And).getOrElse(Literal(true)) readFiles ++= scan.files scan.files&#125;/** Mark the entire table as tainted by this transaction. */def readWholeTable(): Unit = &#123; readPredicates += Literal(true)&#125; org.apache.spark.sql.delta.OptimisticTransactionImpl#filterFiles メソッドは主にファイルを消すときに用いられる。 DELETEするとき、Updateするとき、Overwriteするとき（条件付き含む）など。 org.apache.spark.sql.delta.OptimisticTransactionImpl#readWholeTable メソッドは ストリーム処理で書き込む際に用いられる。 書き込もうとしているテーブルを読んでいる場合はreadPredicatesに真値を追加する。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"Double count of dstat","slug":"Double-count-of-dstat","date":"2021-04-08T23:28:03.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2021/04/09/Double-count-of-dstat/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/04/09/Double-count-of-dstat/","excerpt":"","text":"参考 メモ 結論 事象の確認 関連する実装は？ doolの登場 参考 total disk data include virtual lvm device data (so doubling the calculated values) dstat-real dstat dstatが開発終了した話 dool pcp-dstatのman メモ dstatのtotalが2倍？になる？という話があり簡単に確認。 結論 ディスク単位のI/Oを total に入れるべきなのだが、 正規表現上の仕様により、 xvda1 などのパーティション単位のI/Oも total に足されている。 実は、パーティションの値を取り除く正規表現ではあるのだが、当時の実装（※）としては xvd で始まるディスク名に対応できていなかったようだ。 なお、その後、いくつか正規表現が改変されたような経緯がコミットログやGitHub Issueから見られた。 ※CentOS7等で採用されたdstat0.7.2のリリース時期は2010年ころ 事象の確認 環境情報の確認。 12345678910111213141516171819202122232425262728293031[centos@ip-10-0-0-217 ~]$ uname -aLinux ip-10-0-0-217.ap-northeast-1.compute.internal 3.10.0-1127.13.1.el7.x86_64 #1 SMP Tue Jun 23 15:46:38 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux[centos@ip-10-0-0-217 ~]$ dstat --versionDstat 0.7.2Written by Dag Wieers &lt;dag@wieers.com&gt;Homepage at http://dag.wieers.com/home-made/dstat/Platform posix/linux2Kernel 3.10.0-1127.13.1.el7.x86_64Python 2.7.5 (default, Apr 2 2020, 13:16:51)[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]Terminal type: xterm-256color (color support)Terminal size: 86 lines, 310 columnsProcessors: 4Pagesize: 4096Clock ticks per secs: 100internal: aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock, mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time, udp, unix, vm/usr/share/dstat: battery, battery-remain, cpufreq, dbus, disk-tps, disk-util, dstat, dstat-cpu, dstat-ctxt, dstat-mem, fan, freespace, gpfs, gpfs-ops, helloworld, innodb-buffer, innodb-io, innodb-ops, lustre, memcache-hits, mysql-io, mysql-keys, mysql5-cmds, mysql5-conn, mysql5-io, mysql5-keys, net-packets, nfs3, nfs3-ops, nfsd3, nfsd3-ops, ntp, postfix, power, proc-count, qmail, rpc, rpcd, sendmail, snooze, squid, test, thermal, top-bio, top-bio-adv, top-childwait, top-cpu, top-cpu-adv, top-cputime, top-cputime-avg, top-int, top-io, top-io-adv, top-latency, top-latency-avg, top-mem, top-oom, utmp, vm-memctl, vmk-hba, vmk-int, vmk-nic, vz-cpu, vz-io, vz-ubc, wifi[centos@ip-10-0-0-217 ~]$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTxvda 202:0 0 60G 0 disk└─xvda1 202:1 0 60G 0 part / dstat 0.7.2は2010年にリリースされたようだ。 12$ git log -1 --format=%ai 0.7.22010-06-14 22:25:44 +0000 参考までにpcpのバージョン 12[centos@ip-10-0-0-217 ~]$ pcp --versionpcp version 4.3.2 ここから動作確認。 fioを実行。 123[centos@ip-10-0-0-217 ~]$ fio -rw=randwrite -bs=16k -size=1000m -iodepth=32 -directory=/tmp -direct=1 -invalidate=1 -runtime=300 -numjobs=8 -name=iotest -ioengine=libaio -group_reporting(snip) 裏でdstatを実行 1234567[centos@ip-10-0-0-217 ~]$ dstat -td -D total,xvda ----system---- -dsk/total----dsk/xvda- time | read writ: read writ09-04 08:40:50| 373k 315k: 187k 158k09-04 08:40:51| 0 95M: 0 47M09-04 08:40:52| 0 87M: 0 44M(snip) およそ2倍になっている。 pcp-dstatだと？ 12345678910[centos@ip-10-0-0-217 ~]$ pcp dstat -td -D total,xvda----system---- --dsk/xvda---dsk/total- time | read writ: read writ09-04 09:16:47| :09-04 09:16:48| 0 41M: 0 41M09-04 09:16:49| 0 45M: 0 45M09-04 09:16:50| 0 50M: 0 50M09-04 09:16:51| 0 35M: 0 35M^(snip) ということで、2倍になっていない。 ちなみに、そもそもpcp-dstatは、上記のdstatとは全く異なる実装に見える。 doolだと？（2021/4/8時点のmasterブランチ） 123456789[centos@ip-10-0-0-217 Sources]$ ./dool/dool -td -D total,xvda-----system---- -dsk/total----dsk/xvda- time | read writ: read writApr-09 11:24:54| 577k 3307k: 289k 1653kApr-09 11:24:55| 0 66M: 0 33MApr-09 11:24:56| 0 25M: 0 13MApr-09 11:24:57| 0 44M: 0 22M^(snip) ということで、dstatと同様。 関連する実装は？ dstat-real dstat の実装を確認する。 dstat_disk クラスのコンストラクタに、後に用いられるフィルタ用の正規表現定義がある。 1683 self.diskfilter = re.compile('^(dm-\\d+|md\\d+|[hsv]d[a-z]+\\d+)$') この正規表現を利用する箇所はいくつかあるが、その一つが total の値を計算するところ。 このフィルタにマッチしない場合に、 total に足される計算になっている。 12728 if not self.diskfilter.match(name):729 self.set2['total'] = ( self.set2['total'][0] + long(l[5]), self.set2['total'][1] + long(l[9]) ) ここで正規表現の中身を見ると、例えば dm-0 、 sda1 のような「パーティション名」を表すものとマッチするようになっている。 つまり、 not 条件と合わせて、「パーティション名以外のものを total に足す」という動作になる。 このとき、よく見ると、今回使用した環境で用いている、 xvda1 という名称は正規表現にマッチしないことがわかる。 したがって total に足されてしまい、なぜか total の表示が大きくなる、という事象が生じたと思われる。 total disk data include virtual lvm device data (so doubling the calculated values) にも似たような議論があるようだ。 doolの登場 dstatの源流は今はメンテされておらず、 dool が開発されている。 Dool is a Python3 compatible clone of Dstat. また dstatが開発終了した話 に記載されているが、他にもpcp系列のdstatが残っている。 ただし、doolにおいても、正規表現は同じようなものだった。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Monitering","slug":"Knowledge-Management/Monitering","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Monitering/"}],"tags":[{"name":"dstat","slug":"dstat","permalink":"https://dobachi.github.io/memo-blog/tags/dstat/"}]},{"title":"When DeltaLog ID is created","slug":"When-DeltaLog-ID-is-created","date":"2021-04-03T15:13:19.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2021/04/04/When-DeltaLog-ID-is-created/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/04/04/When-DeltaLog-ID-is-created/","excerpt":"","text":"参考 メモ 前提 createRelationから確認する 参考 メモ Delta LakeのDelta LogのIDがいつ確定するのか、というのが気になり確認した。 前提 Delta Lakeバージョン：0.8.0 createRelationから確認する org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation をエントリポイントとする。 ポイントは、DeltaLog がインスタンス化されるときである。 まず最初にインスタンス化されるのは以下。 org/apache/spark/sql/delta/sources/DeltaDataSource.scala:141 1val deltaLog = DeltaLog.forTable(sqlContext.sparkSession, path) org.apache.spark.sql.delta.DeltaLog は、 org.apache.spark.sql.delta.SnapshotManagement トレイトをミックスインしている。 当該トレイトには、 currentSnapshot というメンバ変数があり、これは org.apache.spark.sql.delta.SnapshotManagement#getSnapshotAtInit メソッドを利用し得られる。 org/apache/spark/sql/delta/SnapshotManagement.scala:47 1@volatile protected var currentSnapshot: Snapshot = getSnapshotAtInit このメソッドは以下のように定義されている。 org/apache/spark/sql/delta/SnapshotManagement.scala:184 123456789101112131415161718192021protected def getSnapshotAtInit: Snapshot = &#123; try &#123; val segment = getLogSegmentFrom(lastCheckpoint) val startCheckpoint = segment.checkpointVersion .map(v =&gt; s\" starting from checkpoint $v.\").getOrElse(\".\") logInfo(s\"Loading version $&#123;segment.version&#125;$startCheckpoint\") val snapshot = createSnapshot( segment, minFileRetentionTimestamp, segment.lastCommitTimestamp) lastUpdateTimestamp = clock.getTimeMillis() logInfo(s\"Returning initial snapshot $snapshot\") snapshot &#125; catch &#123; case e: FileNotFoundException =&gt; logInfo(s\"Creating initial snapshot without metadata, because the directory is empty\") // The log directory may not exist new InitialSnapshot(logPath, this) &#125;&#125; ポイントは、スナップショットを作る際に用いられるセグメントである。 セグメントにバージョン情報が持たれている。 ここでは3行目の 1val segment = getLogSegmentFrom(lastCheckpoint) にて org.apache.spark.sql.delta.SnapshotManagement#getLogSegmentFrom メソッドを用いて、 前回チェックポイントからセグメントの情報が生成される。 なお、参考までにLogSegmentクラスの定義は以下の通り。 org/apache/spark/sql/delta/SnapshotManagement.scala:392 1234567891011case class LogSegment( logPath: Path, version: Long, deltas: Seq[FileStatus], checkpoint: Seq[FileStatus], checkpointVersion: Option[Long], lastCommitTimestamp: Long) &#123; override def hashCode(): Int = logPath.hashCode() * 31 + (lastCommitTimestamp % 10000).toInt (snip) 上記の通り、コンストラクタ引数にバージョン情報が含まれていることがわかる。 インスタンス化の例は以下の通り。 org/apache/spark/sql/delta/SnapshotManagement.scala:140 1234567LogSegment( logPath, newVersion, deltasAfterCheckpoint, newCheckpointFiles, newCheckpoint.map(_.version), lastCommitTimestamp)","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"Power Grid Data","slug":"Power-Grid-Data","date":"2021-02-11T16:03:03.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/02/12/Power-Grid-Data/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/02/12/Power-Grid-Data/","excerpt":"","text":"参考 メモ 参考 以下の資料参照。 メモ 電力データ活用に関連する、割と新しい情報をまとめた。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Power Grid Data","slug":"Knowledge-Management/Power-Grid-Data","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Power-Grid-Data/"}],"tags":[{"name":"Power Grid Data","slug":"Power-Grid-Data","permalink":"https://dobachi.github.io/memo-blog/tags/Power-Grid-Data/"}]},{"title":"Mesh wifi for Home Network","slug":"Mesh-wifi","date":"2021-01-31T13:59:00.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/01/31/Mesh-wifi/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/01/31/Mesh-wifi/","excerpt":"","text":"参考 メッシュWiFiルータ JPNE v6plus So-net au光 動作確認 メモ メッシュWiFi JPNE v6プラス auひかりのHGW（ホームゲートウェイ） so-net IPv6の動作確認 参考 メッシュWiFiルータ メッシュルーターの選び方について整理した Vol.83 メッシュ（Mesh）Wi-Fiって何？メリットと活用方法を紹介 MR2200ac JPNE v6plus v6プラス(IPv6/IPv4インターネットサービス) Synology製Wi-FiルーターMR2200acでv6プラス接続設定 So-net So-net 光 with フレッツ S 東日本 So-net 光 プラスの次世代通信 v6プラス So-net 光 プラス au光 auひかりのホームゲートウェイはいらない? 知っておきたい不都合な真実 au光で市販のルーターを使う ブリッジモードにせずDMZの下でルーターモードで利用するには 動作確認 ipv6 test メモ メッシュWiFi メッシュWiFiとは、Elecomの Vol.83 メッシュ（Mesh）Wi-Fiって何？メリットと活用方法を紹介 記事の通り。 戸建ての家などでは、広く安定してカバーできるはず、ということで。 日本国内においてどの機器を使うか？については、 メッシュルーターの選び方について整理した がわかりやすい。 個人的にはIPv6対応している点で、「Synology MR2200ac」が候補に上がった。 仕様は MR2200ac を参照されたし。 JPNE v6プラス v6プラス(IPv6/IPv4インターネットサービス) によると、 「v6プラス」は、NTT東西の次世代ネットワーク（NGN）を利用しインターネット接続を提供するISP事業者が、 IPv6及びIPv4の設備を持たずに、インターネット接続をお客さま(エンドユーザ)にご提供いただくためのサービスです。 とのこと。 Synology製Wi-FiルーターMR2200acでv6プラス接続設定 によると、MR2200acでv6plusを利用する方法が記載されている。 auひかりのHGW（ホームゲートウェイ） auひかりのホームゲートウェイはいらない? 知っておきたい不都合な真実 にも記載の通り、 先ほどから触れてるとおり、auひかりはホームゲートウェイが無ければインターネットに接続することができません。 その理由は、ホームゲートウェイ内に、KDDIの認証を取る機能があるからです。 とのこと。 そこで、自前のルータを利用したい場合は、 DMZ機能を利用して、ルータ機能をHGWと自前ルータの療法で動かす設定をすることもある。 au光で市販のルーターを使うが参考になる。 ブリッジモードにせずDMZの下でルーターモードで利用するには も参考になる。こちらはtp-linkのブログ。 so-net So-net 光 with フレッツ S 東日本 は、フレッツ光の契約と思われる。 So-net 光 プラスの次世代通信 v6プラス は v6プラス を利用したサービスと思われる。 So-net 光 プラスの次世代通信 v6プラスの対象サービス によると、 So-net 光 プラス などが対応している。 その他フレッツ系のサービスが対応しているようだ。 IPv6の動作確認 ipv6 test にアクセスると良い。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Home Network","slug":"Knowledge-Management/Home-Network","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Home-Network/"}],"tags":[{"name":"WiFi","slug":"WiFi","permalink":"https://dobachi.github.io/memo-blog/tags/WiFi/"},{"name":"WiFi6","slug":"WiFi6","permalink":"https://dobachi.github.io/memo-blog/tags/WiFi6/"},{"name":"IPv6","slug":"IPv6","permalink":"https://dobachi.github.io/memo-blog/tags/IPv6/"}]},{"title":"Memo of How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh","slug":"Memo-of-How-to-Move-Beyond-a-Monolithic-Data-Lake-to-a-Distributed-Data-Mesh","date":"2021-01-18T16:06:28.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/01/19/Memo-of-How-to-Move-Beyond-a-Monolithic-Data-Lake-to-a-Distributed-Data-Mesh/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/01/19/Memo-of-How-to-Move-Beyond-a-Monolithic-Data-Lake-to-a-Distributed-Data-Mesh/","excerpt":"","text":"参考 メモ 参考 メモ Zhamak DehghaniによるHow to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh を読んで簡単にまとめた。 この文章では、ドメインが所有し、提供するデータプロダクトの相互利用に基づく「データメッシュ」の考え方を提唱するものである。 固有技術そのものではなく、アーキテクチャ検討の前の「データを基礎としたサービス開発、ソフトウェア開発のためのデータの取り回し方」に関する示唆を与えようとしたもの、という理解。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Mesh","slug":"Knowledge-Management/Data-Mesh","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Mesh/"}],"tags":[{"name":"Data Mesh","slug":"Data-Mesh","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Mesh/"},{"name":"Data Lake","slug":"Data-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Lake/"}]},{"title":"Memo of Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics","slug":"Memo-of-Lakehouse-A-New-Generation-of-Open-Platforms-that-Unify-Data-Warehousing-and-Advanced-Analytics","date":"2021-01-16T14:18:14.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2021/01/16/Memo-of-Lakehouse-A-New-Generation-of-Open-Platforms-that-Unify-Data-Warehousing-and-Advanced-Analytics/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/01/16/Memo-of-Lakehouse-A-New-Generation-of-Open-Platforms-that-Unify-Data-Warehousing-and-Advanced-Analytics/","excerpt":"","text":"参考 メモ 参考 Lakehouse A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics メモ Lakehouse A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics の論文を確認し、 簡単なメモを作成した。 考え方を節異名しつつ、Delta Lake、Delta Engineを使用した際のTPC−DSの実行結果、クエリ実行コストの情報が記載されていた。 また関連研究、参考文献も過去から現在のトレンドの背景を知る上で参考になるものが多い。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"Getting started of Ansible Vault","slug":"Getting-started-of-Ansible-Vault","date":"2021-01-07T00:43:12.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2021/01/07/Getting-started-of-Ansible-Vault/","link":"","permalink":"https://dobachi.github.io/memo-blog/2021/01/07/Getting-started-of-Ansible-Vault/","excerpt":"","text":"参考 公式サイト ブログ メモ 暗号化されたファイルの作成 （参考）復号して平文化 （参考）暗号化されたファイルの編集 （参考）Vaultパスワードをファイルとして渡す プレイブック内で変数として利用 参考 公式サイト Encrypting content with Ansible Vault ブログ Ansible-Vaultを用いた機密情報の暗号化のTips メモ Ansible内で使用する変数を平文でファイルに記載し、プレイブック集に入れ込むのに不安を感じるときがある。 そのようなとき、Ansible Vaultを利用すると暗号化された状態で変数を管理できる。 Ansible Vaultで管理された変数をプレイブック内で通常の変数と同様に扱えるため見通しが良くなる。 公式サイトの Encrypting content with Ansible Vault が網羅的でわかりやすいのだが、 具体例が足りない気がしたので以下に一例を示しておく。 ここに挙げた以外の使い方は、公式サイトを参照されたし。 Ansible-Vaultを用いた機密情報の暗号化のTips も参考になった。 暗号化されたファイルの作成 secret.yml 内に変数を記述し、暗号化する。 後ほど暗号化されたファイルを変数定義ファイルとして読み込む。 ここでは以下のような内容とする。 secret.yml 12hoge: fuga: foo ファイルを暗号化する。 1$ ansible-vault create secret.yml 上記ファイルを作成する際、Vault用パスワードを聞かれるので適切なパスワードを入力すること。 あとでパスワードは利用する。 （参考）復号して平文化 1$ ansible-vault decrypt secret.yml （参考）暗号化されたファイルの編集 1$ ansible-vault edit secret.yml （参考）Vaultパスワードをファイルとして渡す プロンプトで入力する代わりに、どこかプレイブック集の外などにVaultパスワードを保存し利用することもできる。 ここでは、 ~/.vault_password にパスワードを記載したファイルを準備したものとする。 編集する例を示す。 1$ ansible-vault edit secret.yml --vault-password-file ~/.vault_password プレイブック内で変数として利用 変数定義ファイルとして渡し、プレイブック内で変数として利用する。 以下のようなプレイブックを作る。 test.yml 1234567- hosts: localhost vars_files: - secret.yml tasks: - name: debug debug: msg: \"&#123;&#123; hoge.fuga &#125;&#125;\" 以下、簡単な説明。 変数定義ファイルとして secret.yml を指定（ vars_files の箇所） 今回はdebugモジュールを利用し、変数内の値を表示することとする。 暗号化された変数定義ファイル secret.yml に記載されたとおり、構造化された変数 hoge.fuga の値を利用する。 結果として、foo という内容がSTDOUTに表示されるはず。 プレイブックを実行する。 1$ ansible-playbook test.yml --ask-vault-pass","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Configuration Management","slug":"Knowledge-Management/Configuration-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Configuration-Management/"},{"name":"Ansible","slug":"Knowledge-Management/Configuration-Management/Ansible","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Configuration-Management/Ansible/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://dobachi.github.io/memo-blog/tags/Ansible/"},{"name":"Vault","slug":"Vault","permalink":"https://dobachi.github.io/memo-blog/tags/Vault/"}]},{"title":"Use static image on Hexo blog","slug":"Use-static-image-on-Hexo-blog","date":"2020-12-31T14:58:34.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/12/31/Use-static-image-on-Hexo-blog/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/12/31/Use-static-image-on-Hexo-blog/","excerpt":"","text":"参考 メモ 参考 Hexo 記事に画像を貼り付ける Global-Asset-Folder メモ Hexo 記事に画像を貼り付ける を参考に、もともとCacooのリンクを使っていた箇所を すべてスタティックな画像を利用するようにした。 post_asset_folderを利用して、記事ごとの画像ディレクトリを利用することも考えたが、 画像はひとところに集まっていてほしいので、 Global-Asset-Folder を利用することにした。 なお、上記ブログでは 1プロジェクトトップ/images/site 以下に画像を置き、 1![猫](/images/site/cat.png) のようにリンクを指定していた。 自身の環境では、rootを指定しているので 1プロジェクトトップ/image 以下にディレクトリを置き、 1![猫](memo-blog/images/cat.png) と指定することにした。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"}]},{"title":"Delta Lake with Alluxio","slug":"Delta-Lake-with-Alluxio","date":"2020-12-30T16:13:15.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/12/31/Delta-Lake-with-Alluxio/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/12/31/Delta-Lake-with-Alluxio/","excerpt":"","text":"参考 論文 Hadoop環境 Alluxioドキュメント メモ 疑似分散で動作確認 実行環境の準備 フォーマット、起動 Sparkの起動確認 Delta Lakeを通じて書き込む動作確認 （補足）HDFS上のディレクトリ権限に関するエラー 参考 論文 Delta Lake High-Performance ACID Table Storage over Cloud Object Stores Lakehouse A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics Hadoop環境 ansible-bigdata Alluxioドキュメント Examples: Use Alluxio as Input and Output Alluxio Security メモ Delta Lake High-Performance ACID Table Storage over Cloud Object Stores や Lakehouse A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics の論文の通り、 Delta Lakeはキャッシュとの組み合わせが可能である。 今回は、ストレージにHDFS、キャッシュにAlluxioを使って動作確認する。 疑似分散で動作確認 実行環境の準備 ansible-bigdata あたりを参考に、Hadoopの疑似分散環境を構築する。 Bigtopベースの2.8.5とした。 併せて、同Ansibleプレイブック集などを用いて、Spark3.1.1のコミュニティ版を配備した。 併せて、Alluxioは2.5.0-2を利用。 Alluxioに関しては、以下のようにコンパイルしてパッケージ化して用いることできる。 123456$ sudo -u alluxio mkdir /usr/local/src/alluxio$ sudo -u alluxio chown alluxio:alluxio /usr/local/src/alluxio$ cd /usr/local/src/alluxio$ sudo -u alluxio git clone git://github.com/alluxio/alluxio.git$ sudu -u alluxio git checkout -b v2.5.0-2 refs/tags/v2.5.0-2$ sudo -u alluxio mvn install -Phadoop-2 -Dhadoop.version=2.8.5 -DskipTests コンフィグとしては以下を利用。 123456789101112131415161718192021222324252627$ cat conf/alluxio-site.properties(snip)# Common properties# alluxio.master.hostname=localhostalluxio.master.hostname=localhost# alluxio.master.mount.table.root.ufs=$&#123;alluxio.work.dir&#125;/underFSStorage# alluxio.master.mount.table.root.ufs=/tmpalluxio.master.mount.table.root.ufs=hdfs://localhost:8020/alluxio# Security properties# alluxio.security.authorization.permission.enabled=true# alluxio.security.authentication.type=SIMPLEalluxio.master.security.impersonation.yarn.users=*alluxio.master.security.impersonation.yarn.groups=*\u001c# Worker properties# alluxio.worker.ramdisk.size=1GBalluxio.worker.ramdisk.size=1GB# alluxio.worker.tieredstore.levels=1# alluxio.worker.tieredstore.level0.alias=MEM# alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk# User properties# alluxio.user.file.readtype.default=CACHE# alluxio.user.file.writetype.default=MUST_CACHE ポイントは以下の通り。 疑似分散環境のHDFS利用 Alluxioの使用するディレクトリとして、/alluxioを利用 マスタはローカルホストで起動 フォーマット、起動 12$ sudo -u alluxio ./bin/alluxio format$ sudo -u alluxio ./bin/alluxio-start.sh local SudoMount テストを実行 1$ sudo -u alluxio ./bin/alluxio runTests もしエラーが生じた場合は、例えばHDFSの/alluxioディレクトリに、 適切な権限設定、所有者設定がされているかどうかを確認すること。 Alluxioが起動すると以下のようなUIを確認できる（ポート19999） AlluxioのUI 先程テストで書き込まれたファイル群が見られるはず。 Alluxioに書き込まれたテストファイル群I ここでは、上記の通り、環境を整えた前提で以下説明する。 Sparkの起動確認 Examples: Use Alluxio as Input and Output を参考に、Alluxio経由での読み書きを試す。 予め、今回の動作確認で使用するテキストデータ（AlluxioのREADME）をアップロードしておく。 1$ sudo -u alluxio /opt/alluxio/default//bin/alluxio fs copyFromLocal /opt/alluxio/default/LICENSE /LICENSE 予め、以下のような設定をspark-defaults.confに入れておく。 Alluxioのクライアントライブラリを用いられるように。 12spark.driver.extraClassPath /opt/alluxio/default/client/alluxio-2.5.0-2-client.jarspark.executor.extraClassPath /opt/alluxio/default/client/alluxio-2.5.0-2-client.jar Sparkが起動することを確認する。ここではDelta Lakeも含めて起動する。 1234$ /usr/local/spark/default/bin/spark-shell \\ --packages io.delta:delta-core_2.12:0.8.0 \\ --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" \\ --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" 起動したシェルでAlluxio上のREADMEファイルを読み取り、行数を確認する。 123456scala&gt; val pathOnAlluxio = \"alluxio://localhost:19998/LICENSE\"scala&gt; val testDF = spark.read.text(pathOnAlluxio)scala&gt; testDF.countres0: Long = 482 Delta Lakeを通じて書き込む動作確認 準備として、Alluxio上に、dobachiユーザ用のディレクトリを作成してみる。 123$ sudo -u alluxio /opt/alluxio/default/bin/alluxio fs mkdir /users$ sudo -u alluxio /opt/alluxio/default/bin/alluxio fs mkdir /users/dobachi$ sudo -u alluxio /opt/alluxio/default/bin/alluxio fs chown dobachi:dobachi /users/dobachi 先程起動しておいたシェルで、Delta Lake形式のデータを書き込んで見る。 123456scala&gt; val data = spark.range(0, 5)data: org.apache.spark.sql.Dataset[Long] = [id: bigint]scala&gt; val outputUrl = \"alluxio://localhost:19998/users/dobachi/numbers\"scala&gt; data.write.format(\"delta\").save(outputUrl) すると以下のようなエラーが生じた。 12345678910scala&gt; data.write.format(&quot;delta&quot;).save(outputUrl)21/01/05 22:47:50 ERROR HDFSLogStore: The error typically occurs when the default LogStore implementation, that is, HDFSLogStore, is used to write into a Delta table on a non-HDFS storage system. In order to get the transactional ACID guarantees on table updates, you have to use the correct implementation of LogStore that is appropriate for your storage system. See https://docs.delta.io/latest/delta-storage.html &quot; for details.org.apache.hadoop.fs.UnsupportedFileSystemException: fs.AbstractFileSystem.alluxio.impl=null: No AbstractFileSystem configured for scheme: alluxio(snip) 当たり前だが、Delta Lakeの下回りのストレージとして標準では、 Alluxioが対応しておらず、LogStoreからエラーが生じた、ということのようだ。 一瞬、LogStoreを新たに開発しないといけないか？と思ったものの、よく考えたら、HDFSHadoopFileSystemLogStoreから Alluxioのスキーマを認識させてアクセスできるようにすればよいだけでは？と思った。 そこで、Hadoopの設定でAlluxioFileSystemをalluxioスキーマ（ファイルシステムのスキーマ）に明示的に登録してみる。 /etc/hadoop/conf/core-site.xmlに以下を追記。 12345&lt;property&gt; &lt;name&gt;fs.AbstractFileSystem.alluxio.impl&lt;/name&gt; &lt;value&gt;alluxio.hadoop.AlluxioFileSystem&lt;/value&gt; &lt;description&gt;The FileSystem for alluxio uris.&lt;/description&gt;&lt;/property&gt; 再びSparkを立ち上げ、適当なデータを書き込み。 12345scala&gt; val data = spark.range(0, 5)scala&gt; val outputUrl = \"alluxio://localhost:19998/users/dobachi/numbers\"scala&gt; data.write.format(\"delta\").save(outputUrl) Alluxio上にDelta Lakeで保存された様子 以上のように書き込みに成功した。 つづいて、テーブルとして読み出す。 1234567891011scala&gt; val df = spark.read.format(\"delta\").load(outputUrl)scala&gt; df.show+---+| id|+---+| 1|| 2|| 3|| 4|| 0|+---+ テーブルへの追記。 1234567891011121314151617scala&gt; val addData = spark.range(5, 10)scala&gt; addData.write.format(\"delta\").mode(\"append\").save(outputUrl)scala&gt; df.show+---+| id|+---+| 3|| 9|| 6|| 5|| 1|| 8|| 4|| 2|| 7|| 0|+---+ また、追記書き込みをしたのでDeltaログが増えていることが分かる。 （3回ぶんのログがあるのは、↑には記載していないがミスったため） Alluxio上にDelta LakeのDeltaログ （補足）HDFS上のディレクトリ権限に関するエラー Sparkでの処理実行時にYARNで実行していたところ、Executorにおける処理からAlluxioを呼び出すときにエラー。 yarnユーザでのアクセスとなり、HDFS上の /alluxio へのアクセス権がなかったと考えられる。 1221/01/05 02:54:35 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, hadoop-pseudo, executor 2): alluxio.exception.status.UnauthenticatedException: Channel authentication failed with code:UNAUTHENTICATED. Channel: GrpcChannelKey&#123;ClientType=FileSystemMasterClient, ClientHostname=hadoop-pseudo.mshome.net, ServerAddress=GrpcServerAddress&#123;HostName=localhost, SocketAddress=localhost:19998&#125;, ChannelId=81f7d97f-8e32-4289-bcab-ea6008d5ffac&#125;, AuthType: SIMPLE, Error: alluxio.exception.status.UnauthenticatedException: Plain authentication failed: Failed to authenticate client user=&quot;yarn&quot; connecting to Alluxio server and impersonating as impersonationUser=&quot;vagrant&quot; to access Alluxio file system. User &quot;yarn&quot; is not configured to allow any impersonation. Please read the guide to configure impersonation at https://docs.alluxio.io/os/user/2.4/en/operation/Security.html at alluxio.exception.status.AlluxioStatusException.from(AlluxioStatusException.java:141) Alluxio Security のドキュメント中に「Client-Side Hadoop Impersonation」を読むと、 「なりすまし」を許可する設定があるようだ。 そこで、yarnユーザが様々なユーザになりすませるような簡易設定を以下のように加えることにした。 実際の運用する際は、なりすましのスコープに注意したほうが良さそうだ。 conf/alluxio-site.properties 12alluxio.master.security.impersonation.yarn.users=*alluxio.master.security.impersonation.yarn.groups=* ドキュメントではクライアントで alluxio.security.login.impersonation.username も指定するよう書かれていたが、 起動時にしてしなくてもアクセスできるようになった。 あとで実装を調べたほうが良さそうだ。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"},{"name":"Alluxio","slug":"Alluxio","permalink":"https://dobachi.github.io/memo-blog/tags/Alluxio/"}]},{"title":"Information about trends in 2020","slug":"Information-about-trends-in-2020","date":"2020-12-26T13:27:40.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2020/12/26/Information-about-trends-in-2020/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/12/26/Information-about-trends-in-2020/","excerpt":"","text":"参考 ガートナー データ・ファブリック DNAコンピューティング アダプティブラーニング デジタル・ツイン IOWN メモ 先進テクノロジのハイプ・サイクル：2020年 挙げられていたトレンドと所感 個人的にピックアップしたキーワードと所感 週刊ダイヤモンド 2020年12/12号 IOWN公開情報 参考 ガートナー ガートナー、「先進テクノロジのハイプ・サイクル：2020年」を発表 データ・ファブリック データファブリックを実現するクラウドデータサービス ネットアップとNEC、国内初となるデータファブリックソリューションの提供を開始 NetAppのデータ ファブリックは時代遅れなのか？ 世界のデータファブリックの市場 Talend Data Fabric Splunk エンタープライズ マシン データ ファブリック DNAコンピューティング DNAコンピューティング（日本生物理学会） DNAコンピューティングの原理と展開（小倉裕介） アダプティブラーニング 米軍の教育でもAIが活躍 「アダプティブ・ラーニング」の可能性 (1/4) デジタル・ツイン ヒトと社会のデジタル化世界 ─ デジタルツインコンピューティング ─ IOWNでヒトもデジタル化 “私のコピー”が仮想空間で仕事する世界 IOWN 週刊ダイヤモンド 2020年12/12号 IOWN公式ウェブサイト メモ オープンになっている情報を中心に既存情報を軽くまとめる。 特にデータ処理、データ活用との関連を探る。 先進テクノロジのハイプ・サイクル：2020年 ガートナー、「先進テクノロジのハイプ・サイクル：2020年」を発表 の記事によると、 2020/8/19にハイプ・サイクルが発表された。 記事冒頭で触れられているとおり、COVID-19の影響は否定できない。 例ではヘルス・パスポート、ソーシャル・ディスタンシング・テクノロジが挙げられていた。 記載されているととおり、「過度な期待」として初登場する技術は少ないのだが、 それに該当するということで注目されている。 挙げられていたトレンドと所感 デジタル・ミー つまりは、デジタル・ツイン 多分にもれずデジタル・ツイン関連は取り上げられていた。 過去になかったデータを使うようになるという側面はあるが、 それに合わせた新たなアーキテクチャが必要になるか。 過去のブーム面との例 大量の顧客行動情報、ログ：非構造データ処理に適した並列分散処理フレームワーク（MapReduceなど） 大量の画像データ：深層学習 大量のセンサーデータ：IoT、ストリーム処理 大量のデジタル・ツイン・データ：？ コンポジット・アーキテクチャ 一言で言うとデータ・ファブリック※1を基礎としたアーキテクチャ ※1）つまりオンプレ、クラウドに点在するデータを最適な場所に配置し、いつでも利用可能にする 重要なのは柔軟性を実現するという目論見 柔軟性を求める組織に必要とされるアーキテクチャと考えることもできそうだ フォーマティブAI 一言で言うと状況に動的に対応するAI 動的に適応していく、さらに突き詰めると「自律的に…」というニュアンスが生じそう AIに関する倫理の議論と関連すると考えられる。 アルゴリズムによる信頼 一言で言うと、責任ある権限に基づく信頼モデルから、アルゴリズムによる信頼への転換 シリコンの先へ 一言で言うと、シリコンに変わる新素材に対する期待 参考 DNAコンピューティング（日本生物理学会）、DNAコンピューティングの原理と展開（小倉裕介） 個人的にピックアップしたキーワードと所感 アダプティブな機械学習 「学習」という領域におけるパーソナライズの一つの形と思えばよいか 近年ではAIの普及もあり、機械的な支援をもとに実現することも増えてきたか。 参考 米軍の教育でもAIが活躍 「アダプティブ・ラーニング」の可能性 (1/4) シチズン・ツイン Google検索では適切な結果は得られなかった 人のデジタル・ツイン シチズン・ツインと同様、モノのデジタル化に続いて、ヒトのデジタル化を目指す、ということか。 NTTサービスイノベーション総合研究所の提唱する「デジタルツインコンピューティング」では、 層の中間層を設けることが重要とされている。 参考 ヒトと社会のデジタル化世界 ─ デジタルツインコンピューティング ─ IOWNでヒトもデジタル化 “私のコピー”が仮想空間で仕事する世界 データ・ファブリック 「オンプレ、クラウドに点在するデータを最適な場所に配置し、いつでも利用可能にする」というコンセプトは特徴的だが、 ベンダによって実際の定義が異なるように感じる。 「データ民主化」と同様のコンセプトに感じる。 ポイントはストレージとしての特徴ではなく、「統合」を実現する側面か 参考情報 データファブリックを実現するクラウドデータサービス、ネットアップとNEC、国内初となるデータファブリックソリューションの提供を開始、 NetAppのデータ ファブリックは時代遅れなのか？、世界のデータファブリックの市場、Talend Data Fabric、 Splunk エンタープライズ マシン データ ファブリック 週刊ダイヤモンド 2020年12/12号 週刊ダイヤモンド 2020年12/12号 の中に、 「NTT帝国の逆襲」という特集記事がある。 技術というより、事業職が強いが念の為に（あえて）技術に関連するキーワードだけ述べる。 なお、この書籍で重要なのは、本来はどちらかというとビジネス戦略側面である。 詳細は当該書籍を参照されたし。 技術キーワード IOWN構想 光ファイバー、ネットワーク転送、音声等のNTT特許数は圧倒的である。 電力効率100話い、伝送容量125倍、遅延を200分の1を目指す 光電子融合型の光トランジスタ開発によりリード。 O-RAN 通信規格を主導。オープン化で囲い込みを打破する。 V-RAN 通信の仮想化 IOWN公開情報 あくまで公開された情報ではあるがリスト化しておく。 IOWN公式ウェブサイト 概要や目指しているところを理解できるコンテンツが公開されている 構成要素は以下の通り。 「オールフォトニクス・ネットワーク」 「大容量光伝送システム・デバイス」、「イジングマシン」、「光格子時計ネットワーク」 「デジタルツインコンピューティング」 「多様な産業やモノとヒトのデジタルツインを自在に掛け合わせて演算を行う」ことが特色 「コグニティブ・ファウンデーション」 ICTリソースを全体最適に調和させ、必要な情報をNW内に流通させる。","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Trends","slug":"Research/Trends","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Trends/"}],"tags":[{"name":"Trends","slug":"Trends","permalink":"https://dobachi.github.io/memo-blog/tags/Trends/"}]},{"title":"Reference of connector plugin","slug":"Reference-of-connector-plugin","date":"2020-12-16T14:31:01.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2020/12/16/Reference-of-connector-plugin/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/12/16/Reference-of-connector-plugin/","excerpt":"","text":"参考 メモ kafka-connect-syslog 動作確認 kafka-connect-datagen 概要 実装状況 動作確認 参考 kafka-connect-syslog kafka-connect-datagen kafka-connect-datagenのGitHub ConfluentのGitHub Kafkaのクイックスタート Confluent Platform and Apache Kafka Compatibility avro-random-generator kafka-connect-datagenのサンプルスキーマ メモ 2020/12時点で、Kafka Connectのプラグインの参考になるもの探す。 kafka-connect-syslog kafka-connect-syslog が最も簡易に動作確認できそうだった。 ただ、 ConfluentのGitHub を見る限り、GitHub上には実装が公開されていないようだった。 動作確認 ここでは、ローカルに簡易実験用の1プロセスのKafkaを起動した前提とする。 起動方法は Kafkaのクイックスタート を参照。 kafka-connect-syslog のパッケージをダウンロードして /opt/connectors 以下に展開。 123$ cd /opt/connectors$ sudo unzip confluentinc-kafka-connect-syslog-1.3.2.zip$ sudo chown -R kafka:kafka confluentinc-kafka-connect-syslog-1.3.2 というパッケージが展開される。 動作確認に使用するプロパティは以下。 12345678910111213$ cat etc/minimal-tcp.properties## Copyright [2016 - 2019] Confluent Inc.#name=syslog-tcptasks.max=1connector.class=io.confluent.connect.syslog.SyslogSourceConnectorsyslog.port=5515syslog.listener=TCPconfluent.licenseconfluent.topic.bootstrap.servers=localhost:9092confluent.topic.replication.factor=1 また、Connectの設定には以下を追加する。 /opt/kafka_pseudo/default/config/connect-standalone.properties 123(snip)plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors, プラグインを置く場所として、 /opt/connectors を指定した。 /opt/kafka_pseudo/default/bin/connect-standalone.sh を利用して、 スタンドアローンモードでKafka Connectを起動。 12$ sudo -u kafka /opt/kafka_pseudo/default/bin/connect-standalone.sh /opt/kafka_pseudo/default/config/connect-standalone.properties \\ /opt/connectors/confluentinc-kafka-connect-syslog-1.3.2/etc/minimal-tcp.properties 起動したのを確認し、別の端末から適当なデータを送信。 1$ echo \"&lt;34&gt;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 - Your refrigerator is running\" | nc -v -w 1 localhost 5515 Console Consumerを利用して書き込み状況を確認。 12$ cd $&#123;KAFKA_HOME&#125;$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic syslog --from-beginning 先程書き込んだものが表示されるはずである。 kafka-connect-datagen kafka-connect-datagen もあった。 kafka-connect-datagenのGitHub に実装も公開されているように見える。 ドキュメントのリンクから、当該レポジトリのREADMEにジャンプしたため、そのように判断。 以降、v0.4.0を対象として確認したものである。 概要 指定されたスキーマで、ダミーデータを生成するコネクタ。 avro-random-generator を内部的に利用している。 スキーマ指定はAvroのスキーマファイルを渡す方法もあるし、 組み込みのスキーマを指定する方法もある。 kafka-connect-datagenのサンプルスキーマ を参照。 また、Kafkaに出力する際のフォーマットは指定可能。 Kafka Connect自体の一般的なパラメータである、 value.converter を指定すれば良い。 例えば以下の通り。 1&quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;, 実装状況 2020/12/21時点では本プロジェクトのバージョンは0.4.0であり、 12345&lt;parent&gt; &lt;groupId&gt;io.confluent&lt;/groupId&gt; &lt;artifactId&gt;common-parent&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt;&lt;/parent&gt; の通り、Confluent Platformのバージョンとしては、6系である。 Confluent Platform and Apache Kafka Compatibility によると、 Confluent Platform 6系のKafkaバージョンは2.6.Xである。 io.confluent.kafka.connect.datagen.DatagenConnector io.confluent.kafka.connect.datagen.DatagenConnector クラスは、 org.apache.kafka.connect.source.SourceConnector を継承している。 割と素直な実装。 io.confluent.kafka.connect.datagen.DatagenConnector#start メソッドは特別なことはしておらず、 コンフィグをロードするだけ。 io.confluent.kafka.connect.datagen.DatagenConnector#taskConfigs メソッドも 特別なことはしていない。start時に受け取ったプロパティから taskConfigを生成して返す。 io.confluent.kafka.connect.datagen.DatagenConnector#stop メソッド および、 io.confluent.kafka.connect.datagen.DatagenConnector#config もほぼ何もしない。 タスクには io.confluent.kafka.connect.datagen.DatagenTask クラスを利用する。 io.confluent.kafka.connect.datagen.DatagenTask クラス io.confluent.kafka.connect.datagen.DatagenTask#start メソッドが overrideされている。 以下、ポイントを確認する。 オフセット管理の仕組みあり。 io/confluent/kafka/connect/datagen/DatagenTask.java:133 1234567Map&lt;String, Object&gt; offset = context.offsetStorageReader().offset(sourcePartition);if (offset != null) &#123; // The offset as it is stored contains our next state, so restore it as-is. taskGeneration = ((Long) offset.get(TASK_GENERATION)).intValue(); count = ((Long) offset.get(CURRENT_ITERATION)); random.setSeed((Long) offset.get(RANDOM_SEED));&#125; io.confluent.avro.random.generator.Generator のジェネレータ（のビルダ）を利用する。 io/confluent/kafka/connect/datagen/DatagenTask.java:141 123Generator.Builder generatorBuilder = new Generator.Builder() .random(random) .generation(count); クイックスタートの設定があれば、それに従ってスキーマを読み込む。 io/confluent/kafka/connect/datagen/DatagenTask.java:144 1234567891011121314151617181920String quickstartName = config.getQuickstart();if (quickstartName != \"\") &#123; try &#123; quickstart = Quickstart.valueOf(quickstartName.toUpperCase()); if (quickstart != null) &#123; schemaFilename = quickstart.getSchemaFilename(); schemaKeyField = schemaKeyField.equals(\"\") ? quickstart.getSchemaKeyField() : schemaKeyField; try &#123; generator = generatorBuilder .schemaStream(getClass().getClassLoader().getResourceAsStream(schemaFilename)) .build(); &#125; catch (IOException e) &#123; throw new ConnectException(\"Unable to read the '\" + schemaFilename + \"' schema file\", e); &#125; &#125; &#125; catch (IllegalArgumentException e) &#123; log.warn(\"Quickstart '&#123;&#125;' not found: \", quickstartName, e); &#125; 指定されたクイックスタート名に従い、パッケージに含まれるスキーマファイルを読み込み、 それを適用しながらジェネレータを生成する。 なお、クイックスタートのたぐいはenumで定義されている。 io/confluent/kafka/connect/datagen/DatagenTask.java:75 1234567891011121314151617181920212223242526272829protected enum Quickstart &#123; CLICKSTREAM_CODES(\"clickstream_codes_schema.avro\", \"code\"), CLICKSTREAM(\"clickstream_schema.avro\", \"ip\"), CLICKSTREAM_USERS(\"clickstream_users_schema.avro\", \"user_id\"), ORDERS(\"orders_schema.avro\", \"orderid\"), RATINGS(\"ratings_schema.avro\", \"rating_id\"), USERS(\"users_schema.avro\", \"userid\"), USERS_(\"users_array_map_schema.avro\", \"userid\"), PAGEVIEWS(\"pageviews_schema.avro\", \"viewtime\"), STOCK_TRADES(\"stock_trades_schema.avro\", \"symbol\"), INVENTORY(\"inventory.avro\", \"id\"), PRODUCT(\"product.avro\", \"id\"); private final String schemaFilename; private final String keyName; Quickstart(String schemaFilename, String keyName) &#123; this.schemaFilename = schemaFilename; this.keyName = keyName; &#125; public String getSchemaFilename() &#123; return schemaFilename; &#125; public String getSchemaKeyField() &#123; return keyName; &#125;&#125; クイックスタートが設定されておらず、スキーマの文字列が与えられた場合は、 それを用いてジェネレータが生成される。 io/confluent/kafka/connect/datagen/DatagenTask.java:164 12&#125; else if (schemaString != \"\") &#123; generator = generatorBuilder.schemaString(schemaString).build(); それ以外の場合、つまりスキーマ定義の書かれたファイルを指定する場合は、 以下の通り。 io/confluent/kafka/connect/datagen/DatagenTask.java:166 1234567891011121314151617&#123; String err = \"Unable to read the '\" + schemaFilename + \"' schema file\"; try &#123; generator = generatorBuilder.schemaStream(new FileInputStream(schemaFilename)).build(); &#125; catch (FileNotFoundException e) &#123; // also look in jars on the classpath try &#123; generator = generatorBuilder .schemaStream(DatagenTask.class.getClassLoader().getResourceAsStream(schemaFilename)) .build(); &#125; catch (IOException inner) &#123; throw new ConnectException(err, e); &#125; &#125; catch (IOException e) &#123; throw new ConnectException(err, e); &#125; &#125; 最後のAvroに関連する情報を生成して終了。 io/confluent/kafka/connect/datagen/DatagenTask.java:184 123avroSchema = generator.schema();avroData = new AvroData(1);ksqlSchema = avroData.toConnectSchema(avroSchema); io.confluent.kafka.connect.datagen.DatagenTask#poll メソッドもoverrideされている。 以下、ポイントを記載する。 インターバル機能あり。 io/confluent/kafka/connect/datagen/DatagenTask.java:192 12345678if (maxInterval &gt; 0) &#123; try &#123; Thread.sleep((long) (maxInterval * Math.random())); &#125; catch (InterruptedException e) &#123; Thread.interrupted(); return null; &#125;&#125; ジェネレータを利用し、オブジェクトを生成する。 io/confluent/kafka/connect/datagen/DatagenTask.java:201 12345678final Object generatedObject = generator.generate();if (!(generatedObject instanceof GenericRecord)) &#123; throw new RuntimeException(String.format( \"Expected Avro Random Generator to return instance of GenericRecord, found %s instead\", generatedObject.getClass().getName() ));&#125;final GenericRecord randomAvroMessage = (GenericRecord) generatedObject; 生成されたオブジェクトから、スキーマ定義に基づいてフィールドの値を取り出し、 バリューのArrayListを生成する。 io/confluent/kafka/connect/datagen/DatagenTask.java:210 123456789101112final List&lt;Object&gt; genericRowValues = new ArrayList&lt;&gt;();for (org.apache.avro.Schema.Field field : avroSchema.getFields()) &#123; final Object value = randomAvroMessage.get(field.name()); if (value instanceof Record) &#123; final Record record = (Record) value; final Object ksqlValue = avroData.toConnectData(record.getSchema(), record).value(); Object optionValue = getOptionalValue(ksqlSchema.field(field.name()).schema(), ksqlValue); genericRowValues.add(optionValue); &#125; else &#123; genericRowValues.add(value); &#125;&#125; キーも同様に取り出し、Kafka Connectの形式に変換する。 io/confluent/kafka/connect/datagen/DatagenTask.java:224 1234567SchemaAndValue key = new SchemaAndValue(DEFAULT_KEY_SCHEMA, null);if (!schemaKeyField.isEmpty()) &#123; key = avroData.toConnectData( randomAvroMessage.getSchema().getField(schemaKeyField).schema(), randomAvroMessage.get(schemaKeyField) );&#125; 先程ArrayListとして取り出したバリューもKafka Connect形式に変換する。 io/confluent/kafka/connect/datagen/DatagenTask.java:233 12final org.apache.kafka.connect.data.Schema messageSchema = avroData.toConnectSchema(avroSchema);final Object messageValue = avroData.toConnectData(avroSchema, randomAvroMessage).value(); イテレートのたびに、メタデータを更新する。 io/confluent/kafka/connect/datagen/DatagenTask.java:246 123456789// The source offsets will be the values that the next task lifetime will restore from// Essentially, the \"next\" state of the connector after this loop completesMap&lt;String, Object&gt; sourceOffset = new HashMap&lt;&gt;();// The next lifetime will be a member of the next generation.sourceOffset.put(TASK_GENERATION, taskGeneration + 1);// We will have produced this recordsourceOffset.put(CURRENT_ITERATION, count + 1);// This is the seed that we just re-seeded for our own next iteration.sourceOffset.put(RANDOM_SEED, seed); 最後に、SourceRecordのリスト形式に変換し、 レコードとして生成して戻り値として返す。 io/confluent/kafka/connect/datagen/DatagenTask.java:261 12345678910111213141516final List&lt;SourceRecord&gt; records = new ArrayList&lt;&gt;();SourceRecord record = new SourceRecord( sourcePartition, sourceOffset, topic, null, key.schema(), key.value(), messageSchema, messageValue, null, headers);records.add(record);count += records.size();return records; つづいて、 io.confluent.kafka.connect.datagen.DatagenTask#stop メソッドだが、 これは特に何もしない。 io.confluent.kafka.connect.datagen.DatagenTask#getOptionalSchema という オプショナルなフィールドに関するスキーマを取得するためのヘルパーメソッドもある。 io.confluent.kafka.connect.datagen.DatagenTask#getOptionalValue メソッドもある。 動作確認 confluentinc-kafka-connect-datagen-0.4.0.zip をダウンロードし、 /opt/connectors以下に展開したものとする。 今回は以下の設定ファイルを参考に、データ生成してみる。 なお、イテレーション回数は適度に修正して用いることを推奨する。 confluentinc-kafka-connect-datagen-0.4.0/etc/connector_users.config 1234567891011121314&#123; \"name\": \"datagen-users\", \"config\": &#123; \"connector.class\": \"io.confluent.kafka.connect.datagen.DatagenConnector\", \"kafka.topic\": \"users\", \"quickstart\": \"users\", \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter.schemas.enable\": \"false\", \"max.interval\": 1000, \"iterations\": 10000000, \"tasks.max\": \"1\" &#125;&#125; 上記はconfluentコマンド用で利用する際のコンフィグファイルである。 そこで以下のようなKafka Connect用の設定ファイルを生成する。 /opt/connectors/confluentinc-kafka-connect-datagen-0.4.0/etc/connector_users.properties 12345678910name=usersconnector.class=io.confluent.kafka.connect.datagen.DatagenConnectorkafka.topic=usersquickstart=userskey.converter=org.apache.kafka.connect.storage.StringConvertervalue.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=falsemax.interval=1000iterations=10tasks.max=1 スタンドアローンモードでKafka Connectを起動する。 123$ sudo -u kafka /opt/kafka_pseudo/default/bin/connect-standalone.sh \\ /opt/kafka_pseudo/default/config/connect-standalone.properties \\ /opt/connectors/confluentinc-kafka-connect-datagen-0.4.0/etc/connector_users.properties 停止した後、結果を確認する。 トピックが作られたことがわかる。 12345$ /opt/kafka_pseudo/default/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list__consumer_offsets_confluent-commandsyslogusers データを確認する。 1234567891011$ /opt/kafka_pseudo/default/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic users --from-beginning&#123;\"registertime\":1501850210149,\"userid\":\"User_8\",\"regionid\":\"Region_3\",\"gender\":\"FEMALE\"&#125;&#123;\"registertime\":1516539876299,\"userid\":\"User_2\",\"regionid\":\"Region_7\",\"gender\":\"OTHER\"&#125;&#123;\"registertime\":1505292095234,\"userid\":\"User_4\",\"regionid\":\"Region_1\",\"gender\":\"OTHER\"&#125;&#123;\"registertime\":1502118362741,\"userid\":\"User_3\",\"regionid\":\"Region_1\",\"gender\":\"FEMALE\"&#125;&#123;\"registertime\":1503193759324,\"userid\":\"User_9\",\"regionid\":\"Region_5\",\"gender\":\"MALE\"&#125;&#123;\"registertime\":1507693509191,\"userid\":\"User_1\",\"regionid\":\"Region_8\",\"gender\":\"OTHER\"&#125;&#123;\"registertime\":1497764008309,\"userid\":\"User_1\",\"regionid\":\"Region_6\",\"gender\":\"OTHER\"&#125;&#123;\"registertime\":1514606256206,\"userid\":\"User_1\",\"regionid\":\"Region_3\",\"gender\":\"MALE\"&#125;&#123;\"registertime\":1492595638722,\"userid\":\"User_2\",\"regionid\":\"Region_6\",\"gender\":\"MALE\"&#125;&#123;\"registertime\":1500602208014,\"userid\":\"User_3\",\"regionid\":\"Region_1\",\"gender\":\"MALE\"&#125; ダミーデータが生成されていることが確認できた。 （WIP）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"Kafak Connect","slug":"Kafak-Connect","permalink":"https://dobachi.github.io/memo-blog/tags/Kafak-Connect/"}]},{"title":"Create projects includes sbt launcher","slug":"Create-project-includes-sbt-launcher","date":"2020-12-06T16:12:13.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/12/07/Create-project-includes-sbt-launcher/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/12/07/Create-project-includes-sbt-launcher/","excerpt":"","text":"参考 メモ Coursierプロジェクト （補足）sbt-launcher-packageプロジェクト （補足）launcherプロジェクト 参考 launcher sbt-launcher-package Coursier を使って最速でScalaの開発環境を整える csのインストール メモ プロジェクト内に、sbt自体を含めてクローンしただけでビルドできるようにしたい、という動機。 現時点ではCoursierプロジェクトが汎用性※が高く便利であると、個人的には感じた。 ※ここでは、多くの環境で実行可能で、様々なツールを一度にセットアップ可能という意味。 Coursierプロジェクト Coursier を使って最速でScalaの開発環境を整える のブログに記載されているとおり、 Scala開発環境を簡単に整えられる。 ひとまずDocker内で試す。 123$ sudo docker pull ubuntu:18.04$ sudo docker run -rm -it -d --name ubu ubuntu:18.04$ sudo docker exec -it ubu /bin/bash Dockerコンテナ内でインストール。 12345# apt update# apt install -y curl# curl -fLo cs https://git.io/coursier-cli-linux &amp;&amp; chmod +x cs &amp;&amp; ./cs 結果として以下がインストールされた様子。 123456789Checking if the standard Scala applications are installed Installed ammonite Installed cs Installed coursier Installed scala Installed scalac Installed sbt Installed sbtn Installed scalafmt ~/.profileに環境変数等がまとまっているので有効化する。 1# source ~/.profile これでインストールされたコマンドが使用できるようになった。 なお、参考までに上記でダウンロードしたcsコマンドは以下の通り。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364$ ./cs setup --helpCommand: setupUsage: cs setup --jvm &lt;string?&gt; --jvm-dir &lt;string?&gt; --system-jvm &lt;bool?&gt; --local-only &lt;bool&gt; --update &lt;bool&gt; --jvm-index &lt;string?&gt; --graalvm-home &lt;string?&gt; --graalvm-option &lt;string*&gt; --graalvm-default-version &lt;string?&gt; --install-dir | --dir &lt;string?&gt; --install-platform &lt;string?&gt; Platform for prebuilt binaries (e.g. &quot;x86_64-pc-linux&quot;, &quot;x86_64-apple-darwin&quot;, &quot;x86_64-pc-win32&quot;) --install-prefer-prebuilt &lt;bool&gt; --only-prebuilt &lt;bool&gt; Require prebuilt artifacts for native applications, don&apos;t try to build native executable ourselves --repository | -r &lt;maven|sonatype:$repo|ivy2local|bintray:$org/$repo|bintray-ivy:$org/$repo|typesafe:ivy-$repo|typesafe:$repo|sbt-plugin:$repo|ivy:$pattern&gt; Repository - for multiple repositories, separate with comma and/or add this option multiple times (e.g. -r central,ivy2local -r sonatype:snapshots, or equivalently -r central,ivy2local,sonatype:snapshots) --default-repositories &lt;bool&gt; --proguarded &lt;bool?&gt; --channel &lt;org:name&gt; Channel for apps --default-channels &lt;bool&gt; Add default channels --contrib &lt;bool&gt; Add contrib channel --file-channels &lt;bool&gt; Add channels read from the configuration directory --cache &lt;string?&gt; Cache directory (defaults to environment variable COURSIER_CACHE, or ~/.cache/coursier/v1 on Linux and ~/Library/Caches/Coursier/v1 on Mac) --mode | -m &lt;offline|update-changing|update|missing|force&gt; Download mode (default: missing, that is fetch things missing from cache) --ttl | -l &lt;duration&gt; TTL duration (e.g. &quot;24 hours&quot;) --parallel | -n &lt;int&gt; Maximum number of parallel downloads (default: 6) --checksum &lt;checksum1,checksum2,...&gt; Checksum types to check - end with none to allow for no checksum validation if no checksum is available, example: SHA-256,SHA-1,none --retry-count &lt;int&gt; Retry limit for Checksum error when fetching a file --cache-file-artifacts | --cfa &lt;bool&gt; Flag that specifies if a local artifact should be cached. --follow-http-to-https-redirect &lt;bool&gt; Whether to follow http to https redirections --credentials &lt;host(realm) user:pass|host user:pass&gt; Credentials to be used when fetching metadata or artifacts. Specify multiple times to pass multiple credentials. Alternatively, use the COURSIER_CREDENTIALS environment variable --credential-file &lt;string*&gt; Path to credential files to read credentials from --use-env-credentials &lt;bool&gt; Whether to read credentials from COURSIER_CREDENTIALS (env) or coursier.credentials (Java property), along those passed with --credentials and --credential-file --quiet | -q &lt;counter&gt; Quiet output --verbose | -v &lt;counter&gt; Increase verbosity (specify several times to increase more) --progress | -P &lt;bool&gt; Force display of progress bars --env &lt;bool&gt; --user-home &lt;string?&gt; --banner &lt;bool?&gt; --yes | -y &lt;bool?&gt; --try-revert &lt;bool&gt; --apps &lt;string*&gt; （補足）sbt-launcher-packageプロジェクト sbt-launcher-package をビルドすることでSBTランチャーを提供できそうだった。 （補足）launcherプロジェクト https://github.com/sbt/launcher/blob/1.x/.java-version にも記載されているとおり、 JDK7系にしか公式に対応していなかった。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Scala","slug":"Knowledge-Management/Scala","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Scala/"},{"name":"SBT","slug":"Knowledge-Management/Scala/SBT","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Scala/SBT/"}],"tags":[{"name":"SBT","slug":"SBT","permalink":"https://dobachi.github.io/memo-blog/tags/SBT/"},{"name":"Scala","slug":"Scala","permalink":"https://dobachi.github.io/memo-blog/tags/Scala/"}]},{"title":"Read and write data on Delta Lake streaming manner","slug":"Read-and-write-data-on-Delta-Lake-streaming-manner","date":"2020-11-18T15:11:42.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2020/11/19/Read-and-write-data-on-Delta-Lake-streaming-manner/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/11/19/Read-and-write-data-on-Delta-Lake-streaming-manner/","excerpt":"","text":"参考 メモ 準備（SBT） 簡単なビルドと実行の例 書き込み Appendモード Completeモード 読み出し 準備 ストリームで読み込んでみる 読み込み時の maxFilesPerTrigger オプション 読み込み時の maxBytesPerTrigger オプション 追記ではなく更新したらどうなるか？ 読み込み時の ignoreDeletes オプション 読み込み時の ignoreChanges オプション org.apache.spark.sql.delta.sources.DeltaSource クラスについて org.apache.spark.sql.delta.sources.DeltaSource#getChanges メソッド org.apache.spark.sql.delta.sources.DeltaSource#getSnapshotAt メソッド org.apache.spark.sql.delta.sources.DeltaSource#getChangesWithRateLimit メソッド org.apache.spark.sql.delta.sources.DeltaSource#getStartingOffset メソッド org.apache.spark.sql.delta.sources.DeltaSource#latestOffset メソッド org.apache.spark.sql.delta.sources.DeltaSource#verifyStreamHygieneAndFilterAddFiles メソッド org.apache.spark.sql.delta.sources.DeltaSource#getBatch メソッド org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits クラスについて org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits#toReadLimit メソッド org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits#admit メソッド 参考 dobachi's StructuredStreamingDeltaLakeExample sbt SparkのUnitTest作成でspark-testing-base spark-testing-base dobachi's sparkProjectTemplate.g8 dobachi's spark-sbt StructuredNetworkWordCount Delta table as a sink Delta table as a stream source Ignore updates and deletes メモ 準備（SBT） 今回はSBTを利用してSalaを用いたSparkアプリケーションで試すことにする。 sbt を参考にSBTをセットアップする。（基本的には対象バージョンをダウンロードして置くだけ） 123$ mkdir -p ~/Sources$ cd ~/Sources$ sbt new dobachi/spark-sbt.g8 ここでは、 [dobachi's spark-sbt.g8] を利用してSpark3.0.1の雛形を作った。 対話的に色々聞かれるので、ほぼデフォルトで生成。 ここでは、 StructuredNetworkWordCount を参考に、Word Countした結果をDelta Lakeに書き込むことにする。 なお、以降の例で示しているアプリケーションは、すべて dobachi's StructuredStreamingDeltaLakeExample に含まれている。 簡単なビルドと実行の例 以下のように予めncコマンドを起動しておく。 1$ nc -lk 9999 続いて、別のターミナルでアプリケーションをビルドして実行。（ncコマンドを起動したターミナルは一旦保留） 123$ cd structured_streaming_deltalake$ sbt assembly$ /opt/spark/default/bin/spark-submit --class com.example.StructuredStreamingDeltaLakeExample target/scala-2.12/structured_streaming_deltalake-assembly-0.0.1.jar localhost 9999 先程起動したncコマンドの引数に合わせ、9999ポートに接続する。 Structured Streamingが起動したら、ncコマンド側で、適当な文字列（単語）を入力する（以下は例） 123hoge hoge fugafuga fugafuga fuga hoge もうひとつターミナルを開き、spark-shellを起動する。 1$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" アプリケーションで出力先としたディレクトリ内のDelta Lakeテーブルを参照すると、以下のようにテーブルが更新されていることがわかる。 ひとまずncコマンドでいろいろな単語を入力して挙動を試すと良い。 12345678910scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta/wordcount\")df: org.apache.spark.sql.DataFrame = [value: string, count: bigint]scala&gt; df.show+-----+-----+|value|count|+-----+-----+| fuga| 5|| hoge| 3|+-----+-----+ (wip) 書き込み Delta table as a sink の通り、Delta LakeテーブルをSink（つまり書き込み先）として利用する際には、 AppendモードとCompleteモードのそれぞれが使える。 Appendモード 例のごとく、ncコマンドを起動し、アプリケーションを実行。（予めsbt assemblyしておく） もうひとつターミナルを開き、spark-shellでDelta Lakeテーブルの中身を確認する。 ターミナル1 1$ nc -lk 9999 ターミナル2 12$ /opt/spark/default/bin/spark-submit --class com.example.StructuredStreamingDeltaLakeAppendExampletarget/scala-2.12/structured_streaming_deltalake-assembly-0.0.1.jar localhost 9999 ターミナル3 1$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" ターミナル1のncで適当な単語列を入力する。 1hoge hoge ターミナル3のspark-shellでDelta Lakeテーブルの中身を確認する。 123456789scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta/wordcount_per_line\")df: org.apache.spark.sql.DataFrame = [unixtime: bigint, count: bigint]scala&gt; df.show+-------------+-----+| unixtime|count|+-------------+-----+|1605968491821| 2|+-------------+-----+ 何度かncコマンド経由でテキストを流し込むと、以下のように行が加わるkとがわかる。 12345678910111213141516scala&gt; df.show+-------------+-----+| unixtime|count|+-------------+-----+|1605968491821| 2|+-------------+-----+scala&gt; df.show+-------------+-----+| unixtime|count|+-------------+-----+|1605968518584| 2||1605968522461| 3||1605968491821| 2|+-------------+-----+ Completeモード Completeモードは、上記のWordCountの例がそのまま例になっている。 com.example.StructuredStreamingDeltaLakeExample 参照。 読み出し Delta table as a stream source の通り、既存のDelta Lakeテーブルからストリームデータを取り出す。 準備 ひとまずDelta Lakeのテーブルを作る。 ここではspark-shellで適当に作成する。 1$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" 12345678910scala&gt; val df = spark.read.format(\"parquet\").load(\"/opt/spark/default/examples/src/main/resources/users.parquet\")scala&gt; df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+scala&gt; df.write.format(\"delta\").save(\"/tmp/delta/users\") 上記のDataFrameのスキーマは以下の通り。 123456scala&gt; df.printSchemaroot |-- name: string (nullable = true) |-- favorite_color: string (nullable = true) |-- favorite_numbers: array (nullable = true) | |-- element: integer (containsNull = true) ストリームで読み込んでみる もうひとつターミナルを立ち上げる。 1$ /opt/spark/default/bin/spark-submit --class com.example.StructuredStreamingDeltaLakeReadExample target/scala-2.12/structured_streaming_deltalake-assembly-0.0.1.jar /tmp/delta/users 先程作成しておいたテーブルの中身が読み込まれる。 1234567891011-------------------------------------------Batch: 0-------------------------------------------20/11/22 00:29:41 INFO CodeGenerator: Code generated in 3.212 ms20/11/22 00:29:41 INFO CodeGenerator: Code generated in 4.1238 ms+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+ 先程の、Delta Lakeテーブルを作成したターミナルのspark-shellで、 追加データを作り、Delta Lakeテーブルに挿入する。 まず、追加データ用のスキーマを持つcase classを作成する。 1scala&gt; case class User(name: String, favorite_color: String, favorite_numbers: Array[Int]) つづいて、作られたcase classを利用して、追加用のDataFrameを作る。 （SeqからDataFrameを生成する） 1scala&gt; val addRecord = Seq(User(\"Bob\", \"yellow\", Array(1,2))).toDF appendモードで既存のDelta Lakeテーブルに追加する。 1scala&gt; addRecord.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/users\") ここで、ストリーム処理を動かしている方のターミナルを見ると、 以下のように追記された内容が表示されていることがわかる。 12345678-------------------------------------------Batch: 1-------------------------------------------+----+--------------+----------------+|name|favorite_color|favorite_numbers|+----+--------------+----------------+| Bob| yellow| [1, 2]|+----+--------------+----------------+ 読み込み時の maxFilesPerTrigger オプション ストリーム読み込み時には、 maxFilesPerTrigger オプションを指定できる。 このオプションは、以下のパラメータとして利用される。 org/apache/spark/sql/delta/DeltaOptions.scala:98 123456val maxFilesPerTrigger = options.get(MAX_FILES_PER_TRIGGER_OPTION).map &#123; str =&gt; Try(str.toInt).toOption.filter(_ &gt; 0).getOrElse &#123; throw DeltaErrors.illegalDeltaOptionException( MAX_FILES_PER_TRIGGER_OPTION, str, \"must be a positive integer\") &#125;&#125; このパラメータは org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits#toReadLimit メソッド内で用いられる。 org/apache/spark/sql/delta/sources/DeltaSource.scala:350 12345678910111213 def toReadLimit: ReadLimit = &#123; if (options.maxFilesPerTrigger.isDefined &amp;&amp; options.maxBytesPerTrigger.isDefined) &#123; CompositeLimit( ReadMaxBytes(options.maxBytesPerTrigger.get), ReadLimit.maxFiles(options.maxFilesPerTrigger.get).asInstanceOf[ReadMaxFiles]) &#125; else if (options.maxBytesPerTrigger.isDefined) &#123; ReadMaxBytes(options.maxBytesPerTrigger.get) &#125; else &#123; ReadLimit.maxFiles( options.maxFilesPerTrigger.getOrElse(DeltaOptions.MAX_FILES_PER_TRIGGER_OPTION_DEFAULT)) &#125; &#125;&#125; このメソッドの呼び出し階層は以下の通り。 123AdmissionLimits in DeltaSource.toReadLimit() (org.apache.spark.sql.delta.sources) DeltaSource.getDefaultReadLimit() (org.apache.spark.sql.delta.sources) MicroBatchExecution MicroBatchExecutionは、 org.apache.spark.sql.execution.streaming.MicroBatchExecution クラスであり、 この仕組みを通じてSparkのStructured Streamingの持つレートコントロールの仕組みに設定値をベースにした値が渡される。 読み込み時の maxBytesPerTrigger オプション 原則的には、 maxFilesPerTrigger と同じようなもの。 指定された値を用いてcase classを定義し、最大バイトサイズの目安を保持する。 追記ではなく更新したらどうなるか？ Delta Lakeでoverwriteモードで書き込む際に、パーティションカラムに対して条件を指定できることを利用し、 部分更新することにする。 まず name をパーティションカラムとしたDataFrameを作ることにする。 12345678910scala&gt; val df = spark.read.format(\"parquet\").load(\"/opt/spark/default/examples/src/main/resources/users.parquet\")scala&gt; df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+scala&gt; df.write.format(\"delta\").partitionBy(\"name\").save(\"/tmp/delta/partitioned_users\") 上記例と同様に、ストリームでデータを読み込むため、もうひとつターミナルを起動し、以下を実行。 1$ /opt/spark/default/bin/spark-submit --class com.example.StructuredStreamingDeltaLakeReadExample target/scala-2.12/structured_streaming_deltalake-assembly-0.0.1.jar /tmp/delta/partitioned_users spark-shellを起動したターミナルで、更新用のデータを準備。 1scala&gt; val updateRecord = Seq(User(\"Ben\", \"green\", Array(1,2))).toDF 条件付きのoverwriteモードで既存のDelta Lakeテーブルの一部レコードを更新する。 123456scala&gt; updateRecord .write .format(\"delta\") .mode(\"overwrite\") .option(\"replaceWhere\", \"name == 'Ben'\") .save(\"/tmp/delta/partitioned_users\") 上記を実行したときに、ストリーム処理を実行しているターミナル側で、 以下のエラーが生じ、プロセスが停止した。 12345620/11/23 22:10:55 ERROR MicroBatchExecution: Query [id = 13cf0aa0-116c-4c95-aea0-3e6f779e02c8, runId = 6f71a4bb-c067-4f6d-aa17-6bf04eea3520] terminatedwith errorjava.lang.UnsupportedOperationException: Detected a data update in the source table. This is currently not supported. If you&apos;d like to ignore updates, set the option &apos;ignoreChanges&apos; to &apos;true&apos;. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory. at org.apache.spark.sql.delta.sources.DeltaSource.verifyStreamHygieneAndFilterAddFiles(DeltaSource.scala:273) at org.apache.spark.sql.delta.sources.DeltaSource.$anonfun$getChanges$1(DeltaSource.scala:117)(snip) 後ほど検証するが、Delta Lakeのストリーム処理では、既存レコードのupdate、deleteなどの既存レコードの更新となる処理は基本的にサポートされていない。 オプションを指定することで、更新を無視することができる。 読み込み時の ignoreDeletes オプション 基本的には、上記の通り、元テーブルに変化があった場合はエラーを生じるようになっている。 Ignore updates and deletes に記載の通り、update、merge into、delete、overwriteがエラーの対象である。 ひとまずdeleteでエラーになるのを避けるため、 ignoreDeletes で無視するオプションを指定できる。 動作確認する。 spark-shellを立ち上げ、検証用のテーブルを作成する。 1$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" 12345678910scala&gt; val df = spark.read.format(\"parquet\").load(\"/opt/spark/default/examples/src/main/resources/users.parquet\")scala&gt; df.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+scala&gt; df.write.format(\"delta\").save(\"/tmp/delta/users_for_delete\") 別のターミナルを立ち上げ、当該テーブルをストリームで読み込む。 1$ /opt/spark/default/bin/spark-submit --class com.example.StructuredStreamingDeltaLakeReadExample target/scala-2.12/structured_streaming_deltalake-assembly-0.0.1.jar /tmp/delta/users_for_delete spark-shellでテーブルとして読み込む。 12scala&gt; import io.delta.tables._scala&gt; val deltaTable = DeltaTable.forPath(spark, \"/tmp/delta/users_for_delete\") 1scala&gt; deltaTable.delete(\"name == 'Ben'\") 上記の通り、テーブルを更新（削除）した結果、ストリーム処理が以下のエラーを出力して終了した。 12345620/11/23 22:26:21 ERROR MicroBatchExecution: Query [id = 660b82a9-ca40-4b91-8032-d75807b11c18, runId = 7e46e9a7-1ba2-48b2-b264-53c254cfa6fc] terminatedwith errorjava.lang.UnsupportedOperationException: Detected a data update in the source table. This is currently not supported. If you&apos;d like to ignore updates, set the option &apos;ignoreChanges&apos; to &apos;true&apos;. If you would like the data update to be reflected, please restart this query with a fresh checkpoint directory. at org.apache.spark.sql.delta.sources.DeltaSource.verifyStreamHygieneAndFilterAddFiles(DeltaSource.scala:273) at org.apache.spark.sql.delta.sources.DeltaSource.$anonfun$getChanges$1(DeltaSource.scala:117) at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)(snip) そこで、同じことをもう一度 ignoreDelte オプションを利用して実行してみる。 1scala&gt; df.write.format(&quot;delta&quot;).partitionBy(&quot;name&quot;).save(&quot;/tmp/delta/users_for_delete2&quot;) なお、 ignoreDelete オプションはパーティションカラムに指定されたカラムに対し、 where句を利用して削除するものに対して有効である。 （パーティションカラムに指定されていないカラムをwhere句に使用してdeleteしたところ、エラーが生じた） 別のターミナルを立ち上げ、当該テーブルをストリームで読み込む。 1$ /opt/spark/default/bin/spark-submit --class com.example.StructuredStreamingDeltaLakeDeleteExample target/scala-2.12/structured_streaming_deltalake-assembly-0.0.1.jar /tmp/delta/users_for_delete2 spark-shellでテーブルとして読み込む。 12scala&gt; import io.delta.tables._scala&gt; val deltaTable = DeltaTable.forPath(spark, \"/tmp/delta/users_for_delete2\") 1scala&gt; deltaTable.delete(\"name == 'Ben'\") このとき、特にエラーなくストリーム処理が続いた。 またテーブルを見たところ、 123456scala&gt; deltaTable.toDF.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|+------+--------------+----------------+ 削除対象のレコードが消えていることが確かめられる。 読み込み時の ignoreChanges オプション ignoreChanges オプションは、いったん概ね ignoreDelete オプションと同様だと思えば良い。 ただしdeleteに限らない。（deleteも含む） 細かな点は後で調査する。 org.apache.spark.sql.delta.sources.DeltaSource クラスについて Delta Lake用のストリームData Sourceである。 親クラスは以下の通り。 123DeltaSource (org.apache.spark.sql.delta.sources) Source (org.apache.spark.sql.execution.streaming) SparkDataStream (org.apache.spark.sql.connector.read.streaming) 気になったメンバ変数は以下の通り。 1234567891011121314151617181920212223242526/** A check on the source table that disallows deletes on the source data. */private val ignoreChanges = options.ignoreChanges || ignoreFileDeletion--&gt; レコードの変更（ファイル変更）を無視するかどうか/** A check on the source table that disallows commits that only include deletes to the data. */private val ignoreDeletes = options.ignoreDeletes || ignoreFileDeletion || ignoreChanges--&gt; レコードの削除（ファイル削除）を無視するかどうかprivate val excludeRegex: Option[Regex] = options.excludeRegex--&gt; ADDファイルのリストする際、無視するファイルを指定するoverride val schema: StructType = deltaLog.snapshot.metadata.schema--&gt; 対象テーブルのスキーマ(snip)private val tableId = deltaLog.snapshot.metadata.id--&gt; テーブルのIDprivate var previousOffset: DeltaSourceOffset = null--&gt; バッチの取得時のオフセットを保持する// A metadata snapshot when starting the query.private var initialState: DeltaSourceSnapshot = nullprivate var initialStateVersion: Long = -1L--&gt; org.apache.spark.sql.delta.sources.DeltaSource#getBatch などから呼ばれる、スナップショットを保持するための変数 org.apache.spark.sql.delta.sources.DeltaSource#getChanges メソッド ストリーム処理のバッチ取得メソッド org.apache.spark.sql.delta.sources.DeltaSource#getBatch などから呼ばれるメソッド。 スナップショットなどの情報から、開始時点のバージョンから現在までの変化を返す。 org.apache.spark.sql.delta.sources.DeltaSource#getSnapshotAt メソッド 上記の org.apache.spark.sql.delta.sources.DeltaSource#getChanges メソッドなどで利用される、 指定されたバージョンでのスナップショットを返す。 org.apache.spark.sql.delta.SnapshotManagement#getSnapshotAt を内部的に利用する。 org.apache.spark.sql.delta.sources.DeltaSource#getChangesWithRateLimit メソッド org.apache.spark.sql.delta.sources.DeltaSource#getChanges メソッドに比べ、 レート制限を考慮した上での変化を返す。 org.apache.spark.sql.delta.sources.DeltaSource#getStartingOffset メソッド org.apache.spark.sql.delta.sources.DeltaSource#latestOffset メソッド内で呼ばれる。 ストリーム処理でマイクロバッチを構成する際、対象としているデータのオフセットを返すのだが、 それらのうち、初回の場合に呼ばれる。★要確認 当該メソッド内で、「開始時点か、変化をキャプチャして処理しているのか」、「コミット内のオフセット位置」などの確認が行われ、 org.apache.spark.sql.delta.sources.DeltaSourceOffset にラップした上でメタデータが返される。 org.apache.spark.sql.delta.sources.DeltaSource#latestOffset メソッド org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl#latestOffset メソッドをオーバーライドしている。 今回のマイクロバッチで対象となるデータのうち、最後のデータを示すオフセットを返す。 org.apache.spark.sql.delta.sources.DeltaSource#verifyStreamHygieneAndFilterAddFiles メソッド org.apache.spark.sql.delta.sources.DeltaSource#getChanges メソッドで呼ばれる。 getChanges メソッドは指定されたバージョン以降の「変更」を取得するものである。 その中において、verifyStreamHygieneAndFilterAddFiles メソッドは関係あるアクションだけ取り出すために用いられる。 org.apache.spark.sql.delta.sources.DeltaSource#getBatch メソッド org.apache.spark.sql.execution.streaming.Source#getBatch メソッドをオーバライドしたもの。 与えられたオフセット（開始、終了）をもとに、そのタイミングで処理すべきバッチとなるDataFrameを返す。 org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits クラスについて レート管理のためのクラス。 org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits#toReadLimit メソッド org.apache.spark.sql.delta.sources.DeltaSource#getDefaultReadLimit メソッド内で呼ばれる。 オプションで与えられたレート制限のヒント値を、 org.apache.spark.sql.connector.read.streaming.ReadLimit に変換する。 org.apache.spark.sql.delta.sources.DeltaSource.AdmissionLimits#admit メソッド org.apache.spark.sql.delta.sources.DeltaSource#getChangesWithRateLimit メソッド内で呼ばれる。 上記メソッドは、レート制限を考慮して変化に関する情報のイテレータを返す。 admit メソッドはその中において、レートリミットに達しているかどうかを判断するために用いられる。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"BigDL memory usage","slug":"BigDL-memory-usage","date":"2020-11-13T07:04:02.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/11/13/BigDL-memory-usage/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/11/13/BigDL-memory-usage/","excerpt":"","text":"参考 メモ 前提 Spark起動 サンプルを動かす サンプルの中身 Trainクラス Optimizerの種類 参考 Download Issue-3070 use pre-bulid libs bigdl-SPARK_2.4 Run Examples Lenet Train LeNet Example メモ 前提 Download によると、Sparkは2.4系まで対応しているようだ。 Issue-3070 によると、Spark3対応も進んでいるようだ。 Spark起動 Run で記載の通り、以下のように起動した。 123$ export SPARK_HOME=/usr/local/spark/default$ export BIGDL_HOME=/usr/local/bigdl/default$ $&#123;BIGDL_HOME&#125;/bin/spark-shell-with-bigdl.sh --master local[*] 12scala&gt; import com.intel.analytics.bigdl.utils.Enginescala&gt; Engine.init とりあえず初期化までは動いた。 サンプルを動かす Examples に記載のサンプルを動かす。 LeNet Train にあるLeNetの例が良さそう。 MNISTの画像をダウンロードして、展開した。 1234567$ mkdir ~/tmp$ cd ~/tmp$ wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz$ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz$ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz$ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz$ gunzip *.gz 展開したディレクトリを指定しながら、LeNetの学習を実行。 12345678$ spark-submit \\ --master local[6] \\ --driver-class-path $&#123;BIGDL_HOME&#125;/lib/bigdl-SPARK_2.4-0.11.1-jar-with-dependencies.jar \\ --class com.intel.analytics.bigdl.models.lenet.Train \\ $&#123;BIGDL_HOME&#125;/lib/bigdl-SPARK_2.4-0.11.1-jar-with-dependencies.jar \\ -f $HOME/tmp \\ -b 12 \\ --checkpoint ./model 実行中の様子は以下の通り。 ヒストリのキャプチャ 今回はローカルモードで実行したが、入力された学習データと同様のサイズのキャッシュがメモリ上に展開されていることがわかる。 サンプルの中身 2020/11/15時点のmasterブランチを確認する。 Trainクラス 上記サンプルで実行されているTrainを見る。 モデルを定義している箇所は以下の通り。 com/intel/analytics/bigdl/models/lenet/Train.scala:48 123456789101112val model = if (param.modelSnapshot.isDefined) &#123; Module.load[Float](param.modelSnapshot.get)&#125; else &#123; if (param.graphModel) &#123; LeNet5.graph(classNum = 10) &#125; else &#123; Engine.getEngineType() match &#123; case MklBlas =&gt; LeNet5(10) case MklDnn =&gt; LeNet5.dnnGraph(param.batchSize / Engine.nodeNumber(), 10) &#125; &#125;&#125; Modelインスタンスは以下の通り、Optimizerオブジェクトに渡され、 データセットに合わせたOptimizerが返される。 （例：データセットが分散データセットかどうか、など） com/intel/analytics/bigdl/models/lenet/Train.scala:83 1234val optimizer = Optimizer( model = model, dataset = trainSet, criterion = criterion) 参考までに、Optimizerの種類と判定の処理は以下の通り。 com/intel/analytics/bigdl/optim/Optimizer.scala:688 12345678910111213141516171819202122232425dataset match &#123; case d: DistributedDataSet[_] =&gt; Engine.getOptimizerVersion() match &#123; case OptimizerV1 =&gt; new DistriOptimizer[T]( _model = model, _dataset = d.toDistributed().asInstanceOf[DistributedDataSet[MiniBatch[T]]], _criterion = criterion ).asInstanceOf[Optimizer[T, D]] case OptimizerV2 =&gt; new DistriOptimizerV2[T]( _model = model, _dataset = d.toDistributed().asInstanceOf[DistributedDataSet[MiniBatch[T]]], _criterion = criterion ).asInstanceOf[Optimizer[T, D]] &#125; case d: LocalDataSet[_] =&gt; new LocalOptimizer[T]( model = model, dataset = d.toLocal().asInstanceOf[LocalDataSet[MiniBatch[T]]], criterion = criterion ).asInstanceOf[Optimizer[T, D]] case _ =&gt; throw new UnsupportedOperationException&#125; 返されたOptimizerの、Optimizer#optimizeメソッドを利用し学習が実行される。 com/intel/analytics/bigdl/models/lenet/Train.scala:98 12345678optimizer .setValidation( trigger = Trigger.everyEpoch, dataset = validationSet, vMethods = Array(new Top1Accuracy, new Top5Accuracy[Float], new Loss[Float])) .setOptimMethod(optimMethod) .setEndWhen(Trigger.maxEpoch(param.maxEpoch)) .optimize() Optimizerの種類 上記の内容を見るに、Optimizerにはいくつか種類がありそうだ。 DistriOptimizer DistriOptimizerV2 LocalOptimizer DistriOptimizer ひとまずメモリ使用に関連する箇所ということで、入力データの準備の処理を確認する。 com.intel.analytics.bigdl.optim.DistriOptimizer#optimize メソッドには 以下のような箇所がある。 com/intel/analytics/bigdl/optim/DistriOptimizer.scala:870 1prepareInput() これは com.intel.analytics.bigdl.optim.DistriOptimizer#prepareInput メソッドであり、 内部的に com.intel.analytics.bigdl.optim.AbstractOptimizer#prepareInput メソッドを呼び出し、 入力データをSparkのキャッシュに載せるように処理する。 com/intel/analytics/bigdl/optim/DistriOptimizer.scala:808 1234if (!dataset.toDistributed().isCached) &#123; DistriOptimizer.logger.info(\"caching training rdd ...\") DistriOptimizer.prepareInput(this.dataset, this.validationDataSet)&#125; キャシュに載せると箇所は以下の通り。 com/intel/analytics/bigdl/optim/AbstractOptimizer.scala:279 1234567private[bigdl] def prepareInput[T: ClassTag](dataset: DataSet[MiniBatch[T]], validationDataSet: Option[DataSet[MiniBatch[T]]]): Unit = &#123; dataset.asInstanceOf[DistributedDataSet[MiniBatch[T]]].cache() if (validationDataSet.isDefined) &#123; validationDataSet.get.toDistributed().cache() &#125;&#125; 上記の DistributedDataSet の chache メソッドは以下の通り。 com/intel/analytics/bigdl/dataset/DataSet.scala:216 123456def cache(): Unit = &#123; if (originRDD() != null) &#123; originRDD().count() &#125; isCached = true&#125; originRDD の戻り値に対して、count を読んでいる。 ここで count を呼ぶのは、入力データである originRDD の戻り値に入っているRDDをメモリ上にマテリアライズするためである。 count を呼ぶだけでマテリアライズできるのは、予め入力データを定義したときに Spark RDDの cache を利用してキャッシュ化することを指定されているからである。 今回の例では、Optimizerオブジェクトのapplyを利用する際に渡されるデータセット trainSet を 定義する際に予め cache が呼ばれる。 com/intel/analytics/bigdl/models/lenet/Train.scala:79 123val trainSet = DataSet.array(load(trainData, trainLabel), sc) -&gt; BytesToGreyImg(28, 28) -&gt; GreyImgNormalizer(trainMean, trainStd) -&gt; GreyImgToBatch( param.batchSize) trainSet を定義する際、 com.intel.analytics.bigdl.dataset.DataSet$#array(T[], org.apache.spark.SparkContext) メソッドが 呼ばれるのだが、その中で以下のように Sparkの RDD#cache が呼ばれていることがわかる。 com/intel/analytics/bigdl/dataset/DataSet.scala:343 123456789101112def array[T: ClassTag](localData: Array[T], sc: SparkContext): DistributedDataSet[T] = &#123; val nodeNumber = Engine.nodeNumber() new CachedDistriDataSet[T]( sc.parallelize(localData, nodeNumber) // Keep this line, or the array will be send to worker every time .coalesce(nodeNumber, true) .mapPartitions(iter =&gt; &#123; Iterator.single(iter.toArray) &#125;).setName(\"cached dataset\") .cache() )&#125; 以下、一例。 具体的には、Optimizerのインスタンスを生成するための apply メソッドはいくつかあるが、 以下のように引数にデータセットを指定する箇所がある。（再掲） com/intel/analytics/bigdl/optim/Optimizer.scala:619 1_dataset = (DataSet.rdd(sampleRDD) -&gt; ここで用いられている com.intel.analytics.bigdl.dataset.DataSet#rdd メソッドは以下の通り。 com/intel/analytics/bigdl/dataset/DataSet.scala:363 12345678910def rdd[T: ClassTag](data: RDD[T], partitionNum: Int = Engine.nodeNumber() ): DistributedDataSet[T] = &#123; new CachedDistriDataSet[T]( data.coalesce(partitionNum, true) .mapPartitions(iter =&gt; &#123; Iterator.single(iter.toArray) &#125;).setName(\"cached dataset\") .cache() )&#125; com.intel.analytics.bigdl.dataset.CachedDistriDataSet#CachedDistriDataSet のコンストラクタ引数に、 org.apache.spark.rdd.RDD#cache を用いてキャッシュ化することを指定したRDDを渡していることがわかる。","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Machine Learning","slug":"Research/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/"},{"name":"BigDL","slug":"Research/Machine-Learning/BigDL","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/BigDL/"}],"tags":[{"name":"BigDL","slug":"BigDL","permalink":"https://dobachi.github.io/memo-blog/tags/BigDL/"}]},{"title":"Generate PDF using pandoc","slug":"Generate-PDF-using-pandoc","date":"2020-10-29T13:17:31.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2020/10/29/Generate-PDF-using-pandoc/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/10/29/Generate-PDF-using-pandoc/","excerpt":"","text":"参考 メモ 参考 TeX Liveのインストール手順 default.latex メモ TeX Liveのインストール手順 の通り、最新版をインストールする。 なお、フルセットでインストールした。 PDFを作成するときは以下のようにする。 1$ pandoc test.md -o test.pdf --pdf-engine=lualatex -V documentclass=bxjsarticle -V classoption=pandoc ただし、 default.latex に示すとおり、リンクに色を付けるなどしたいので、 上記マークダウンファイルには先頭部分にYAMLでコンフィグを記載することとした。 12345678910111213141516---title: テスト文章date: 2020-10-30author: dobachicolorlinks: yesnumbersections: yestoc: yes---# test## これはテスト[Google](https://www.google.com)&lt;!-- vim: set ft=markdown : --&gt;","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Documentation","slug":"Knowledge-Management/Documentation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Documentation/"}],"tags":[{"name":"pandoc","slug":"pandoc","permalink":"https://dobachi.github.io/memo-blog/tags/pandoc/"}]},{"title":"Access AWS S3 from Hadoop3 and Spark3","slug":"Access-AWS-S3-from-Hadoop3","date":"2020-09-14T14:01:29.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/09/14/Access-AWS-S3-from-Hadoop3/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/09/14/Access-AWS-S3-from-Hadoop3/","excerpt":"","text":"参考 メモ パッケージインポート profileを使ったクレデンシャル設定 （補足）パラメータと指定するクラスについて 参考 Hadoopの公式ドキュメント Sparkの公式ドキュメント メモ パッケージインポート Sparkの公式ドキュメント には、org.apache.spark:hadoop-cloud_2.12を利用するよう記載されているが、 このMavenパッケージは見当たらなかった。 そこで、 Hadoopの公式ドキュメント の通り、hadoop-awsをインポートすると、それに関連した依存関係をインポートすることにした。 spark-shellで試すとしたら以下の通り。 1$ /opt/spark/default/bin/spark-shell --packages org.apache.hadoop:hadoop-aws:3.2.0 profileを使ったクレデンシャル設定 awscliを利用しているとしたら、profileを設定しながらクレデンシャルを使い分けていることもあると思う。 その場合、起動時の環境変数でプロフィールを指定しながら、以下のようにproviderを指定することで、 profileを利用したクレデンシャル管理の仕組みを利用してアクセスできる。 1$ AWS_PROFILE=s3test /opt/spark/default/bin/spark-shell --packages org.apache.hadoop:hadoop-aws:3.2.0 1scala&gt; spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") （補足）パラメータと指定するクラスについて ちなみに、上記パラメータの説明には、 If unspecified, then the default list of credential provider classes, queried in sequence, is: 1. org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider: Uses the values of fs.s3a.access.key and fs.s3a.secret.key. 2. com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports configuration of AWS access key ID and secret access key in environment variables named AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, as documented in the AWS SDK. 3. com.amazonaws.auth.InstanceProfileCredentialsProvider: supports use of instance profile credentials if running in an EC2 VM. のように記載されている。 また、「Using Named Profile Credentials with ProfileCredentialsProvider」の節には、 Declare com.amazonaws.auth.profile.ProfileCredentialsProvider as the provider. と書かれており、上記Default...でなくても良い。（実際に試したところ動作した）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hadoop","slug":"Knowledge-Management/Hadoop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://dobachi.github.io/memo-blog/tags/Hadoop/"}]},{"title":"qmk_firmware_202009","slug":"qmk-firmware-202009","date":"2020-09-05T18:14:27.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/09/06/qmk-firmware-202009/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/09/06/qmk-firmware-202009/","excerpt":"","text":"参考 メモ 参考 QMK Firmwareでファームウェアをビルドしようとしたらavr-gccでコケた話 公式の環境構築手順 公式のコンパイル手順 メモ 2020/9に久しぶりにコンパイルしようとしたら、だいぶ勝手が変わっていた。 公式の環境構築手順 に従って環境を構築した。 なお、インストールするよう指定されていたパッケージだけでは足りなかったので、 以下のようにインストールした。 1$ pacman --needed --noconfirm --disable-download-timeout -S git mingw-w64-x86_64-toolchain mingw-w64-x86_64-python3-pip python3 python3-pip make diffutils 最初、以下を忘れたため、失敗したので注意・・・。 1$ qmk setup 公式のコンパイル手順 に従うと、qmkコマンドを利用してコンパイル、フラッシュするようだ。 ただ、 QMK Firmwareでファームウェアをビルドしようとしたらavr-gccでコケた話 に記載されているのと同様に、 make git-submodules が必要だった。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Keyboard","slug":"Knowledge-Management/Keyboard","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Keyboard/"},{"name":"QMK","slug":"Knowledge-Management/Keyboard/QMK","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Keyboard/QMK/"}],"tags":[{"name":"QMK","slug":"QMK","permalink":"https://dobachi.github.io/memo-blog/tags/QMK/"},{"name":"keyboard","slug":"keyboard","permalink":"https://dobachi.github.io/memo-blog/tags/keyboard/"}]},{"title":"Use GitHub actions to deploy documents","slug":"Use-GitHub-actions-to-deploy-documents","date":"2020-08-09T14:30:38.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/08/09/Use-GitHub-actions-to-deploy-documents/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/08/09/Use-GitHub-actions-to-deploy-documents/","excerpt":"","text":"参考 メモ レポジトリのDeploy Key設定 GitHub Actionsのワークフローの記述 参考 GitHub Actionsを用いてGitHub Pagesへのデプロイを自動化する マーケットプレイスのsphinx-build メモ GitHub Actionsを用いてGitHub Pagesへのデプロイを自動化する がSphinx JPのユーザ会による記事だったので参考になるが、 依存関係を都度pipインストールしているのが気になった。 マーケットプレイスのsphinx-build を利用すると良さそうだが、 この手順ではGITHUB TOKENを利用してデプロイしているのが気になった。 レポジトリごとに設定できる、Deploy Keyを利用したい。 そこでビルドには マーケットプレイスのsphinx-build を利用し、デプロイには GitHub Actionsを用いてGitHub Pagesへのデプロイを自動化する の手順を利用することにした。 レポジトリのDeploy Key設定 GitHub Actionsを用いてGitHub Pagesへのデプロイを自動化する の「Deploy keys の設定」章を参考に、 当該レポジトリのDeploy Keyを登録する。 任意の環境（ここではWSLのUbuntu18を利用した）で、 以下のコマンドを実行。 1$ ssh-keygen -t rsa -b 4096 -C \"＜レポジトリで使用しているメールアドレス＞\" -f ＜任意の名前＞ -N \"\" 上記記事の通り、秘密鍵と公開鍵をGitHubのウェブUIで登録する。 なお、「Secrets」タブで登録した名称は、後ほどGitHub Actionsのワークフロー内で使用する。 GitHub Actionsのワークフローの記述 マーケットプレイスのsphinx-build の例を参考に、 ワークフローを記述する。 なお、最後のデプロイする部分は、 GitHub Actionsを用いてGitHub Pagesへのデプロイを自動化する を参考に、 Deploy Keyを利用するよう修正した。 12345678910111213141516171819202122232425262728293031323334353637383940414243name: CI# 今回はマスタブランチへのPushをトリガとする。on: push: branches: - masterjobs: build: runs-on: ubuntu-latest # 今回はmasterブランチへのpushをトリガとしているので不要だが、gh-pagesへのpushをトリガとする場合など # 無限ループを回避する際には以下のように記述する。 if: \"!contains(github.event.head_commit.message, 'Update documentation via GitHub Actions')\" steps: - uses: actions/checkout@v1 # 今回はMakefileを利用するので、makeコマンドを使用するよう元ネタから修正した。 # またドキュメントのソースが含まれているディレクトリは各自の定義に依存する。 - uses: ammaraskar/sphinx-action@master with: build-command: \"make html\" docs-folder: \"documents/\" # 先ほどGitHubのウェブUIで定義した秘密鍵名を使用する。 - name: Commit documentation changes and push it run: | mkdir ~/.ssh ssh-keyscan -t rsa github.com &gt;&gt; ~/.ssh/known_hosts echo \"$&#123;&#123; secrets.＜先ほどGitHubウェブUIで定義した秘密鍵名＞ &#125;&#125;\" &gt; ~/.ssh/id_rsa chmod 400 ~/.ssh/id_rsa git clone git@github.com:$&#123;GITHUB_REPOSITORY&#125;.git --branch gh-pages --single-branch gh-pages cp -r documents/_build/html/* gh-pages/ cd gh-pages git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add . git commit -m \"Update documentation via GitHub Actions\" -a || true git push origin HEAD:gh-pages # The above command will fail if no changes were present, so we ignore # that. # ===============================","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Documentation","slug":"Knowledge-Management/Documentation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Documentation/"}],"tags":[{"name":"GitHub Actions","slug":"GitHub-Actions","permalink":"https://dobachi.github.io/memo-blog/tags/GitHub-Actions/"},{"name":"Sphinx","slug":"Sphinx","permalink":"https://dobachi.github.io/memo-blog/tags/Sphinx/"}]},{"title":"Install Bigtop RPMs using Yum","slug":"Install-Bigtop-RPMs-using-Yum","date":"2020-07-21T13:50:06.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2020/07/21/Install-Bigtop-RPMs-using-Yum/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/07/21/Install-Bigtop-RPMs-using-Yum/","excerpt":"","text":"参考 メモ 参考 Apache Bigtop 1.4.0のパッケージバージョン レポジトリ関連の資材置き場 CentOS7のbigtop.repo メモ 今回は、2020/7/21時点の最新バージョンである Apache Bigtop 1.4.0のパッケージバージョン の Hadoopをインストールできるかどうかを試してみることとする。 Yumのrepoファイルは レポジトリ関連の資材置き場 以下にある。 例えば、今回はCentOS7を利用することにするので、 CentOS7のbigtop.repo あたりを利用する。 12$ cd /etc/yum.repos.d$ sudo wget https://downloads.apache.org/bigtop/bigtop-1.4.0/repos/centos7/bigtop.repo ひとまずパッケージが見つかるかどうか、確認。 123456789$ sudo yum search hadoop-conf-pseudo読み込んだプラグイン:fastestmirrorLoading mirror speeds from cached hostfile * base: d36uatko69830t.cloudfront.net * epel: d2lzkl7pfhq30w.cloudfront.net * extras: d36uatko69830t.cloudfront.net * updates: d36uatko69830t.cloudfront.net=========================================== N/S matched: hadoop-conf-pseudo ============================================hadoop-conf-pseudo.x86_64 : Pseudo-distributed Hadoop configuration 確認できたので、試しにインストール。 1$ sudo yum install hadoop-conf-pseudo 自分の手元の環境では、依存関係で以下のパッケージがインストールされた。 12345678910111213141516171819202122232425262728293031323334======================================================================================================================== Package アーキテクチャー バージョン リポジトリー 容量========================================================================================================================インストール中: hadoop-conf-pseudo x86_64 2.8.5-1.el7 bigtop 20 k依存性関連でのインストールをします: at x86_64 3.1.13-24.el7 base 51 k bc x86_64 1.06.95-13.el7 base 115 k bigtop-groovy noarch 2.4.10-1.el7 bigtop 9.8 M bigtop-jsvc x86_64 1.0.15-1.el7 bigtop 29 k bigtop-utils noarch 1.4.0-1.el7 bigtop 11 k cups-client x86_64 1:1.6.3-43.el7 base 152 k ed x86_64 1.9-4.el7 base 72 k hadoop x86_64 2.8.5-1.el7 bigtop 24 M hadoop-hdfs x86_64 2.8.5-1.el7 bigtop 24 M hadoop-hdfs-datanode x86_64 2.8.5-1.el7 bigtop 5.7 k hadoop-hdfs-namenode x86_64 2.8.5-1.el7 bigtop 5.8 k hadoop-hdfs-secondarynamenode x86_64 2.8.5-1.el7 bigtop 5.8 k hadoop-mapreduce x86_64 2.8.5-1.el7 bigtop 34 M hadoop-mapreduce-historyserver x86_64 2.8.5-1.el7 bigtop 5.8 k hadoop-yarn x86_64 2.8.5-1.el7 bigtop 20 M hadoop-yarn-nodemanager x86_64 2.8.5-1.el7 bigtop 5.7 k hadoop-yarn-resourcemanager x86_64 2.8.5-1.el7 bigtop 5.6 k libpcap x86_64 14:1.5.3-12.el7 base 139 k m4 x86_64 1.4.16-10.el7 base 256 k mailx x86_64 12.5-19.el7 base 245 k nmap-ncat x86_64 2:6.40-19.el7 base 206 k patch x86_64 2.7.1-12.el7_7 base 111 k psmisc x86_64 22.20-16.el7 base 141 k redhat-lsb-core x86_64 4.1-27.el7.centos.1 base 38 k redhat-lsb-submod-security x86_64 4.1-27.el7.centos.1 base 15 k spax x86_64 1.5.2-13.el7 base 260 k time x86_64 1.7-45.el7 base 30 k zookeeper x86_64 3.4.6-1.el7 bigtop 7.0 M initスクリプトがインストールされていることがわかる。 1234567891011$ ls -1 /etc/init.d/READMEfunctionshadoop-hdfs-datanodehadoop-hdfs-namenodehadoop-hdfs-secondarynamenodehadoop-mapreduce-historyserverhadoop-yarn-nodemanagerhadoop-yarn-resourcemanagernetconsolenetwork ひとまずHDFSをフォーマット。 1$ sudo -u hdfs hdfs namenode -format あとは、上記の各種Hadoopサービスを立ち上げれば良い。","categories":[],"tags":[]},{"title":"Delta Lake 0.7.0","slug":"Delta-Lake-0-7-0","date":"2020-06-19T00:46:13.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/06/19/Delta-Lake-0-7-0/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/06/19/Delta-Lake-0-7-0/","excerpt":"","text":"参考 メモ SQL DDLへの対応やHive メタストアの対応 起動時のオプション例 パーサの呼び出し流れ カスタムカタログ ScalaやPythonでの例 SQLでのマージ処理 Presto / Athena用のメタデータの自動生成 テーブル履歴の切り詰めの管理 ユーザメタデータ AzureのData Lake Storage Gen2 ストリーム処理のone-timeトリガの改善 参考 Delta Lake リリースノート 0.7.0のテーブル読み書き 0.6.1のテーブル読み書き Spark3系でないとHiveメタストアに対応できない理由 SQLを用いたマージの例 Table Properties Table Properties 0.7.0で対応したAzure Data Lake Storage Gen2 メモ 0.7.0が出たので、本リリースの特徴を確認する。 SQL DDLへの対応やHive メタストアの対応 0.6系まではScala、Python APIのみであったが、SQL DDLにも対応した。 0.7.0のテーブル読み書き と 0.6.1のテーブル読み書き を見比べると、SQLの例が載っていることがわかる。 対応するSQL構文については src/main/antlr4/io/delta/sql/parser/DeltaSqlBase.g4 あたりを見ると良い。 なお、 Spark3系でないとHiveメタストアに対応できない理由 を見る限り、 Spark3系のAPI（や、DataSourceV2も、かな）を使わないと、Data SourceのカスタムAPIを利用できないため、 これまでHiveメタストアのような外部メタストアと連携したDelta Lakeのメタデータ管理ができなかった、とのこと。 なお、今回の対応でSparkのカタログ機能を利用することになったので、起動時もしくはSparkSession生成時の オプション指定が必要になった。 その代わり、ライブラリの明示的なインポートが不要であり、クエリはDelta Lakeのパーサで解釈された後、 解釈できないようであれば通常のパーサで処理されるようになる。 起動時のオプション例 例： 1$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.12:0.7.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" なお、ここでは SparkSessionExtensions を利用し、SparkSession生成時にカスタムルール等を挿入している。 この機能は2020/06/19時点でSpark本体側でExperimentalであることに注意。 今後もSpark本体側の仕様変更に引きずられる可能性はある。 パーサの呼び出し流れ セッション拡張機能を利用し、パーサが差し替えられている。 io/delta/sql/DeltaSparkSessionExtension.scala:73 1234567class DeltaSparkSessionExtension extends (SparkSessionExtensions =&gt; Unit) &#123; override def apply(extensions: SparkSessionExtensions): Unit = &#123; extensions.injectParser &#123; (session, parser) =&gt; new DeltaSqlParser(parser) &#125;(snip) io.delta.sql.parser.DeltaSqlParser クラスでは デリゲート用のパーサを受け取り、自身のパーサで処理できなかった場合に処理をデリゲートパーサに渡す。 io/delta/sql/parser/DeltaSqlParser.scala:66 1234class DeltaSqlParser(val delegate: ParserInterface) extends ParserInterface &#123; private val builder = new DeltaSqlAstBuilder(snip) 例えば、 SparkSession の sql メソッドを使って呼び出す場合を例にする。 このとき、内部では、 org.apache.spark.sql.catalyst.parser.ParserInterface#parsePlan メソッドが呼ばれて、 渡されたクエリ文 sqlText が処理される。 org/apache/spark/sql/SparkSession.scala:601 1234567def sql(sqlText: String): DataFrame = withActive &#123; val tracker = new QueryPlanningTracker val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) &#123; sessionState.sqlParser.parsePlan(sqlText) &#125; Dataset.ofRows(self, plan, tracker)&#125; この parsePlan がoverrideされており、以下のように定義されている。 io/delta/sql/parser/DeltaSqlParser.scala:69 123456override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) &#123; parser =&gt; builder.visit(parser.singleStatement()) match &#123; case plan: LogicalPlan =&gt; plan case _ =&gt; delegate.parsePlan(sqlText) &#125;&#125; まずは io.delta.sql.parser.DeltaSqlParser#parse メソッドを利用してパースがここ見られるが、 LogicalPlanが戻らなかったときは、デリゲート用パーサが呼び出されるようになっている。 カスタムカタログ Spark3ではDataSourvV2の中で、プラガブルなカタログに対応した。 Delta Lake 0.7.0はこれを利用し、カスタムカタログを用いる。（これにより、Hiveメタストアを経由してDelta Lake形式のデータを読み書きできるようになっている） 使われているカタログは org.apache.spark.sql.delta.catalog.DeltaCatalog である。 （SparkSessionのインスタンス生成する際、もしくは起動時のオプション指定） 当該カタログ内部では、例えば org.apache.spark.sql.delta.catalog.DeltaCatalog#createDeltaTable メソッドが定義されており、 org.apache.spark.sql.delta.catalog.DeltaCatalog#createTable ※ しようとするときなどに呼び出されるようになっている。 ※ org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension#createTable をoverrideしている なお、このクラスもデリゲート用のカタログを用いるようになっている。 org.apache.spark.sql.delta.catalog.DeltaCatalog#createTable メソッドは以下のようになっており、 データソースが delta 出ない場合は、親クラスの createTable （つまり標準的なもの）が呼び出されるようになっている。 org/apache/spark/sql/delta/catalog/DeltaCatalog.scala:149 123456789101112override def createTable( ident: Identifier, schema: StructType, partitions: Array[Transform], properties: util.Map[String, String]): Table = &#123; if (DeltaSourceUtils.isDeltaDataSourceName(getProvider(properties))) &#123; createDeltaTable( ident, schema, partitions, properties, sourceQuery = None, TableCreationModes.Create) &#125; else &#123; super.createTable(ident, schema, partitions, properties) &#125;&#125; ScalaやPythonでの例 代表的にScalaの例を出す。公式サイトには以下のように載っている。 123df.write.format(\"delta\").saveAsTable(\"events\") // create table in the metastoredf.write.format(\"delta\").save(\"/delta/events\") // create table by path Hiveメタストア経由で書き込むケースと、ストレージ上に直接書き出すケースが載っている。 SQLでのマージ処理 SQLを用いたマージの例 の通り、Delta Lakeの特徴であるマージ機能もSQLから呼び出させる。 123456spark.sql(s\"\"\"MERGE INTO $tableName USING newData ON $&#123;tableName&#125;.id = newData.id WHEN MATCHED THEN UPDATE SET $&#123;tableName&#125;.id = newData.id WHEN NOT MATCHED THEN INSERT *\"\"\") Spark SQLのカタログに登録されたDelta LakeのテーブルからDeltaTableを生成することもできる。 12scala&gt; import io.delta.tables.DeltaTablescala&gt; val tbl = DeltaTable.forName(tableName) Presto / Athena用のメタデータの自動生成 Delta LakeはPresto、Athena用のメタデータを生成できるが、更新があった際にパーティションごとに自動で再生成できるようになった。 テーブル履歴の切り詰めの管理 Delta Lakeは更新の履歴を保持することも特徴の一つだが、 データ本体とログのそれぞれの切り詰め対象期間を指定できる。 CREATEやALTER句内で、TBLPROPERTIESとして指定することになっている。 例えば以下。 12spark.sql(s\"CREATE TABLE $tableName(id LONG) USING delta TBLPROPERTIES ('delta.logRetentionDuration' = 'interval 1 day', 'delta.deletedFileRetentionDuration' = 'interval 1 day')\")spark.sql(s\"ALTER TABLE $tableName SET TBLPROPERTIES ('delta.logRetentionDuration' = 'interval 1 day', 'delta.deletedFileRetentionDuration' = 'interval 1 day')\") ユーザメタデータ spark.databricks.delta.commitInfo.userMetadata プロパティを利用して、ユーザメタデータを付与できる。 1df.write.option(\"spark.databricks.delta.commitInfo.userMetadata\", \"test\").format(\"delta\").mode(\"append\").save(\"/tmp/test\") 12scala&gt; spark.sql(\"SET spark.databricks.delta.commitInfo.userMetadata=test\")scala&gt; spark.sql(s\"INSERT INTO $tableName VALUES 0, 1, 2, 3, 4\") AzureのData Lake Storage Gen2 対応した。 しかし、 0.7.0で対応したAzure Data Lake Storage Gen2 の通り、 前提となっている各種ソフトウェアバージョンは高め。 ストリーム処理のone-timeトリガの改善 DataStreamReaderのオプションでmaxFilesPerTriggerを設定しているとしても、 one-time triggerでは一度に溜まったすべてのデータを読み込むようになった。（Spark 3系の話） \u001c","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"About tream table theory","slug":"About-tream-table-theory","date":"2020-05-19T15:12:43.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/05/20/About-tream-table-theory/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/05/20/About-tream-table-theory/","excerpt":"","text":"参考 メモ Tyler AkidauらによるApache Beamにまつわる情報 Matthias J. Sax、Guozhang Wangらによる論文 概要 1 Introduction 2 Background 3 Duality of streams and tables 4 STREAM PROCESSING OPERATORS 5 CASE STUDY: APACHE KAFKA 6 RELATED WORK 参考 Foundations of streaming SQL stream &amp; table theory メモ Tyler AkidauらによるApache Beamにまつわる情報 Foundations of streaming SQL stream &amp; table theory (General theory) の通り、 ストリームデータとテーブルデータの関係性を論じたもの。 上記の内容の全体は、 OReillyのStreaming Systems に記載あり。 これらは、ストリーム・テーブル理論をベースに、Apache Beamを利用したストリームデータ処理・活用システムについて論じたもの。 Matthias J. Sax、Guozhang Wangらによる論文 Streams and Tables Two Sides of the Same Coin 概要 ストリームデータの処理モデルを提案。 論理・物理の間の一貫性の崩れに対処する。 1 Introduction データストリームのチャレンジ 物理的なアウト・オブ・オーダへの対応 処理のオペレータがステートフルでないといけない、レイテンシ、正しさ、処理コストなどのトレードオフがある、など 分散処理への対応 物理的な並びが保証されない条件下で、 十分に表現力のあるオペレータ（モデル）を立案する。 2 Background ストリームデータのモデル。 ポイント。レコード配下の構成。 オフセット：物理の並び タイムスタンプ：論理の並び（生成時刻） キー バリュー 3 Duality of streams and tables レイテンシは、データストリームの特性に依存する。 論理、物理の並びの差異を解決するため、結果を継続的に更新するモデルを提案した。 Dual Streaming Model DUality of streams and tables テーブルは、テーブルのバージョンのコレクションと考えることもできる。 4 STREAM PROCESSING OPERATORS モデル定義を解説。 フィルターなどのステートレスな処理から、アグリゲーションなどのステートフルな処理まで。 out-of-order なレコードが届いても、最終的な出力結果テーブルがout-of-orderレコードがないときと同一になるように動くことを期待する。 Watermarkと似たような機構として、「retention time」を導入。 結果テーブル内の「現在時刻 - retention time」よりも古い結果をメンテナンス対象から外す。 Stream - Table Joinのケースで、「遅れテーブル更新」が生じると、 下流システムに対して結果の上書きデータを出力する必要がある。 5 CASE STUDY: APACHE KAFKA Kafka Streamsの例。 Kafka Streamsの利用企業：The New York Times, Pinterest, LINE, Trivago, etc Kafka Streamsのオペレーションはすべてノンブロッキングであり、 レコードを受領次第処理し、KTableにマテリアライズされたり、Topicに書き戻したりされる。 Window集計の例を題材に、retention timeを利用。 retention timeを長くするほどストレージを利用するが、 out-of-orderレコードに対してロバストになる。 また、retention timeが長いほど、ウィンドウ結果が確定したと言えるタイミングが遅くなる。 6 RELATED WORK Relationを取り扱うモデル、out-of-orderを取り扱うモデル、テーブルバージョン管理を取り扱うモデルなど。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"Handle pictures in delta lake and hudi","slug":"Handle-pictures-in-delta-lake","date":"2020-05-12T13:58:43.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2020/05/12/Handle-pictures-in-delta-lake/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/05/12/Handle-pictures-in-delta-lake/","excerpt":"","text":"参考 Delta Lake まずはjpgをSparkで読み込む Delta Lakeで扱う 書き込み 読み出し 更新 Parquetの内容を確認する スキーマ メタデータ Pythonで画像処理してみる Hudi 画像読み込み 書き込み 読み込み 更新 Parquetの内容を確認する スキーマ メタデータ 参考 Spark公式ドキュメントのImage data source Kaggleのmnistデータ How to Load and Manipulate Images for Deep Learning in Python With PIL/Pillow Hudiのquick-start-guide parquet-mr Delta Lake まずはjpgをSparkで読み込む 予め、Delta Lakeを読み込みながら起動する。 1$ /opt/spark/default/bin/spark-shell --packages io.delta:delta-core_2.11:0.6.0 Spark公式ドキュメントのImage data source を利用し、jpgをDataFrameとして読み込んでみる。 データは、 Kaggleのmnistデータ を利用。このデータはjpgになっているので便利。 123scala&gt; val home = sys.env(\"HOME\")scala&gt; val imageDir = \"Downloads/mnist_jpg/trainingSet/trainingSet/0\"scala&gt; val df = spark.read.format(\"image\").option(\"dropInvalid\", true).load(home + \"/\" + imageDir) データをDataFrameにする定義ができた。 内容は以下の通り。 12345678scala&gt; df.select(\"image.origin\", \"image.width\", \"image.height\").show(3, truncate=false)+-------------------------------------------------------------------------------+-----+------+|origin |width|height|+-------------------------------------------------------------------------------+-----+------+|file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_20188.jpg|28 |28 ||file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_12634.jpg|28 |28 ||file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_26812.jpg|28 |28 |+-------------------------------------------------------------------------------+-----+------+ 公式ドキュメントのとおりだが、カラムは以下の通り。 origin: StringType (represents the file path of the image) height: IntegerType (height of the image) width: IntegerType (width of the image) nChannels: IntegerType (number of image channels) mode: IntegerType (OpenCV-compatible type) data: BinaryType (Image bytes in OpenCV-compatible order: row-wise BGR in most cases) 123456789scala&gt; df.printSchemaroot |-- image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = true) | |-- width: integer (nullable = true) | |-- nChannels: integer (nullable = true) | |-- mode: integer (nullable = true) | |-- data: binary (nullable = true) Delta Lakeで扱う 書き込み これをDelta LakeのData Sourceで書き出してみる。 12scala&gt; val deltaImagePath = \"/tmp/delta-lake-image\"scala&gt; df.write.format(\"delta\").save(deltaImagePath) 読み出し できた。試しにDeltaTableとして読み出してみる。 123scala&gt; import io.delta.tables._scala&gt; import org.apache.spark.sql.functions._scala&gt; val deltaTable = DeltaTable.forPath(deltaImagePath) 更新 mergeを試す。 マージのためにキーを明示的に与えつつデータを準備する。 12345scala&gt; val targetDf = df.select($\"image.origin\", $\"image\")scala&gt; val targetImagePath = \"/tmp/delta-table\"scala&gt; targetDf.write.format(\"delta\").save(targetImagePath)scala&gt; val sourcePath = \"Downloads/mnist_jpg/trainingSet/trainingSet/1\"scala&gt; val sourceDf = spark.read.format(\"image\").option(\"dropInvalid\", true).load(home + \"/\" + sourcePath).select($\"image.origin\", $\"image\") Delta Tableを定義。 1234567891011121314scala&gt; val deltaTable = DeltaTable.forPath(targetImagePath)scala&gt; deltaTable .as(\"target\") .merge( sourceDf.as(\"source\"), \"target.origin = source.origin\") .whenMatched .updateExpr(Map( \"image\" -&gt; \"source.image\")) .whenNotMatched .insertExpr(Map( \"origin\" -&gt; \"source.origin\", \"image\" -&gt; \"source.image\")) .execute() 実際に、追加データが入っていることが確かめられる。 123456789scala&gt; deltaTable.toDF.where($\"origin\" contains \"trainingSet/1\").select(\"image.origin\", \"image.width\", \"image.height\").show(3, truncate=false)+-------------------------------------------------------------------------------+-----+------+|origin |width|height|+-------------------------------------------------------------------------------+-----+------+|file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/1/img_10266.jpg|28 |28 ||file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/1/img_10665.jpg|28 |28 ||file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/1/img_10772.jpg|28 |28 |+-------------------------------------------------------------------------------+-----+------+only showing top 3 rows SparkのImage Data Sourceは、メタデータと画像本体（バイナリ）を構造で扱っているだけなので、 実は特に工夫することなく扱えた。 公式ドキュメントによると、OpenCV形式バイナリで保持されている。 これから先は、Python使って処理したいので、改めて開き直す。 Parquetの内容を確認する parquet-mr の parquet-tools を利用して、Parquetの内容を確認する。 なお、Hudi v0.5.2におけるParquetのバージョンは以下の通り、1.10.1である。 pom.xml:84 1&lt;parquet.version&gt;1.10.1&lt;/parquet.version&gt; 当該バージョンに対応するタグをチェックアウトし、予めparquet-toolsをパッケージしておくこと。 スキーマ 1234567891011$ java -jar target/parquet-tools-1.10.1.jar schema /tmp/delta-lake-image/part-00000-c3d03d70-1785-435f-b055-b2a78903e732-c000.snappy.parquetmessage spark_schema &#123; optional group image &#123; optional binary origin (UTF8); optional int32 height; optional int32 width; optional int32 nChannels; optional int32 mode; optional binary data; &#125;&#125; シンプルにParquet内に保持されていることがわかる。 Spark SQLのParquet取扱機能を利用している。 メタデータ ファイル名やクリエイタなど。 123file: file:/tmp/delta-lake-image/part-00000-c3d03d70-1785-435f-b055-b2a78903e732-c000.snappy.parquetcreator: parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)extra: org.apache.spark.sql.parquet.row.metadata = &#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;image&quot;,&quot;type&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;origin&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;height&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;width&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;nChannels&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;mode&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;,&#123;&quot;name&quot;:&quot;data&quot;,&quot;type&quot;:&quot;binary&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;]&#125;,&quot;nullable&quot;:true,&quot;metadata&quot;:&#123;&#125;&#125;]&#125; ファイルスキーマ 123456789file schema: spark_schema--------------------------------------------------------------------------------image: OPTIONAL F:6.origin: OPTIONAL BINARY O:UTF8 R:0 D:2.height: OPTIONAL INT32 R:0 D:2.width: OPTIONAL INT32 R:0 D:2.nChannels: OPTIONAL INT32 R:0 D:2.mode: OPTIONAL INT32 R:0 D:2.data: OPTIONAL BINARY R:0 D:2 Rowグループ 1234567891011row group 1: RC:32 TS:29939 OFFSET:4--------------------------------------------------------------------------------image:.origin: BINARY SNAPPY DO:0 FPO:4 SZ:620/2838/4.58 VC:32 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_10203.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9098.jpg, num_nulls: 0].height: INT32 SNAPPY DO:0 FPO:624 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0].width: INT32 SNAPPY DO:0 FPO:698 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0].nChannels: INT32 SNAPPY DO:0 FPO:772 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 1, max: 1, num_nulls: 0].mode: INT32 SNAPPY DO:0 FPO:846 SZ:74/70/0.95 VC:32 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0].data: BINARY SNAPPY DO:0 FPO:920 SZ:21175/26821/1.27 VC:32 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 0x00(snip) Pythonで画像処理してみる とりあえずOpenCVをインストールしたPython環境のJupyterでPySparkを起動。 1$ PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --ip 0.0.0.0\" /opt/spark/default/bin/pyspark --packages io.delta:delta-core_2.11:0.6.0 --conf spark.pyspark.driver.python=$HOME/venv/jupyter/bin/jupyter この後はJupyter内で処理する。 12345from delta.tables import *from pyspark.sql.functions import *delta_image_path = \"/tmp/delta-lake-image\"deltaTable = DeltaTable.forPath(spark, delta_image_path)deltaTable.toDF().select(\"image.origin\", \"image.width\", \"image.height\").show(3, truncate=False) とりあえず読み込めたことがわかる。 12345678+-------------------------------------------------------------------------------+-----+------+|origin |width|height|+-------------------------------------------------------------------------------+-----+------+|file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_20188.jpg|28 |28 ||file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_12634.jpg|28 |28 ||file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_26812.jpg|28 |28 |+-------------------------------------------------------------------------------+-----+------+only showing top 3 rows 中のデータを利用できることを確かめるため、1件だけ取り出して処理してみる。 1img = deltaTable.toDF().where(\"image.origin == 'file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg'\").select(\"image.origin\", \"image.width\", \"image.height\", \"image.nChannels\", \"image.data\").collect()[0] とりあえずエッジ抽出をしてみる。 1234567import numpy as npimport cv2from matplotlib import pyplot as pltnparr = np.frombuffer(img.data, np.uint8).reshape(img.height, img.width, img.nChannels)edges = cv2.Canny(nparr,100,200)plt.imshow(edges) Jupyter上でエッジ抽出されたことが確かめられただろうか。 エッジ抽出された様 これをSparkでUDFで実行するようにする。 （とりあえず雑にライブラリをインポート…） 1234567891011def get_edge(data, h, w, c): import numpy as np import cv2 nparr = np.frombuffer(data, np.uint8).reshape(h, w, c) edge = cv2.Canny(nparr,100,200) return edge.tobytes()from pyspark.sql.functions import udffrom pyspark.sql.types import BinaryTypeget_edge_udf = udf(get_edge, BinaryType()) エッジ抽出されたndarrayをバイト列に変換し、Spark上ではBinaryTypeで扱うように指定した。 123with_edge = deltaTable.toDF().select(\"image.origin\", \"image.width\", \"image.height\", \"image.nChannels\", get_edge_udf(\"image.data\", \"image.height\", \"image.width\", \"image.nChannels\").alias(\"edge\"), \"image.data\")with_edge.show(3) 12345678+--------------------+-----+------+---------+--------------------+--------------------+| origin|width|height|nChannels| edge| data|+--------------------+-----+------+---------+--------------------+--------------------+|file:///home/cent...| 28| 28| 1|[00 00 00 00 00 0...|[05 00 04 08 00 0...||file:///home/cent...| 28| 28| 1|[00 00 00 00 00 0...|[05 00 0B 00 00 0...||file:///home/cent...| 28| 28| 1|[00 00 00 00 00 0...|[06 00 02 00 02 0...|+--------------------+-----+------+---------+--------------------+--------------------+only showing top 3 rows こんな感じで、OpenCVを用いた処理をDataFrameに対して実行できる。 なお、当然ながら処理されたデータを取り出せば、また画像として表示も可能。 123new_img = with_edge.where(\"origin == 'file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg'\").select(\"origin\", \"width\", \"height\", \"nChannels\", \"data\", \"edge\").collect()[0]new_nparr = np.frombuffer(new_img.edge, np.uint8).reshape(new_img.height, new_img.width)plt.imshow(new_nparr) Hudi 画像読み込み まずはjpgをSparkで読み込む と同様に データを読み込む。 Hudiのquick-start-guide を参考にしながら、まずはHudiを読み込みながら、シェルを起動する。 12$ /opt/spark/default/bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \\ --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' ライブラリをインポート。 1234scala&gt; import org.apache.spark.sql.SaveMode._scala&gt; import org.apache.hudi.DataSourceReadOptions._scala&gt; import org.apache.hudi.DataSourceWriteOptions._scala&gt; import org.apache.hudi.config.HoodieWriteConfig._ 画像データを読み込み 12345scala&gt; val tableName = \"hudi_images\"scala&gt; val basePath = \"file:///tmp/hudi_images\"scala&gt; val home = sys.env(\"HOME\")scala&gt; val imageDir = \"Downloads/mnist_jpg/trainingSet/trainingSet/*\"scala&gt; val df = spark.read.format(\"image\").option(\"dropInvalid\", true).load(home + \"/\" + imageDir) 今回は、Hudiにおけるパーティション構成を確認するため、上記のようにワイルドカード指定した。 書き込み Hudi書き込み用にデータを変換。パーティション用キーなどを定義する。 12345scala&gt; import org.apache.spark.sql.functions.&#123;col, udf&#125;scala&gt; val partitionPath = udf((str: String) =&gt; &#123; str.split(\"/\").takeRight(2).head &#125;)scala&gt; val hudiDf = df.select($\"image.origin\", partitionPath($\"image.origin\") as \"partitionpath\", $\"*\") 書き込み。 1234567scala&gt; hudiDf.write.format(\"hudi\"). option(TABLE_NAME, tableName). option(PRECOMBINE_FIELD_OPT_KEY, \"origin\"). option(RECORDKEY_FIELD_OPT_KEY, \"origin\"). option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). mode(Overwrite). save(basePath) 書き込まれたデータは以下のような感じ。 123456789101112131415161718192021222324252627282930313233$ ls -R /tmp/hudi_images//tmp/hudi_images/:0 1 2 3 4 5 6 7 8 9/tmp/hudi_images/0:86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-29-14074_20200517135104.parquet/tmp/hudi_images/1:4912032f-3365-4852-b2ae-91ffb5ea9806-0_1-29-14075_20200517135104.parquet/tmp/hudi_images/2:0636bb93-dd32-40f1-b5c8-328130386528-0_2-29-14076_20200517135104.parquet/tmp/hudi_images/3:6f0154cd-4a9f-4001-935b-47d067416797-0_3-29-14077_20200517135104.parquet/tmp/hudi_images/4:b6f29c2c-7df0-481a-8149-2bbc93e92e2c-0_4-29-14078_20200517135104.parquet/tmp/hudi_images/5:24115172-10db-4c85-808a-bf4048e0f533-0_5-29-14079_20200517135104.parquet/tmp/hudi_images/6:64823f4b-26fb-4679-87e8-cd81d01b1181-0_6-29-14080_20200517135104.parquet/tmp/hudi_images/7:d0c097e3-9ab7-477a-b591-593c6cb7880f-0_7-29-14081_20200517135104.parquet/tmp/hudi_images/8:13a48b38-cbfb-4076-957d-9c580765bfca-0_8-29-14082_20200517135104.parquet/tmp/hudi_images/9:5d3ba9ae-8ede-410f-95f3-e8d69e8c0478-0_9-29-14083_20200517135104.parquet メタデータは以下のような感じ。 123456789101112131415$ ls -1 /tmp/hudi_images/.hoodie/20200517135104.clean20200517135104.clean.inflight20200517135104.clean.requested20200517135104.commit20200517135104.commit.requested20200517135104.inflight20200517140006.clean20200517140006.clean.inflight20200517140006.clean.requested20200517140006.commit20200517140006.commit.requested20200517140006.inflightarchivedhoodie.properties 読み込み 読み込んで見る。 1234scala&gt; val hudiImageDf = spark. read. format(\"hudi\"). load(basePath + \"/*\") スキーマは以下の通り。 12345678910111213141516scala&gt; hudiImageDf.printSchemaroot |-- _hoodie_commit_time: string (nullable = true) |-- _hoodie_commit_seqno: string (nullable = true) |-- _hoodie_record_key: string (nullable = true) |-- _hoodie_partition_path: string (nullable = true) |-- _hoodie_file_name: string (nullable = true) |-- origin: string (nullable = true) |-- partitionpath: string (nullable = true) |-- image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = true) | |-- width: integer (nullable = true) | |-- nChannels: integer (nullable = true) | |-- mode: integer (nullable = true) | |-- data: binary (nullable = true) すべてのただの書き込みAPIを試しただけだが、想定通り書き込み・読み込みできている。 更新 Hudiといえば、テーブルを更新可能であることも特徴のひとつなので、試しに更新をしてみる。 元は同じデータであるが、「0」の画像データを新しいデータとしてDataFrameを定義し、更新する。 意図的に、一部のデータだけ更新する。 12345678910scala&gt; val updateImageDir = \"Downloads/mnist_jpg/trainingSet/trainingSet/0\"scala&gt; val updateDf = spark.read.format(\"image\").option(\"dropInvalid\", true).load(home + \"/\" + updateImageDir)scala&gt; val updateHudiDf = updateDf.select($\"image.origin\", partitionPath($\"image.origin\") as \"partitionpath\", $\"*\")scala&gt; updateHudiDf.write.format(\"hudi\"). option(TABLE_NAME, tableName). option(PRECOMBINE_FIELD_OPT_KEY, \"origin\"). option(RECORDKEY_FIELD_OPT_KEY, \"origin\"). option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). mode(Append). save(basePath) 以下のように、「0」のデータが更新されていることがわかる。 123456789101112$ ls -R /tmp/hudi_images//tmp/hudi_images/:0 1 2 3 4 5 6 7 8 9/tmp/hudi_images/0:86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-29-14074_20200517135104.parquet86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_.parquet/tmp/hudi_images/1:4912032f-3365-4852-b2ae-91ffb5ea9806-0_1-29-14075_20200517135104.parquet(snip) つづいて、更新されたことを確かめるため、改めて読み込みDataFrameを定義し、 1234scala&gt; val updatedHudiImageDf = spark. read. format(\"hudi\"). load(basePath + \"/*\") 試しに、差分がわかるように「1」と「0」のデータをそれぞれ読んで見る。 1234567891011121314151617181920scala&gt; updatedHudiImageDf.select($\"_hoodie_commit_time\", $\"_hoodie_commit_seqno\", $\"_hoodie_partition_path\").filter(\"partitionpath == 1\").show(3)+-------------------+--------------------+----------------------+|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_partition_path|+-------------------+--------------------+----------------------+| 20200517135104| 20200517135104_1_1| 1|| 20200517135104| 20200517135104_1_12| 1|| 20200517135104| 20200517135104_1_45| 1|+-------------------+--------------------+----------------------+only showing top 3 rowsscala&gt; updatedHudiImageDf.select($\"_hoodie_commit_time\", $\"_hoodie_commit_seqno\", $\"_hoodie_partition_path\").filter(\"partitionpath == 0\").show(3)+-------------------+--------------------+----------------------+|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_partition_path|+-------------------+--------------------+----------------------+| 20200517140006|20200517140006_0_...| 0|| 20200517140006|20200517140006_0_...| 0|| 20200517140006|20200517140006_0_...| 0|+-------------------+--------------------+----------------------+only showing top 3 rows 上記のように、更新されたことがわかる。最新の更新日時は、 2020/05/17 14:00:06UTC である。 ここで読み込まれたデータも、Delta Lake同様に画像を元にしたベクトルデータとして扱える。 Pythonであれば ndarray に変換して用いれば良い。 Parquetの内容を確認する parquet-mr の parquet-tools を利用して、Parquetの内容を確認する。 なお、Hudi v0.5.2におけるParquetのバージョンは以下の通り、1.10.1である。 pom.xml:84 1&lt;parquet.version&gt;1.10.1&lt;/parquet.version&gt; 当該バージョンに対応するタグをチェックアウトし、予めparquet-toolsをパッケージしておくこと。 スキーマ 123456789101112131415161718$ java -jar target/parquet-tools-1.10.1.jar schema /tmp/hudi_images/0/86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquetmessage hoodie.hudi_images.hudi_images_record &#123; optional binary _hoodie_commit_time (UTF8); optional binary _hoodie_commit_seqno (UTF8); optional binary _hoodie_record_key (UTF8); optional binary _hoodie_partition_path (UTF8); optional binary _hoodie_file_name (UTF8); optional binary origin (UTF8); optional binary partitionpath (UTF8); optional group image &#123; optional binary origin (UTF8); optional int32 height; optional int32 width; optional int32 nChannels; optional int32 mode; optional binary data; &#125;&#125; Hudiが独自に付加したカラムが追加されていることが見て取れる。 メタデータ つづいて、メタデータを確認する。 ファイル名やクリエイタなど。 12file: file:/tmp/hudi_images/0/86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquetcreator: parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1) ブルームフィルタはここでは省略 123extra: org.apache.hudi.bloomfilter = ///(snip) レコードキーやAvroスキーマ 1234extra: hoodie_min_record_key = file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpgextra: parquet.avro.schema = &#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_images_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_images&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;_hoodie_commit_time&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_commit_seqno&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_record_key&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_partition_path&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_file_name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;origin&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;image&quot;,&quot;type&quot;:[&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;image&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_images.hudi_images_record&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;origin&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;height&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;width&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;nChannels&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;mode&quot;,&quot;type&quot;:[&quot;int&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;data&quot;,&quot;type&quot;:[&quot;bytes&quot;,&quot;null&quot;]&#125;]&#125;,&quot;null&quot;]&#125;]&#125;extra: writer.model.name = avroextra: hoodie_max_record_key = file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg 上記の通り、Hudiでは parquet-avro を利用し、Avro形式でParquet内のデータを保持する。 スキーマ詳細 12345678910111213141516file schema: hoodie.hudi_images.hudi_images_record--------------------------------------------------------------------------------_hoodie_commit_time: OPTIONAL BINARY O:UTF8 R:0 D:1_hoodie_commit_seqno: OPTIONAL BINARY O:UTF8 R:0 D:1_hoodie_record_key: OPTIONAL BINARY O:UTF8 R:0 D:1_hoodie_partition_path: OPTIONAL BINARY O:UTF8 R:0 D:1_hoodie_file_name: OPTIONAL BINARY O:UTF8 R:0 D:1origin: OPTIONAL BINARY O:UTF8 R:0 D:1partitionpath: OPTIONAL BINARY O:UTF8 R:0 D:1image: OPTIONAL F:6.origin: OPTIONAL BINARY O:UTF8 R:0 D:2.height: OPTIONAL INT32 R:0 D:2.width: OPTIONAL INT32 R:0 D:2.nChannels: OPTIONAL INT32 R:0 D:2.mode: OPTIONAL INT32 R:0 D:2.data: OPTIONAL BINARY R:0 D:2 Rowグループ 123456789101112131415161718row group 1: RC:4132 TS:4397030 OFFSET:4--------------------------------------------------------------------------------_hoodie_commit_time: BINARY GZIP DO:0 FPO:4 SZ:165/127/0.77 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 20200517140006, max: 20200517140006, num_nulls: 0]_hoodie_commit_seqno: BINARY GZIP DO:0 FPO:169 SZ:10497/107513/10.24 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 20200517140006_0_42001, max: 20200517140006_0_46132, num_nulls: 0]_hoodie_record_key: BINARY GZIP DO:0 FPO:10666 SZ:16200/342036/21.11 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg, num_nulls: 0]_hoodie_partition_path: BINARY GZIP DO:0 FPO:26866 SZ:100/62/0.62 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0]_hoodie_file_name: BINARY GZIP DO:0 FPO:26966 SZ:448/419/0.94 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquet, max: 86c74288-e541-4a25-addf-0c00e17ef6bf-0_0-60-27759_20200517140006.parquet, num_nulls: 0]origin: BINARY GZIP DO:0 FPO:27414 SZ:16200/342036/21.11 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg, num_nulls: 0]partitionpath: BINARY GZIP DO:0 FPO:43614 SZ:100/62/0.62 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0]image:.origin: BINARY GZIP DO:0 FPO:43714 SZ:16200/342036/21.11 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_1.jpg, max: file:///home/centos/Downloads/mnist_jpg/trainingSet/trainingSet/0/img_9996.jpg, num_nulls: 0].height: INT32 GZIP DO:0 FPO:59914 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0].width: INT32 GZIP DO:0 FPO:60025 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 28, max: 28, num_nulls: 0].nChannels: INT32 GZIP DO:0 FPO:60136 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 1, max: 1, num_nulls: 0].mode: INT32 GZIP DO:0 FPO:60247 SZ:111/73/0.66 VC:4132 ENC:RLE,PLAIN_DICTIONARY,BIT_PACKED ST:[min: 0, max: 0, num_nulls: 0].data: BINARY GZIP DO:0 FPO:60358 SZ:1609341/3262447/2.03 VC:4132 ENC:RLE,PLAIN,BIT_PACKED ST:[min: 0x00(snip)","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"}]},{"title":"Research about BigTop","slug":"Research-about-BigTop","date":"2020-04-24T12:51:36.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2020/04/24/Research-about-BigTop/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/04/24/Research-about-BigTop/","excerpt":"","text":"参考 メモ Trunkのビルド方法 デプロイ周りを読み解いてみる Dockerベースのデプロイを試す Ambariのモジュールを確認する。 ホットIssue 参考 公式Wikiトップページ How to build Bigtop-trunk reuse_images Deployment and Integration Testing Dockerベースのデプロイ方法 Puppetベースのデプロイ方法 メモ Trunkのビルド方法 How to build Bigtop-trunk を見ると、現在ではDocker上でビルドする方法が推奨されているようだ。 ただ、 1# ./gradlew spark-pkg-ind で実行されるとき、都度Mavenキャッシュのない状態から開始される。 これが待ち時間長いので工夫が必要。 なお、上記コマンドで実行されるタスクは、以下の通り。 packages.gradle:629 12345678 task \"$target-pkg-ind\" ( description: \"Invoking a native binary packaging for $target in Docker. Usage: \\$ ./gradlew \" + \"-POS=[centos-7|fedora-26|debian-9|ubuntu-16.04|opensuse-42.3] \" + \"-Pprefix=[trunk|1.2.1|1.2.0|1.1.0|...] $target-pkg-ind \" + \"-Pnexus=[true|false]\", group: PACKAGES_GROUP) doLast &#123; def _prefix = project.hasProperty(\"prefix\") ? prefix : \"trunk\"(snip) bigtop-ci/build.sh がタスク内で実行されるスクリプトである。 この中でDockerが呼び出されてビルドが実行される。 これをいじれば、一応ビルド時に使ったDockerコンテナを残すことができそうではある。 -&gt; reuse_images というブランチに、コンテナンをリユースする機能をつけた。 デプロイ周りを読み解いてみる Deployment and Integration Testing を見る限り、 Dockerベースのデプロイ方法 と Puppetベースのデプロイ方法 があるように見える。 Dockerベースのデプロイを試す Dockerベースのデプロイ方法 に従って、Dockerベースでデプロイしてみる。 ソースコードは20200427時点でのmasterを利用。 Ubuntu16でDocker、Java、Ruby環境を整え、以下を実行した。 1$ sudo ./gradlew -Pconfig=config_ubuntu-16.04.yaml -Pnum_instances=1 docker-provisioner なお、20200427時点で Dockerベースのデプロイ方法 ではコンフィグファイルが config_ubuntu_xenial.yaml となっていたが、 BIGTOP-2814 の際に変更されたのに合わせた。 プロビジョニング完了後、コンテナを確認し、接続。 1234$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7311ae86181a bigtop/puppet:trunk-ubuntu-16.04 \"/sbin/init\" 4 minutes ago Up 4 minutes 20200426_151302_r30103_bigtop_1$ sudo docker exec -it 20200426_151302_r30103_bigtop_1 bash 以下のように hdfs コマンドを実行できることがわかる。 123456789root@7311ae86181a:/# hdfs dfs -ls /Found 7 itemsdrwxr-xr-x - hdfs hadoop 0 2020-04-26 15:15 /appsdrwxrwxrwx - hdfs hadoop 0 2020-04-26 15:15 /benchmarksdrwxr-xr-x - hbase hbase 0 2020-04-26 15:15 /hbasedrwxr-xr-x - solr solr 0 2020-04-26 15:15 /solrdrwxrwxrwt - hdfs hadoop 0 2020-04-26 15:15 /tmpdrwxr-xr-x - hdfs hadoop 0 2020-04-26 15:15 /userdrwxr-xr-x - hdfs hadoop 0 2020-04-26 15:15 /var なお、 -Pnum_instances=1 の値を3に変更すると、コンテナが3個立ち上がる。 1$ sudo ./gradlew -Pconfig=config_ubuntu-16.04.yaml -Pnum_instances=3 docker-provisioner 試しに、dfsadmin を実行してみる。 123456789101112131415161718192021222324252627282930313233root@53211353deec:/# sudo -u hdfs hdfs dfsadmin -reportConfigured Capacity: 374316318720 (348.61 GB)Present Capacity: 345302486224 (321.59 GB)DFS Remaining: 345300606976 (321.59 GB)DFS Used: 1879248 (1.79 MB)DFS Used%: 0.00%Under replicated blocks: 0Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0Pending deletion blocks: 0-------------------------------------------------Live datanodes (3):Name: 172.17.0.2:50010 (53211353deec.bigtop.apache.org)Hostname: 53211353deec.bigtop.apache.orgDecommission Status : NormalConfigured Capacity: 124772106240 (116.20 GB)DFS Used: 626416 (611.73 KB)Non DFS Used: 9637679376 (8.98 GB)DFS Remaining: 115100246016 (107.20 GB)DFS Used%: 0.00%DFS Remaining%: 92.25%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Sun Apr 26 15:33:12 UTC 2020(snip) また、 --stack オプションを利用し、デプロイするコンポーネントを指定できる。 ここではHadoopに加え、Sparkをプロビジョニングしてみる。 1$ sudo ./gradlew -Pconfig=config_ubuntu-16.04.yaml -Pstack=hdfs,yarn,spark -Pnum_instances=1 docker-provisioner コンテナに接続し、Sparkを動かす。 1234$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES56e1ff5670be bigtop/puppet:trunk-ubuntu-16.04 \"/sbin/init\" 3 minutes ago Up 3 minutes 20200426_155010_r21667_bigtop_1$ sudo docker exec -it 20200426_155010_r21667_bigtop_1 bash 123root@56e1ff5670be:/# spark-shellscala&gt; spark.sparkContext.masterres1: String = yarn 上記のように、マスタ=YARNで起動していることがわかる。 参考までに、 spark-env.sh は以下の通り。 12345678910111213141516171819root@56e1ff5670be:/# cat /etc/spark/conf/spark-env.shexport SPARK_HOME=$&#123;SPARK_HOME:-/usr/lib/spark&#125;export SPARK_LOG_DIR=$&#123;SPARK_LOG_DIR:-/var/log/spark&#125;export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=NONE\"export HADOOP_HOME=$&#123;HADOOP_HOME:-/usr/lib/hadoop&#125;export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-/etc/hadoop/conf&#125;export HIVE_CONF_DIR=$&#123;HIVE_CONF_DIR:-/etc/hive/conf&#125;export STANDALONE_SPARK_MASTER_HOST=56e1ff5670be.bigtop.apache.orgexport SPARK_MASTER_PORT=7077export SPARK_MASTER_IP=$STANDALONE_SPARK_MASTER_HOSTexport SPARK_MASTER_URL=yarnexport SPARK_MASTER_WEBUI_PORT=8080export SPARK_WORKER_DIR=$&#123;SPARK_WORKER_DIR:-/var/run/spark/work&#125;export SPARK_WORKER_PORT=7078export SPARK_WORKER_WEBUI_PORT=8081export SPARK_DIST_CLASSPATH=$(hadoop classpath) なお、構築したクラスタを破棄する際には以下の通り。（公式ドキュメントの通り） 1$ sudo ./gradlew docker-provisioner-destroy 仕様確認 上記のプロビジョナのタスクは、 build.gradle 内にある。 build.gradle:263 123task &quot;docker-provisioner&quot;(type:Exec,(snip) 上記の中で、 ./docker-hadoop.sh が用いられている。 build.gradle:296 12345def command = [ './docker-hadoop.sh', '-C', _config, '--create', _num_instances,] --create オプションが用いられているので、 create 関数が呼ばれる。 provisioner/docker/docker-hadoop.sh:387 123if [ \"$READY_TO_LAUNCH\" = true ]; then create $NUM_INSTANCESfi create 内では、以下のように docker-compose が用いられている。 provisioner/docker/docker-hadoop.sh:78 1docker-compose -p $PROVISION_ID up -d --scale bigtop=$1 --no-recreate また、コンポーネントの内容に応じて、Puppetマニフェスト（正確には、hieraファイル）が生成されるようになっている。 provisioner/docker/docker-hadoop.sh:101 1generate-config \"$hadoop_head_node\" \"$repo\" \"$components\" また、最終的には、 provision 関数、さらに bigtop-puppet 関数を通じ、各コンテナ内でpuppetが実行されるようになっている。 123456bigtop-puppet() &#123; if docker exec $1 bash -c \"puppet --version\" | grep ^3 &gt;/dev/null ; then future=\"--parser future\" fi docker exec $1 bash -c \"puppet apply --detailed-exitcodes $future --modulepath=/bigtop-home/bigtop-deploy/puppet/modules:/etc/puppet/modules:/usr/share/puppet/modules /bigtop-home/bigtop-deploy/puppet/manifests\"&#125; Ambariのモジュールを確認する。 bigtop-deploy/puppet/modules/ambari/manifests/init.pp にAmbariデプロイ用のモジュールがある。 内容は短い。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class ambari &#123; class deploy ($roles) &#123; if (\"ambari-server\" in $roles) &#123; include ambari::server &#125; if (\"ambari-agent\" in $roles) &#123; include ambari::agent &#125; &#125; class server &#123; package &#123; \"ambari-server\": ensure =&gt; latest, &#125; exec &#123; \"mpack install\": command =&gt; \"/bin/bash -c 'echo yes | /usr/sbin/ambari-server install-mpack --purge --verbose --mpack=/var/lib/ambari-server/resources/odpi-ambari-mpack-1.0.0.0-SNAPSHOT.tar.gz'\", require =&gt; [ Package[\"ambari-server\"] ] &#125; exec &#123; \"server setup\": command =&gt; \"/usr/sbin/ambari-server setup -j $(readlink -f /usr/bin/java | sed 's@jre/bin/java@@') -s\", require =&gt; [ Package[\"ambari-server\"], Package[\"jdk\"], Exec[\"mpack install\"] ] &#125; service &#123; \"ambari-server\": ensure =&gt; running, require =&gt; [ Package[\"ambari-server\"], Exec[\"server setup\"] ], hasrestart =&gt; true, hasstatus =&gt; true, &#125; &#125; class agent($server_host = \"localhost\") &#123; package &#123; \"ambari-agent\": ensure =&gt; latest, &#125; file &#123; \"/etc/ambari-agent/conf/ambari-agent.ini\": content =&gt; template('ambari/ambari-agent.ini'), require =&gt; [Package[\"ambari-agent\"]], &#125; service &#123; \"ambari-agent\": ensure =&gt; running, require =&gt; [ Package[\"ambari-agent\"], File[\"/etc/ambari-agent/conf/ambari-agent.ini\"] ], hasrestart =&gt; true, hasstatus =&gt; true, &#125; &#125;&#125; なお、上記の通り、サーバとエージェントそれぞれのマニフェストが存在する。 ODPiのMpackをインストールしているのが特徴。 逆にいうと、それをインストールしないと使えるVersionが存在しない。（ベンダ製のVDFを読み込めば使えるが） また、ODPiのMpackをインストールした上でクラスタを構成しようとしたところ、 ODPiのレポジトリが見当たらなかった。 プライベートレポジトリを立てる必要があるのだろうか。 いったん、公式のAmbariをインストールした上で動作確認することにする。 ホットIssue BIGTOP-3123 が1.5リリース向けのIssue","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hadoop","slug":"Knowledge-Management/Hadoop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/"},{"name":"BigTop","slug":"Knowledge-Management/Hadoop/BigTop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/BigTop/"}],"tags":[{"name":"BigTop","slug":"BigTop","permalink":"https://dobachi.github.io/memo-blog/tags/BigTop/"}]},{"title":"機械学習向けのFeature StoreないしStorage Layer Software","slug":"Storage-Layer-Software-for-Machine-Learning","date":"2020-04-10T02:43:58.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/04/10/Storage-Layer-Software-for-Machine-Learning/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/04/10/Storage-Layer-Software-for-Machine-Learning/","excerpt":"","text":"参考 プロダクト 企業アーキテクチャ まとめ メモ 傾向 Feature Storeとして挙げられている特徴・機能 主に、featuer storeとしての特徴 rawデータストアを含めた特徴 特徴量エンジニアリングの例 feature storeにおける画像の取扱は？ Feastにおけるデータフロー概要 hopsworksにおけるfeature store ストレージ製品の動向 Netapp Dell EMC 参考 プロダクト Feast Feast Feast Bridging ML Models and Data メモ Feature Store for Machine Learning https://feast.dev GoJek/Google released Feast in early 2019 and it is built around Google Cloud services: Big Query (offline) and Big Table (online) and Redis (low-latency), using Beam for feature engineering. Delta Lake メモ Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads. Hopsworks メモ The Platform for Data-Intensive AI Feature Storeに限らない Hopsworks HopsworksのGitHub Hopsworks Feature Store The missing data layer in ML pipelines? Hopsworksの公式ドキュメントのFeature Store Metaflow メモ Netflixの機械学習パイプライン管理用のライブラリ 特徴量を保存するためのライブラリを内容（割と素朴に保存する仕組み） Petastorm メモ いろいろなフレームワークから利用できるデータ入出力のためのライブラリ Parquetを利用する。 Zadara どちらかということ純粋にストレージ Netapp メモ いわゆる「ストレージ」におけるアプローチの例 Accelerated AI and deep learning pipelines across edge, core, and cloud Edge to Core to Cloud Architecture for AI Dell EMC メモ 単独の技術というより、コンピューティングと合わせてのソリューション、サーバ ENTERPRISE MACHINE &amp; DEEP LEARNING WITH INTELLIGENT STORAGE IBM メモ Watsonの名のもとに様々なソリューションを集結 IBM Storage for AI and big data IBM Spectrum Storage for AI with Power Systems Kafka メモ 推論用のイベントをやり取りするためのハブとして用いる 最近開発されているTiered Storage機能を利用し、長期保存用のストレージと組み合わせた使い方が可能になる つまり学習データ（ヒストリカルデータ）を含めて、Kafkaでデータをサーブすることが描かれている。 Streaming Machine Learning with Tiered Storage and Without a Data Lake Tiered Storageの紹介と機械学習への応用例 Cognitive SSD メモ USENIX ATC'19 非構造データを記憶装置の階層間で移動するのが無駄。そこでSSDのNAND Flash横にDeep Learning用、グラフ検索用のエンジンを積む Ignite Ignite Machine Larning Bandana Bandana Using Non-Volatile Memory for Storing Deep Learning Models Facebook Research 深層学習モデルをストアするためのストレージの提案。 NVMを活用。一緒に読み込まれるベクトルを物理的近く配置し、プリフェッチの効果を向上。 キャッシュポリシーをシミュレーションに基づいて最適化。 企業アーキテクチャ Pinterest - Big Data Machine Learning Platform at Pinterest Michelangelo メモ UberのMLプラットフォーム。必ずしもFeature Storeに限らない。 Feature Storeに関して特筆すると、「online」と「offline」のデータを統合して扱う、という発想。 Michelangelo_0 Michelangelo_1 Twiter 特徴量をライブラリとして保持、アプリケーションから使いやすくした？ Comcas ReidsをFeature Storeに利用 Pinterest Pinterest - Big Data Machine Learning Platform at Pinterest Zipline メモ 特徴量エンジニアリングパイプラインを補助するライブラリ バックフィルが特徴？ OSSではないように見える Zipline_0 Zipline_1 まとめ Feature Stores for ML 割とよくまとまっている。観点が参考になる。 Rethinking Feature Stores プロダクトは、 Feature Stores for ML と重なっているが、考察が載っている。 Feature Stores Components of a Data Science Factory Feature Storeの要件を整理しようとしている Accelerating Machine Learning as a Service with Automated Feature Engineering Feature Storeの定義、ビジネスメリットまで言及されている。 Data Storage Architectures for Machine Learning and Artificial Intelligence ベンダリストが載っていて便利そう。発行が2019/11なので比較的最近。 AI/ML向けのストレージアーキテクチャを「2層型」、「1層型」で分けている。 2層型は性能層と容量層に別れる。また性能層は1層型として用いられることもある。 各層に用いられる、ベンダ製品を例示している。 ベンダリスト Dell EMC（Isilon、ECS） Qumulo WekaIO Scality RING DataDirect Networks IBM（Spectrum Scale、COS） Minio Netapp OpenIO Pavilion Data Systems Pure Storage AIRI Quobyte VAST Data メモ 傾向 Google Big Query、Big Table、Redisあたりを特徴量置き場として使っている例が見られた。 Feature Storeとして挙げられている特徴・機能 主に、featuer storeとしての特徴 機能・分析補助 オンライン・オフライン統合（一貫性の実現、共通API） Feature Stores for ML Rethinking Feature Stores Feature Stores Components of a Data Science Factory Feast Bridging ML Models and Data Hopsworksの公式ドキュメントのFeature Store Hopsworksではオンライン用にMySQL、オフライン用にHiveを利用 また一方でHudiにも対応 バージョンニング、point-in-time correctness（特定のタイミングのレコードに対するラベルの更新）、タイムトラベル Feature Stores for ML Rethinking Feature Stores Hopsworksの公式ドキュメントのFeature Store ストレージレイヤ（実体の保存方法） Hopsworks Feature Store The missing data layer in ML pipelines? 自動特徴量分析、ドキュメンテーション、特徴量のテスト Hopsworks Feature Store The missing data layer in ML pipelines? Hopsworksの公式ドキュメントのFeature Store HopsworksではDeequを使った特徴量のユニットテストが可能 マルチテナンシ（ネームスペース、リソース) Hopsworks Feature Store The missing data layer in ML pipelines? Feast Bridging ML Models and Data 読み書きAPI Hopsworks Feature Store The missing data layer in ML pipelines? アクセス管理 Hopsworksの公式ドキュメントのFeature Store クエリプランナ（複数の特徴量グループの自動結合） Hopsworksの公式ドキュメントのFeature Store 必要な特徴量だけ選んでデータセットを定義（DBMSでいうビュー） Hopsworksの公式ドキュメントのFeature Store 計算 遅延評価（必要なタイイングでの計算実行） Rethinking Feature Stores 自動再計算（auto backfill） Hopsworks Feature Store The missing data layer in ML pipelines? Hopsworksの公式ドキュメントのFeature Store 性能 オンラインFeature Storeとしての良いレスポンス、スケーラビリティ Feature Stores for ML Feature Stores Components of a Data Science Factory Feast Bridging ML Models and Data Hopsworksの公式ドキュメントのFeature Store オフラインデータストアの性能（スケーラビリティ） Feature Stores Components of a Data Science Factory Feast Bridging ML Models and Data Hopsworksの公式ドキュメントのFeature Store 特徴量サービングの分散化・非中央集権化（サービングのコピー） Feast Bridging ML Models and Data 連係 特徴量エンジニアリング手段との連係 Feature Stores for ML Hopsworks Feature Store The missing data layer in ML pipelines? 共通フォーマットでのデータのマテリアライズ、複数のフレームワークから読み書き可能なフォーマット Feature Stores for ML Hopsworksの公式ドキュメントのFeature Store メタデータ管理との統合、データカタログ、登録・探索、探索用のGUI Feature Stores for ML Feature Stores Components of a Data Science Factory Hopsworks Feature Store The missing data layer in ML pipelines? Hopsworksの公式ドキュメントのFeature Store rawデータストアを含めた特徴 画像、動画、音声など非テキストデータとテキストデータの統合的な取り扱い 特徴量エンジニアリングの例 Hopsworks Feature Store The missing data layer in ML pipelines? に一例が載っていたのでついでに転記。 Converting categorical data into numeric data; Normalizing data (to alleviate ill-conditioned optimization when features originate from different distributions); One-hot-encoding/binarization; Feature binning (e.g., convert continuous features into discrete); Feature hashing (e.g., to reduce the memory footprint of one-hot-encoded features); Computing polynomial features; Representation learning (e.g., extract features using clustering, embeddings, or generative models); Computing aggregate features (e.g., count, min, max, stdev). feature storeにおける画像の取扱は？ feature storeのレベルになると行列化されているので、画像を特別なものとして扱わない？ rawデータストア上では画像は画像として扱う。 Feastにおけるデータフロー概要 ※Feastから幾つか図を引用。 Feast Bridging ML Models and Data に載っていたイメージ。 Feastのデータフローから引用 データオーナ側はストリームデータ（Kafka）、DWH（BigQuery）、File（BigQuery）が書かれている。 また真ん中にはApache Beamが書かれており、ストリームETLを経ながらデータがサービングシステムに渡されている。 データは基本的にはストリームとして扱うようだ。 また特徴量を取得するときは以下のようにする。 特徴量の取得 hopsworksにおけるfeature store ※Hopsworksから幾つか図を引用。 Hopsworksの公式ドキュメントのFeature Store に掲載されていたイメージは以下の通り。 Rawデータストアとは異なる位置づけ。 hopsworksでのfeature storeの位置づけ Feastでも言われているが、データエンジニアとデータサイエンティストの間にあるもの、とされている。 データストアする部分の全体アーキテクチャ。 feature storeのアーキテクチャ feature storeのレイヤ構成 複数のコンポーネントを組み合わせて、ひとつのfeature storeを構成しているようである。 ストレージ製品の動向 Netapp Accelerated AI and deep learning pipelines across edge, core, and cloud では、 Create a smooth, secure flow of data for your AI workloads. Unify AI compute and data silos across sites and regions.​ Your data, always available: right place, right time. が挙げられている。 また、クラウド・オンプレ、エッジ・センタを統合する、というのが重要なアピールポイントに見えた。 詳しくは、 Edge to Core to Cloud Architecture for AI を読めばわかりそう。 Dell EMC 単独の技術というより、コンピューティングの工夫を含めてのソリューションのようにみえる。 ENTERPRISE MACHINE &amp; DEEP LEARNING WITH INTELLIGENT STORAGE に思想が書いてありそう。まだ読んでいない。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Storage Layer Software","slug":"Storage-Layer-Software","permalink":"https://dobachi.github.io/memo-blog/tags/Storage-Layer-Software/"}]},{"title":"Hudi","slug":"Hudi","date":"2020-03-25T14:40:12.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2020/03/25/Hudi/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/03/25/Hudi/","excerpt":"","text":"参考 メモ 公式ドキュメント クイックスタートから確認（version 0.5.2前提） org.apache.hudi.DefaultSource#createRelation（書き込み） org.apache.hudi.DefaultSource#createRelation（読み込み） IncrementalRelation Hudiへの書き込み オペレーション種類 DeltaStreamer 動作確認 実装確認 参考 公式ドキュメント Quick Start Guide Writing Hudi Tables 公式ドキュメントのData Streamer apurvam streams-prototyping メモ 公式ドキュメント 載っている特徴は、以下の通り。 Upsert support with fast, pluggable indexing. Atomically publish data with rollback support. Snapshot isolation between writer &amp; queries. Savepoints for data recovery. Manages file sizes, layout using statistics. Async compaction of row &amp; columnar data. Timeline metadata to track lineage. クイックスタートから確認（version 0.5.2前提） Quick Start Guide を参考に進める。 公式ドキュメントではSpark2.4.4を利用しているが、ここでは2.4.5を利用する。 1234$ export SPARK_HOME=/opt/spark/default$ $&#123;SPARK_HOME&#125;/bin/spark-shell \\ --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.2-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \\ --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' 必要なライブラリをインポート 12345678910scala&gt; import org.apache.hudi.QuickstartUtils._scala&gt; import scala.collection.JavaConversions._scala&gt; import org.apache.spark.sql.SaveMode._scala&gt; import org.apache.hudi.DataSourceReadOptions._scala&gt; import org.apache.hudi.DataSourceWriteOptions._scala&gt; import org.apache.hudi.config.HoodieWriteConfig._scala&gt; scala&gt; val tableName = \"hudi_trips_cow\"scala&gt; val basePath = \"file:///tmp/hudi_trips_cow\"scala&gt; val dataGen = new DataGenerator ダミーデータには org.apache.hudi.QuickstartUtils.DataGenerator クラスを利用する。 以下の例では、 org.apache.hudi.QuickstartUtils.DataGenerator#generateInserts メソッドを利用しデータを生成するが、 どういうレコードが生成されるかは、 org.apache.hudi.QuickstartUtils.DataGenerator#generateRandomValue メソッドあたりを見るとわかる。 ダミーデータを生成し、Spark DataFrameに変換。 12scala&gt; val inserts = convertToStringList(dataGen.generateInserts(10))scala&gt; val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2)) 中身は以下。 123456789101112131415scala&gt; df.show+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+| begin_lat| begin_lon| driver| end_lat| end_lon| fare| partitionpath| rider| ts| uuid|+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+| 0.4726905879569653|0.46157858450465483|driver-213| 0.754803407008858| 0.9671159942018241|34.158284716382845|americas/brazil/s...|rider-213|0.0|28432dec-53eb-402...|| 0.6100070562136587| 0.8779402295427752|driver-213| 0.3407870505929602| 0.5030798142293655| 43.4923811219014|americas/brazil/s...|rider-213|0.0|1bd3905e-a6c4-404...|| 0.5731835407930634| 0.4923479652912024|driver-213|0.08988581780930216|0.42520899698713666| 64.27696295884016|americas/united_s...|rider-213|0.0|c9cc8f4b-acee-413...||0.21624150367601136|0.14285051259466197|driver-213| 0.5890949624813784| 0.0966823831927115| 93.56018115236618|americas/united_s...|rider-213|0.0|4be1c199-86dc-489...|| 0.40613510977307| 0.5644092139040959|driver-213| 0.798706304941517|0.02698359227182834|17.851135255091155| asia/india/chennai|rider-213|0.0|83f4d3df-46c1-48a...|| 0.8742041526408587| 0.7528268153249502|driver-213| 0.9197827128888302| 0.362464770874404|19.179139106643607|americas/united_s...|rider-213|0.0|cb8b392d-c9d0-445...|| 0.1856488085068272| 0.9694586417848392|driver-213|0.38186367037201974|0.25252652214479043| 33.92216483948643|americas/united_s...|rider-213|0.0|66aaf87d-4786-4d0...|| 0.0750588760043035|0.03844104444445928|driver-213|0.04376353354538354| 0.6346040067610669| 66.62084366450246|americas/brazil/s...|rider-213|0.0|c5a335f5-c57f-4f5...|| 0.651058505660742| 0.8192868687714224|driver-213|0.20714896002914462|0.06224031095826987| 41.06290929046368| asia/india/chennai|rider-213|0.0|53026eda-28c4-4d8...||0.11488393157088261| 0.6273212202489661|driver-213| 0.7454678537511295| 0.3954939864908973| 27.79478688582596|americas/united_s...|rider-213|0.0|cd42df54-5215-402...|+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+ 12345678scala&gt; df.write.format(\"hudi\"). options(getQuickstartWriteConfigs). option(PRECOMBINE_FIELD_OPT_KEY, \"ts\"). option(RECORDKEY_FIELD_OPT_KEY, \"uuid\"). option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). option(TABLE_NAME, tableName). mode(Overwrite). save(basePath) なお、生成されたファイルは以下の通り。 PARTITIONPATH_FIELD_OPT_KEY で指定したカラムをパーティションキーとして用いていることがわかる。 123456789101112131415161718192021222324252627$ ls -R /tmp/hudi_trips_cow//tmp/hudi_trips_cow/:americas asia/tmp/hudi_trips_cow/americas:brazil united_states/tmp/hudi_trips_cow/americas/brazil:sao_paulo/tmp/hudi_trips_cow/americas/brazil/sao_paulo:ae28c85a-38f0-487f-a42d-3a0babc9d321-0_0-21-25_20200329002247.parquet/tmp/hudi_trips_cow/americas/united_states:san_francisco/tmp/hudi_trips_cow/americas/united_states/san_francisco:849db286-1cbe-4a1f-b544-9939893e99f8-0_1-21-26_20200329002247.parquet/tmp/hudi_trips_cow/asia:india/tmp/hudi_trips_cow/asia/india:chennai/tmp/hudi_trips_cow/asia/india/chennai:2ebfbab0-4f8f-42db-b79e-1c0cbcc3cf39-0_2-21-27_20200329002247.parquet 保存したデータを読み出してみる。 12345scala&gt; val tripsSnapshotDF = spark. read. format(\"hudi\"). load(basePath + \"/*/*/*/*\")scala&gt; tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\") 中身は以下の通り。 元データに対し、Hudiのカラムが追加されていることがわかる。 123456789101112131415scala&gt; tripsSnapshotDF.show+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+|_hoodie_commit_time|_hoodie_commit_seqno| _hoodie_record_key|_hoodie_partition_path| _hoodie_file_name| begin_lat| begin_lon| driver| end_lat| end_lon| fare| partitionpath| rider| ts| uuid|+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+| 20200329002247| 20200329002247_1_1|7695c291-8530-473...| americas/united_s...|849db286-1cbe-4a1...|0.21624150367601136|0.14285051259466197|driver-213| 0.5890949624813784| 0.0966823831927115| 93.56018115236618|americas/united_s...|rider-213|0.0|7695c291-8530-473...|| 20200329002247| 20200329002247_1_3|2f06fcd2-8296-423...| americas/united_s...|849db286-1cbe-4a1...| 0.5731835407930634| 0.4923479652912024|driver-213|0.08988581780930216|0.42520899698713666| 64.27696295884016|americas/united_s...|rider-213|0.0|2f06fcd2-8296-423...|| 20200329002247| 20200329002247_1_5|6ebc4028-9aae-420...| americas/united_s...|849db286-1cbe-4a1...| 0.8742041526408587| 0.7528268153249502|driver-213| 0.9197827128888302| 0.362464770874404|19.179139106643607|americas/united_s...|rider-213|0.0|6ebc4028-9aae-420...|| 20200329002247| 20200329002247_1_6|8bf60390-ad41-4b0...| americas/united_s...|849db286-1cbe-4a1...|0.11488393157088261| 0.6273212202489661|driver-213| 0.7454678537511295| 0.3954939864908973| 27.79478688582596|americas/united_s...|rider-213|0.0|8bf60390-ad41-4b0...|| 20200329002247| 20200329002247_1_7|762e8cb2-8806-47d...| americas/united_s...|849db286-1cbe-4a1...| 0.1856488085068272| 0.9694586417848392|driver-213|0.38186367037201974|0.25252652214479043| 33.92216483948643|americas/united_s...|rider-213|0.0|762e8cb2-8806-47d...|| 20200329002247| 20200329002247_0_8|28622337-d76b-442...| americas/brazil/s...|ae28c85a-38f0-487...| 0.6100070562136587| 0.8779402295427752|driver-213| 0.3407870505929602| 0.5030798142293655| 43.4923811219014|americas/brazil/s...|rider-213|0.0|28622337-d76b-442...|| 20200329002247| 20200329002247_0_9|33aec15d-356f-475...| americas/brazil/s...|ae28c85a-38f0-487...| 0.0750588760043035|0.03844104444445928|driver-213|0.04376353354538354| 0.6346040067610669| 66.62084366450246|americas/brazil/s...|rider-213|0.0|33aec15d-356f-475...|| 20200329002247| 20200329002247_0_10|2d71c9a3-26a3-40b...| americas/brazil/s...|ae28c85a-38f0-487...| 0.4726905879569653|0.46157858450465483|driver-213| 0.754803407008858| 0.9671159942018241|34.158284716382845|americas/brazil/s...|rider-213|0.0|2d71c9a3-26a3-40b...|| 20200329002247| 20200329002247_2_2|a997a8f0-4ab6-4d5...| asia/india/chennai|2ebfbab0-4f8f-42d...| 0.40613510977307| 0.5644092139040959|driver-213| 0.798706304941517|0.02698359227182834|17.851135255091155| asia/india/chennai|rider-213|0.0|a997a8f0-4ab6-4d5...|| 20200329002247| 20200329002247_2_4|271de424-a0f8-427...| asia/india/chennai|2ebfbab0-4f8f-42d...| 0.651058505660742| 0.8192868687714224|driver-213|0.20714896002914462|0.06224031095826987| 41.06290929046368| asia/india/chennai|rider-213|0.0|271de424-a0f8-427...|+-------------------+--------------------+--------------------+----------------------+--------------------+-------------------+-------------------+----------+-------------------+-------------------+------------------+--------------------+---------+---+--------------------+ 上記の通り、SparkのData Source機能を利用している。 中では、org.apache.hudi.DefaultSource#createRelation メソッドが用いられる。 つづいて、更新を試す。 12345678910scala&gt; val updates = convertToStringList(dataGen.generateUpdates(10))scala&gt; val df = spark.read.json(spark.sparkContext.parallelize(updates, 2))scala&gt; df.write.format(\"hudi\"). options(getQuickstartWriteConfigs). option(PRECOMBINE_FIELD_OPT_KEY, \"ts\"). option(RECORDKEY_FIELD_OPT_KEY, \"uuid\"). option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). option(TABLE_NAME, tableName). mode(Append). save(basePath) もう一度、DataFrameとして読み出すと、レコードが追加されていることを確かめられる。（ここでは省略） この後の、 incremental クエリタイプの実験のため、上記の更新を幾度か実行しておく。 つづいて、 incremental クエリタイプで読み出す。 一度読み出し、最初のコミット時刻を取り出す。 12345678scala&gt; spark. read. format(\"hudi\"). load(basePath + \"/*/*/*/*\"). createOrReplaceTempView(\"hudi_trips_snapshot\")scala&gt; val commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_trips_snapshot order by commitTime\").map(k =&gt; k.getString(0)).take(50)scala&gt; val beginTime = commits(commits.length - 2) // commit time we are interested in 今回は、初回書き込みに加えて2回更新したので、 commits は以下の通り。 12scala&gt; commitsres12: Array[String] = Array(20200330002239, 20200330002354, 20200330003142) また、今回「読み込みの最初」とするコミットは、以下の通り。 つまり、2回目の更新時。 12scala&gt; beginTimeres13: String = 20200330002354 では、 incremental クエリタイプで読み出す。 1234567scala&gt; val tripsIncrementalDF = spark.read.format(\"hudi\"). option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL). option(BEGIN_INSTANTTIME_OPT_KEY, beginTime). load(basePath)scala&gt; tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")scala&gt; spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_incremental where fare &gt; 20.0\").show() 結果は以下のようなイメージ。 1234567891011scala&gt; spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_incremental where fare &gt; 20.0\").show()+-------------------+------------------+--------------------+-------------------+---+|_hoodie_commit_time| fare| begin_lon| begin_lat| ts|+-------------------+------------------+--------------------+-------------------+---+| 20200330003142| 87.68271062363665| 0.9273857651526887| 0.1620033132033215|0.0|| 20200330003142| 40.44073446276323|9.842943407509797E-4|0.47631824594751015|0.0|| 20200330003142| 45.39370966816483| 0.65888271115305| 0.8535610661589833|0.0|| 20200330003142|47.332186591003044| 0.8006023508896579| 0.9025851737325563|0.0|| 20200330003142| 93.34457064050349| 0.6331319396951335| 0.5375953886834237|0.0|| 20200330003142|31.065524210209226| 0.7608842984578864| 0.9514417909802292|0.0|+-------------------+------------------+--------------------+-------------------+---+ なお、ここでbeginTimeを1遡ることにすると…。 1scala&gt; val beginTime = commits(commits.length - 3) // commit time we are interested in 以下のように、2回目のコミットも含まれるようになる。 1234567891011121314151617scala&gt; spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_incremental where fare &gt; 20.0\").show()+-------------------+------------------+--------------------+-------------------+---+|_hoodie_commit_time| fare| begin_lon| begin_lat| ts|+-------------------+------------------+--------------------+-------------------+---+| 20200330003142| 87.68271062363665| 0.9273857651526887| 0.1620033132033215|0.0|| 20200330003142| 40.44073446276323|9.842943407509797E-4|0.47631824594751015|0.0|| 20200330003142| 45.39370966816483| 0.65888271115305| 0.8535610661589833|0.0|| 20200330003142|47.332186591003044| 0.8006023508896579| 0.9025851737325563|0.0|| 20200330002354| 39.09858962414072| 0.08151154133724581|0.21729959707372848|0.0|| 20200330003142| 93.34457064050349| 0.6331319396951335| 0.5375953886834237|0.0|| 20200330002354| 80.87869643345753| 0.0748253615757305| 0.9787639413761751|0.0|| 20200330003142|31.065524210209226| 0.7608842984578864| 0.9514417909802292|0.0|| 20200330002354|21.602186045036387| 0.772134626462835| 0.3291184473506418|0.0|| 20200330002354| 43.41497201940956| 0.6226833057042072| 0.5501675314928346|0.0|| 20200330002354| 35.71294622426758| 0.6696123015022845| 0.7318572150654761|0.0|| 20200330002354| 67.30906296028802| 0.16768228612130764|0.29666655980198253|0.0|+-------------------+------------------+--------------------+-------------------+---+ org.apache.hudi.DefaultSource#createRelation（書き込み） クイックスタートで、例えば更新などする際の動作を確認する。 12345678scala&gt; df.write.format(\"hudi\"). | options(getQuickstartWriteConfigs). | option(PRECOMBINE_FIELD_OPT_KEY, \"ts\"). | option(RECORDKEY_FIELD_OPT_KEY, \"uuid\"). | option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\"). | option(TABLE_NAME, tableName). | mode(Append). | save(basePath) のような例を実行する際、内部的には org.apache.hudi.DefaultSource#createRelation が呼ばれる。 org/apache/hudi/DefaultSource.scala:85 123456789override def createRelation(sqlContext: SQLContext, mode: SaveMode, optParams: Map[String, String], df: DataFrame): BaseRelation = &#123; val parameters = HoodieSparkSqlWriter.parametersWithWriteDefaults(optParams) HoodieSparkSqlWriter.write(sqlContext, mode, parameters, df) createRelation(sqlContext, parameters, df.schema)&#125; 上記メソッド内では、 org.apache.hudi.HoodieSparkSqlWriter$#write メソッドが呼ばれており、 これが書き込みの実態である。 なお、その下の org.apache.hudi.DefaultSource#createRelation は、読み込み時に呼ばれるものと同一。 ここでは org.apache.hudi.HoodieSparkSqlWriter#write メソッドを確認する。 当該メソッドの冒頭では、オペレーションの判定などいくつか前処理が行われた後、 以下の箇所から実際に書き出す処理が定義されている。 org/apache/hudi/HoodieSparkSqlWriter.scala:85 123456789101112val (writeStatuses, writeClient: HoodieWriteClient[HoodieRecordPayload[Nothing]]) = if (!operation.equalsIgnoreCase(DELETE_OPERATION_OPT_VAL)) &#123; // register classes &amp; schemas val structName = s\"$&#123;tblName.get&#125;_record\" val nameSpace = s\"hoodie.$&#123;tblName.get&#125;\" sparkContext.getConf.registerKryoClasses( Array(classOf[org.apache.avro.generic.GenericData], classOf[org.apache.avro.Schema])) val schema = AvroConversionUtils.convertStructTypeToAvroSchema(df.schema, structName, nameSpace) sparkContext.getConf.registerAvroSchemas(schema) (snip) まず delete オペレーションかどうかで処理が別れるが、上記の例では upsert オペレーションなので一旦そのまま読み進める。 ネームスペース（データベースやテーブル？）を取得した後、SparkのStructTypeで保持されたスキーマ情報を、AvroのSchemaに変換する。 変換されたスキーマをSparkで登録する。 つづいて、DataFrameをRDDに変換する。 org/apache/hudi/HoodieSparkSqlWriter.scala:97 123456789// Convert to RDD[HoodieRecord]val keyGenerator = DataSourceUtils.createKeyGenerator(toProperties(parameters))val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)val hoodieAllIncomingRecords = genericRecords.map(gr =&gt; &#123; val orderingVal = DataSourceUtils.getNestedFieldValAsString( gr, parameters(PRECOMBINE_FIELD_OPT_KEY), false).asInstanceOf[Comparable[_]] DataSourceUtils.createHoodieRecord(gr, orderingVal, keyGenerator.getKey(gr), parameters(PAYLOAD_CLASS_OPT_KEY))&#125;).toJavaRDD() RDDに一度変換した後、mapメソッドで加工する。 まず、 genericRecords の内容は以下のようなものが含まれる。 1234567891011121314result = &#123;GenericRecord[1]@27822&#125; 0 = &#123;GenericData$Record@27827&#125; &quot;&#123;&quot;begin_lat&quot;: 0.09632451474505643, &quot;begin_lon&quot;: 0.8989273848550128, &quot;driver&quot;: &quot;driver-164&quot;, &quot;end_lat&quot;: 0.6431885917325862, &quot;end_lon&quot;: 0.6664889106258252, &quot;fare&quot;: 86.865568091804, &quot;partitionpath&quot;: &quot;americas/brazil/sao_paulo&quot;, &quot;rider&quot;: &quot;rider-164&quot;, &quot;ts&quot;: 0.0, &quot;uuid&quot;: &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot;&#125;&quot; schema = &#123;Schema$RecordSchema@27835&#125; &quot;&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_trips_cow_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_trips_cow&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;begin_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;begin_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;driver&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;fare&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;rider&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;&quot; values = &#123;Object[10]@27836&#125; 0 = &#123;Double@27838&#125; 0.09632451474505643 1 = &#123;Double@27839&#125; 0.8989273848550128 2 = &#123;Utf8@27840&#125; &quot;driver-164&quot; 3 = &#123;Double@27841&#125; 0.6431885917325862 4 = &#123;Double@27842&#125; 0.6664889106258252 5 = &#123;Double@27843&#125; 86.865568091804 6 = &#123;Utf8@27844&#125; &quot;americas/brazil/sao_paulo&quot; 7 = &#123;Utf8@27845&#125; &quot;rider-164&quot; 8 = &#123;Double@27846&#125; 0.0 9 = &#123;Utf8@27847&#125; &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot; 上記の通り、これは入ロクレコードそのものである。 その後、mapメソッドを使ってHudiで利用するキーを含む、Hudiのレコード形式に変換する。 変換された hoodieAllIncomingRecords は以下のような内容になる。 1234567891011result = &#123;Wrappers$SeqWrapper@27881&#125; size = 1 0 = &#123;HoodieRecord@27883&#125; &quot;HoodieRecord&#123;key=HoodieKey &#123; recordKey=5d49cfb5-0db4-4172-bff4-e581eb1f9783 partitionPath=americas/brazil/sao_paulo&#125;, currentLocation=&apos;null&apos;, newLocation=&apos;null&apos;&#125;&quot; key = &#123;HoodieKey@27892&#125; &quot;HoodieKey &#123; recordKey=5d49cfb5-0db4-4172-bff4-e581eb1f9783 partitionPath=americas/brazil/sao_paulo&#125;&quot; recordKey = &quot;5d49cfb5-0db4-4172-bff4-e581eb1f9783&quot; partitionPath = &quot;americas/brazil/sao_paulo&quot; data = &#123;OverwriteWithLatestAvroPayload@27893&#125; recordBytes = &#123;byte[142]@27895&#125; orderingVal = &quot;0.0&quot; currentLocation = null newLocation = null sealed = false 上記の例の通り、ペイロードは org.apache.hudi.common.model.OverwriteWithLatestAvroPayload で保持される。 その後、いくつかモードの確認が行われた後、もしテーブルがなければ org.apache.hudi.common.table.HoodieTableMetaClient#initTableType を用いて テーブルを初期化する。 その後、重複レコードを必要に応じて落とす。 org/apache/hudi/HoodieSparkSqlWriter.scala:132 123456789val hoodieRecords = if (parameters(INSERT_DROP_DUPS_OPT_KEY).toBoolean) &#123; DataSourceUtils.dropDuplicates( jsc, hoodieAllIncomingRecords, mapAsJavaMap(parameters), client.getTimelineServer) &#125; else &#123; hoodieAllIncomingRecords &#125; レコードが空かどうかを改めて確認しつつ、 最後に書き込み実施。 org.apache.hudi.DataSourceUtils#doWriteOperation が実態である。 org/apache/hudi/HoodieSparkSqlWriter.scala:147 1val writeStatuses = DataSourceUtils.doWriteOperation(client, hoodieRecords, commitTime, operation) org.apache.hudi.DataSourceUtils#doWriteOperation メソッドは以下の通り。 org/apache/hudi/DataSourceUtils.java:162 1234567891011public static JavaRDD&lt;WriteStatus&gt; doWriteOperation(HoodieWriteClient client, JavaRDD&lt;HoodieRecord&gt; hoodieRecords, String commitTime, String operation) &#123; if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) &#123; return client.bulkInsert(hoodieRecords, commitTime); &#125; else if (operation.equals(DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())) &#123; return client.insert(hoodieRecords, commitTime); &#125; else &#123; // default is upsert return client.upsert(hoodieRecords, commitTime); &#125;&#125; 今回の例だと、 upseart オペレーションなので org.apache.hudi.client.HoodieWriteClient#upsert メソッドが呼ばれる。 このメソッドは以下のとおりだが、ポイントは、 org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal メソッドである。 12345678910111213141516171819public JavaRDD&lt;WriteStatus&gt; upsert(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, final String commitTime) &#123; HoodieTable&lt;T&gt; table = getTableAndInitCtx(OperationType.UPSERT); try &#123; // De-dupe/merge if needed JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords = combineOnCondition(config.shouldCombineBeforeUpsert(), records, config.getUpsertShuffleParallelism()); Timer.Context indexTimer = metrics.getIndexCtx(); // perform index loop up to get existing location of records JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; taggedRecords = getIndex().tagLocation(dedupedRecords, jsc, table); metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop())); return upsertRecordsInternal(taggedRecords, commitTime, table, true); &#125; catch (Throwable e) &#123; if (e instanceof HoodieUpsertException) &#123; throw (HoodieUpsertException) e; &#125; throw new HoodieUpsertException(\"Failed to upsert for commit time \" + commitTime, e); &#125;&#125; org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal メソッド内のポイントは、 以下の箇所。 org.apache.spark.api.java.AbstractJavaRDDLike#mapPartitionsWithIndex メソッドで、upsertやinsertの処理を定義している。 org/apache/hudi/client/HoodieWriteClient.java:470 123456789JavaRDD&lt;WriteStatus&gt; writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -&gt; &#123; if (isUpsert) &#123; return hoodieTable.handleUpsertPartition(commitTime, partition, recordItr, partitioner); &#125; else &#123; return hoodieTable.handleInsertPartition(commitTime, partition, recordItr, partitioner); &#125;&#125;, true).flatMap(List::iterator);return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime); ここでは、 org.apache.hudi.table.HoodieCopyOnWriteTable#handleUpsertPartition メソッドを確認してみる。 org/apache/hudi/table/HoodieCopyOnWriteTable.java:253 12345678910111213141516171819public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String commitTime, Integer partition, Iterator recordItr, Partitioner partitioner) &#123; UpsertPartitioner upsertPartitioner = (UpsertPartitioner) partitioner; BucketInfo binfo = upsertPartitioner.getBucketInfo(partition); BucketType btype = binfo.bucketType; try &#123; if (btype.equals(BucketType.INSERT)) &#123; return handleInsert(commitTime, binfo.fileIdPrefix, recordItr); &#125; else if (btype.equals(BucketType.UPDATE)) &#123; return handleUpdate(commitTime, binfo.fileIdPrefix, recordItr); &#125; else &#123; throw new HoodieUpsertException(\"Unknown bucketType \" + btype + \" for partition :\" + partition); &#125; &#125; catch (Throwable t) &#123; String msg = \"Error upserting bucketType \" + btype + \" for partition :\" + partition; LOG.error(msg, t); throw new HoodieUpsertException(msg, t); &#125;&#125; 真ん中あたりに、INSERTかUPDATEかで条件分岐しているが、ここでは例としてINSERT側を確認する。 org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsert メソッドがポイントとなる。 なお、当該メッソッドには同期的な実装と、非同期的な実装があるよう。 ここでは上記呼び出しに基づき、非同期的な実装の方を確認する。 org/apache/hudi/table/HoodieCopyOnWriteTable.java:233 123456789public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String commitTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) throws Exception &#123; // This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records if (!recordItr.hasNext()) &#123; LOG.info(\"Empty partition\"); return Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator(); &#125; return new CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, commitTime, this, idPfx);&#125; 戻り値が、 org.apache.hudi.execution.CopyOnWriteLazyInsertIterable クラスのインスタンスになっていることがわかる。 このイテレータは、 org.apache.hudi.client.utils.LazyIterableIterator アブストラクトクラスを継承している。 org.apache.hudi.client.utils.LazyIterableIterator では、nextメソッドが org/apache/hudi/client/utils/LazyIterableIterator.java:116 12345678@Overridepublic O next() &#123; try &#123; return computeNext(); &#125; catch (Exception ex) &#123; throw new RuntimeException(ex); &#125;&#125; のように定義されており、実態が org.apache.hudi.client.utils.LazyIterableIterator#computeNext であることがわかる。 当該メソッドは、 org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#CopyOnWriteLazyInsertIterable クラスではオーバーライドされており、 以下のように定義されている。 org/apache/hudi/execution/CopyOnWriteLazyInsertIterable.java:93 1234567891011121314151617181920@Overrideprotected List&lt;WriteStatus&gt; computeNext() &#123; // Executor service used for launching writer thread. BoundedInMemoryExecutor&lt;HoodieRecord&lt;T&gt;, HoodieInsertValueGenResult&lt;HoodieRecord&gt;, List&lt;WriteStatus&gt;&gt; bufferedIteratorExecutor = null; try &#123; final Schema schema = new Schema.Parser().parse(hoodieConfig.getSchema()); bufferedIteratorExecutor = new SparkBoundedInMemoryExecutor&lt;&gt;(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema)); final List&lt;WriteStatus&gt; result = bufferedIteratorExecutor.execute(); assert result != null &amp;&amp; !result.isEmpty() &amp;&amp; !bufferedIteratorExecutor.isRemaining(); return result; &#125; catch (Exception e) &#123; throw new HoodieException(e); &#125; finally &#123; if (null != bufferedIteratorExecutor) &#123; bufferedIteratorExecutor.shutdownNow(); &#125; &#125;&#125; どうやら内部でFutureパターンを利用し、非同期化して書き込みを行っているようだ。（これが筋よしなのかどうかは要議論。update、つまりマージも同様になっている。） 処理内容を知る上でポイントとなるのは、 12bufferedIteratorExecutor = new SparkBoundedInMemoryExecutor&lt;&gt;(hoodieConfig, inputItr, getInsertHandler(), getTransformFunction(schema)); の箇所。 org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#getInsertHandler あたり。 中で用いられている、 org.apache.hudi.execution.CopyOnWriteLazyInsertIterable.CopyOnWriteInsertHandler クラスがポイントとなる。 このクラスは、書き込みデータのキュー（要確認）からレコードを受け取って、処理していると考えられる。 1234567891011121314151617181920protected void consumeOneRecord(HoodieInsertValueGenResult&lt;HoodieRecord&gt; payload) &#123; final HoodieRecord insertPayload = payload.record; // lazily initialize the handle, for the first time if (handle == null) &#123; handle = new HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, insertPayload.getPartitionPath(), getNextFileId(idPrefix)); &#125; if (handle.canWrite(payload.record)) &#123; // write the payload, if the handle has capacity handle.write(insertPayload, payload.insertValue, payload.exception); &#125; else &#123; // handle is full. statuses.add(handle.close()); // Need to handle the rejected payload &amp; open new handle handle = new HoodieCreateHandle(hoodieConfig, commitTime, hoodieTable, insertPayload.getPartitionPath(), getNextFileId(idPrefix)); handle.write(insertPayload, payload.insertValue, payload.exception); // we should be able to write 1 payload. &#125;&#125; 下の方にある org.apache.hudi.io.HoodieCreateHandle クラスを用いているあたりがポイント。 そのwriteメソッドは以下の通り。 org.apache.hudi.io.storage.HoodieStorageWriter#writeAvroWithMetadata を用いて書き出しているように見える。 （実際には org.apache.hudi.io.storage.HoodieParquetWriter ） 12345678910111213141516171819202122232425262728public void write(HoodieRecord record, Option&lt;IndexedRecord&gt; avroRecord) &#123; Option recordMetadata = record.getData().getMetadata(); try &#123; if (avroRecord.isPresent()) &#123; // Convert GenericRecord to GenericRecord with hoodie commit metadata in schema IndexedRecord recordWithMetadataInSchema = rewriteRecord((GenericRecord) avroRecord.get()); storageWriter.writeAvroWithMetadata(recordWithMetadataInSchema, record); // update the new location of record, so we know where to find it next record.unseal(); record.setNewLocation(new HoodieRecordLocation(instantTime, writeStatus.getFileId())); record.seal(); recordsWritten++; insertRecordsWritten++; &#125; else &#123; recordsDeleted++; &#125; writeStatus.markSuccess(record, recordMetadata); // deflate record payload after recording success. This will help users access payload as a // part of marking // record successful. record.deflate(); &#125; catch (Throwable t) &#123; // Not throwing exception from here, since we don't want to fail the entire job // for a single record writeStatus.markFailure(record, t, recordMetadata); LOG.error(\"Error writing record \" + record, t); &#125;&#125; org.apache.hudi.io.storage.HoodieParquetWriter#writeAvroWithMetadata メソッドは以下の通りである。 つまり、 org.apache.parquet.hadoop.ParquetWriter#write を用いてParquet内に、Avroレコードを書き出していることがわかる。 12345678910@Overridepublic void writeAvroWithMetadata(R avroRecord, HoodieRecord record) throws IOException &#123; String seqId = HoodieRecord.generateSequenceId(commitTime, TaskContext.getPartitionId(), recordIndex.getAndIncrement()); HoodieAvroUtils.addHoodieKeyToRecord((GenericRecord) avroRecord, record.getRecordKey(), record.getPartitionPath(), file.getName()); HoodieAvroUtils.addCommitMetadataToRecord((GenericRecord) avroRecord, commitTime, seqId); super.write(avroRecord); writeSupport.add(record.getRecordKey());&#125; 今回のクイックスタートの例では、 avroRecord には以下のような内容が含まれていた。 123456789101112131415161718result = &#123;GenericData$Record@18566&#125; &quot;&#123;&quot;_hoodie_commit_time&quot;: &quot;20200331002133&quot;, &quot;_hoodie_commit_seqno&quot;: &quot;20200331002133_0_44&quot;, &quot;_hoodie_record_key&quot;: &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;, &quot;_hoodie_partition_path&quot;: &quot;asia/india/chennai&quot;, &quot;_hoodie_file_name&quot;: &quot;317a54b0-70b8-4bdc-bfde-12ba4fde982b-0_0-207-301_20200331002133.parquet&quot;, &quot;begin_lat&quot;: 0.4789745387904072, &quot;begin_lon&quot;: 0.14781856144057215, &quot;driver&quot;: &quot;driver-022&quot;, &quot;end_lat&quot;: 0.10509642405359532, &quot;end_lon&quot;: 0.07682825311613706, &quot;fare&quot;: 30.429177017810616, &quot;partitionpath&quot;: &quot;asia/india/chennai&quot;, &quot;rider&quot;: &quot;rider-022&quot;, &quot;ts&quot;: 0.0, &quot;uuid&quot;: &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot;&#125;&quot; schema = &#123;Schema$RecordSchema@18582&#125; &quot;&#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;hudi_trips_cow_record&quot;,&quot;namespace&quot;:&quot;hoodie.hudi_trips_cow&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;_hoodie_commit_time&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_commit_seqno&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_record_key&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_partition_path&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_file_name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;begin_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;begin_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;driver&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lat&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;end_lon&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;fare&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;partitionpath&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;rider&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;double&quot;,&quot;null&quot;]&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;string&quot;,&quot;null&quot;]&#125;]&#125;&quot; values = &#123;Object[15]@18583&#125; 0 = &quot;20200331002133&quot; 1 = &quot;20200331002133_0_44&quot; 2 = &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot; 3 = &quot;asia/india/chennai&quot; 4 = &quot;317a54b0-70b8-4bdc-bfde-12ba4fde982b-0_0-207-301_20200331002133.parquet&quot; 5 = &#123;Double@18596&#125; 0.4789745387904072 6 = &#123;Double@18597&#125; 0.14781856144057215 7 = &#123;Utf8@18598&#125; &quot;driver-022&quot; 8 = &#123;Double@18599&#125; 0.10509642405359532 9 = &#123;Double@18600&#125; 0.07682825311613706 10 = &#123;Double@18601&#125; 30.429177017810616 11 = &#123;Utf8@18602&#125; &quot;asia/india/chennai&quot; 12 = &#123;Utf8@18603&#125; &quot;rider-022&quot; 13 = &#123;Double@18604&#125; 0.0 14 = &#123;Utf8@18605&#125; &quot;7b887fb5-2837-4cac-b075-a8a8450f453d&quot; org.apache.hudi.DefaultSource#createRelation（読み込み） 当該メソッドのポイントを確認する。 hoodie.datasource.query.type の種類によって返すRelationが変わる。 org/apache/hudi/DefaultSource.scala:60 1234567891011if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_SNAPSHOT_OPT_VAL)) &#123;(snip)&#125; else if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_INCREMENTAL_OPT_VAL)) &#123;(snip)&#125; else &#123; throw new HoodieException(\"Invalid query type :\" + parameters(QUERY_TYPE_OPT_KEY))&#125; 上記の通り、 snapshot 、もしくは incremental クエリタイプである。 なお、以下の通り、 MERGE_ON_READ テーブルに対する snapshot クエリタイプは利用できない。 もし使いたければ、SparkのData Source機能ではなく、Hiveテーブルとして読み込むこと。 org/apache/hudi/DefaultSource.scala:69 12log.warn(\"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. \" + \"Please query the Hive table registered using Spark SQL.\") まずクエリタイプが snapshot である場合は、 以下の通り、Parquetとして読み込みが定義され、Relationが返される。 org/apache/hudi/DefaultSource.scala:72 123456DataSource.apply( sparkSession = sqlContext.sparkSession, userSpecifiedSchema = Option(schema), className = \"parquet\", options = parameters) .resolveRelation() 例えば、クイックスタートの例 12345scala&gt; val tripsSnapshotDF = spark. read. format(\"hudi\"). load(basePath + \"/*/*/*/*\")scala&gt; tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\") では、こちらのタイプ。 ParquetベースのRelation（実際には、HadoopFsRelation）が返される。 上記の例では、当該RelationのrootPathsに、以下のような値が含まれる。 1234567rootPaths = &#123;$colon$colon@14885&#125; &quot;::&quot; size = 6 0 = &#123;Path@15421&#125; &quot;file:/tmp/hudi_trips_cow/americas/brazil/sao_paulo/ae28c85a-38f0-487f-a42d-3a0babc9d321-0_0-21-25_20200329002247.parquet&quot; 1 = &#123;Path@15422&#125; &quot;file:/tmp/hudi_trips_cow/americas/brazil/sao_paulo/.hoodie_partition_metadata&quot; 2 = &#123;Path@15423&#125; &quot;file:/tmp/hudi_trips_cow/americas/united_states/san_francisco/849db286-1cbe-4a1f-b544-9939893e99f8-0_1-21-26_20200329002247.parquet&quot; 3 = &#123;Path@15424&#125; &quot;file:/tmp/hudi_trips_cow/americas/united_states/san_francisco/.hoodie_partition_metadata&quot; 4 = &#123;Path@15425&#125; &quot;file:/tmp/hudi_trips_cow/asia/india/chennai/2ebfbab0-4f8f-42db-b79e-1c0cbcc3cf39-0_2-21-27_20200329002247.parquet&quot; 5 = &#123;Path@15426&#125; &quot;file:/tmp/hudi_trips_cow/asia/india/chennai/.hoodie_partition_metadata&quot; 次にクエリタイプが incremental である場合は、 以下の通り、 org.apache.hudi.IncrementalRelation#IncrementalRelation が返される。 org/apache/hudi/DefaultSource.scala:79 1new IncrementalRelation(sqlContext, path.get, optParams, schema) クイックスタートの例 1234567scala&gt; val tripsIncrementalDF = spark.read.format(\"hudi\"). option(QUERY_TYPE_OPT_KEY, QUERY_TYPE_INCREMENTAL_OPT_VAL). option(BEGIN_INSTANTTIME_OPT_KEY, beginTime). load(basePath)scala&gt; tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")scala&gt; spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_incremental where fare &gt; 20.0\").show() では、 org.apache.hudi.IncrementalRelation#IncrementalRelation が戻り値として返される。 IncrementalRelation コンストラクタ Parquetをファイルを単純に読めば良いのと比べて、格納された最新データを返すようにしないとならないので それなりに複雑なRelationとなっている。 以下、簡単にコンストラクタのポイントを確認する。 最初にメタデータを取得するクライアント。 コミット、セーブポイント、コンパクションなどの情報を得られるようになる。 org/apache/hudi/IncrementalRelation.scala:51 1val metaClient = new HoodieTableMetaClient(sqlContext.sparkContext.hadoopConfiguration, basePath, true) クイックスタートの例では、 metaPath は、 file:/tmp/hudi_trips_cow/.hoodie だった。 続いてテーブル情報のインスタンスを取得する。 テーブル情報からタイムラインを取り出す。 org/apache/hudi/IncrementalRelation.scala:57 12345678910private val hoodieTable = HoodieTable.getHoodieTable(metaClient, HoodieWriteConfig.newBuilder().withPath(basePath).build(), sqlContext.sparkContext)val commitTimeline = hoodieTable.getMetaClient.getCommitTimeline.filterCompletedInstants()if (commitTimeline.empty()) &#123; throw new HoodieException(\"No instants to incrementally pull\")&#125;if (!optParams.contains(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY)) &#123; throw new HoodieException(s\"Specify the begin instant time to pull from using \" + s\"option $&#123;DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY&#125;\")&#125; クイックスタートの例で実際に生成されたタイムラインは以下の通り。 1234instants = &#123;ArrayList@25586&#125; size = 3 0 = &#123;HoodieInstant@25589&#125; &quot;[20200330002239__commit__COMPLETED]&quot; 1 = &#123;HoodieInstant@25590&#125; &quot;[20200330002354__commit__COMPLETED]&quot; 2 = &#123;HoodieInstant@25591&#125; &quot;[20200330003142__commit__COMPLETED]&quot; オプションとして与えられた「はじめ」と「おわり」から、 対象となるタイムラインを構成する。 タイムライン上、最も新しいインスタンスを取得し、 Parquetファイルからスキーマを読んでいる。 org/apache/hudi/IncrementalRelation.scala:68 1234567891011121314151617val lastInstant = commitTimeline.lastInstant().get()val commitsToReturn = commitTimeline.findInstantsInRange( optParams(DataSourceReadOptions.BEGIN_INSTANTTIME_OPT_KEY), optParams.getOrElse(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, lastInstant.getTimestamp)) .getInstants.iterator().toList// use schema from a file produced in the latest instantval latestSchema = &#123; // use last instant if instant range is empty val instant = commitsToReturn.lastOption.getOrElse(lastInstant) val latestMeta = HoodieCommitMetadata .fromBytes(commitTimeline.getInstantDetails(instant).get, classOf[HoodieCommitMetadata]) val metaFilePath = latestMeta.getFileIdAndFullPaths(basePath).values().iterator().next() AvroConversionUtils.convertAvroSchemaToStructType(ParquetUtils.readAvroSchema( sqlContext.sparkContext.hadoopConfiguration, new Path(metaFilePath)))&#125; クイックスタートの例では、 commitsToReturn は以下の通り。 12345result = &#123;$colon$colon@25626&#125; &quot;::&quot; size = 1 0 = &#123;HoodieInstant@25591&#125; &quot;[20200330003142__commit__COMPLETED]&quot; state = &#123;HoodieInstant$State@25602&#125; &quot;COMPLETED&quot; action = &quot;commit&quot; timestamp = &quot;20200330003142&quot; また、少々気になるのは、 12AvroConversionUtils.convertAvroSchemaToStructType(ParquetUtils.readAvroSchema( sqlContext.sparkContext.hadoopConfiguration, new Path(metaFilePath))) という箇所で、もともとParquet形式のデータからAvro形式のスキーマを取り出し、それをさらにSparkのStructTypeに変換しているところ。 実際にParquetのfooterから取り出したスキーマ情報を、AvroのSchemaに変換しているのは以下の箇所。 org/apache/hudi/common/util/ParquetUtils.java:140 123public static Schema readAvroSchema(Configuration configuration, Path parquetFilePath) &#123; return new AvroSchemaConverter().convert(readSchema(configuration, parquetFilePath));&#125; Parquet自身にAvroへの変換器 org.apache.parquet.avro.AvroSchemaConverter が備わっているので便利？ SparkのData Source機能でDataFrame化してからスキーマを取り出すと、一度読み込みが生じていしまうから非効率？ という理由が想像されるが、やや回りくどいような印象を持った。 ★要確認 本編に戻る。続いてフィルタを定義。 org/apache/hudi/IncrementalRelation.scala:86 12345678910val filters = &#123; if (optParams.contains(DataSourceReadOptions.PUSH_DOWN_INCR_FILTERS_OPT_KEY)) &#123; val filterStr = optParams.getOrElse( DataSourceReadOptions.PUSH_DOWN_INCR_FILTERS_OPT_KEY, DataSourceReadOptions.DEFAULT_PUSH_DOWN_FILTERS_OPT_VAL) filterStr.split(\",\").filter(!_.isEmpty) &#125; else &#123; Array[String]() &#125;&#125; ここまでがコンストラクタ。 buildScan 実際にSparkのData Sourceで読み込むときに用いられる読み込みの手段が定義されている。 以下にポイントを述べる。 org/apache/hudi/IncrementalRelation.scala:99 123override def buildScan(): RDD[Row] = &#123;(snip) ファイルIDとフルPATHのマップを作る。 123456val fileIdToFullPath = mutable.HashMap[String, String]()for (commit &lt;- commitsToReturn) &#123; val metadata: HoodieCommitMetadata = HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commit) .get, classOf[HoodieCommitMetadata]) fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap&#125; 上記マップに対し、必要に応じてフィルタを適用する。 org/apache/hudi/IncrementalRelation.scala:106 123456789val pathGlobPattern = optParams.getOrElse( DataSourceReadOptions.INCR_PATH_GLOB_OPT_KEY, DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)val filteredFullPath = if(!pathGlobPattern.equals(DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)) &#123; val globMatcher = new GlobPattern(\"*\" + pathGlobPattern) fileIdToFullPath.filter(p =&gt; globMatcher.matches(p._2))&#125; else &#123; fileIdToFullPath&#125; コンストラクタで定義されたフィルタを適用しながら、 対象となるParquetファイルを読み込み、RDDを生成する。 org/apache/hudi/IncrementalRelation.scala:117 1234567891011121314sqlContext.sparkContext.hadoopConfiguration.unset(\"mapreduce.input.pathFilter.class\")val sOpts = optParams.filter(p =&gt; !p._1.equalsIgnoreCase(\"path\"))if (filteredFullPath.isEmpty) &#123; sqlContext.sparkContext.emptyRDD[Row]&#125; else &#123; log.info(\"Additional Filters to be applied to incremental source are :\" + filters) filters.foldLeft(sqlContext.read.options(sOpts) .schema(latestSchema) .parquet(filteredFullPath.values.toList: _*) .filter(String.format(\"%s &gt;= '%s'\", HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitsToReturn.head.getTimestamp)) .filter(String.format(\"%s &lt;= '%s'\", HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitsToReturn.last.getTimestamp)))((e, f) =&gt; e.filter(f)) .toDF().rdd&#125; Hudiへの書き込み Writing Hudi Tables をベースに調べる。 オペレーション種類 書き込みのオペレーション種類は、upsert、insert、bulk_insert。 クイックスタートにはbulk_insertはなかった。 DeltaStreamer ユーティリティとして付属するDeltaStreamerを用いると、 Kafka等からデータを取り込める。 Avro等のスキーマのデータを読み取れる。 動作確認 パッケージ化 公式ドキュメントのData Streamer の手順に基づくと、 ビルドされたユーティリティを使うことになるので、 予めパッケージ化しておく。 123456$ mkdir -p ~/Sources$ cd ~/Sources$ git clone https://github.com/apache/incubator-hudi.git incubator-hudi-052$ cd incubator-hudi-052$ git checkout -b release-0.5.2-incubating refs/tags/release-0.5.2-incubating$ mvn clean package -DskipTests -DskipITs 実行 公式ドキュメントのData Streamerに基づくと、Confluentメンバが作成した （ apurvam streams-prototyping ）サンプルデータ作成用のAvroスキーマと Confluent PlatformのKSQLのユーティリティを 使ってサンプルデータを作成する。 ついては。予めConfluent Platformをインストールしておくこと。 まずはスキーマをダウンロードする。 1$ curl https://raw.githubusercontent.com/apurvam/streams-prototyping/master/src/main/resources/impressions.avro &gt; /tmp/impressions.avro テストデータを生成する。 1$ ksql-datagen schema=/tmp/impressions.avro format=avro topic=impressions key=impressionid 別の端末を開き、ユーティリティを起動する。 1234567891011$ export SPARK_HOME=/opt/spark/default$ cd ~/Sources/incubator-hudi-052$ $&#123;SPARK_HOME&#125;/bin/spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.5.2-incubating.jar \\ --props file://$&#123;PWD&#125;/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\ --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\ --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\ --source-ordering-field impresssiontime \\ --target-base-path file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ --target-table uber.impressions \\ --table-type COPY_ON_WRITE \\ --op BULK_INSERT なお、 公式ドキュメントのData Streamer から2箇所修正した。（JarファイルPATH、 --table-type オプション追加。 Kafkaから読み込んで書き出したデータ（ /tmp/hudi-deltastreamer-op ）を確認してみる。 123$ $&#123;SPARK_HOME&#125;/bin/spark-shell \\ --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.2-incubating,org.apache.spark:spark-avro_2.11:2.4.5 \\ --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' シェルが起動したら、以下の通り読み込んで見る。 なお、ここでは userid がパーティションキーとなっているので、ロード時にそれを指定した。 12345scala&gt; val basePath = \"file:///tmp/hudi-deltastreamer-op\"scala&gt; val impressionDF = spark. read. format(\"hudi\"). load(basePath + \"/*/*\") 内容は以下の通り。 123456789101112scala&gt; impressionDF.show+-------------------+--------------------+------------------+----------------------+--------------------+---------------+--------------+-------+-----+|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path| _hoodie_file_name|impresssiontime| impressionid| userid| adid|+-------------------+--------------------+------------------+----------------------+--------------------+---------------+--------------+-------+-----+| 20200406002420|20200406002420_1_...| impression_106| user_83|fb381e12-f9ec-4fb...| 1586096500438|impression_106|user_83|ad_57|| 20200406002420|20200406002420_1_...| impression_107| user_83|fb381e12-f9ec-4fb...| 1586096464324|impression_107|user_83|ad_11|| 20200406002420|20200406002420_1_...| impression_111| user_83|fb381e12-f9ec-4fb...| 1586096366450|impression_111|user_83|ad_14|| 20200406002420|20200406002420_1_...| impression_111| user_83|fb381e12-f9ec-4fb...| 1586099019181|impression_111|user_83|ad_38|| 20200406002420|20200406002420_1_...| impression_116| user_83|fb381e12-f9ec-4fb...| 1586099146437|impression_116|user_83|ad_48|| 20200406002420|20200406002420_1_...| impression_121| user_83|fb381e12-f9ec-4fb...| 1586098316334|impression_121|user_83|ad_26|(snip) 実装確認 org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer クラスの実装を確認する。 まずmainは以下の通り。 org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java:298 1234567891011121314151617public static void main(String[] args) throws Exception &#123; final Config cfg = new Config(); JCommander cmd = new JCommander(cfg, null, args); if (cfg.help || args.length == 0) &#123; cmd.usage(); System.exit(1); &#125; Map&lt;String, String&gt; additionalSparkConfigs = SchedulerConfGenerator.getSparkSchedulingConfigs(cfg); JavaSparkContext jssc = UtilHelpers.buildSparkContext(\"delta-streamer-\" + cfg.targetTableName, cfg.sparkMaster, additionalSparkConfigs); try &#123; new HoodieDeltaStreamer(cfg, jssc).sync(); &#125; finally &#123; jssc.stop(); &#125;&#125; 上記の通り、 org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer#sync メソッドがエントリポイント。 org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java:116 123456789101112131415161718public void sync() throws Exception &#123; if (cfg.continuousMode) &#123; deltaSyncService.start(this::onDeltaSyncShutdown); deltaSyncService.waitForShutdown(); LOG.info(\"Delta Sync shutting down\"); &#125; else &#123; LOG.info(\"Delta Streamer running only single round\"); try &#123; deltaSyncService.getDeltaSync().syncOnce(); &#125; catch (Exception ex) &#123; LOG.error(\"Got error running delta sync once. Shutting down\", ex); throw ex; &#125; finally &#123; deltaSyncService.close(); LOG.info(\"Shut down delta streamer\"); &#125; &#125;&#125; 上記の通り、 continous モードかどうかで動作が変わる。 ここでは一旦、ワンショットの場合を確認する。 上記の通り、 org.apache.hudi.utilities.deltastreamer.DeltaSync#syncOnce メソッドがエントリポイント。 当該メソッドは以下のようにシンプルな内容。 org/apache/hudi/utilities/deltastreamer/DeltaSync.java:218 12345678910111213141516171819202122232425262728public Option&lt;String&gt; syncOnce() throws Exception &#123; Option&lt;String&gt; scheduledCompaction = Option.empty(); HoodieDeltaStreamerMetrics metrics = new HoodieDeltaStreamerMetrics(getHoodieClientConfig(schemaProvider)); Timer.Context overallTimerContext = metrics.getOverallTimerContext(); // Refresh Timeline refreshTimeline(); Pair&lt;SchemaProvider, Pair&lt;String, JavaRDD&lt;HoodieRecord&gt;&gt;&gt; srcRecordsWithCkpt = readFromSource(commitTimelineOpt); if (null != srcRecordsWithCkpt) &#123; // this is the first input batch. If schemaProvider not set, use it and register Avro Schema and start // compactor if (null == schemaProvider) &#123; // Set the schemaProvider if not user-provided this.schemaProvider = srcRecordsWithCkpt.getKey(); // Setup HoodieWriteClient and compaction now that we decided on schema setupWriteClient(); &#125; scheduledCompaction = writeToSink(srcRecordsWithCkpt.getRight().getRight(), srcRecordsWithCkpt.getRight().getLeft(), metrics, overallTimerContext); &#125; // Clear persistent RDDs jssc.getPersistentRDDs().values().forEach(JavaRDD::unpersist); return scheduledCompaction;&#125; 最初にメトリクスの準備、データソースから読み出してRDD化する定義（ org.apache.hudi.utilities.deltastreamer.DeltaSync#readFromSource メソッド） その後、 org.apache.hudi.utilities.deltastreamer.DeltaSync#writeToSink メソッドにより、定義されたRDDの内容を実際に書き込む。 ここでは上記メソッドを確認する。 まず与えられたRDDから重複排除する。 org/apache/hudi/utilities/deltastreamer/DeltaSync.java:352 1234567891011121314 private Option&lt;String&gt; writeToSink(JavaRDD&lt;HoodieRecord&gt; records, String checkpointStr, HoodieDeltaStreamerMetrics metrics, Timer.Context overallTimerContext) throws Exception &#123; Option&lt;String&gt; scheduledCompactionInstant = Option.empty(); // filter dupes if needed if (cfg.filterDupes) &#123; // turn upserts to insert cfg.operation = cfg.operation == Operation.UPSERT ? Operation.INSERT : cfg.operation; records = DataSourceUtils.dropDuplicates(jssc, records, writeClient.getConfig()); &#125; boolean isEmpty = records.isEmpty();(snip) その後実際の書き込みになるが、そのとき採用したオペレーション種類によって動作が異なる。 org/apache/hudi/utilities/deltastreamer/DeltaSync.java:369 123456789if (cfg.operation == Operation.INSERT) &#123; writeStatusRDD = writeClient.insert(records, instantTime);&#125; else if (cfg.operation == Operation.UPSERT) &#123; writeStatusRDD = writeClient.upsert(records, instantTime);&#125; else if (cfg.operation == Operation.BULK_INSERT) &#123; writeStatusRDD = writeClient.bulkInsert(records, instantTime);&#125; else &#123; throw new HoodieDeltaStreamerException(\"Unknown operation :\" + cfg.operation);&#125; bulkInsert ここではためしに org.apache.hudi.client.HoodieWriteClient#bulkInsert メソッドを確認してみる。 当該メソッドでは、最初にCOPY_ON_WRITEかMERGE_ON_READかに応じて、それぞれの種類のテーブル情報を取得する。 その後、 org.apache.hudi.client.HoodieWriteClient#bulkInsertInternal メソッドを使ってデータを書き込む。 なお、その間で重複排除されているが、上記の通り、もともと重複排除しているはずなので、要確認。（重複排除のロジックが異なるのかどうか、など） パット見た感じ、 org.apache.hudi.DataSourceUtils#dropDuplicates メソッドはロケーション情報（インデックス？）がない場合をドロップする。 org.apache.hudi.client.HoodieWriteClient#combineOnCondition メソッドはキーに基づきreduce処理する。 という違いがあるようだ。 org/apache/hudi/client/HoodieWriteClient.java:300 1234567891011121314151617public JavaRDD&lt;WriteStatus&gt; bulkInsert(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, final String instantTime, Option&lt;UserDefinedBulkInsertPartitioner&gt; bulkInsertPartitioner) &#123; HoodieTable&lt;T&gt; table = getTableAndInitCtx(WriteOperationType.BULK_INSERT); setOperationType(WriteOperationType.BULK_INSERT); try &#123; // De-dupe/merge if needed JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords = combineOnCondition(config.shouldCombineBeforeInsert(), records, config.getInsertShuffleParallelism()); return bulkInsertInternal(dedupedRecords, instantTime, table, bulkInsertPartitioner); &#125; catch (Throwable e) &#123; if (e instanceof HoodieInsertException) &#123; throw e; &#125; throw new HoodieInsertException(\"Failed to bulk insert for commit time \" + instantTime, e); &#125;&#125; 上記の通り、 org.apache.hudi.client.HoodieWriteClient#bulkInsertInternal メソッドが中で用いられている。 当該メソッドでは、再パーティションないしソートが行われた後、書き込みが実行される。 org/apache/hudi/client/HoodieWriteClient.java:412 123JavaRDD&lt;WriteStatus&gt; writeStatusRDD = repartitionedRecords .mapPartitionsWithIndex(new BulkInsertMapFunction&lt;T&gt;(instantTime, config, table, fileIDPrefixes), true) .flatMap(List::iterator); ポイントは、org.apache.hudi.execution.BulkInsertMapFunction クラスである。 このクラスが関数として渡されている。 org.apache.hudi.execution.BulkInsertMapFunction#call メソッドは以下の通り。 org/apache/hudi/execution/BulkInsertMapFunction.java:52 1234public Iterator&lt;List&lt;WriteStatus&gt;&gt; call(Integer partition, Iterator&lt;HoodieRecord&lt;T&gt;&gt; sortedRecordItr) &#123; return new CopyOnWriteLazyInsertIterable&lt;&gt;(sortedRecordItr, config, instantTime, hoodieTable, fileIDPrefixes.get(partition), hoodieTable.getSparkTaskContextSupplier());&#125; org.apache.hudi.execution.CopyOnWriteLazyInsertIterable クラスについては、別の節で書いたとおり。 insert org.apache.hudi.client.HoodieWriteClient#insert メソッド。 大まかな構造は、 bulkInsert と同様。 ポイントは、 org.apache.hudi.client.HoodieWriteClient#upsertRecordsInternal メソッド。 org/apache/hudi/client/HoodieWriteClient.java:229 12345678910111213141516public JavaRDD&lt;WriteStatus&gt; insert(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; records, final String instantTime) &#123; HoodieTable&lt;T&gt; table = getTableAndInitCtx(WriteOperationType.INSERT); setOperationType(WriteOperationType.INSERT); try &#123; // De-dupe/merge if needed JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; dedupedRecords = combineOnCondition(config.shouldCombineBeforeInsert(), records, config.getInsertShuffleParallelism()); return upsertRecordsInternal(dedupedRecords, instantTime, table, false); &#125; catch (Throwable e) &#123; if (e instanceof HoodieInsertException) &#123; throw e; &#125; throw new HoodieInsertException(\"Failed to insert for commit time \" + instantTime, e); &#125;&#125; 上記の通り、挿入対象のデータを表すRDDを引数に取り、データを書き込む。 これは、upsertのときと同じメソッドである。第4引数でinsertかupsertかを分ける。 当該メソッドは以下の通り。 bulkInsert と同様にリパーティションなどを経て、 org.apache.hudi.table.HoodieTable#handleUpsertPartition が呼び出される。 org/apache/hudi/client/HoodieWriteClient.java:457 1234567891011121314 private JavaRDD&lt;WriteStatus&gt; upsertRecordsInternal(JavaRDD&lt;HoodieRecord&lt;T&gt;&gt; preppedRecords, String instantTime, HoodieTable&lt;T&gt; hoodieTable, final boolean isUpsert) &#123;(snip) JavaRDD&lt;WriteStatus&gt; writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -&gt; &#123; if (isUpsert) &#123; return hoodieTable.handleUpsertPartition(instantTime, partition, recordItr, partitioner); &#125; else &#123; return hoodieTable.handleInsertPartition(instantTime, partition, recordItr, partitioner); &#125; &#125;, true).flatMap(List::iterator);(snip) org.apache.hudi.table.HoodieTable#handleUpsertPartition と org.apache.hudi.table.HoodieTable#handleInsertPartition が用いられている。 今回は、insertなので後者。 なお、 org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsertPartition は以下の通り、実態としては org.apache.hudi.table.HoodieCopyOnWriteTable#handleUpsertPartition である。 1234public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsertPartition(String instantTime, Integer partition, Iterator recordItr, Partitioner partitioner) &#123; return handleUpsertPartition(instantTime, partition, recordItr, partitioner);&#125; 当該メソッドは以下の通り。 insertやupsertでは、RDDひとつを1バケットと表現している。 バケットの情報から、insertやupdateの情報を取得して用いる。 12345678910111213141516171819public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleUpsertPartition(String instantTime, Integer partition, Iterator recordItr, Partitioner partitioner) &#123; UpsertPartitioner upsertPartitioner = (UpsertPartitioner) partitioner; BucketInfo binfo = upsertPartitioner.getBucketInfo(partition); BucketType btype = binfo.bucketType; try &#123; if (btype.equals(BucketType.INSERT)) &#123; return handleInsert(instantTime, binfo.fileIdPrefix, recordItr); &#125; else if (btype.equals(BucketType.UPDATE)) &#123; return handleUpdate(instantTime, binfo.partitionPath, binfo.fileIdPrefix, recordItr); &#125; else &#123; throw new HoodieUpsertException(\"Unknown bucketType \" + btype + \" for partition :\" + partition); &#125; &#125; catch (Throwable t) &#123; String msg = \"Error upserting bucketType \" + btype + \" for partition :\" + partition; LOG.error(msg, t); throw new HoodieUpsertException(msg, t); &#125;&#125; 例えば、insertの場合は、 org.apache.hudi.table.HoodieCopyOnWriteTable#handleInsert が呼び出される。 当該メソッドでは、戻り値として org.apache.hudi.execution.CopyOnWriteLazyInsertIterable#CopyOnWriteLazyInsertIterable が返される。 org/apache/hudi/table/HoodieCopyOnWriteTable.java:186 123456789public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) throws Exception &#123; // This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records if (!recordItr.hasNext()) &#123; LOG.info(\"Empty partition\"); return Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator(); &#125; return new CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, this, idPfx, sparkTaskContextSupplier);&#125; このメソッドについては上記ですでに説明したとおり。 HoodieCopyOnWriteTable と HoodieMergeOnReadTable テーブルの種類によって、書き込みの実装上どういう違いがあるかを確認する。 例えば、handleInsert メソッドを確認する。なお、当該メソッドには同期的、非同期的な処理方式がそれぞれ実装されている。 HoodieCopyOnWriteTableの場合は以下の通り。 org/apache/hudi/table/HoodieCopyOnWriteTable.java:186 1234567891011121314151617public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) throws Exception &#123; // This is needed since sometimes some buckets are never picked in getPartition() and end up with 0 records if (!recordItr.hasNext()) &#123; LOG.info(\"Empty partition\"); return Collections.singletonList((List&lt;WriteStatus&gt;) Collections.EMPTY_LIST).iterator(); &#125; return new CopyOnWriteLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, this, idPfx, sparkTaskContextSupplier);&#125;public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String partitionPath, String fileId, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) &#123; HoodieCreateHandle createHandle = new HoodieCreateHandle(config, instantTime, this, partitionPath, fileId, recordItr, sparkTaskContextSupplier); createHandle.write(); return Collections.singletonList(Collections.singletonList(createHandle.close())).iterator();&#125; 上が非同期的な方式、下が同期的な方式と見られる。 なお、実装上は同期的な処理方式は今は使われていないようにもみえるが、要確認。 HoodieMergeOnReadTableの場合は、非同期的な処理だけoverrideされている。 org/apache/hudi/table/HoodieMergeOnReadTable.java:120 123456789public Iterator&lt;List&lt;WriteStatus&gt;&gt; handleInsert(String instantTime, String idPfx, Iterator&lt;HoodieRecord&lt;T&gt;&gt; recordItr) throws Exception &#123; // If canIndexLogFiles, write inserts to log files else write inserts to parquet files if (index.canIndexLogFiles()) &#123; return new MergeOnReadLazyInsertIterable&lt;&gt;(recordItr, config, instantTime, this, idPfx, sparkTaskContextSupplier); &#125; else &#123; return super.handleInsert(instantTime, idPfx, recordItr); &#125;&#125;","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Hudi","slug":"Knowledge-Management/Storage-Layer/Hudi","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Hudi/"}],"tags":[{"name":"Storage Layer","slug":"Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/tags/Storage-Layer/"},{"name":"Apache Hudi","slug":"Apache-Hudi","permalink":"https://dobachi.github.io/memo-blog/tags/Apache-Hudi/"}]},{"title":"pandoc template and css","slug":"pandoc-template-and-css","date":"2020-03-06T13:46:59.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/03/06/pandoc-template-and-css/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/03/06/pandoc-template-and-css/","excerpt":"","text":"参考 メモ 参考 Pandocを使ってMarkdownを整形されたHTMLに変換する メモ Pandocのバージョンは、 2.9.2を使用。 Pandocを使ってMarkdownを整形されたHTMLに変換する を参考に、テンプレートを作成してCSSを用いた。 12$ mkdir -p ~/.pandoc/templates$ pandoc -D html5 &gt; ~/.pandoc/templates/mytemplate.html テンプレートを適当にいじる。 その後、HTMLを以下のように生成。 1$ pandoc --css ./pandoc-github.css --template=mytemplate -i ./README.md -o ./README.html なお、GitHub風になるCSSは、 dashed/github-pandoc.css に公開されていたものを利用。 --css はcssのURLを表すだけなので、上記の例ではREADME.htmlと同じディレクトリに pandoc-github.css があることを期待する。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Documentation","slug":"Knowledge-Management/Documentation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Documentation/"}],"tags":[{"name":"pandoc","slug":"pandoc","permalink":"https://dobachi.github.io/memo-blog/tags/pandoc/"}]},{"title":"Flow Engine for ML","slug":"Flow-Engine-for-ML","date":"2020-02-16T13:31:14.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2020/02/16/Flow-Engine-for-ML/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/02/16/Flow-Engine-for-ML/","excerpt":"","text":"参考 総合 Azkaban メモ よく名前の挙がるもの 機械的な検索 Airflowの代替 参考 総合 alternativetoでAirflowを検索した結果 Azkaban Azkabanのフロー書き方 メモ 機械学習で利用されるフロー管理ツールを軽くさらってみる。 よく名前の挙がるもの Apache Airflow DigDag Oozie 機械的な検索 Airflowの代替 alternativetoでAirflowを検索した結果 では以下の通り。 RunDeck OSSだが商用版もある。自動化ツール。ワークフローも管理できるようだ StackStorm どちらかというとIFTTTみたいなものか？ Zenaton ワークフローエンジン。JavaScriptで記述できるようだ Apache Oozie Hadoopエコシステムのワークフローエンジン Azkaban ワークフローエンジン Azkabanのフロー書き方 の通り、YAMLで書ける。 LinkedIn が主に開発 Metaflow ★ ワークフローエンジン 機械学習にフォーカス Netflix と AWS が主に開発 luigi ワークフローエンジン Pythonモジュール Spotify が主に開発","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Flow Engine","slug":"Knowledge-Management/Machine-Learning/Flow-Engine","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Flow-Engine/"}],"tags":[{"name":"Flow Engine","slug":"Flow-Engine","permalink":"https://dobachi.github.io/memo-blog/tags/Flow-Engine/"}]},{"title":"Kafka Streamsの始め方","slug":"Starting-Kafka-Streams","date":"2020-02-14T05:56:58.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/02/14/Starting-Kafka-Streams/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/02/14/Starting-Kafka-Streams/","excerpt":"","text":"参考 メモ はじめに読む文献 レファレンスとして使う文献 環境準備 プロジェクト作成 wordcount/Pipe.java wordcount/LineSplit.java wordcount/WordCount.java 参考 公式ドキュメント 公式チュートリアル Confluent Platformドキュメント Confluent CLI 公式API説明（groupBy） 公式API説明（count） 公式ドキュメント（Step 5: Process some data） 公式ドキュメント（Developer Guide） 公式ドキュメント（Kafka Streams DSL） 公式ドキュメント（Kafka Streams Processor API） 公式ドキュメント（Kafka Streams Test Utils） メモ まとまった情報が無いような気がするので、初心者向けのメモをここに書いておくことにする。 はじめに読む文献 公式チュートリアル 最初にこのあたりを読み、イメージをつかむのが良い 公式ドキュメント（Developer Guide） つづいて開発者ガイドを読むと良い レファレンスとして使う文献 公式ドキュメント（Kafka Streams DSL） チュートリアルを終えたあとくらいに使用し始めると良い 公式ドキュメント（Kafka Streams Processor API） Kafka Streams DSLでは対応しきれないときにProcessor APIを用いるときに使う 公式ドキュメント（Kafka Streams Test Utils） Kafka Streamsのテスト作るときに使用 環境準備 Apache Kafka、もしくはConfluent Platformで環境構築しておくことを前提とする。 Apache Kafkaであれば、 公式ドキュメント のインストール手順。 Confluent Platformであれば、 Confluent Platformドキュメントのインストール手順。 また、Confluent Platformを用いるときは、 Confluent CLI をインストールしておくと便利である。 1$ confluent local start だけでKafka関連のサービスを開発用にローカル環境に起動できる。 具体的には、以下のサービスを立ち上げられる。 1234567control-center is [UP]ksql-server is [UP]connect is [UP]kafka-rest is [UP]schema-registry is [UP]kafka is [UP]zookeeper is [UP] ちなみに、 org.apache.kafka.connect.cli.ConnectDistributed が意外とメモリを使用するので注意。 また、デフォルトでは /tmp 以下にワーキングディレクトリを作成する。 また実行時には /tmp/confluent.current を作成し、その時に使用しているワーキングディレクトリを識別できるようになっている。 tmpwatch等により、ワーキングディレクトリを乱してしまい、 confluent local start によりKafkaクラスタを起動できなくなったときは、 /tmp/confluent.current を削除してもう一度起動すると良い。 以降の説明では、Confluent Platformをインストールしたものとして説明する。 プロジェクト作成 公式チュートリアル が最初は参考になるはず。 MavenのArchetypeを使い、プロジェクトを生成する。 12345678$ mvn archetype:generate \\ -DarchetypeGroupId=org.apache.kafka \\ -DarchetypeArtifactId=streams-quickstart-java \\ -DarchetypeVersion=2.4.0 \\ -DgroupId=net.dobachi.kafka.streams.examples \\ -DartifactId=firstapp \\ -Dversion=0.1 \\ -Dpackage=wordcount 適宜パッケージ名などを変更して用いること。 雛形に基づいたプロジェクトには、簡単なアプリが含まれている。 最初はこれらを修正しながら、アプリの書き方に慣れるとよい。 wordcount/Pipe.java Kafka Streamsのアプリは通常のJavaアプリと同様に、1プロセスからスタンドアローンで起動する。 ここでは、Pipe.javaの内容を確認しよう。 以下、ポイントとなるソースコードとその説明を並べる。 wordcount/Pipe.java:36 12345Properties props = new Properties();props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"streams-pipe\");props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); メインの中では最初にストリームを作るための設定が定義される。 上記の例では、ストリーム処理アプリの名前、Kafkaクラスタのブートストラップサーバ（つまり、Broker）、 またキーやバリューのデフォルトのシリアライゼーションの仕組みを指定します。 今回はキー・バリューともにStringであることがわかります。 wordcount/Pipe.java:42 123final StreamsBuilder builder = new StreamsBuilder();builder.stream(\"streams-plaintext-input\").to(\"streams-pipe-output\"); つづいて、ストリームのビルダをインスタンス化。 このとき、入力・出力トピックを指定する。 wordcount/Pipe.java:46 123final Topology topology = builder.build();final KafkaStreams streams = new KafkaStreams(topology, props);final CountDownLatch latch = new CountDownLatch(1); ビルダでストリームをビルドし、トポロジを定義する。 wordcount/Pipe.java:46 12345678// attach shutdown handler to catch control-cRuntime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") &#123; @Override public void run() &#123; streams.close(); latch.countDown(); &#125;&#125;); シャットダウンフックを定義。 wordcount/Pipe.java:59 1234567try &#123; streams.start(); latch.await();&#125; catch (Throwable e) &#123; System.exit(1);&#125;System.exit(0); ストリーム処理を開始。 上記アプリを実行するには、事前に streams-plaintext-input streams-pipe-output の2種類のトピックを生成しておく。 12$ kafka-topics --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic streams-plaintext-input$ kafka-topics --create --zookeeper localhost:2181 --partitions 1 --replication-factor 1 --topic streams-pipe-output トピックが作られたかどうかは、以下のように確認する。 1$ kafka-topics --list --zookeeper localhost:2181 なお、ユーザが明示的に作るトピックの他にも、Kafkaの動作等のために作られるトピックがあるので、 上記コマンドを実行するとずらーっと出力されるはず。 コンパイル、パッケージングする。 1$ mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies 入力ファイルを作成し、入ロトピックに書き込み。 12$ echo -e \"all streams lead to kafka\\nhello kafka streams\\njoin kafka summit\" &gt; /tmp/file-input.txt$ cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input アプリを実行する。 1$ java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.Pipe 別のターミナルを改めて開き、コンソール上に出力トピックの内容を出力する。 1$ kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning --property print.key=true --topic streams-pipe-output 以下のような結果が見られるはずである。なお、今回はキーを使用しないアプリだから、左側（キーを表示する場所）には null が並ぶ。 123null all streams lead to kafkanull hello kafka streamsnull join kafka summit さて、ここでキーを使うようにしてみる。 今回使用したアプリをコピーし、 wordcount/PipeWithKey.java を作る。 ここで変更点は以下の通り。 12345678910111213141516171819202122232425262728293031323334--- src/main/java/wordcount/Pipe.java 2020-02-14 15:23:23.808282200 +0900+++ src/main/java/wordcount/PipeWithKey.java 2020-02-14 16:54:17.623090500 +0900@@ -17,10 +17,8 @@ package wordcount; import org.apache.kafka.common.serialization.Serdes;-import org.apache.kafka.streams.KafkaStreams;-import org.apache.kafka.streams.StreamsBuilder;-import org.apache.kafka.streams.StreamsConfig;-import org.apache.kafka.streams.Topology;+import org.apache.kafka.streams.*;+import org.apache.kafka.streams.kstream.KStream; import java.util.Properties; import java.util.concurrent.CountDownLatch;@@ -30,7 +28,7 @@ * that reads from a source topic &quot;streams-plaintext-input&quot;, where the values of messages represent lines of text, * and writes the messages as-is into a sink topic &quot;streams-pipe-output&quot;. */-public class Pipe &#123;+public class PipeWithKey &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties();@@ -41,7 +39,8 @@ final StreamsBuilder builder = new StreamsBuilder();- builder.stream(&quot;streams-plaintext-input&quot;).to(&quot;streams-pipe-output&quot;);+ KStream&lt;String, String&gt; raw = builder.stream(&quot;streams-plaintext-input&quot;);+ raw.map((key, value ) -&gt; new KeyValue&lt;&gt;(value.split(&quot; &quot;)[0], value)).to(&quot;streams-pipe-output&quot;); final Topology topology = builder.build(); final KafkaStreams streams = new KafkaStreams(topology, props); 主な変更は、ストリームビルダから定義されたストリームをいったん、 raw にバインドし、 mapメソッドを使って変換している箇所である。 ここでは、バリューをスペースで区切り、先頭の単語をキーとすることにした。 このアプリをコンパイル、パッケージ化し実行すると、以下のような結果が得られる。 123$ mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies$ cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input$ java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.PipeWithKey 実行結果の例 123all all streams lead to kafkahello hello kafka streamsjoin join kafka summit wordcount/LineSplit.java 先程作成したPipeWithKeyとほぼ同じ。 実行すると、 streams-linesplit-output というトピックに結果が出力される。 1$ java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.LineSplit 結果の例 1234567$ kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning --property print.key=true --topic streams-linesplit-outputnull allnull streamsnull leadnull tonull kafka(snip) wordcount/WordCount.java 最後にWordCountを確認する。 ほぼ他のアプリと同じだが、ポイントはストリームを加工する定義の部分である。 wordcount/WordCount.java:53 123456builder.&lt;String, String&gt;stream(\"streams-plaintext-input\") .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(\"\\\\W+\"))) .groupBy((key, value) -&gt; value) .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(\"counts-store\")) .toStream() .to(\"streams-wordcount-output\", Produced.with(Serdes.String(), Serdes.Long())); 以下、上記実装を説明する。 1builder.&lt;String, String&gt;stream(\"streams-plaintext-input\") ストリームビルダを利用し、入力トピックからストリームを定義 1.flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(\"\\\\W+\"))) バリューに入っている文字列をスペース等で分割し、配列にする。 合わせて配列をflattenする。 1.groupBy((key, value) -&gt; value) キーバリューから新しいキーを生成し、新しいキーに基づいてグループ化する。 今回の例では、分割されて生成された単語（バリューに入っている）をキーとしてグループ化する。 詳しくは、 公式API説明（groupBy） 1.count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(\"counts-store\")) groupByにより生成された KGroupedStream の count メソッドを呼び出し、 キーごとの合計値を求める。 今回はキーはString型であり、合計値はLong型。 また集計結果を保持するストアは counts-store という名前とする。 詳しくは、 公式API説明（count） 12.toStream().to(\"streams-wordcount-output\", Produced.with(Serdes.String(), Serdes.Long())); count の結果は KTable になるので、これをストリームに変換し、出力先トピックを指定する。 実行してみる。 12$ mvn clean assembly:assembly -DdescriptorId=jar-with-dependencies$ java -cp target/firstapp-0.1-jar-with-dependencies.jar wordcount.WordCount 別のターミナルを改めて立ち上げ、入力トピックに書き込む。 1$ cat /tmp/file-input.txt | kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input 出力は以下のようになる。 123456789$ kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning --property print.key=true --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --topic streams-wordcount-outputall 19lead 19to 19hello 19streams 38join 19kafka 57summit 19 なお、ここでは kafka-console-consumer にプロパティ value.deserializer=org.apache.kafka.common.serialization.LongDeserializer を渡した。 アプリケーションでは集計した値はLong型だったためである。 詳しくは、 公式ドキュメント（Step 5: Process some data） 参照。 なお、指定しない場合は入力されたバイト列がそのまま標準出力に渡されるようになっている。 その結果、期待する出力が得られないことになるので注意。 kafka/tools/ConsoleConsumer.scala:512 123456def write(deserializer: Option[Deserializer[_]], sourceBytes: Array[Byte], topic: String): Unit = &#123; val nonNullBytes = Option(sourceBytes).getOrElse(\"null\".getBytes(StandardCharsets.UTF_8)) val convertedBytes = deserializer.map(_.deserialize(topic, nonNullBytes).toString. getBytes(StandardCharsets.UTF_8)).getOrElse(nonNullBytes) output.write(convertedBytes)&#125; なお、別の方法として WordCount の実装を修正する方法がある。以下、参考までに修正方法を紹介する。 想定と異なる表示だが、これは今回バリューの方にLongを用いたため。 kafka-console-consumer で表示させるために以下のように実装を修正する。 1234567891011121314151617@@ -50,12 +44,15 @@ public class WordCount &#123; final StreamsBuilder builder = new StreamsBuilder();- builder.&lt;String, String&gt;stream(\"streams-plaintext-input\")+ KStream&lt;String, Long&gt; wordCount = builder.&lt;String, String&gt;stream(\"streams-plaintext-input\") .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase(Locale.getDefault()).split(\"\\\\W+\"))) .groupBy((key, value) -&gt; value)- .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(\"counts-store\"))- .toStream()- .to(\"streams-wordcount-output\", Produced.with(Serdes.String(), Serdes.Long()));+ .count(Materialized.as(\"counts-store\"))+ .toStream();++ wordCount.foreach((key, value) -&gt; System.out.println(\"key: \" + key + \", value: \" + value));++ wordCount.map((key, value) -&gt; new KeyValue&lt;&gt;(key, String.valueOf(value))).to(\"streams-wordcount-output\", Produced.with(Serdes.String(), Serdes.String())); つまり、もともと to で終えていたところを、いったん変数にバインドし、 foreach を使ってストリームの内容を標準出力に表示させるようにしている。 また、 map メソッドを利用し、バリューの型をLongからStringに変換してから to で書き出すようにしている。 上記修正を加えた上で、改めてパッケージ化して実行したところ、以下のような表示が得られる。 kafka-console-consumer での表示例 12345678all 9lead 9to 9hello 9streams 18join 9kafka 27summit 9 ストリーム処理アプリの標準出力例 12345678key: all, value: 9key: lead, value: 9key: to, value: 9key: hello, value: 9key: streams, value: 18key: join, value: 9key: kafka, value: 27key: summit, value: 9 無事に表示できたことが確かめられただろうか。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"Kafka Streams","slug":"Kafka-Streams","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka-Streams/"}]},{"title":"CDC Kafka and master table cache","slug":"CDC-Kafka-and-master-table-cache","date":"2020-01-25T14:32:13.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/01/25/CDC-Kafka-and-master-table-cache/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/25/CDC-Kafka-and-master-table-cache/","excerpt":"","text":"参考 メモ やりたいこと RDBMSからのデータ取り込み JDBC Kafka Connector 更新時間とユニークIDを利用したキャプチャ Kafka Connect Kafka Stramsでテーブルに変換 GlobalKTableへの読み込み 過去の古いメモを少し細くしてアップロード。 参考 KafkaConnectを試す その2 No More Silos: How to Integrate Your Databases with Apache Kafka and CDC JDBC Connector (Source and Sink) for Confluent Platform JDBC Connector Prerequisites JDBC Connector Incremental Query Modes JDBC Connector Message Keys Confluent PlatformのUbuntuへのインストール手順 Confluent CLIのインストール手順 PostgreSQLで更新時のtimestampをアップデートするには PostgreSQL で連番の数字のフィールドを作る方法 (sequence について) postgres - シーケンス inser時に自動採番 Kafka Streams例 GlobalKTablesExample.java メモ やりたいこと マスタデータを格納しているRDBMSからKafkaにデータを取り込み、 KTableとしてキャッシュしたうえで、ストリームデータとJoinする。 RDBMSからのデータ取り込み KafkaConnectを試す その2 や No More Silos: How to Integrate Your Databases with Apache Kafka and CDC を 参考に、Kafka Connectのjdbcコネクタを使用してみようと思った。 しかしこの例で載っているのは、差分取り込みのためにシーケンシャルなIDを持つカラムが必要であることが要件を満たさないかも、と思った。 やりたいのは、Updateも含むChange Data Capture。 test_db.json 内の該当箇所は以下の通り。 1234(snip) \"mode\" : \"incrementing\", \"incrementing.column.name\" : \"seq\",(snip) ということで、 JDBC Connector (Source and Sink) for Confluent Platform を確認してみた。 結論としては、更新時間とユニークIDを使うモードを利用すると良さそうだ。 JDBC Kafka Connector JDBC Connector Prerequisites によると、 Kafka と Schema Registryがあればよさそうだ。 JDBC Connector Incremental Query Modes によると、モードは以下の通り。 Incrementing Column ユニークで必ず値が大きくなるカラムを持つことを前提としたモード 更新はキャプチャできない。そのためDWHのfactテーブルなどをキャプチャすることを想定している。 Timestamp Column 更新時間のカラムを持つことを前提としたモード 更新時間はユニークではないことを起因とした注意点がある。 同一時刻に2個のレコードが更新された場合、もし1個目のレコードが処理されたあとに障害が生じたとしたら、 復旧時にもう1個のレコードが処理されないことが起こりうる。 疑問点：処理後に「処理済みTimestamp」を更新できないのだろうか Timestamp and Incrementing Columns 更新時刻カラム、ユニークIDカラムの両方を前提としたモード 先のTimestamp Columnモードで問題となった、同一時刻に複数レコードが生成された場合における、 部分的なキャプチャ失敗を防ぐことができる。 確認点：先に更新時刻を見て、ユニークIDで確認するのだろうか。だとしたら、更新もキャプチャできそう。 Custom Query クエリを用いてフィルタされた結果をキャプチャするモード Incrementing Column、Timestamp Columnなどと比べ、オフセットをトラックしないので、 クエリ自身がオフセットをトラックするのと同等の処理内容になっていないと行けない。 Bulk テーブルをまるごとキャプチャするモード レコードが消える場合になどに対応 補足：他のモードで、レコードの消去に対応するには、実際に消去するのではなく、消去フラグを立てる、などの工夫が必要そう 当然だが、必要に応じて元のRDBMS側でインデックスが貼られていないとならない。 timestamp.delay.interval.ms 設定を使い、更新時刻に対し、実際に取り込むタイミングを遅延させられる。 これはトランザクションを伴うときに、一連のレコードが更新されるのを待つための機能。 なお、 JDBC Connector Message Keys によると、レコードの値や特定のカラムから、Kafkaメッセージのキーを生成できる。 更新時間とユニークIDを利用したキャプチャ No More Silos: How to Integrate Your Databases with Apache Kafka and CDC のあたりを参考に、 別のモードを使って試してみる。 まずはKafka環境を構築するが、ここでは簡単化のためにConfluent Platformを用いることとした。 Confluent PlatformのUbuntuへのインストール手順 あたりを参考にすすめると良い。 また、 confluent コマンドがあると楽なので、 Confluent CLIのインストール手順 を参考にインストールしておく。 インストールしたら、シングルモードでKafka環境を起動しておく。 1$ confluent local start KafkaConnectを試す その2 あたりを参考に、Kafkaと同一マシンにPostgreSQLの環境を構築しておく。 12345$ sudo apt install -y postgresql$ sudo vim /etc/postgresql/10/main/postgresql.conf$ sudo cp /usr/share/postgresql/10/pg_hba.conf&#123;.sample,&#125;$ sudo vim /usr/share/postgresql/10/pg_hba.conf$ sudo systemctl restart postgresql /etc/postgresql/10/main/postgresql.conf に追加する内容は以下の通り。 1listen_addresses = '*' /usr/share/postgresql/10/pg_hba.conf に追加する内容は以下の通り。 1234# PostgreSQL Client Authentication Configuration File# ===================================================local all all trusthost all all 127.0.0.1/32 trust Kakfa用のデータベースとテーブルを作る。 1$ psql -c \"alter user postgres with password 'kafkatest'\" 12$ sudo -u postgres psql -U postgres -W -c \"CREATE DATABASE testdb\";Password for user postgres: テーブルを作る際、Timestampとインクリメンタルな値を使ったデータキャプチャを実現するためのカラムを含むようにする。 PostgreSQLで更新時のtimestampをアップデートするには 、 PostgreSQL で連番の数字のフィールドを作る方法 (sequence について) 、 postgres - シーケンス inser時に自動採番 あたりを参考とする。 以下、テーブルを作り、ユニークID用のシーケンスを作り、タイムスタンプを作る流れ。 タイムスタンプはレコード更新時に合わせて更新されるようにトリガを設定する。 1$ sudo -u postgres psql -U postgres testdb 1234567891011121314151617181920212223CREATE TABLE test_table ( seq SERIAL PRIMARY KEY, ts timestamp NOT NULL DEFAULT now(), item varchar(256), price integer, category varchar(256));CREATE FUNCTION set_update_time() RETURNS OPAQUE AS ' begin new.ts := ''now''; return new; end;' LANGUAGE 'plpgsql';CREATE TRIGGER update_tri BEFORE UPDATE ON test_table FOR EACH ROW EXECUTE PROCEDURE set_update_time();CREATE USER connectuser WITH password 'connectuser';GRANT ALL ON test_table TO connectuser;INSERT INTO test_table(item, price, category) VALUES ('apple', 400, 'fruit');INSERT INTO test_table(item, price, category) VALUES ('banana', 160, 'fruit');UPDATE test_table SET item='orange', price=100 where seq = 2;INSERT INTO test_table(item, price, category) VALUES ('banana', 200, 'fruit');INSERT INTO test_table(item, price, category) VALUES ('pork', 400, 'meet');INSERT INTO test_table(item, price, category) VALUES ('beef', 800, 'meet'); 以下のような結果が得られるはずである。 123456789testdb=# SELECT * FROM test_table; seq | ts | item | price | category-----+----------------------------+--------+-------+---------- 1 | 2020-02-02 13:31:12.065458 | apple | 400 | fruit 2 | 2020-02-02 13:31:49.220178 | orange | 100 | fruit 3 | 2020-02-02 13:32:32.324241 | banana | 200 | fruit 4 | 2020-02-02 13:33:06.560747 | pork | 400 | meet 5 | 2020-02-02 13:33:06.561966 | beef | 800 | meet(5 rows) Kafka Connect JDBC Connector Incremental Query Modes を参考に、タイムスタンプ＋インクリメンティングモードを用いる。 1234567891011121314151617181920cat &lt;&lt; EOF &gt; test_db.json&#123; &quot;name&quot;: &quot;load-test-table&quot;, &quot;config&quot;: &#123; &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSourceConnector&quot;, &quot;connection.url&quot; : &quot;jdbc:postgresql://localhost:5432/testdb&quot;, &quot;connection.user&quot; : &quot;connectuser&quot;, &quot;connection.password&quot; : &quot;connectuser&quot;, &quot;mode&quot; : &quot;timestamp+incrementing&quot;, &quot;incrementing.column.name&quot; : &quot;seq&quot;, &quot;timestamp.column.name&quot; : &quot;ts&quot;, &quot;table.whitelist&quot; : &quot;test_table&quot;, &quot;topic.prefix&quot; : &quot;db_&quot;, &quot;tasks.max&quot; : &quot;1&quot; &#125;&#125;EOF$ curl -X DELETE http://localhost:8083/connectors/load-test-table$ curl -X POST -H &quot;Content-Type: application/json&quot; http://localhost:8083/connectors -d @test_db.json$ curl http://localhost:8083/connectors 上記コネクタでは、KafkaにAvro形式で書き込むので、 kafka-avro-console-consumerで確認する。 1$ kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic db_test_table --from-beginning 上記を起動した後、PostgreSQL側で適当にレコードを挿入・更新すると、 以下のような内容がコンソールコンシューマの出力に表示される。 変化がキャプチャされて取り込まれることがわかる。 挿入だけではなく、更新したものも取り込まれる。メッセージには、シーケンスとタイムスタンプの療法が含まれている。 123456&#123;&quot;seq&quot;:1,&quot;ts&quot;:1580650272065,&quot;item&quot;:&#123;&quot;string&quot;:&quot;apple&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:400&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;&#123;&quot;seq&quot;:2,&quot;ts&quot;:1580650296666,&quot;item&quot;:&#123;&quot;string&quot;:&quot;banana&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:160&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;&#123;&quot;seq&quot;:2,&quot;ts&quot;:1580650309220,&quot;item&quot;:&#123;&quot;string&quot;:&quot;orange&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:100&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;&#123;&quot;seq&quot;:3,&quot;ts&quot;:1580650352324,&quot;item&quot;:&#123;&quot;string&quot;:&quot;banana&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:200&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;fruit&quot;&#125;&#125;&#123;&quot;seq&quot;:4,&quot;ts&quot;:1580650386560,&quot;item&quot;:&#123;&quot;string&quot;:&quot;pork&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:400&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;meet&quot;&#125;&#125;&#123;&quot;seq&quot;:5,&quot;ts&quot;:1580650386561,&quot;item&quot;:&#123;&quot;string&quot;:&quot;beef&quot;&#125;,&quot;price&quot;:&#123;&quot;int&quot;:800&#125;,&quot;category&quot;:&#123;&quot;string&quot;:&quot;meet&quot;&#125;&#125; Kafka Stramsでテーブルに変換 上記の通り、RDBMSからデータを取り込んだものに対し、 マスタテーブルとして使うため、KTableに変換してみる。 GlobalKTableへの読み込み Kafka Streams例 あたりを参考にする。 特に、 GlobalKTablesExample.java あたりが参考になるかと思う。 今回は、上記レポジトリをベースに少しいじって、本例向けのサンプルアプリを作る。 [WIP]","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"RDBMS","slug":"RDBMS","permalink":"https://dobachi.github.io/memo-blog/tags/RDBMS/"},{"name":"CDC","slug":"CDC","permalink":"https://dobachi.github.io/memo-blog/tags/CDC/"},{"name":"Kafka Connect","slug":"Kafka-Connect","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka-Connect/"}]},{"title":"ThirdEye LinkedIn’s business-wide monitoring platform","slug":"ThirdEye-LinkedIns-business-wide-monitoring-platform","date":"2020-01-15T15:41:28.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/01/16/ThirdEye-LinkedIns-business-wide-monitoring-platform/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/16/ThirdEye-LinkedIns-business-wide-monitoring-platform/","excerpt":"","text":"参考 メモ セッションメモ ブログメモ 参考 ThirdEye LinkedIn’s business-wide monitoring platform thirdeyeの実装 メモ セッションメモ Strataの ThirdEye LinkedIn’s business-wide monitoring platform のメモ。 LinkedInで運用されている異常検知と原因分析のためのプラットフォーム。 オープンソースになっているようだ。 -&gt; thirdeyeの実装 主に以下の内容が記載されていた。 MTTD: Mean time to detect MTTR: Mean time to repair 50 チーム以上がThirdEyeを利用。 何千もの時系列データが監視されている？ 攻撃を検知したり、AIモデル周りの監視。 アーキ図あり。 異常検知における課題：スケーラビリティ、性能 手動でのコンフィグ、監視は現実的でない。 ルールベースの単純な仕組みは不十分。 多すぎるアラートは邪魔。 ブログメモ Analyzing anomalies with ThirdEye のメモ。 データキューブについて。 LinkedInではPinotを使って事前にキューブ化されている。 ディメンジョン・ヒートマップの利用。 あるディメンジョンにおける問題の原因分析に有用。 変化の検出の仕方について。 ただし単独のディメンジョンの問題を検出するだけでは不十分。 複数のディメンジョンにまたがって分析してわかる問題を検出したい。 ディメンジョンをツリー構成。 ベースラインと現在の値でツリーを構成。 ノードの親子関係を利用。 各ノードとその親ノードの変化を式に組み入れることで、木構造に基づく傾向（？）を考慮しながら、 変化の重大さを算出する。 上記仕組みを利用することで、データマネージャのバグ、機械学習モデルサービングにおけるバグを見つけた。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Monitering","slug":"Knowledge-Management/Monitering","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Monitering/"}],"tags":[{"name":"LinkedIn","slug":"LinkedIn","permalink":"https://dobachi.github.io/memo-blog/tags/LinkedIn/"},{"name":"Monitering","slug":"Monitering","permalink":"https://dobachi.github.io/memo-blog/tags/Monitering/"}]},{"title":"Automating ML model training and deployments via metadata-driven data, infrastructure, feature engineering, and model management","slug":"Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management","date":"2020-01-15T15:05:53.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2020/01/16/Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/16/Automating-ML-model-training-and-deployments-via-metadata-driven-data-infrastructure-feature-engineering-and-model-management/","excerpt":"","text":"参考 メモ 参考 Automating ML model training and deployments via metadata-driven data, infrastructure, feature engineering, and model management メモ 以下の内容が記載されている。 ワークフロー。 よくあるワークフローなので特筆なし。 無数のストリームイベント。 数億のレコードアップデート。 数千万の顧客アカウント。 メタデータ管理、特徴量ストア、モデルサービング、パイプラインオートメーション。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Comcast","slug":"Comcast","permalink":"https://dobachi.github.io/memo-blog/tags/Comcast/"}]},{"title":"Questioning the Lambda Architecture","slug":"Questioning-the-Lambda-Architecture","date":"2020-01-13T12:50:11.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2020/01/13/Questioning-the-Lambda-Architecture/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/13/Questioning-the-Lambda-Architecture/","excerpt":"","text":"参考 メモ まとめ 気になった文言を引用 所感 参考 Questioning the Lambda Architecture メモ Questioning the Lambda Architecture にてJay Krepsが初めて言及したとされているようだ。 過去に読んだが忘れたので改めて、いかにメモを記載する。 まとめ 主張としては、ストリーム処理はKafkaの登場により、十分に使用に耐えうるものになったため、 バッチ処理と両方使うのではなく、ストリーム処理1本で勝負できるのでは？ということだった。 気になった文言を引用 ラムダアーキテクチャについて： The Lambda Architecture is an approach to building stream processing applications on top of MapReduce and Storm or similar systems. Lambdaアーキテクチャのイメージ バッチ処理とストリーム処理で2回ロジックを書く。： You implement your transformation logic twice, once in the batch system and once in the stream processing system. レコメンデーションシステムを例にとる： A good example would be a news recommendation system that needs to crawl various news sources, process and normalize all the input, and then index, rank, and store it for serving. データを取り込み、イミュターブルなものとして扱うことはありだと思う： I’ve written some of my thoughts about capturing and transforming immutable data streams I have found that many people who attempt to build real-time data processing systems don’t put much thought into this problem and end-up with a system that simply cannot evolve quickly because it has no convenient way to handle reprocessing. リアルタイム処理が本質的に近似であり、バッチ処理よりも弱く、損失しがち、という意見があるよね、と。： One is that real-time processing is inherently approximate, less powerful, and more lossy than batch processing. ラムダアーキテクチャの利点にCAP定理との比較が持ち出されることを引き合いに出し、ラムダアーキテクチャがCAP定例を克服するようなものではない旨を説明。： Long story short, although there are definitely latency/availability trade-offs in stream processing, this is an architecture for asynchronous processing, so the results being computed are not kept immediately consistent with the incoming data. The CAP theorem, sadly, remains intact. 結局、StormとHadoopの両方で同じ結果を生み出すアプリケーションを 実装するのがしんどいという話。： Programming in distributed frameworks like Storm and Hadoop is complex. ひとつの解法は抽象化。： Summingbird とはいえ、2重運用はしんどい。デバッグなど。： the operational burden of running and debugging two systems is going to be very high. 結局のところ、両方を同時に使わないでくれ、という結論： These days, my advice is to use a batch processing framework like MapReduce if you aren’t latency sensitive, and use a stream processing framework if you are, but not to try to do both at the same time unless you absolutely must. ストリーム処理はヒストリカルデータの高スループットでの処理に向かない、という話もあるが…： When I’ve discussed this with people, they sometimes tell me that stream processing feels inappropriate for high-throughput processing of historical data. バッチ処理もストリーム処理も抽象化の仕方は、DAGをベースにしたものであり、 その点では共通である、と。： But there is no reason this should be true. The fundamental abstraction in stream processing is data flow DAGs, which are exactly the same underlying abstraction in a traditional data warehouse (a la Volcano) as well as being the fundamental abstraction in the MapReduce successor Tez. ということでKafka。： Use Kafka 提案アーキテクチャ Kafkaに入れた後は、HDFS等に簡単に保存できる。： Kafka has good integration with Hadoop, so mirroring any Kafka topic into HDFS is easy. このあと少し、Kafkaの説明が続く。 この時点では、Event Sourcing、CQRSについての言及あり。 Indeed, a lot of people are familiar with similar patterns that go by the name Event Sourcing or CQRS. LinkedInにて、JayはSamzaを利用。 I know this approach works well using Samza as the stream processing system because we do it at LinkedIn. 提案手法の難点として、一時的に2倍の出力ストレージサイズが必要になる。 However, my proposal requires temporarily having 2x the storage space in the output database and requires a database that supports high-volume writes for the re-load. 単純さを大事にする。： So, in cases where simplicity is important, consider this approach as an alternative to the Lambda Architecture. 所感 当時よりも、最近のワークロードは複雑なものも含めて期待されるようになっており、 ますます「バッチ処理とストリーム処理で同じ処理を実装する」というのがしんどくなっている印象。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"},{"name":"Kappa Architecture","slug":"Knowledge-Management/Stream-Processing/Kappa-Architecture","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/Kappa-Architecture/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Kappa Architecture","slug":"Kappa-Architecture","permalink":"https://dobachi.github.io/memo-blog/tags/Kappa-Architecture/"},{"name":"Lambda Architecture","slug":"Lambda-Architecture","permalink":"https://dobachi.github.io/memo-blog/tags/Lambda-Architecture/"}]},{"title":"ML Ops: Machine Learning as an Engineering Discipline","slug":"ML-Ops-Machine-Learning-as-an-Engineering-Discipline","date":"2020-01-11T13:26:40.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2020/01/11/ML-Ops-Machine-Learning-as-an-Engineering-Discipline/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/11/ML-Ops-Machine-Learning-as-an-Engineering-Discipline/","excerpt":"","text":"参考 メモ 感想 気になる文言の抜粋 参考 ML Ops Machine Learning as an Engineering Discipline メモ 読んでみた感想をまとめる。 感想 結論としては、まとめ表がよくまとまっているのでそれでよい気がする。 この表をベースに、アクティビティから必要なものを追加するか？ -&gt; まとめ表 演繹的ではなく、帰納的な手法であるため、もとになったコードとデータの両方が重要。 データサイエンティスト、MLエンジニア、DevOpsエンジニア、データエンジニア。 MLエンジニア。あえてエンジニアと称しているのは、ソフトウェア開発のスキルを有していることを期待するから。 フェアネスの考慮、というか機械学習の倫理考慮をどうやって機械的に実現するのか、というのはかねてより気になっていた。 フェアネスの考慮などを達成するためには、テストデータセットの作り方、メトリクスの作り方に工夫するとよい。つまり、男女でそれぞれ個別にもテストするなど。 まだ一面ではあるが、参考になった。 気になる文言の抜粋 以下の文言が印象に残った。 ほかのレポートでも言われていることに見える。 Deeplearning.ai reports that “only 22 percent of companies using machine learning have successfully deployed a model”. このレポートがどれか気になった。 Deeplearning.aiのレポート か。 確かに以下のように記載されている。 Although AI budgets are on the rise, only 22 percent of companies using machine learning have successfully deployed a model, the study found. データが大切論： ML is not just code, it’s code plus data training data, which will affect the behavior of the model in production ML = Code + Dataの図 It never stops changing, and you can’t control how it will change. 確かに、データはコントロールできない。 Data Engineering ML Opsの概念図 チーム構成について言及あり。： But the most likely scenario right now is that a successful team would include a Data Scientist or ML Engineer, a DevOps Engineer and a Data Engineer. データサイエンティストのほかに、明確にMLエンジニア、DevOpsエンジニア、データエンジニアを入れている。 基盤エンジニアはデータエンジニアに含まれるのだろうか。 Even if an organization includes all necessary skills, it won’t be successful if they don’t work closely together. ノートブックに殴り書かれたコードは不十分の話： getting a model to work great in a messy notebook is not enough. ML Engineers データパイプラインの話： data pipeline 殴り書きのコードではなく、適切なパイプラインはメリットいくつかあるよね、と。： Switching to proper data pipelines provides many advantages in code reuse, run time visibility, management and scalability. トレーニングとサービングの両方でパイプラインがあるけど、 入力データや変換内容は、場合によっては微妙に異なる可能性がある、と。： Most models will need 2 versions of the pipeline: one for training and one for serving. ML Pipelineは特定のデータから独立しているためCICDと連携可能 For example, the training pipeline usually runs over batch files that contain all features, while the serving pipeline often runs online and receives only part of the features in the requests, retrieving the rest from a database. いくつかTensorFlow関係のツールが紹介されている。確認したほうがよさそう。： TensorFlow Pipeline TensorFlow Transform バージョン管理について： In ML, we also need to track model versions, along with the data used to train it, and some meta-information like training hyperparameters. Models and metadata can be tracked in a standard version control system like Git, but data is often too large and mutable for that to be efficient and practical. コードのラインサイクルと、モデルのライフサイクルは異なる： It’s also important to avoid tying the model lifecycle to the code lifecycle, since model training often happens on a different schedule. It’s also necessary to version data and tie each trained model to the exact versions of code, data and hyperparameters that were used. Having comprehensive automated tests can give great confidence to a team, accelerating the pace of production deployments dramatically. モデルの検証は本質的に、統計に基づくものになる。というのも、そもそもモデルの出力が確率的だし、入力データも変動する。： model validation tests need to be necessarily statistical in nature Just as good unit tests must test several cases, model validation needs to be done individually for relevant segments of the data, known as slices. Data validation is analogous to unit testing in the code domain. ML pipelines should also validate higher level statistical properties of the input. TensorFlow Data Validation Therefore, in addition to monitoring standard metrics like latency, traffic, errors and saturation, we also need to monitor model prediction performance. An obvious challenge with monitoring model performance is that we usually don’t have a verified label to compare our model’s predictions to, since the model works on new data. まとめ表","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"OpML","slug":"Knowledge-Management/Machine-Learning/OpML","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/OpML/"}],"tags":[{"name":"ML Ops","slug":"ML-Ops","permalink":"https://dobachi.github.io/memo-blog/tags/ML-Ops/"}]},{"title":"jvm profiler for Spark at Uber","slug":"jvm-profiler-for-Spark-at-Uber","date":"2020-01-06T15:20:37.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/01/07/jvm-profiler-for-Spark-at-Uber/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/07/jvm-profiler-for-Spark-at-Uber/","excerpt":"","text":"参考 メモ 概要 動作確認 参考 公式GitHub Strata NY 2019のセッション ブログ メモ 概要 Sparkのエグゼキュータ等のJVMプロファイリングを行うためのライブラリ。 JVM起動時に、agentとしてアタッチするようにする。 executorプロセスのUUIDを付与しながらメトリクスを取得できるようだ。 Kafkaに流すことも可能。 公式GitHub のREADMEによると、オフヒープの使用量なども計測できており、チューニングに役立ちそう。 20200107時点では、2か月前ほどに更新されており、まだ生きているプロジェクトのようだ。 動作確認 公式GitHub のREADMEに記載されていた手順で、パッケージをビルドし、ローカルモードのSparkで利用してみた。 ビルド方法： 1$ mvn clean package パッケージビルド結果は、 target/jvm-profiler-1.0.0.jar に保存される。 これをjavaagentとして用いる。 渡すオプションは、 --conf spark.executor.driverJavaOptions=-javaagent:${JVMPROFILER_HOME}/target/jvm-profiler-1.0.0.jar である。 環境変数 ${JVMPROFILER_HOME} は先ほどビルドしたレポジトリのPATHとする。 また、今回は com.uber.profiling.reporters.FileOutputReporter を用いて、ファイル出力を試みることとする。 結果的に、Sparkの起動コマンドは、以下のような感じになる。： 1$ $&#123;SPARK_HOME&#125;/bin/spark-shell --conf spark.driver.extraJavaOptions=-javaagent:/home/ubuntu/Sources/jvm-profiler/target/jvm-profiler-1.0.0.jar=reporter=com.uber.profiling.reporters.FileOutputReporter,outputDir=/tmp/jvm-profile ここで 環境変数 ${SPARK_HOME} はSparkを配備したPATHである ディレクトリ /tmp/jvm-profile は予め作成しておく とする。 生成されるレコードは、以下のようなJSONである。CpuAndMemory.jsonの例は以下の通り。： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&#123; &quot;heapMemoryMax&quot;: 954728448, &quot;role&quot;: &quot;driver&quot;, &quot;nonHeapMemoryTotalUsed&quot;: 156167536, &quot;bufferPools&quot;: [ &#123; &quot;totalCapacity&quot;: 20572, &quot;name&quot;: &quot;direct&quot;, &quot;count&quot;: 10, &quot;memoryUsed&quot;: 20575 &#125;, &#123; &quot;totalCapacity&quot;: 0, &quot;name&quot;: &quot;mapped&quot;, &quot;count&quot;: 0, &quot;memoryUsed&quot;: 0 &#125; ], &quot;heapMemoryTotalUsed&quot;: 400493400, &quot;vmRSS&quot;: 812081152, &quot;epochMillis&quot;: 1578408135107, &quot;nonHeapMemoryCommitted&quot;: 157548544, &quot;heapMemoryCommitted&quot;: 744488960, &quot;memoryPools&quot;: [ &#123; &quot;peakUsageMax&quot;: 251658240, &quot;usageMax&quot;: 251658240, &quot;peakUsageUsed&quot;: 37649152, &quot;name&quot;: &quot;Code Cache&quot;, &quot;peakUsageCommitted&quot;: 38010880, &quot;usageUsed&quot;: 37649152, &quot;type&quot;: &quot;Non-heap memory&quot;, &quot;usageCommitted&quot;: 38010880 &#125;, &#123; &quot;peakUsageMax&quot;: -1, &quot;usageMax&quot;: -1, &quot;peakUsageUsed&quot;: 104054944, &quot;name&quot;: &quot;Metaspace&quot;, &quot;peakUsageCommitted&quot;: 104857600, &quot;usageUsed&quot;: 104054944, &quot;type&quot;: &quot;Non-heap memory&quot;, &quot;usageCommitted&quot;: 104857600 &#125;, &#123; &quot;peakUsageMax&quot;: 1073741824, &quot;usageMax&quot;: 1073741824, &quot;peakUsageUsed&quot;: 14463440, &quot;name&quot;: &quot;Compressed Class Space&quot;, &quot;peakUsageCommitted&quot;: 14680064, &quot;usageUsed&quot;: 14463440, &quot;type&quot;: &quot;Non-heap memory&quot;, &quot;usageCommitted&quot;: 14680064 &#125;, &#123; &quot;peakUsageMax&quot;: 336592896, &quot;usageMax&quot;: 243269632, &quot;peakUsageUsed&quot;: 247788352, &quot;name&quot;: &quot;PS Eden Space&quot;, &quot;peakUsageCommitted&quot;: 250085376, &quot;usageUsed&quot;: 218352416, &quot;type&quot;: &quot;Heap memory&quot;, &quot;usageCommitted&quot;: 239075328 &#125;, &#123; &quot;peakUsageMax&quot;: 58195968, &quot;usageMax&quot;: 55050240, &quot;peakUsageUsed&quot;: 43791112, &quot;name&quot;: &quot;PS Survivor Space&quot;, &quot;peakUsageCommitted&quot;: 58195968, &quot;usageUsed&quot;: 43791112, &quot;type&quot;: &quot;Heap memory&quot;, &quot;usageCommitted&quot;: 55050240 &#125;, &#123; &quot;peakUsageMax&quot;: 716177408, &quot;usageMax&quot;: 716177408, &quot;peakUsageUsed&quot;: 138349872, &quot;name&quot;: &quot;PS Old Gen&quot;, &quot;peakUsageCommitted&quot;: 450363392, &quot;usageUsed&quot;: 138349872, &quot;type&quot;: &quot;Heap memory&quot;, &quot;usageCommitted&quot;: 450363392 &#125; ], &quot;processCpuLoad&quot;: 0.02584087025382403, &quot;systemCpuLoad&quot;: 0.026174300837744344, &quot;processCpuTime&quot;: 49500000000, &quot;vmHWM&quot;: 812081152, &quot;appId&quot;: &quot;local-1578407721611&quot;, &quot;vmPeak&quot;: 4925947904, &quot;name&quot;: &quot;24974@ubuec2&quot;, &quot;host&quot;: &quot;ubuec2&quot;, &quot;processUuid&quot;: &quot;38d5c63f-d70d-4e4d-9d54-a2381b9c37a7&quot;, &quot;nonHeapMemoryMax&quot;: -1, &quot;vmSize&quot;: 4925947904, &quot;gc&quot;: [ &#123; &quot;collectionTime&quot;: 277, &quot;name&quot;: &quot;PS Scavenge&quot;, &quot;collectionCount&quot;: 16 &#125;, &#123; &quot;collectionTime&quot;: 797, &quot;name&quot;: &quot;PS MarkSweep&quot;, &quot;collectionCount&quot;: 4 &#125; ]&#125; nonヒープのメモリ使用量についても情報あり ヒープについては、RSSに関する情報もある ヒープ内の領域に関する情報もあり、GCに関する情報もある","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Spark","slug":"Knowledge-Management/Spark","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Spark/"}],"tags":[{"name":"Apache Spark","slug":"Apache-Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Apache-Spark/"},{"name":"Java","slug":"Java","permalink":"https://dobachi.github.io/memo-blog/tags/Java/"},{"name":"Profiler","slug":"Profiler","permalink":"https://dobachi.github.io/memo-blog/tags/Profiler/"}]},{"title":"The evolution of metadata: LinkedIn’s story","slug":"The-evolution-of-metadata-LinkedIns-story","date":"2020-01-04T06:42:04.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2020/01/04/The-evolution-of-metadata-LinkedIns-story/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/04/The-evolution-of-metadata-LinkedIns-story/","excerpt":"","text":"参考 メモ GMA Metadata Serving Metadata Ingestion Architecture 参考 The evolution of metadata LinkedIn’s story スライド LinkedInのdatahubの20200104時点での実装 アーキテクチャ Generalized Metadata Architecture (GMA) Metadata Serving MAE(Metadata Audit Event) メモ LinkedInが提唱する Generalized Metadata Architecture (GMA) を基盤としたメタデータ管理システム。 コンセプトは、 スライド に記載されているが、ざっとアーキテクチャのイメージをつかむには、 アーキテクチャ がよい。 GMA メタデータは自動収集。 これは標準化されたメタデータモデルとアクセスレイヤによるものである。 また標準モデルが、モデルファーストのアプローチを促進する。 Metadata Serving Metadata Serving に記載あり。 RESTサービスは、LinkedInが開発していると思われるREST.liが用いられており、 DAOもその中の「Pegasus」という仕組みを利用している。 Key-Value DAO、Search DAO、Query DAOが定義されている。 上記GMAの通り、この辺りのDAOによるアクセスレイヤの標準化が見て取れる。 Metadata Ingestion Architecture そもそも、メタデータに対する変更は MAE(Metadata Audit Event) としてキャプチャされる。 それがKafka Streamsのジョブで刈り取られ処理される。なお、シーケンシャルに処理されるための工夫もあるようだ。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Metadata Management","slug":"Knowledge-Management/Metadata-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Metadata-Management/"}],"tags":[{"name":"Metadata Management","slug":"Metadata-Management","permalink":"https://dobachi.github.io/memo-blog/tags/Metadata-Management/"},{"name":"LinkedIn","slug":"LinkedIn","permalink":"https://dobachi.github.io/memo-blog/tags/LinkedIn/"}]},{"title":"Pinot: Enabling Real-time Analytics Applications","slug":"Pinot-Enabling-Real-time-Analytics-Applications","date":"2020-01-01T14:01:36.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2020/01/01/Pinot-Enabling-Real-time-Analytics-Applications/","link":"","permalink":"https://dobachi.github.io/memo-blog/2020/01/01/Pinot-Enabling-Real-time-Analytics-Applications/","excerpt":"","text":"参考 メモ オンライン分析のアーキテクチャ ユースケース ワークフロー 分散処理とIngestion 分散処理とクエリルーティング リアルタイムサーバとオフラインサーバ アーキテクチャの特徴 インデックス（Cube）の特徴 性能特性 Druidとの簡単な比較 Star-Tree Indexについて 参考 Pinot Enabling Real-time Analytics Applications atLinkedIn's Scale Star-Tree Index Powering Fast Aggregations on Pinot メモ オンライン分析のアーキテクチャ Join on the fly ベーステーブルからクエリ実行のタイミングでデータ生成 Pre Join + Pre Aggregate 分析で必要となるテーブルをストリーム処理等で事前作成 事前処理はマスタデータのテーブルとの結合、など Pre Join + Pre Aggregate + Pre Cube さらに分析で求められる結果データを予め作成、インデックス化 例えばAggregationしておく、など レイテンシと柔軟性のトレードオフの関係： Latency vs. Flexibility 「Who View」のアーキテクチャ ユースケース LinkedInのメンバーが見る分析レポート QPSが高い（数千/秒 級） レイテンシは数十ms～sub秒 インタラクティブダッシュボード 様々なアグリゲーション 異常検知 LinkedIn以外の企業では、 Uber slack MS Teams Weibo factual あたりが利用しているようだ。 Uberは、自身の技術ブログでも触れていた。 ワークフロー ワークフロー概要 Pinotは、原則としてPre Aggregation、Pre Cubeを前提とする仕組みなので、 スキーマの定義が非常に重要。 分散処理とIngestion またバッチとストリーム両方に対応しているので、 それぞれデータ入力（Ingestion）を定義する。 データはセグメントに分けられ、サーバに分配される。 Segment Assignment 分散処理とクエリルーティング セグメントに基づき、Brokerによりルーティングされる。 リアルタイムサーバとオフラインサーバ ストリームから取り込んだデータとバッチで取り込んだデータは、 共通のスキーマを用いることになる。 したがって、統一的にクエリできる？ アーキテクチャの特徴 インデックス（Cube）の特徴 Scan、Inverted Index、Sorted Index、Star-Tree Indexを併用可能。 データ処理上の工夫 比較対象として、Druidがよく挙げられているが、Sorted Index、Star-Tree Indexがポイント。 カラムナデータフォーマットを用いるのは前提。 それに加え、Dictionary Encodeing、Bit Compressionを使ってデータ構造に基づいた圧縮を採用。 Inverted Index 転置インデックスを作っておく、という定番手法。 Sorted Index 予め、Dimensionに基づいてソートしておくことで、 フィルタする際にスキャン量を減らしながら、かつデータアクセスを効率化。 Sroted Indexの特徴 Star-Tree Index すべてをCube化するとデータ保持のスペースが大量に必要になる。 そこで部分的にCube化する。 ★要確認 Star-Tree Indexの特徴 性能特性 Druidとの簡単な比較 Druidと簡易的に比較した結果が載っている。 まずは、レイテンシの小ささが求められるインタラクティブな分析における性能特徴： Druidとの簡易比較 ミリ秒単位での分析を取り扱うことに関してDruidと共通だが、 各種インデックスのおかげか、Druidよりもパーセンタイルベースの比較で レイテンシが小さいとされている。 つづいて、あらかじめ定義されたクエリを大量にさばくユースケース： Druidとの簡易比較2 レイテンシを小さく保ったまま、高いQPSを実現していることを 示すグラフが載っている。 この辺りは、工夫として載っていた各種インデックスを予め定義していることが強く効きそうだ。 続いて異常検知ユースケースの例： Druidとの簡易比較3 データがSkewしていることが強調されているが、その意図はもう少し読み解く必要がありそう。 ★要確認 Star-Tree Indexについて Star-Tree Index Powering Fast Aggregations on Pinot に記載あり。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Pinot","slug":"Knowledge-Management/Pinot","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Pinot/"}],"tags":[{"name":"Pinot","slug":"Pinot","permalink":"https://dobachi.github.io/memo-blog/tags/Pinot/"},{"name":"LinkedIn","slug":"LinkedIn","permalink":"https://dobachi.github.io/memo-blog/tags/LinkedIn/"},{"name":"OLAP","slug":"OLAP","permalink":"https://dobachi.github.io/memo-blog/tags/OLAP/"},{"name":"Query Engine","slug":"Query-Engine","permalink":"https://dobachi.github.io/memo-blog/tags/Query-Engine/"},{"name":"Druid","slug":"Druid","permalink":"https://dobachi.github.io/memo-blog/tags/Druid/"}]},{"title":"Uber's 2019 highlights","slug":"Uber-s-2019-highlights","date":"2019-12-29T14:52:34.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/12/29/Uber-s-2019-highlights/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/12/29/Uber-s-2019-highlights/","excerpt":"","text":"参考 メモ Ludwig AresDB QUIC導入 Kotlinの性能調査 グラフ処理と機械学習を用いたUber Eatsの改善 Uber’s Data Platform in 2019: Transforming Information to Intelligence 参考 Year in Review: 2019 Highlights from the Uber Engineering Blog Introducing Ludwig, a Code-Free Deep Learning Toolbox Uber’s Data Platform in 2019 Transforming Information to Intelligence メモ Ludwig モデル開発と比較を単純にするために作られたソフトウェア。 詳しくは、 Introducing Ludwig, a Code-Free Deep Learning Toolbox を参照。 AresDB GPUを活用した処理エンジン。 QUIC導入 TCPとHTTP/2の置き換え。 Kotlinの性能調査 Javaとの比較など。かなり細かく調査したようだ（まだ読んでいない） グラフ処理と機械学習を用いたUber Eatsの改善 グラフ構造に基づいた機械学習により、 レコメンデーションの効果を向上させる取り組みについて。 Uber’s Data Platform in 2019: Transforming Information to Intelligence Uber’s Data Platform in 2019 Transforming Information to Intelligence UberでもData Platformという言い方をするようだ。 データの用途は、スクーターの位置情報や店舗の最新メニューをトラックするだけではない。 トレンドを把握することなどにも用いられる。 データの鮮度の品質は大事。 リアルタイムの処理のニーズ・仕組み、ヒストリカルなデータの処理のニーズ・仕組み。両方ある。 補足）それぞれがあることを前提とした最適化を施すことが前提となっているようだ。 データがどこから来たのかをトラックする。 Uberの内部プロダクト uLineageにて実現。 Apache HBaseをグローバルインデックスとして利用。 これにより、高いスケーラビリティ、強い一貫性、水平方向へのスケーラビリティを獲得する。 DBEventsというプロダクトで、CDCを実現する。 トラブルシュートとプロアクティブな防止。 もしリアルタイムにデータ分析できれば、例えば利用者にドライバ替わり当たらず一定時間を過ぎてしまったケースを発見し、 オペレーションチームが助けられるかもしれない。 AresDB、Apache Pinotなど。★要確認","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Uber","slug":"Clipping/Uber","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Uber/"}],"tags":[{"name":"Uber","slug":"Uber","permalink":"https://dobachi.github.io/memo-blog/tags/Uber/"}]},{"title":"Configure Python of PySpark in Zeppelin","slug":"Configure-Python-of-PySpark-in-Zeppelin","date":"2019-12-27T14:43:45.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/12/27/Configure-Python-of-PySpark-in-Zeppelin/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/12/27/Configure-Python-of-PySpark-in-Zeppelin/","excerpt":"","text":"参考 メモ 参考）Pythonバージョンの確かめ方 参考 メモ ウェブフロントエンドのSpark Interpreterの設定において、 以下の2項目を設定した。 spark.pyspark.python zeppelin.pyspark.python 上記2項目が同じ値で設定されていないと、実行時エラーを生じた。 参考）Pythonバージョンの確かめ方 ドライバのPythonバージョンの確かめ方 12345%spark.pysparkimport sysprint(&apos;Zeppelin python: &#123;&#125;&apos;.format(sys.version)) ExecutorのPythonバージョンの確かめ方 1234567%spark.pysparkdef print_version(x): import sys return sys.versionspark.sparkContext.parallelize(range(1, 3)).map(print_version).collect()","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Zeppelin","slug":"Knowledge-Management/Zeppelin","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Zeppelin/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"PySpark","slug":"PySpark","permalink":"https://dobachi.github.io/memo-blog/tags/PySpark/"},{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Zeppelin","slug":"Zeppelin","permalink":"https://dobachi.github.io/memo-blog/tags/Zeppelin/"}]},{"title":"markdown preview of vim","slug":"markdown-preview-of-vim","date":"2019-12-21T08:04:53.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/12/21/markdown-preview-of-vim/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/12/21/markdown-preview-of-vim/","excerpt":"","text":"参考 メモ 試したツール その他試したいもの iamcco/markdown-preview.nvim を試す際に気をつけたこと 参考 Vim + Markdown iamcco/markdown-preview.nvim shime/vim-livedown previm/previm iwataka/minidown.vim メモ Vim + Markdown のページに色々と載っていた。 前半はMarkdown編集のための補助ツール、後半はプレビューのツール。 プレビューとしては、結論として iamcco/markdown-preview.nvim を選んだ。 試したツール shime/vim-livedown 外部ツールが必要だがnpmで導入可能なので簡易。 悪くなかったが、動作が不安定だったので一旦保留。 iamcco/markdown-preview.nvim 外部ツールが必要だが予めプラグインディレクトリ内にインストールされるような手順になっている ただし手元のWSL環境ではnpmインストールに失敗していたため手動インストールした 今のところ馴染んでいるので利用している previm/previm 外部依存がなく悪くなさそうだった WindowsのWSL環境で動作が不安定だったので一旦保留。 その他試したいもの iwataka/minidown.vim Vim + Markdown のブログ内で紹介されていた、ブログ主の作ったツール ミニマムな機能で必要十分そう。外部依存が殆どないのが大きい iamcco/markdown-preview.nvim を試す際に気をつけたこと プラグイン内でnodejsを利用するのだが、そのライブラリインストールにyarnを 使うようになっている。 公式READMEの文言を以下に引用。 12call dein#add(&apos;iamcco/markdown-preview.nvim&apos;, &#123;&apos;on_ft&apos;: [&apos;markdown&apos;, &apos;pandoc.markdown&apos;, &apos;rmd&apos;], \\ &apos;build&apos;: &apos;cd app &amp; yarn install&apos; &#125;) 個人的にはnpmを利用しているのでyarnではなくnpmでインストールするよう修正し実行した。 しかいｓ上記の通り、一部の環境ではプラグイン内にインストールする予定だったnpmライブラリが インストールされない事象が生じた。 その際は、自身で npm install すればよい。 12$ cd ~/vimfiles/bundles/repos/github.com/iamcco/markdown-preview.nvim/app$ npm install なお、 ~/vimfiles/bundles のところは、各自のdeinレポジトリの場所を指定してほしい。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"vim","slug":"Knowledge-Management/vim","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"},{"name":"markdown","slug":"markdown","permalink":"https://dobachi.github.io/memo-blog/tags/markdown/"}]},{"title":"Data Platform for Machine Learning","slug":"Data-Platform-for-Machine-Learning","date":"2019-12-09T23:23:12.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/12/10/Data-Platform-for-Machine-Learning/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/12/10/Data-Platform-for-Machine-Learning/","excerpt":"","text":"参考 メモ Abstract Introduction Motivation 関連研究 SYSTEM ARCHITECTURE AND DESIGN FUTURE WORK 参考 Data Platform for Machine Learning メモ Data Platform for Machine Learning を読んでみた。個人的要約メモを以下に記載する。 Abstract これまでは数学的な試行錯誤（ここでは機械学習も含んでそう表現していると思われる）に適したデータプラットフォーム（データを保持し、サーブする仕組み、と解釈）がなかった。 既存のデータプラットフォームの課題はいくつかある。 いずれにせよ、データ管理の負荷はユーザビリティを下げる。 また、昨今求められるコンプライアンス機能（例：terms of use、privacy measure、auditなど）を すべて利用できるわけではない。 本論文では、Machine Learning Data Platform（MLdp）に求められるものをまとめ、Appleにおける経験とソリューションを紹介し、将来のアクションを紹介する。 Introduction MLdp = Machine Learningのためのデータ管理システム Motivation 前提とするワークフロー： data collection annotation exploration feature engineering experimentation evaluation finally deployment （補足）Appleにおけるワークフローのアブストラクションの考え方が分かるので参考として 強い課題感：サイロ化されたデータストア、サイロ化された知見・アクティビティ ★重要 サイロ化されたストアとアクティビティのイメージ 3種類の動機 エンジニアリングチームのサポート MLライフサイクルのサポート MLフレームワーク、データのサポート ロール3種類 + 1 MLエンジニア ソフトウェアエンジニア データサイエンティスト Legal Team （補足）よく聞くのは、データサイエンティスト、データエンジニアだが、ここでは、 MLエンジニア、ソフトウェアエンジニア、Legal Teamが挙げられているのが特徴的。 MLでは、データはトラックされること、バージョン管理されることが必要 ★重要 MLdpが満たすべき要件 ★重要 以下の2種類のデータに対するコンセプチャルな表現方法 ほとんど変化しない大量の永続データ 学習データのセットのこと 再現性担保のため学習データ自体はイミュータブルに扱う 揮発性のデータ アノテーションやラベルのこと これらの情報は、業務次第で値を変える可能性がある 学習データセット本体と比べて総量が小さい 上記2種類の要件を満たすハイブリッドデータストア さらにバージョニング可能な物理レイアウト インクリメンタルな更新、デルタトラッキング、ML学習ワークロードへの最適化 継続的なデータ入力に対するシンプルな機構 データ、特徴量エンジニアリングのためのDSL 著名なMLフレームワークへのインターフェース 著名なMLフレームワークでは独自のデータ表現をもっているが、一方でMLdpでは フレームワークをまたいだ共通的なデータ表現が必要、など データ型も多様である データセンタ、エッジの両方への対応。 データへの距離を意識しないとならない 説明可能性を担保するためのバージョニング データリネージのトラック、データソースのトラック データ探索と発見 MLdpでは省力化のための中央集権的なアプローチを採用 コンプライアンスãプライバシーの考慮 MLプラットフォームのエコシステム一覧 3Vの話 関連研究 （ここでは一旦省略） SYSTEM ARCHITECTURE AND DESIGN アーキテクチャは、コントロールプレーン、データプレーンによる構成。 ★重要 MLdpのアーキテクチャ概要 各コンポーネントの特徴 ★重要 概念データモデル ローデータ、ローデータから生成されたデータ（アノテーション、特徴量）の両方とも表現可能とする バージョン管理の仕組み イミュータブルなデータのスナップショットを用いて、実験の再現性を担保する データアクセスインターフェース MLフレームワークや他のデータ処理エンジンとシームレスに連係可能とする ハイブリッドなデータストア 高頻度でのデータ入力とあまり変化しない大量のデータの両方に対応可能とする ストレージレイアウト バージョン間のデルタトラッキング 分散学習のためのデータ並列化 データ探索と発見を可能とするインデックス デバイスとデータセンタの両方での学習のためのストリームI/O 学習タスクのためのキャッシュ 4種類のデータセット：dataset, annotation, split, and package に分けて考える。 ★重要 dataset: データ本体。学習データに用いる、など。量、サイズが大きい。 annotatoin: データ本体に付与したラベル、属性、メタデータなど。量、サイズはdatasetと比べて小さい。 split: データ本体を分割したもの。 （補足）実体はdatasetであり、split自体は仮想的な定義と考えられる。 package: dataset / split、annotationを組み合わせた仮想的なまとまり。 再現性担保のため、学習セットをまとめて管理するために利用。 以下補足。 datasetに対し、annotationやsplitは「weak object」。つまり、それ単体では存在しない。 annotationやsplitをdatasetとは別に管理することメリットは「multifold」である。 annotationもsplitもdatasetを変更せずに変更可能である。 annotationは個別のコンプライアンスポリシーを持つことがある。 バージョンの表現 -&gt; &lt;schema&gt;.&lt;revision&gt;.&lt;patch&gt; ★重要 これにより、スキーマ変更あり・なしを明示しながら、バージョン管理を可能とする。 バージョン管理はユーザが明示的に行う。 ★重要 MLプロジェクト間でのデータシェアにおいて以下を実現。 ★重要 他のプロジェクトへの影響を気にせず、そのプロジェクトの都合でデータをシェアし、スキーマを変更できる。 特定のバージョンをピンどめする。再現性確保。 データのモデルバージョンの依存関係をトラック可能とする MLプロジェクトにおいてはデータはインターフェースである。 ★名言 MLdpではオブジェクトは以下の状態を遷移する。 ★重要 draft published archived purged publishされたデータはイミュータブルになる。 ★重要 これは再現性を担保するため。 （補足）このオブジェクトのライフサイクルは、S3等のパブリッククラウドのオブジェクトストレージの機能と相性が良さそう。 MLdpでは、表形式のデータ表現を採用。 ★重要 カラムナ表現により、圧縮効率やIO効率の改善だけではなく、 カラムの追加削除にも対応しやすくなる。 Spark RDD、DataFrame、Apache DataFrame、Pandas、R DataFrameなどと互換性あり。 MLdpではファイルはバイト列として扱う。 またファイルに対して、ストリーム形式でのアクセスも可能とする。 ★重要 データアクセスのインタフェースのうち、ハイレベルAPIはサーバサイドのバッチ処理向き。つまり前処理などに適している。 ローレベルAPIは各種フレームワークとの連係向き。 クライアントでのデータ利用のため、MLdpはキャッシュ機能を持つ。 ★重要 （補足）このあたり、Appleだからこそ必要であり、一般企業は果たして必要だろうか。 SQLライクなインタフェースを提供。（その他のDSLもあるようだ） 現在、Pythonと連係可能 入力となるデータの例 データセット定義のクエリ例 既存モデルを読み込んで推論するクエリ例 MLのデータセットは、複数のファイルをまとめ、MLモデルから扱うことがある。 MLdpをマウントして扱うことができる。 いまのところクライアントサイドのシンプルなキャッシュ機構を有している。 またエッジでの利用を想定し、REST APIも提供。 MLdpをマウントしPATHに対して画像処理するクエリ例 セカンダリインデックスも利用可能。 セカンダリインデックスを使い、求める画像のみを扱うクエリ例 分散学習のためのスプリットを生成することもできる。 MLdpをマウントし、TFの機能を使ってスライスを作成 ストレージレイヤのデザインのポイントは以下の通り。 頻繁に更新されるデータにも対応しながら、学習時の高スループットでの処理にも対応 スケーラビリティを持ち、バージョン管理可能であり、バージョン間の差分をトラックできる ダイナミックレンジクエリ、ポイントクエリに対応し、ストリームI/Oアクセスも可能 in-flightのデータ保持にはKVSを使用し、高頻度での並列での書き込みに対応する。 ★重要 スナップショットが取られ、クラウドストレージ上に置かれる。 ★重要 おかれたデータは読み取りに最適化されたものになる。 スナップショットに対する変更は、新しいスナップショットとなって書き込まれる（論文ではcurateされた、と表現されていた）。 つまり、スナップショットはイミュータブルな扱いとなる。 in-flightのデータとcurateされたデータをつなぐサブシステムを論文ではdata-pipeと呼ぶ。 MLdpでは複数のデータストアを取り扱うが、ユニファイドなアクセス手段を提供する。 ★重要 in-flightのデータストアは頻繁に更新されるため、それをそのまま機械学習の入力データに することはあまりない。というのも、再現性を担保するのが難しいからだ。 MLdpではデータをパーティション化して保存するが、パーティションキーはユーザが 直接指定するわけではない。 その代わりソートキーを指定する。 ただし、ソートキーが指定されなかった場合は、システムがハッシュをキーに割り当て パーティションを構成するようにする。 パーティションは複数のブロックで構成される。 データの変更が少ないケースでは、バージョン間で多くのブロックを共有できる。 copy-on-writeで変更のあったブロックが生成される。 パーティションとブロック （感想）多くの分散ファイルシステムで見られる構成。 このブロックで構成することで、MLdpのクライアントは興味のあるブロックにだけアクセスすればよくなる。 またブロックに対するストリームI/Oも可能なので、MLフレームワークからストリームI/Oでアクセスできる。 インデックスもある。そのためブロック単位でのスキャンも可能だし、インデックスに基づくピンポイントでの検索も可能。 セカンダリインデックスの仕組みもある。 ★重要 セカンダリインデックスの仕組み セカンダリインデックスを用いて、各ブロック内のレコードを取りに行くことになる。 MLの計算クラスタにはキャッシュの仕組みが組み込まれており、 透過的に利用できるようになっている。 データアクセスし、もしキャッシュヒットしたら、キャッシュからデータを返す。 キャッシュシステムが停止している場合は、通常のデータアクセスにフォールバックする。 キャッシュシステムの目的は、レイテンシの低減とReadのスケールアウト。 キャッシュはイミュータブルデータを対象とし、読み取り専用。この単純化のおかげで、結果一貫性などに伴う異常を排除。 FUTURE WORK 将来の課題：データ探索性、システム改善、エコシステムインテグレーション。 データ分析者は、適切なデータを探す。 human-in-the-loop。 カタログは知っている情報を探し当てるもの。データ探索は知らない情報を明らかにすること。両者は異なる。 データ探索の活動はそれ自体が機械学習タスクであることもある。 この手のデータ探索はドメインスペシフィックではあるが、共通的な？システム要件もある。 例えばコンピュータビジョンなど。 ヘテロジーニアスなデータフォーマットをユニファイドな表現に変換できるようにしたい。 システム的な改善に関する2種類の観点： レイテンシを下げ、スループットを上げる human-in-the-loopを減らす インテントベースのキャッシュを開発したい。つまり、明示的にキャッシュ化するのではなく、頻繁に利用されるデータをキャッシュするようにする、など。 プリフェッチの改善、ローカルバッファリングも改善ネタ。 しかし学習の際にランダマイズが必要なのが難しいところ。 Spark等との連携も課題。 各フレームワークは独自のデータ保持方式を持っている。 MLdpを利用しやすいようにしたい。できるだけユーザコードを変更せずに利用できるようにしたい。 データ保持方式に大きく分けて2種類： \" MLdpの独自の方式を採用し、都度変換する。 \" もうひとつはTFRecordなどのフレームワークネイティブの方式を採用すること 一長一短。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Data Platform","slug":"Knowledge-Management/Machine-Learning/Data-Platform","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Data-Platform/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Data Platform","slug":"Data-Platform","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Platform/"},{"name":"Apple","slug":"Apple","permalink":"https://dobachi.github.io/memo-blog/tags/Apple/"}]},{"title":"Abstract of Analytics Zoo","slug":"Abstract-of-Analytics-Zoo","date":"2019-12-02T13:13:42.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/12/02/Abstract-of-Analytics-Zoo/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/12/02/Abstract-of-Analytics-Zoo/","excerpt":"","text":"参考 メモ 動作確認 Python環境構築 参考 Analytics Zooの公式ドキュメント Analytics Zoo 0.6.0の公式ドキュメント メモ Analytics Zooの公式ドキュメント の内容から、ひとことで書いてある文言を拾うと...以下の通り。 0.2.0時代： Analytics + AI Platform for Apache Spark and BigDL. 2019/12現在： A unified analytics + AI platform for distributed TensorFlow, Keras, PyTorch and BigDL on Apache Spark とのこと。対応範囲が拡大していることがわかる。 動作確認 Python環境構築 今回は簡易的にPythonから触る。 pipenvで3.6.9環境を作り、以下のライブラリをインストールした。 numpy scipy pandas scikit-learn matplotlib seaborn wordcloud jupyter pipenvファイルは以下の通り。 https://github.com/dobachi/analytics-zoo-example/blob/master/Pipfile なお、 Analytics Zooの公式ドキュメント では、Hadoop YARNでローンチする方法などいくつかの方法が掲載されている。 Exampleを動かしてみようとするが、pipでインストールしたところ jupyter-with-zoo.sh は以下の箇所にあった。 1$HOME/.local/share/virtualenvs/analytics-zoo-example-hS4gvn-H/lib/python3.6/site-packages/zoo/share/bin/jupyter-with-zoo.sh そこで環境変数を以下のように設定した。 12$ export ANALYTICS_ZOO_HOME=`pipenv --venv`/lib/python3.6/site-packages/zoo/share$ export SPARK_HOME=`pipenv --venv`/lib/python3.6/site-packages/pyspark この状態で、 ${ANALYTICS_ZOO_HOME}/bin/jupyter-with-zoo.sh --master local[*] を実行しようとしたのだが、 jupyter-with-zoo.sh 内で用いられている analytics-zoo-base.sh の中で、 1234if [[ ! -f $&#123;ANALYTICS_ZOO_PY_ZIP&#125; ]]; then echo &quot;Cannot find $&#123;ANALYTICS_ZOO_PY_ZIP&#125;&quot; exit 1fi の箇所に引っかかりエラーになってしまう。 そもそも環境変数 ANALYTICS_ZOO_PY_ZIP は任意のオプションの値を指定するための環境変数であるため、このハンドリングがおかしいと思われる。 どうやら、pipenvでインストールした0.6.0では、Jupyterとの連係周りがまだ付け焼き刃のようだ。 とりあえず、今回の動作確認では利用しない環境変数であるため、関係個所をコメントアウトして実行することとする。 上記対応の後、以下のようにpipenv経由でJupyter PySparkを起動。 1$ pipenv run $&#123;ANALYTICS_ZOO_HOME&#125;/bin/jupyter-with-zoo.sh --master local[*] 試しに実行しようとしたが、以下のエラー 1Py4JError: com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames does not exist in the JVM","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Analytics Zoo","slug":"Knowledge-Management/Machine-Learning/Analytics-Zoo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Analytics-Zoo/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Analytics Zoo","slug":"Analytics-Zoo","permalink":"https://dobachi.github.io/memo-blog/tags/Analytics-Zoo/"},{"name":"Intel","slug":"Intel","permalink":"https://dobachi.github.io/memo-blog/tags/Intel/"},{"name":"BigDL","slug":"BigDL","permalink":"https://dobachi.github.io/memo-blog/tags/BigDL/"}]},{"title":"Kafka Streams with ML","slug":"Kafka-Streams-with-ML","date":"2019-11-25T13:14:32.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/11/25/Kafka-Streams-with-ML/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/25/Kafka-Streams-with-ML/","excerpt":"","text":"参考 kaiwaehner kafka-streams-machine-learning-examples メモ kaiwaehner/kafka-streams-machine-learning-examples Convolutional Neural Network (CNN) with TensorFlow for Image Recognition Iris Prediction using a Neural Network with DeepLearning4J (DL4J) Python + Keras + TensorFlow + DeepLearning4j 参考 kaiwaehner kafka-streams-machine-learning-examples kaiwaehner kafka-streams-machine-learning-examples How to Build and Deploy Scalable Machine Learning in Production with Apache Kafka Using Apache Kafka to Drive Cutting-Edge Machine Learning Machine Learning with Python, Jupyter, KSQL and TensorFlow kaiwaehner ksql-udf-deep-learning-mqtt-iot Iris Prediction using a Neural Network with DeepLearning4J (DL4J) Python + Keras + TensorFlow + DeepLearning4j メモ kaiwaehner/kafka-streams-machine-learning-examples kaiwaehner kafka-streams-machine-learning-examples には、TensorFlow、Keras、H2O、Python、DL4JをKafka Streamsと併用するサンプルが含まれている。 上記レポジトリのREADMEには、いくつか参考文献が記載されている。 How to Build and Deploy Scalable Machine Learning in Production with Apache Kafka には、 デザインの概略みたいなものが載っていた 学習と推論のモデルやりとりをどうつなぐのか？の考察 が記載されていた。 Using Apache Kafka to Drive Cutting-Edge Machine Learning にはモデルの組み込み方の種類、 AutoMLとの組み合わせについて考察（Not 具体例）が掲載されている。 Machine Learning with Python, Jupyter, KSQL and TensorFlow には、以下のような記述がある。 いつもの論文とセット。 Impedance mismatch between data scientists, data engineers and production engineers これを解決する手段としていくつか例示。 ONNX、PMMLなどを利用 DL4Jなど開発者視点の含まれるプロダクトの利用 AutoML モデルを出力してアプリに埋め込んで利用（TensorFlowだったらJava APIでモデルを利用する、など） パブリッククラウドのマネージドサービス利用 ConfluentのKafka Python KSQL APIを使い、Jupyter上でKafkaからデータロードし分析する例も記載されていた。 kaiwaehner ksql-udf-deep-learning-mqtt-iot には、UDF内でTensorFlowを使う例が記載されている。 kaiwaehner ksql-fork-with-deep-learning-function には、エンドツーエンドで動作を確認してみるためのサンプル実装が載っている。 kaiwaehner tensorflow-serving-java-grpc-kafka-streams には、gRPC経由でKafka StreamsとTensorFlow Servingを連係する例が記載されている。 Convolutional Neural Network (CNN) with TensorFlow for Image Recognition com.github.megachucky.kafka.streams.machinelearning.Kafka_Streams_TensorFlow_Image_Recognition_Example クラスを確認する。 概要 本クラスは、予め学習されたTensorFlowのモデルを読み出して利用する 上記モデルを利用するときには、TensorFlowのJava APIを利用する 画像はどこかのストレージに保存されている前提となっており、そのPATHがメッセージとしてKafkaに入ってくるシナリオである 画像に対するラベルのProbabilityを計算し、最大のProbabilityを持つラベルを戻り値として返す 詳細メモ main内では特別なことはしていない。トポロジを組み立てるための getStreamTopology が呼ばれる。 getStreamTopology メソッドを確認する。 当該メソッドでは、最初にモデル本体やモデルの定義が読み込まれる。 com/github/megachucky/kafka/streams/machinelearning/Kafka_Streams_TensorFlow_Image_Recognition_Example.java:83 1234567String modelDir = &quot;src/main/resources/generatedModels/CNN_inception5h&quot;;Path pathGraph = Paths.get(modelDir, &quot;tensorflow_inception_graph.pb&quot;);byte[] graphDef = Files.readAllBytes(pathGraph);Path pathModel = Paths.get(modelDir, &quot;imagenet_comp_graph_label_strings.txt&quot;);List&lt;String&gt; labels = Files.readAllLines(pathModel, Charset.forName(&quot;UTF-8&quot;)); 続いてストリームのインスタンスが生成される。 その後、ストリーム処理の内容が定義される。 最初はメッセージ内に含まれる画像のPATHを用いて、実際の画像をバイト列で読み出す。 com/github/megachucky/kafka/streams/machinelearning/Kafka_Streams_TensorFlow_Image_Recognition_Example.java:104 123456789101112KStream&lt;String, Object&gt; transformedMessage =imageInputLines.mapValues(value -&gt; &#123; String imageClassification = &quot;unknown&quot;; String imageProbability = &quot;unknown&quot;; String imageFile = value; Path pathImage = Paths.get(imageFile); byte[] imageBytes; try &#123; imageBytes = Files.readAllBytes(pathImage); つづいていくつかのヘルパメソッドを使って、画像に対するラベル（推論結果）を算出する。 com/github/megachucky/kafka/streams/machinelearning/Kafka_Streams_TensorFlow_Image_Recognition_Example.java:117 1234567891011try (Tensor image = constructAndExecuteGraphToNormalizeImage(imageBytes)) &#123; float[] labelProbabilities = executeInceptionGraph(graphDef, image); int bestLabelIdx = maxIndex(labelProbabilities); imageClassification = labels.get(bestLabelIdx); imageProbability = Float.toString(labelProbabilities[bestLabelIdx] * 100f); System.out.println(String.format(&quot;BEST MATCH: %s (%.2f%% likely)&quot;, imageClassification, labelProbabilities[bestLabelIdx] * 100f));&#125; constructAndExecuteGraphToNormalizeImage メソッドは、グラフを構成し、前処理を実行する。 com/github/megachucky/kafka/streams/machinelearning/Kafka_Streams_TensorFlow_Image_Recognition_Example.java:171 123456789final Output input = b.constant(&quot;input&quot;, imageBytes);final Output output = b .div(b.sub( b.resizeBilinear(b.expandDims(b.cast(b.decodeJpeg(input, 3), DataType.FLOAT), b.constant(&quot;make_batch&quot;, 0)), b.constant(&quot;size&quot;, new int[] &#123; H, W &#125;)), b.constant(&quot;mean&quot;, mean)), b.constant(&quot;scale&quot;, scale));try (Session s = new Session(g)) &#123; return s.runner().fetch(output.op().name()).run().get(0);&#125; executeInceptionGraph メソッドは、予め学習済みのモデルと画像を引数にとり、 ラベルごとのProbabilityを算出する。 com/github/megachucky/kafka/streams/machinelearning/Kafka_Streams_TensorFlow_Image_Recognition_Example.java:184 1234567891011121314151617181920try (Graph g = new Graph()) &#123; // Model loading: Using Graph.importGraphDef() to load a pre-trained Inception // model. g.importGraphDef(graphDef); // Graph execution: Using a Session to execute the graphs and find the best // label for an image. try (Session s = new Session(g); Tensor result = s.runner().feed(&quot;input&quot;, image).fetch(&quot;output&quot;).run().get(0)) &#123; final long[] rshape = result.shape(); if (result.numDimensions() != 2 || rshape[0] != 1) &#123; throw new RuntimeException(String.format( &quot;Expected model to produce a [1 N] shaped tensor where N is the number of labels, instead it produced one with shape %s&quot;, Arrays.toString(rshape))); &#125; int nlabels = (int) rshape[1]; return result.copyTo(new float[1][nlabels])[0]; &#125;&#125; executeInceptionGraph メソッドにより、ある画像に対するラベルごとのProbabilityが得られた後、 最大のProbabilityを持つラベルを算出。 それを戻り値とする。 Iris Prediction using a Neural Network with DeepLearning4J (DL4J) Iris Prediction using a Neural Network with DeepLearning4J (DL4J) を確認する。 com.github.megachucky.kafka.streams.machinelearning.models.DeepLearning4J_CSV_Model_Inference クラスを確認したが、これはKafka Streamsアプリには見えなかった。 中途半端な状態で止まっている？ これをベースに Kafka Streams のアプリを作ってみろ、ということか。 もしくはunitテスト側を見ろ、ということか？ -&gt; そのようだ。 Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java クラスの内容を確認する。 まずテストのセットアップ。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:46 1234567891011@ClassRulepublic static final EmbeddedKafkaCluster CLUSTER = new TestEmbeddedKafkaCluster(1);private static final String inputTopic = &quot;IrisInputTopic&quot;;private static final String outputTopic = &quot;IrisOutputTopic&quot;;// Generated DL4J modelprivate File locationDL4JModel = new File(&quot;src/main/resources/generatedModels/DL4J/DL4J_Iris_Model.zip&quot;);// Prediction Valueprivate static String irisPrediction = &quot;unknown&quot;; TestEmbeddedKafkaClusterはテスト用のKafkaクラスタを起動するヘルパークラス。 内部的には、Kafka Streamsのテストに用いられている補助機能である org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster クラスを継承して作られている。 機械学習モデルは、予め学習済みのものが含まれているのでそれを読み込んで用いる テストコードの実態は、 com.github.megachucky.kafka.streams.machinelearning.test.Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest#shouldPredictIrisFlowerType である。 以降、当該メソッド内を確認する。 メソッドの冒頭で、Kafka Streamsの設定を定義している。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:67 12345678 // Iris input data (the model returns probabilities for input being each of Iris // Type 1, 2 and 3) List&lt;String&gt; inputValues = Arrays.asList(&quot;5.4,3.9,1.7,0.4&quot;, &quot;7.0,3.2,4.7,1.4&quot;, &quot;4.6,3.4,1.4,0.3&quot;);(snip) streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); このサンプルは動作確認用のため簡易な設定になっている。 実際のアプリケーション開発時にはきちんと設定必要。 なお、途中でDL4Jのモデルを読み込んでいる。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:86 12// Create DL4J object (see DeepLearning4J_CSV_Model.java)MultiLayerNetwork model = ModelSerializer.restoreMultiLayerNetwork(locationDL4JModel); その後、ビルダのインスタンスを生成し、Irisデータを入力とするストリームを定義する。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:97, 104 12final StreamsBuilder builder = new StreamsBuilder();final KStream&lt;String, String&gt; irisInputLines = builder.stream(inputTopic); その後はストリームのメッセージに対し、DL4Jのモデルによる推論を実行する。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:108 12345678910111213141516171819202122irisInputLines.foreach((key, value) -&gt; &#123; if (value != null &amp;&amp; !value.equals(&quot;&quot;)) &#123; System.out.println(&quot;#####################&quot;); System.out.println(&quot;Iris Input:&quot; + value); // TODO Easier way to map from String[] to double[] !!! String[] stringArray = value.split(&quot;,&quot;); Double[] doubleArray = Arrays.stream(stringArray).map(Double::valueOf).toArray(Double[]::new); double[] irisInput = Stream.of(doubleArray).mapToDouble(Double::doubleValue).toArray(); // Inference INDArray input = Nd4j.create(irisInput); INDArray result = model.output(input); System.out.println(&quot;Probabilities: &quot; + result.toString()); irisPrediction = result.toString(); &#125;&#125;); ここでは入力されたテキストデータをDouble型の配列に変換し、さらにND4JのINDArrayに変換する。 それをモデルに入力し、推論を得る。 その後、テキストを整形し、出力用トピックに書き出し。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:132 123456// Transform message: Add prediction informationKStream&lt;String, Object&gt; transformedMessage = irisInputLines .mapValues(value -&gt; &quot;Prediction: Iris Probability =&gt; &quot; + irisPrediction);// Send prediction information to Output TopictransformedMessage.to(outputTopic); ビルダを渡し、ストリーム処理をスタート。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:140 12345final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfiguration);streams.cleanUp();streams.start();System.out.println(&quot;Iris Prediction Microservice is running...&quot;);System.out.println(&quot;Input to Kafka Topic &apos;IrisInputTopic&apos;; Output to Kafka Topic &apos;IrisOutputTopic&apos;&quot;); その後、これはテストコードなので、入力となるデータをアプリケーション内で生成し、入力トピックに書き込む。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_MachineLearning_DL4J_DeepLearning_Iris_IntegrationTest.java:149 1234567Properties producerConfig = new Properties();producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());producerConfig.put(ProducerConfig.ACKS_CONFIG, &quot;all&quot;);producerConfig.put(ProducerConfig.RETRIES_CONFIG, 0);producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);IntegrationTestUtils.produceValuesSynchronously(inputTopic, inputValues, producerConfig, new MockTime()); ここでは結合テスト用のヘルパーメソッドを利用。 その後、出力トピックから結果を取り出し、確認する。（実装解説は省略） Python + Keras + TensorFlow + DeepLearning4j 例のごとく、テスト側が実装本体。 Kafka_Streams_TensorFlow_Keras_Example_IntegrationTest クラスを確認する。 実態は shouldPredictValues メソッド。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_TensorFlow_Keras_Example_IntegrationTest.java:64 1public void shouldPredictValues() throws Exception &#123; 上記メソッド内では、最初にモデルを読み込む。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_TensorFlow_Keras_Example_IntegrationTest.java:69 1234String simpleMlp = new ClassPathResource(&quot;generatedModels/Keras/simple_mlp.h5&quot;).getFile().getPath();System.out.println(simpleMlp.toString());MultiLayerNetwork model = KerasModelImport.importKerasSequentialModelAndWeights(simpleMlp); 上記では、HDF形式で予め保存されたモデルを読み込む。 読み込みの際にはDeeplearning4Jの KerasModelImport#importKerasSequentialModelAndWeights メソッドが用いられる。 続いて、Kafka Streamsのコンフィグを定める。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_TensorFlow_Keras_Example_IntegrationTest.java:81 1234567Properties streamsConfiguration = new Properties();streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;kafka-streams-tensorflow-keras-integration-test&quot;);streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); 次にKafka Streamsのビルダを定義し、入力トピックを渡して入力ストリームを定義する。 123final StreamsBuilder builder = new StreamsBuilder();final KStream&lt;String, String&gt; inputEvents = builder.stream(inputTopic); 以降、メッセージを入力とし、推論を行う処理の定義が続く。 先程定義したストリームの中は、カンマ区切りのテキストになっている。 これをカンマで区切り、配列に変換する。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_TensorFlow_Keras_Example_IntegrationTest.java:104 123456inputEvents.foreach((key, value) -&gt; &#123; // Transform input values (list of Strings) to expected DL4J parameters (two // Integer values): String[] valuesAsArray = value.split(&quot;,&quot;); INDArray input = Nd4j.create(Integer.parseInt(valuesAsArray[0]), Integer.parseInt(valuesAsArray[1])); 配列への変換には、Nd4Jを用いる。 配列の定義が完了したら、それを入力としてモデルを用いた推論を行う処理を定義する。 12345 output = model.output(input); prediction = output.toString();&#125;); 最後は出力メッセージに変換し、出力トピックへの書き出しを定義する。 その後、独自のユーティリティクラスを使って、ビルダーに基づいてストリームをビルド。 ストリーム処理を開始する。 com/github/megachucky/kafka/streams/machinelearning/test/Kafka_Streams_TensorFlow_Keras_Example_IntegrationTest.java:118 12345678910111213// Transform message: Add prediction resultKStream&lt;String, Object&gt; transformedMessage = inputEvents.mapValues(value -&gt; &quot;Prediction =&gt; &quot; + prediction);// Send prediction result to Output TopictransformedMessage.to(outputTopic);// Start Kafka Streams Application to process new incoming messages from// Input Topicfinal KafkaStreams streams = new TestKafkaStreams(builder.build(), streamsConfiguration);streams.cleanUp();streams.start();System.out.println(&quot;Prediction Microservice is running...&quot;);System.out.println(&quot;Input to Kafka Topic &quot; + inputTopic + &quot;; Output to Kafka Topic &quot; + outputTopic);","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Stream Processing","slug":"Knowledge-Management/Machine-Learning/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Stream-Processing/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://dobachi.github.io/memo-blog/tags/TensorFlow/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Kafka Streams","slug":"Kafka-Streams","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka-Streams/"}]},{"title":"Behavioral economics","slug":"Behavioral-economics","date":"2019-11-15T00:55:04.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/11/15/Behavioral-economics/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/15/Behavioral-economics/","excerpt":"","text":"参考 メモ 参考 わずか15年で3件ものノーベル賞を出した「行動経済学」の知見が、仕事の役に立ちすぎる件。 メモ それは「人は、出された質問が難しいと、それを簡単な質問に置き換えてしまう」ように脳ができているからだ。 これは、心理学と経済学の融合を果たした「行動経済学」という学問分野の知見による。 という記述を皮切りに、行動経済学の初歩を紹介するブログ。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Management","slug":"Clipping/Management","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Management/"}],"tags":[{"name":"Behavioral Economics","slug":"Behavioral-Economics","permalink":"https://dobachi.github.io/memo-blog/tags/Behavioral-Economics/"}]},{"title":"Studying Software Engineering Patterns for Designing Machine Learning Systems","slug":"Studying-Software-Engineering-Patterns-for-Designing-Machine-Learning-Systems","date":"2019-11-12T13:54:19.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/11/12/Studying-Software-Engineering-Patterns-for-Designing-Machine-Learning-Systems/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/12/Studying-Software-Engineering-Patterns-for-Designing-Machine-Learning-Systems/","excerpt":"","text":"参考 メモ 興味深かった図 参考 Studying Software Engineering Patterns for Designing Machine Learning Systems スライド メモ スライド に大まかな内容が記載されている。 機械学習に関するソフトウェアエンジニアリングやアーキテクチャデザインパターンに関する調査。 良い/悪いソフトウェアエンジニアリングデザインパターンを調査。 「リサーチ質問」（RQ）という形でいくつか確認事項を用意し、 ヒアリングや検索エンジンによる調査を実施。 SLR（Systematic literature review）手法にのっとり、調査結果を検証。 ヒアリングは760超の開発者を対処に実施し、1%の回答者を得た。 調査は、23個の論文、追加の6個の論文、48個のグレードキュメントを調査。 アカデミックではシステムエンジニアリングデザインパターンの文献は少ない。 グレードキュメントでは多数。データマネジメント観点が多い。 パターン整理では以下の軸を用いた。 Microsoftのパイプライン（9ステージ） ISO/IEC/IEEE 12207:2008 standard ドキュメント群から69個のパターンを抽出。MLのアーキテクチャやデザインパターン関係は33個。 興味深かった図 業務ロジックとMLロジックを分離。 メンテナンス性が上がるだろうが、果たしてロジックを現実的に分離可能かどうかはやや疑問。 Fig. 2. Structure of Distinguish Business Logic from ML Model pattern モデルに関するドキュメントが多数。 TABLE III CLASSIFICATION OF THE IDENTIFIED PATTERNS","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Software Engineering Patterns","slug":"Knowledge-Management/Machine-Learning/Software-Engineering-Patterns","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Software-Engineering-Patterns/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Software Engineering Patterns","slug":"Software-Engineering-Patterns","permalink":"https://dobachi.github.io/memo-blog/tags/Software-Engineering-Patterns/"}]},{"title":"Git SDK for Windows","slug":"Git-SDK-for-Windows","date":"2019-11-09T13:42:58.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/11/09/Git-SDK-for-Windows/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/09/Git-SDK-for-Windows/","excerpt":"","text":"参考 メモ 参考 git-sdk-1.0.5 メモ pacmanをgit bash上で使いたいと思ったのだが、git-sdkはいまいち。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"Git","slug":"Knowledge-Management/Tools/Git","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/Git/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://dobachi.github.io/memo-blog/tags/Windows/"},{"name":"Git","slug":"Git","permalink":"https://dobachi.github.io/memo-blog/tags/Git/"}]},{"title":"Docker on VMWare","slug":"Docker-on-VMWare","date":"2019-11-09T12:33:42.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/11/09/Docker-on-VMWare/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/09/Docker-on-VMWare/","excerpt":"","text":"参考 メモ Docker環境 参考 Docker ToolboxをVMware Workstationで使う Hyper-VでなくVMwareでDocker for Windows を使う Get Docker Engine - Community for Ubuntu メモ Hyper-VでなくVMwareでDocker for Windows を使う に習った。 Docker環境 手元の環境は Windows + VMWare だったので、Vagrant + VMWareで環境を構築し、その中にDockerをインストールすることにした。 Get Docker Engine - Community for Ubuntu を参考に、dockerをインストールするようにした。 参考までに、Vagrantfileは以下のような感じである。 ubuntu18系をベースとしつつ、プロビジョニングの際にDockerをインストールする。 ただし、以下のVagrantfileのプロビジョニングでは、DockerサービスにTCPで接続することを許可するようにしている。 これはセキュリティ上問題があるため、注意して取り扱うこと。（あくまで動作確認程度に留めるのがよさそう） Vagrantfile 123456789101112131415161718192021222324252627Vagrant.configure(&quot;2&quot;) do |config| config.vm.define &quot;docker-01&quot; do |config| config.vm.box = &quot;bento/ubuntu-18.04&quot; config.vm.network :private_network, ip: &quot;172.16.19.220&quot; config.vm.hostname = &quot;docker-01&quot; end config.vm.provision &quot;shell&quot;, inline: &lt;&lt;-SHELL apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - apt-key fingerprint 0EBFCD88 add-apt-repository \\ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable&quot; apt-get update apt-get install -y docker-ce docker-ce-cli containerd.io sed -i &quot;s;fd://;tcp://0.0.0.0:2375;g&quot; /lib/systemd/system/docker.service SHELLend WSLなどから以下のようにすることで、VM内のDockerにアクセスできる。 1$ sudo docker --host tcp://172.16.19.220:2375 run hello-world 環境変数として、DOCKER_HOSTを設定しておいても良いだろう。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Windows","slug":"Knowledge-Management/Windows","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/"},{"name":"Docker","slug":"Knowledge-Management/Windows/Docker","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/Docker/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://dobachi.github.io/memo-blog/tags/Windows/"},{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"},{"name":"VMWare","slug":"VMWare","permalink":"https://dobachi.github.io/memo-blog/tags/VMWare/"}]},{"title":"Kafka cluster information","slug":"Kafka-cluster-information","date":"2019-11-05T14:55:22.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/11/05/Kafka-cluster-information/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/05/Kafka-cluster-information/","excerpt":"","text":"参考 メモ 例としてBroker その他 参考 メモ kafka.clusterパッケージ以下には、Kafkaクラスタの情報を格納するためのクラス群が存在している。 具体的には以下の通り。 Broker BrokerEndPoint Cluster EndPoint Partition.scala Replica 例としてBroker 例えば Broker クラスについて。 case classである。 usageを確認すると、例えば以下のように KafkaServer#createBrokerInfo メソッド内で用いられている。 kafka/server/KafkaServer.scala:430 1BrokerInfo(Broker(config.brokerId, updatedEndpoints, config.rack), config.interBrokerProtocolVersion, jmxPort) その他 わかりやすい例だと、PartitionやReplicaなどが挙げられる。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"Kafka Admin Commands","slug":"Kafka-Admin-Commands","date":"2019-11-04T14:12:19.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/11/04/Kafka-Admin-Commands/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/04/Kafka-Admin-Commands/","excerpt":"","text":"参考 メモ Confluentドキュメント 実装 参考 Kafka公式ドキュメント Confluentドキュメント Clouderaのドキュメント（Kafka Administration Using Command Line Tools） ConfluentドキュメントのAdminister章 メモ 意外とまとまった説明は Kafka公式ドキュメント や Confluentドキュメント にはない。 Getting startedや運用面のドキュメントに一部含まれている。 丁寧なのは、 Clouderaのドキュメント（Kafka Administration Using Command Line Tools） である。 Confluentドキュメント ConfluentドキュメントのAdminister章 には、ツールとしてのまとまりではなく、 Admin作業単位で説明があり、その中にいくつかツールの説明が含まれている。 実装 kafka.admin パッケージ以下にAminコマンドの実装が含まれている。 またそれらのクラスは、 bin以下に含まれている。 例えば、 kafka-log-dirs コマンドでは、kafka.admin.LogDirsCommand クラスが使われている、など。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"KafkaのJavaバージョン","slug":"Kafka-Java-version","date":"2019-11-03T14:17:21.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/11/03/Kafka-Java-version/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/03/Kafka-Java-version/","excerpt":"","text":"参考 メモ 関連しそうなIssue、メーリングリストのエントリ 参考 6.4 Java Version メモ 6.4 Java Version の通り、JDK1.8の最新リリースバージョンを使うように、とされている。 2019/11/03時点の公式ドキュメントでは、LinkedInでは1.8u5を使っているとされているが…。 このあたりの記述が最も最近でいつ編集されたか、というと、 以下の通り2018/5/21あたり。 1234567891011121314151617181920212223commit e70a191d3038e00790aa95fbd1e16e78c32b79a4Author: Ismael Juma &lt;ismael@juma.me.uk&gt;Date: Mon May 21 23:17:42 2018 -0700 KAFKA-4423: Drop support for Java 7 (KIP-118) and update deps (#5046)(snip)diff --git a/docs/ops.html b/docs/ops.htmlindex 450a268a2..95b9a9601 100644--- a/docs/ops.html+++ b/docs/ops.html@@ -639,9 +639,7 @@ From a security perspective, we recommend you use the latest released version of JDK 1.8 as older freely available versions have disclosed security vulnerabilities.- LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. If you decide to use the G1 collector (the current default) and you are still on JDK 1.7, make sure you are on u51 or newer. LinkedIn tried out u21 in testing, but they had a number of problems with the GC implementation in that version.-- LinkedIn&apos;s tuning looks like this:+ LinkedIn is currently running JDK 1.8 u5 (looking to upgrade to a newer version) with the G1 collector. LinkedIn&apos;s tuning looks like this: &lt;pre class=&quot;brush: text;&quot;&gt; -Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M 関連しそうなIssue、メーリングリストのエントリ あまり活発な議論はない。公式ドキュメントを参照せよ、というコメントのみ。 Jay Kepsによると2013年くらいはLinkedInでは1.6を使っていた、ということらしい。 順当に更新されている。 https://issues.apache.org/jira/browse/KAFKA-7328 https://sematext.com/opensee/m/Kafka/uyzND19j3Ec5wqz42?subj=Re+kafka+0+9+0+java+version https://sematext.com/opensee/m/Kafka/uyzND138NFX1w26SP1?subj=Re+java+version+for+kafka+clients","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"OpenMLを軽く確認してみる","slug":"OpenML","date":"2019-11-01T06:33:39.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/11/01/OpenML/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/11/01/OpenML/","excerpt":"","text":"参考 メモ 概要 Dataset Task Flow Run scikit-learnで試す トラブルシュート 所感 参考 OpenML公式ウェブサイト OpenML公式ドキュメント scikit-learnでの使用 Pythonガイド github.ioのPythonガイド FlowとRunのチュートリアル 実行に用いたpipenvの環境ファイル dobachi openml_sklearn_example 公式Examples メモ 概要 OpenML公式ドキュメント によると、以下の定義。 An open, collaborative, frictionless, automated machine learning environment. OpenML公式ウェブサイト によると、 Dataset Task データセットと機械学習としての達成すべきこと Flow 各種フレームワークに則った処理パイプライン Run あるFlowについてハイパーパラメータを指定し、あるTaskに対して実行したもの のレポジトリが提供されている。 これらはウェブサイトから探索可能。 Dataset Datasetが登録されると機械的にアノテーションされたり、分析されたりする。 パット見でデータの品質がわかるようになっている。 Task データセットに対する目標（と言ってよいのか。つまり識別、回帰など）、学習用、テスト用のデータのスプリット、 どのカラムを目的変数とするか、などが セットになったコンテナである。 機械的に読めるようになっている。 Flow 処理パイプラインはFlowとして登録され、再利用可能になる。 特定のTaskに紐づく処理として定義される。 フレームワークにひもづく。 Run OpenML APIを用いて実行ごとに自動登録される。 scikit-learnで試す Pythonガイド を参考に試してみる。 一通り、Pandas、scikit-learn、openml、pylzmaをインストールする。 なお、あらかじめliblzma-devをインストールしておくこと。 詳しくは以下の「lzmaがインポートできない」節を参照。 実行に用いたpipenvの環境ファイル に用いたライブラリが載っている。 Pythonガイド を眺めながら進めようと思ったが、 https://openml.github.io/openml-python/develop/examples/introduction_tutorial.html のリンクが切れていた。 何となく見るとgithub.ioだったので探索してみたら、 github.ioのPythonガイド が見つかった。 こっちを参考にしてみる。 FlowとRunのチュートリアル を元に、チュートリアルを実施。 また特に、 公式Examples あたりを参考にした。 詳しくは、 dobachi openml_sklearn_example 内のnotebookを参照。 トラブルシュート lzmaがインポートできない 以下の内容のエラーが生じた。 12/home/dobachi/.local/share/virtualenvs/openml_sklearn_example-YW762zpK/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError. warnings.warn(msg) lzma関連のlibをインストールしておいてから、Python環境を構築すること。 つまりpyenvとpipenvで環境構築する前にliblzma-devをインストールしておく。 1$ sudo apt install liblzma-dev 参考： lzmaライブラリをインポートできません。 所感 歴史があり、情報が集まっている。 ドキュメントが中途半端なせいで、初見ではとっつきづらいかもしれないが、ライブラリひとつでデータ発掘・共有に始まり、処理パイプラインの機械化まで対応しているのは有益。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"OpenML","slug":"OpenML","permalink":"https://dobachi.github.io/memo-blog/tags/OpenML/"},{"name":"ML Model Management","slug":"ML-Model-Management","permalink":"https://dobachi.github.io/memo-blog/tags/ML-Model-Management/"}]},{"title":"pyenv を使う","slug":"pyenv","date":"2019-10-25T14:31:45.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/10/25/pyenv/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/10/25/pyenv/","excerpt":"","text":"参考 メモ インストール手順 トラブルシューティング 古い内容のバックアップ 参考 環境構築 - pipenv で Python 環境の構築方法 (2018年11月版) pyenvのGitHub pyenvのインストール手順 自動インストール Common-build-problems メモ インストール手順 WSL / WSL2にインストールするには、 pyenvのGitHub の通り、以下のようにインストールする。 1234$ git clone https://github.com/pyenv/pyenv.git ~/.pyenv$ echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc$ echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bashrc$ echo -e 'if command -v pyenv 1&gt;/dev/null 2&gt;&amp;1; then\\n eval \"$(pyenv init -)\"\\nfi' &gt;&gt; ~/.bashrc トラブルシューティング Common-build-problems に載っているが、前提となるパッケージのインストールが必要なことに注意。 古い内容のバックアップ 環境構築 - pipenv で Python 環境の構築方法 (2018年11月版) ではpipenvを使う前に、pyenvを使ってPython3最新環境を導入している。 自動インストール に自動インストール手順が記載されている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Python","slug":"Knowledge-Management/Python","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Python/"},{"name":"pyenv","slug":"Knowledge-Management/Python/pyenv","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Python/pyenv/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"pyenv","slug":"pyenv","permalink":"https://dobachi.github.io/memo-blog/tags/pyenv/"}]},{"title":"pipenvを試す","slug":"pipenv","date":"2019-10-22T13:59:34.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/10/22/pipenv/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/10/22/pipenv/","excerpt":"","text":"参考 メモ WSL上でのpipenv 参考 Pipenvことはじめ pyenv、pyenv-virtualenv、venv、Anaconda、Pipenv。私はPipenvを使う。 Pipenvを使ったPython開発まとめ pyenvのインストール手順 Pipenvことはじめ メモ 上記の参考情報を見て試した。 WSL上でのpipenv まずOS標準のPython環境を汚さないため、pyenvを導入した。 導入手段は、 pyenvのインストール手順 を参照。 導入した後、pyenvを使ってPython3.7.5を導入する。 123$ pyenv install 3.7.5$ pyenv global 3.7.5$ pip install pipenv さて、ここでpipenvを使おうとしたらエラーが生じた。 以下の参考に、 libffi-devを導入してPythonを再インストールしたらうまくいった。 https://stackoverflow.com/questions/27022373/python3-importerror-no-module-named-ctypes-when-using-value-from-module-mul Pipenvことはじめ に大まかな使い方が載っている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Python","slug":"Knowledge-Management/Python","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Python/"},{"name":"Pipenv","slug":"Knowledge-Management/Python/Pipenv","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Python/Pipenv/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Pipenv","slug":"Pipenv","permalink":"https://dobachi.github.io/memo-blog/tags/Pipenv/"}]},{"title":"Automagicaを試してみる","slug":"Automagica","date":"2019-10-22T13:07:48.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/10/22/Automagica/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/10/22/Automagica/","excerpt":"","text":"参考 メモ 総合的感想 Getting startedを試す 環境構築 Example 1の実行 参考 Automagicaの公式ドキュメント AutomagicaのGetting started Example 1 メモ 総合的感想 用途ごとに個別に開発されているライブラリを統合して使えるのは便利。エントリポイントが統一化され、 from automagica import * ですべての機能が使えるのは画期的である。 しかし以下の点は少し課題を感じた。 ユーザ目線で似たもの同士に感じる機能に関し、APIが異なる 具体的にはWord、Excel操作 ただし内部的に用いられるライブラリが別々のものなので致し方ない。（うまくすれば、APIレベルでも統合可能かもしれない） 内部的に用いられているライブラリのAPIを直接利用したいケースがあるが少しわかりづらい Automagicaが提供しているAPIよりも、内部的に用いられている元ネタのライブラリのAPIの方が機能豊富（少なくとも docx ライブラリはそうだと思う） ということから、まずはAutomagicaのAPIを基本として使用しつつ、必要に応じて内部で用いられているライブラリのAPIを直接利用するのが良さそう。 実際に OpenWordDocument メソッドの戻り値は docx.Document クラスのインスタンス。 しかし1個目の課題と関連するが、その使い方を想起させるドキュメントやAPI仕様に見えない。 Word activities以外ではどうなのか、は、細かくは未確認。 ドキュメントにバグがちらほら見られるのと、ドキュメント化されていない機能がある。 これは改善されるだろう Getting startedを試す 環境構築 AutomagicaのGetting started を参考にしつつ、動かしてみる。 Anacondaにはパッケージが含まれていなかった（conda-forge含む）のでpipenvを使って環境づくり。 123&gt; pipenv.exe --python 3.7&gt; pipenv.exe install jupyter&gt; pipenv.exe install https://github.com/OakwoodAI/automagica/tarball/master なお、Python3.8系では依存関係上の問題が生じたので、いったん切り分けも兼ねて3.7で環境構築している。 AutomagicaのGetting started では、Tesseract4のインストールもオプションとして示されていたが、いったん保留とする。 Example 1の実行 Example 1 に載っていた例を動かそうとしたが、一部エラーが生じたので手直しして実行した。 エラー生じた箇所1： ドキュメント上は、以下のとおり。 1lookup_terms.append(ExcelReadCell(excel_path, 2, col)) しかし、行と列の番号で指定するAPIは、 ExcelReadRowCol である。 また、例に示されていたエクセルでは1行目に検索対象文字列が含まれているため、第2引数は 2 ではなく、 1 である。 したがって、 1lookup_terms.append(ExcelReadRowCol(excel_path, 1, col)) とした。 なお、ExcelReadCellメソッド、ExcelReadRowColメソッドの実装は以下の通り。（2019/10/24現在） automagica/activities.py:639 123456789101112131415def ExcelReadCell(path, cell=&quot;A1&quot;, sheet=None): &apos;&apos;&apos; Read a cell from an Excel file and return its value. Make sure you enter a valid path e.g. &quot;C:\\\\Users\\\\Bob\\\\Desktop\\\\RPA Examples\\\\data.xlsx&quot;. The cell you want to read needs to be defined by a cell name e.g. &quot;A2&quot;. The third variable is a string with the name of the sheet that needs to be read. If omitted, the function reads the entered cell of the active sheet. &apos;&apos;&apos; workbook = load_workbook(path) if sheet: worksheet = workbook.get_sheet_by_name(sheet) else: worksheet = workbook.active return worksheet[cell].value automagica/activities.py:656 12345678910111213141516def ExcelReadRowCol(path, r=1, c=1, sheet=None): &apos;&apos;&apos; Read a Cell from an Excel file and return its value. Make sure you enter a valid path e.g. &quot;C:\\\\Users\\\\Bob\\\\Desktop\\\\RPA Examples\\\\data.xlsx&quot;. The cell you want to read needs to be row and a column. E.g. r = 2 and c = 3 refers to cell C3. The third variable needs to be a string with the name of the sheet that needs to be read. If omitted, the function reads the entered cell of the active sheet. First row is defined row number 1 and first column is defined column number 1. &apos;&apos;&apos; workbook = load_workbook(path) if sheet: worksheet = workbook.get_sheet_by_name(sheet) else: worksheet = workbook.active return worksheet.cell(row=r, column=c).value エラーが生じた箇所2： ドキュメント上では、読み込んだ文字列のリストをそのまま使っているが、 そのままではリスト後半に None が混ざることがある。 そこで、リストをフィルタするようにした。 1lookup_terms = list(filter(lambda x: x is not None, lookup_terms)) エラーが生じた箇所3： ドキュメント上では、以下の通り。 1ExcelWriteCell(excel_path, row=i+2, col=j+2, write_value=url) これも読み込みの場合と同様にAPIが異なる。合わせて引数名が異なる。そこで、 1ExcelWriteRowCol(excel_path, r=i+2, c=j+2, write_value=url) とした。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"Selenium","slug":"Knowledge-Management/Tools/Selenium","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/Selenium/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Automagica","slug":"Automagica","permalink":"https://dobachi.github.io/memo-blog/tags/Automagica/"},{"name":"RPA","slug":"RPA","permalink":"https://dobachi.github.io/memo-blog/tags/RPA/"}]},{"title":"Seleniumを試してみる","slug":"Selenium","date":"2019-10-19T12:51:25.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/10/19/Selenium/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/10/19/Selenium/","excerpt":"","text":"参考 メモ 環境構築 便利な情報源 Selenium関連 おまけ 参考 【超便利】PythonとSeleniumでブラウザを自動操作する方法まとめ ブラウザのヘッドレスモードでスクショ 【WSL】にFirefox&amp;Google-Chromeをインストールする方法！ geckodriver geckodriver-v0.26.0-linux64.tar.gz PythonからSeleniumを使ってFirefoxを操作してみる selenium_getting_started PythonでSeleniumを使ってスクレイピング (基礎) Seleniumで要素を選択する方法まとめ 4. 要素を見つける Pythonで取得したWebページのHTMLを解析するはじめの一歩 Seleniumで待機処理するお話 メモ 日常生活の簡単化のために使用。 【超便利】PythonとSeleniumでブラウザを自動操作する方法まとめ を試す。 環境構築 Condaを使って環境を作り、seleniumをインストールした。 WebDriverは自分の環境に合わせて、Chrome77向けのものを使用したかっただが、 WSLでChromeを使おうとすると、--no-sandboxをつけないと行けないようだ。 詳しくは、 ブラウザのヘッドレスモードでスクショ 及び 【WSL】にFirefox&amp;Google-Chromeをインストールする方法！ を参照。 geckodriver を使うことにした。 また、当初は 【超便利】PythonとSeleniumでブラウザを自動操作する方法まとめ を参考にしようとしたが、 途中から PythonからSeleniumを使ってFirefoxを操作してみる を参考にした。 動作確認ログは、 selenium_getting_started を参照。 便利な情報源 Selenium関連 PythonからSeleniumを使ってFirefoxを操作してみる Firefoxを使ってみる例 geckodriver FirefoxのWebDriverを得る場所 【WSL】にFirefox&amp;Google-Chromeをインストールする方法！ WSLにブラウザを導入する例 Chromeでの苦労についても触れられている PythonでSeleniumを使ってスクレイピング (基礎) スクレイピングの面で使いそうな基本機能の説明 Seleniumで要素を選択する方法まとめ 要素選択の基本的な例 汎用性が高いXPathを使う例も記載 4. 要素を見つける 上記のXPathを使う例なども含め、網羅的に記載されている。 Seleniumで待機処理するお話 waitする方法 おまけ Pythonで取得したWebページのHTMLを解析するはじめの一歩 おそらくBeautiful SoupによるHTML解析も伴うことが多いだろう、ということで。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"Selenium","slug":"Knowledge-Management/Tools/Selenium","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/Selenium/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Slenium","slug":"Slenium","permalink":"https://dobachi.github.io/memo-blog/tags/Slenium/"}]},{"title":"Conferences at Dec. 2019","slug":"Conferences-at-Dec-2019","date":"2019-10-14T06:48:50.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/10/14/Conferences-at-Dec-2019/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/10/14/Conferences-at-Dec-2019/","excerpt":"","text":"参考 Middleware 2019 LISA19 USENIX ATC'20 IEEE Big Data 2019 メモ ACM/IFIP International Conference MIDDLEWARE 2019 趣旨 キーノートスピーカー 採録論文 LISA19 概要 Conference Program 2020 USENIX Conference on Operational Machine Learning USENIX ATC '20 IEEE Big Data 2019 参考 Middleware 2019 ACM/IFIP International Conference MIDDLEWARE 2019 Middleware 2019 - Accepted Papers LISA19 LISA19 LISA19 conference program USENIX ATC'20 USENIX ATC20 IEEE Big Data 2019 IEEE Big Data 2019 メモ 気になるカンファレンスをメモ。 ACM/IFIP International Conference MIDDLEWARE 2019 ACM/IFIP International Conference MIDDLEWARE 2019 2019/12/9 - 13 UC DAVIS, CA, US 悪くなさそう。 趣旨 The annual Middleware conference is a major forum for the discussion of innovations and recent scientific advances of middleware systems with a focus on the design, implementation, deployment, and evaluation of distributed systems, platforms and architectures for computing, storage, and communication. Highlights of the conference will include a high quality single-track technical program, invited speakers, an industrial track, panel discussions involving academic and industry leaders, poster and demonstration presentations, a doctoral symposium, tutorials and workshops. ということから、ミドルウェアに関する技術要素についてのカンファレンスと理解。 キーノートスピーカー Brian F. Cooper (Google) Title: Why can't the bits sit still? The never ending challenge of data migrations. Monica Lam (STANFORD UNIVERSITY) Title: Building the Smartest and Open Virtual Assistant to Protect Privacy Ion Stoica (BERKELEY) Title: To unify or to specialize? 採録論文 Middleware 2019 - Accepted Papers に一覧が載っている。 気になった論文は以下の通り。 Scalable Data-structures with Hierarchical, Distributed Delegatio FfDL: A Flexible Multi-tenant Deep Learning Platform Monitorless: Predicting Performance Degradation in Cloud Applications with Machine Learning Automating Multi-level Performance Elastic Components for IBM Streams Self-adaptive Executors for Big Data Processing Differential Approximation and Sprinting for Multi-Priority Big Data Engines Combining it all: Cost minimal and low-latency stream processing across distributed heterogenous infrastructures ReLAQS: Reducing Latency for Multi-Tenant Approximate Queries via Scheduling Medley: A Novel Distributed Failure Detector for IoT Network LISA19 LISA19 概要 LISA is the premier conference for operations professionals, where we share real-world knowledge about designing, building, securing, and maintaining the critical systems of our interconnected world. とのことであり、開発や運用に関するカンファレンス。 Conference Program LISA19 conference program 気になったセッションは以下の通り。 Pulling the Puppet Strings with Ansible Linux Systems Performance The Challenges of Managing Open Source Infrastructure at Bloomberg Jupyter Notebooks for Ops その他Kubernetes関連のセッションが複数 2020 USENIX Conference on Operational Machine Learning USENIX OpML'20 May 1, 2020 Santa Clara, CA, United States Hyatt Regency Santa Clara USENIX ATC '20 USENIX ATC20 JULY 1517, 2020 BOSTON, MA, USA IEEE Big Data 2019 IEEE Big Data 2019 Los Angeles, CA, USA 9-12, Dec. 気にはなる。 参考： IEEE Big Data 2018の気になった論文リスト 2018年度の気になった論文によると、情報システムというよりは要素技術や分析手法などの方が主旨のようだ。","categories":[],"tags":[]},{"title":"Scraping with session information using Python","slug":"Scraping-with-session-information-using-Python","date":"2019-10-13T23:51:54.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/10/14/Scraping-with-session-information-using-Python/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/10/14/Scraping-with-session-information-using-Python/","excerpt":"","text":"参考 メモ 参考 RequestsでSessionモード メモ あとでメモを追加する","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Open Data","slug":"Knowledge-Management/Open-Data","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Open-Data/"},{"name":"Scraping","slug":"Knowledge-Management/Open-Data/Scraping","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Open-Data/Scraping/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Scraping","slug":"Scraping","permalink":"https://dobachi.github.io/memo-blog/tags/Scraping/"},{"name":"Open Data","slug":"Open-Data","permalink":"https://dobachi.github.io/memo-blog/tags/Open-Data/"},{"name":"Session","slug":"Session","permalink":"https://dobachi.github.io/memo-blog/tags/Session/"}]},{"title":"Redshift vs. Snowflake","slug":"Redshift-vs-Snowflake","date":"2019-09-28T22:42:13.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/09/29/Redshift-vs-Snowflake/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/09/29/Redshift-vs-Snowflake/","excerpt":"","text":"参考 メモ 参考 Redshift vs. Snowflake メモ RedshiftとSnowflakeを長所、短所の面からまとめた記事。 特にSnowflakeにしかない特徴を紹介する部分は、初心者には参考になりそう。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Database","slug":"Clipping/Database","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Database/"}],"tags":[{"name":"Snowflake","slug":"Snowflake","permalink":"https://dobachi.github.io/memo-blog/tags/Snowflake/"},{"name":"Redshift","slug":"Redshift","permalink":"https://dobachi.github.io/memo-blog/tags/Redshift/"},{"name":"RDBMS","slug":"RDBMS","permalink":"https://dobachi.github.io/memo-blog/tags/RDBMS/"}]},{"title":"Efficient and Robust Automated Machine Learning","slug":"Efficient-and-Robust-Automated-Machine-Learning","date":"2019-09-22T08:27:47.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/09/22/Efficient-and-Robust-Automated-Machine-Learning/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/09/22/Efficient-and-Robust-Automated-Machine-Learning/","excerpt":"","text":"参考 論文メモ 概要 1. Introduction 2 AutoML as a CASH problem 3 New methods for increasing efﬁciency and robustness of AutoML 5 Comparing AUTO-SKLEARN to AUTO-WEKA and HYPEROPT-SKLEARN 6 Evaluation of the proposed AutoML improvements 7 Detailed analysis of AUTO-SKLEARN components 動作確認 依存ライブラリのインストール pipenv使って環境構築 依存関係とパッケージをインストールしようとしたらエラーで試行錯誤 依存関係のインストールとパッケージのインストール（2度め） 参考）CentOS7ではSwig3をインストールすること メモリエラー 参考 Efficient and Robust Automated Machine Learning NIPS OpenML auto-sklearnのGitHub auto-sklearnのドキュメント インストール手段 論文メモ 気になったところをメモする。 概要 いわゆるAutoMLの一種。 特徴量処理、アルゴリズム選択、ハイパーパラメータチューニングを実施。 さらに、メタ学習とアンサンブル構成も改善として対応。 scikit-learnベースのAutoML。 auto-sklearnと呼ぶ。 GitHub上にソースコードが公開されている。 同様のデータセットに対する過去の性能を考慮し、アンサンブルを構成する。 ChaLearn AutoMLチャレンジで勝利。 OpenMLのデータセットを用いて、汎用性の高さを確認した。 2019/11時点では、識別にしか対応していない？ 1. Introduction 以下のあたりのがAutoML周りのサービスとして挙げられていた。 12BigML.com, Wise.io, SkyTree.com, RapidMiner.com, Dato.com, Prediction.io, DataRobot.com, Microsoft’s Azure MachineLearning, Google’s Prediction API, and Amazon Machine Learning Auto-wekaをベースとしたもの。 パラメトリックな機械学習フレームワークと、ベイジアン最適化を組み合わせて用いる。 特徴 新しいデータセットに強い。ベイジアン最適化をウォームスタートする。 自動的にモデルをアンサンブルする 高度にパラメタライズされたフレームワーク 数多くのデータセットで実験 2 AutoML as a CASH problem ここで扱う課題をCombined Algorithm Selection and Hyperparameter optimization (CASH) と呼ぶ。 ポイントは、 ひとつの機械学習手法がすべてのデータセットに対して有効ではない 機械学習手法によってはハイパーパラメータチューニングが必要 ということ。 それらを単一の最適化問題として数式化できる。 Auto-wekaで用いられたツリーベースのベイジアン最適化は、ガウシアンプロセスモデルを用いる。 低次元の数値型のハイパパラメータには有効。 しかし、ツリーベースのアルゴリズムは高次元で離散値にも対応可能。 Hyperopt-sklearnのAutoMLシステムで用いられている。 提案手法では、ランダムフォレストベースのSMACを採用。 3 New methods for increasing efﬁciency and robustness of AutoML 提案手法のポイント ベイジアン最適化のウォームスタート アンサンブル構築の自動化 まずウォームスタートについて。 メタ学習は素早いが粗い。ベイジアン最適化はスタートが遅いが精度が高い。 これを組み合わせる。 既存手法でも同様の試みがあったが、複雑な機械学習モデルや多次元のパラメータには適用されていない？ OpenML を利用し、メタ学習。 つづいてアンサンブル構築について。 単一の結果を用いるよりも、アンサンブルを組んだほうが良い。 アンサンブル構成の際には重みを用いる。重みの学習にはいくつかの手法を試した。 Caruana et al. のensemble selectionを利用。 5 Comparing AUTO-SKLEARN to AUTO-WEKA and HYPEROPT-SKLEARN AUTO-SKLEARNをそれぞれ既存手法であるAUTO-WEKAとHYPEROPT-SKLEARNと比較。 （エラーレート？で比較） AUTO-WEKAに比べて概ね良好。 一方HYPEROPT-SKLEARNは正常に動かないことがあった。 6 Evaluation of the proposed AutoML improvements OpenMLレポジトリのデータを使って、パフォーマンスの一般性を確認。 テキスト分類、数字や文字の認識、遺伝子やRNAの分類、広告、望遠鏡データの小片分析、組織からのがん細胞検知。 メタ学習あり・なし、アンサンブル構成あり・なしで動作確認。 7 Detailed analysis of AUTO-SKLEARN components ひとつひとつの手法を解析するのは時間がかかりすぎた。 ランダムフォレスト系統、AdaBoost、Gradient boostingがもっともロバストだった。 一方、SVMが特定のデータセットで性能高かった。 どのモデルも、すべてのケースで性能が高いわけではない。 個別の前処理に対して比較してみても、AUTO-SKLEARNは良かった。 動作確認 依存ライブラリのインストール auto-sklearnのドキュメント を参考に動かしてみる。 予め、ビルドツールをインストールしておく。 1$ sudo apt-get install build-essential swig pipenv使って環境構築 pipenvを使って、おためし用の環境を作る。 1$ pipenv --python 3.7 依存関係とパッケージをインストールしようとしたらエラーで試行錯誤 今回はpipenvを使うので、公式手順を少し修正して実行。（pip -&gt; pipenvとした） 1curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pipenv install なお、requirementsの中身は以下の通り。 1234567891011121314151617181920setuptoolsnoseCythonnumpy&gt;=1.9.0scipy&gt;=0.14.1scikit-learn&gt;=0.21.0,&lt;0.22lockfilejoblibpsutilpyyamlliac-arffpandasConfigSpace&gt;=0.4.0,&lt;0.5pynisher&gt;=0.4.2pyrfr&gt;=0.7,&lt;0.9smac==0.8 論文の通り、SMACが使われるようだ。 つづいて、auto-sklearn自体をインストール。 （最初から、これを実行するのではだめなのだろうか？勝手に依存関係を解決してくれるのでは？） 1$ pipenv run pip install auto-sklearn なお、最初に pipenv install auto-sklearn していたのだがエラーで失敗したので、上記のようにvirtualenv環境下でpipインストールすることにした。 エラー内容は以下の通り。 123456789101112(snip)[pipenv.exceptions.ResolutionFailure]: req_dir=requirements_dir[pipenv.exceptions.ResolutionFailure]: File &quot;/home/dobachi/.pyenv/versions/3.7.5/lib/python3.7/site-packages/pipenv/utils.py&quot;, line 726, in resolve_deps[pipenv.exceptions.ResolutionFailure]: req_dir=req_dir,[pipenv.exceptions.ResolutionFailure]: File &quot;/home/dobachi/.pyenv/versions/3.7.5/lib/python3.7/site-packages/pipenv/utils.py&quot;, line 480, in actually_resolve_deps[pipenv.exceptions.ResolutionFailure]: resolved_tree = resolver.resolve()[pipenv.exceptions.ResolutionFailure]: File &quot;/home/dobachi/.pyenv/versions/3.7.5/lib/python3.7/site-packages/pipenv/utils.py&quot;, line 395, in resolve[pipenv.exceptions.ResolutionFailure]: raise ResolutionFailure(message=str(e))[pipenv.exceptions.ResolutionFailure]: pipenv.exceptions.ResolutionFailure: ERROR: ERROR: Could not find a version that matches scikit-learn&lt;0.20,&lt;0.22,&gt;=0.18.0,&gt;=0.19,&gt;=0.21.0(snip) 作業効率を考え、Jupyterをインストールしておく。 1$ pipenv install jupyter つづいて公式ドキュメントの手順を実行。 以下を実行したらエラーが出た。 1import autosklearn.classification 1ModuleNotFoundError: No module named &apos;_bz2&apos; scikit-learn で No module named '_bz2' というエラーがでる問題 の通り、 bzip2に関する開発ライブラリが足りていない、ということらしい。 パッケージをインストールし、Pythonを再インストール。pipenvのvirtualenv環境も削除。 その後、pipenvで環境を再構築。 1234$ sudo apt install libbz2-dev$ pyenv install 3.7.5$ pipenv --rm$ pipenv install なお、sklearnのバージョンでやはり失敗する？ よくみたら、pypiから2019/11/1時点でインストールできるライブラリは0.5.2であるが、 公式ドキュメントは0.6.0がリリースされているように見えた。 また対応するscikit-learnのバージョンが変わっている…。 依存関係のインストールとパッケージのインストール（2度め） 致し方ないので、依存関係のライブラリはpipenvのPipfileからインストールし、 auto-sklearnはGitHubからダウンロードしたmasterブランチ一式をインストールすることにした。 12$ pipenv install$ pipenv install ~/Downloads.win/auto-sklearn-master.zip\u001c 改めてJupyter notebookを起動 1$ pipenv run jupyter notebook 以下を実行したら、一応動いたけど… 1import autosklearn.classification 少し警告 1Could not import the lzma module. Your installed Python is incomplete. sklearn関係もインポート成功。 123import sklearn.model_selectionimport sklearn.datasetsimport sklearn.metrics 学習データ等の準備 123X, y = sklearn.datasets.load_digits(return_X_y=True)X_train, X_test, y_train, y_test = \\ sklearn.model_selection.train_test_split(X, y, random_state=1) 学習実行 12automl = autosklearn.classification.AutoSklearnClassifier()automl.fit(X_train, y_train) 実行したらPCのCPUが100%に張り付いた！ 12y_hat = automl.predict(X_test)print(&quot;Accuracy score&quot;, sklearn.metrics.accuracy_score(y_test, y_hat)) 参考）CentOS7ではSwig3をインストールすること CentOSで依存関係をインストールするときには、 swig ではなく swig3 を対象とすること。 でないと、以下のようなエラーが生じる。 12345(snip)----------------------------------------&apos;, &apos;ERROR: Command errored out with exit status 1: /home/centos/.local/share/virtualenvs/auto-sklearn-example-hzZc_yaE/bin/python3.7m -u -c \\&apos;import sys, setuptools, tokenize; sys.argv[0] = \\&apos;&quot;\\&apos;&quot;\\&apos;/tmp/pip-install-53cbwkis/pyrfr/setup.py\\&apos;&quot;\\&apos;&quot;\\&apos;; __file__=\\&apos;&quot;\\&apos;&quot;\\&apos;/tmp/pip-install-53cbwkis/pyrfr/setup.py\\&apos;&quot;\\&apos;&quot;\\&apos;;f=getattr(tokenize, \\&apos;&quot;\\&apos;&quot;\\&apos;open\\&apos;&quot;\\&apos;&quot;\\&apos;, open)(__file__);code=f.read().replace(\\&apos;&quot;\\&apos;&quot;\\&apos;\\\\r\\\\n\\&apos;&quot;\\&apos;&quot;\\&apos;, \\&apos;&quot;\\&apos;&quot;\\&apos;\\\\n\\&apos;&quot;\\&apos;&quot;\\&apos;);f.close();exec(compile(code, __file__, \\&apos;&quot;\\&apos;&quot;\\&apos;exec\\&apos;&quot;\\&apos;&quot;\\&apos;))\\&apos; install --record /tmp/pip-record-udozwz5v/install-record.txt --single-version-externally-managed --compile --install-headers /home/centos/.local/share/virtualenvs/auto-sklearn-example-hzZc_yaE/include/site/python3.7/pyrfr Check the logs for full command output.&apos;](snip) メモリエラー EC2インスタンス上で改めて実行したところ、以下のようなエラーを生じた。 123456789(snip) File &quot;/home/centos/.local/share/virtualenvs/auto-sklearn-example-hzZc_yaE/lib/python3.7/site-packages/autosklearn/smbo.py&quot;, line 352, in _calculate_metafeatures_encoded (snip)ImportError: /home/centos/.local/share/virtualenvs/auto-sklearn-example-hzZc_yaE/lib/python3.7/site-packages/sklearn/tree/_criterion.cpython-37m-x86_64-linux-gnu.so: failed to map segment from shared object: Cannot allocate memory(snip) これから切り分ける。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"AutoML","slug":"Knowledge-Management/Machine-Learning/AutoML","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/AutoML/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"AutoML","slug":"AutoML","permalink":"https://dobachi.github.io/memo-blog/tags/AutoML/"},{"name":"Machine Learning Lifecycle/","slug":"Machine-Learning-Lifecycle","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning-Lifecycle/"}]},{"title":"What is OpML","slug":"What-is-OpML","date":"2019-09-02T13:36:25.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/09/02/What-is-OpML/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/09/02/What-is-OpML/","excerpt":"","text":"参考 メモ OpMLについて USENIX OpML'19 の定義 コミッティー セッション セッションをピックアップして紹介 ソリューション関係 その他のセッション傾向 SYSML SYSMLの定義 SYSML'19 参考 USENIX OpML'19 The first conference of Operational Machine Learning OpML '19 メモ OpMLについて USENIX OpML'19のテックジャイアントたちが話題にしている主要なトピックからスコープを推測。 ヘテロジーニアスアーキテクチャの活用（エッジ・クラウドパラダイム） モデルサービング、レイテンシの改善 機械学習パイプライン 機械学習による運用改善（AIOps） 機械学習のデバッガビリティ改善 USENIX OpML'19 の定義 USENIX OpML'19 によると、 OpML = Operational Machine Learning の定義。 カンファレンスの定義は以下の通り。 The 2019 USENIX Conference on Operational Machine Learning (OpML '19) provides a forum for both researchers and industry practitioners to develop and bring impactful research advances and cutting edge solutions to the pervasive challenges of ML production lifecycle management. ML production lifecycle is a necessity for wide-scale adoption and deployment of machine learning and deep learning across industries and for businesses to benefit from the core ML algorithms and research advances. 上記では、機械学習の生産ライフサイクルの管理は、機械学習や深層学習が産業により広く用いられるため、またコアとなる機械学習アルゴリズムや研究からビジネス上のメリットを享受するために必要としている。 コミッティー カンファレンスサイトの情報から集計すると、以下のような割合だった。 多様な企業、組織からコミッティを集めているが、 Googleなど一部企業からのコミッティが多い。 セッション カンファレンスのセッションカテゴリは以下の通り。 カテゴリとしては、今回は実経験に基づく知見を披露するセッションが件数多めだった。 一方で、同じカテゴリでも話している内容がかなり異なる印象は否めない。 もう数段階だけサブ・カテゴライズできるようにも見え、まだトピックが体系化されていないことを伺わせた。 Google、LinkedIn、Microsoft等のテックジャイアントの主要なセッションを見ていて気になったキーワードを並べる。 ヘテロジーニアスアーキテクチャの活用（エッジ・クラウドパラダイム） モデルサービング、レイテンシの改善 機械学習パイプライン 機械学習による運用改善（AIOps） 機械学習のデバッガビリティ改善 その他アルゴリズムの提案も存在。 小さめの企業では、ML as a Serviceを目指した取り組みがいくつか見られた。 セッションをピックアップして紹介 実経験にもとづく知見のセッションを一部メモ。 Opportunities and Challenges Of Machine Learning Accelerators In Production タイトル: Opportunities and Challenges Of Machine Learning Accelerators In Production キーワード: TPU, Google, ヘテロジーニアスアーキテクチャ, 性能 スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_ananthanarayanan.pdf CPUでは近年の深層学習ワークロードの計算アルゴリズム、量において不十分（ボトルネックになりがち）だが、 TPU等のアーキテクチャにより改善。 しかしTPUが入ってくると「ヘテロジーニアスなアーキテクチャ」になる。 これをうまく使うための工夫が必要。 加えて近年のTPUについて紹介。 Accelerating Large Scale Deep Learning Inference through DeepCPU at Microsoft タイトル: Accelerating Large Scale Deep Learning Inference through DeepCPU at Microsoft キーワード: モデルサービング, レイテンシの改善, Microsoft, 性能 スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_zhang-minjia.pdf Microsoft。 CPU上でRNNを高速にサーブするためのライブラリ: DeepCPU 目標としていたレイテンシを実現するためのもの。 例えばTensorFlow Servingで105msかかっていた「質問に答える」という機能に関し、 目標を超えるレイテンシ4.1msを実現した。 スループットも向上。 A Distributed Machine Learning For Giant Hogweed Eradication タイトル: A Distributed Machine Learning For Giant Hogweed Eradication キーワード: Big Data基盤, モデルサービング, ドローン, 事例, NTTデータ スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19-nttd.pdf NTTデータ。 デンマークにおいて、ドローンで撮影した画像を利用し、危険外来種を見つける。 Big Data基盤と機械学習基盤の連係に関する考察。 AIOps: Challenges and Experiences in Azure タイトル: AIOps: Challenges and Experiences in Azure キーワード: Microsoft, 運用, 運用改善, AIOps スライド: 非公開 スライド非公開のため、概要から以下の内容を推測。 Microsoft。 Azureにて、AIOps（AIによるオペレーション？）を実現したユースケースの紹介。 課題と解決方法を例示。 Quasar: A High-Performance Scoring and Ranking Library タイトル: Quasar: A High-Performance Scoring and Ranking Library キーワード: LinkedIn, スコアリング, ランキング, 性能, フレームワーク スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_birkmanis.pdf スコアリングやランキング生成のためにLinkedInで用いられているライブラリ。 Quasar. AI from Labs to Production - Challenges and Learnings タイトル: AI from Labs to Production - Challenges and Learnings キーワード: AIの商用適用 スライド: 非公開 エンタープライズ向けにAIを使うのはB2CにAIを適用するときとは異なる。 MLOp Lifecycle Scheme for Vision-based Inspection Process in Manufacturing タイトル: MLOp Lifecycle Scheme for Vision-based Inspection Process in Manufacturing キーワード: Samsung Research, 運用, 深層学習, 製造業, 運用スキーム スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_lim.pdf Samsung Research。 製造業における画像検知？における運用スキームの提案。 異なるステークホルダが多数存在するときのスキーム。 学習、推論、それらをつなぐ管理機能を含めて提案。 ボルトの検査など。 Deep Learning Vector Search Service タイトル: Deep Learning Vector Search Service キーワード: Microsoft, Bing, 検索, ベクトルサーチ, ANN(Approximae Nearest Neighbor), 深層学習 スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_zhu.pdf 深層学習を用いてコンテンツをベクトル化する。 ベクトル間の関係がコンテンツの関係性を表現。 スケーラビリティを追求。 Signal Fabric An AI-assisted Platform for Knowledge Discovery in Dynamic System タイトル: Signal FabricAn AI-assisted Platform for Knowledge Discovery in Dynamic System キーワード: Microsoft, Azure, 運用改善のためのAI活用 スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_aghajanyan.pdf Microsoft。Azure。 動的に変化するパブリッククラウドのサービスの振る舞いを把握するためのSignal Fabric。 「AI」も含めて様々な手段を活用。 Relevance Debugging and Explaining at LinkedIn タイトル: Relevance Debugging and Explaining at LinkedIn キーワード: LinkedIn, AIのデバッグ, 機械学習基盤, デバッガビリティの改善 スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_qiu.pdf LinkedIn。 分散処理のシステムは複雑。AIエンジニアたちはそれを把握するのが難しい。 そこでそれを支援するツールを開発した。 様々な情報をKafkaを通じて集める。 それを可視化したり、クエリしたり。 Shooting the moving target: machine learning in cybersecurity タイトル: Shooting the moving target: machine learning in cybersecurity キーワード: セキュリティへの機械学習適用, 機械学習のモデル管理 スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_arun.pdf セキュリティへの機械学習適用。 基盤はモジューライズされ？、拡張可能で、 モデルを繰り返し更新できる必要がある。 Deep Learning Inference Service at Microsoft タイトル: Deep Learning Inference Service at Microsoft キーワード: 深層学習, 推論, Microsoft, ヘテロジーニアスアーキテクチャ スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_soifer.pdf Microsoft。 深層学習の推論に関する工夫。インフラ。 ヘテロジーニアス・リソースの管理、リソース分離、ルーティングなど。 数ミリ秒単位のレイテンシを目指す。 モデルコンテナをサーブする仕組み。 ソリューション関係 KnowledgeNet: Disaggregated and Distributed Training and Serving of Deep Neural Networks タイトル: KnowledgeNet: Disaggregated and Distributed Training and Serving of Deep Neural Networks キーワード: エッジ・クラウドパラダイム, DNN, 分散処理, ヘテロジーニアスアーキテクチャ スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_biookaghazadeh.pdf DNNは強化学習、物体検知、ビデオ処理、VR・ARなどに活用されている。 一方でそれらの処理はクラウド中心のパラダイムから、エッジ・クラウドパラダイムに 移りつつある。 そこでDNNの学習とサービングをエッジコンピュータにまたぐ形で提供するようにする、 KnwledgeNetを提案する。 ヘテロジーニアスアーキテクチャ上で学習、サービングする。 クラウドでTeacherモデルを学習し、エッジでStudentモデルを学習する。 Continuous Training for Production ML in the TensorFlow Extended (TFX) Platform タイトル: Continuous Training for Production ML in the TensorFlow Extended (TFX) Platform キーワード: Google, 継続的な機械学習, TFX（TensorFlow Extended）, 機械学習のパイプライン スライド: https://www.usenix.org/system/files/opml19papers-baylor.pdf 大企業は、継続的な機械学習パイプラインに支えられている。 それが止まりモデルが劣化すると、下流のシステムが影響を受ける。 TensorFlow Extendedを使ってパイプラインを構成する。 Reinforcement Learning Based Incremental Web Crawling タイトル: Reinforcement Learning Based Incremental Web Crawling キーワード: ウェブクロール, 機械学習？ スライド: 非公開 Katib: A Distributed General AutoML Platform on Kubernetes タイトル: Katib: A Distributed General AutoML Platform on Kubernetes キーワード: AutoML, Kubernetes, ハイパーパラメータサーチ, ニューラルアーキテクチャサーチ スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_zhou.pdf Kubernetes上に、ハイパーパラメータサーチ、ニューラルアーキテクチャサーチのためのパイプラインをローンチするAutoMLの仕組み。 Machine Learning Models as a Service タイトル: Machine Learning Models as a Service キーワード: 中央集権的な機械学習基盤, 機械学習基盤のセキュリティ, Kafkaを使ったビーコン スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_wenzel.pdf データサイエンティストがモデル設計や学習よりも、デプロイメントに時間をかけている状況を受け、 中央集権的な機械学習基盤を開発。 PyPIに悪意のあるライブラリが混入した件などを考慮し、中央集権的にセキュリティを担保。 Kafkaにビーコンを流す。リアルタイムに異常検知するフロー、マイクロバッチでデータレイクに格納し、再学習をするフロー。 その他、ロギング、トレーシング（トラッキング）、モニタリング。 Stratum: A Serverless Framework for the Lifecycle Management of Machine Learning-based Data Analytics Tasks タイトル: Stratum: A Serverless Framework for the Lifecycle Management of Machine Learning-based Data Analytics Tasks キーワード: 機械学習向けのサーバレスアーキテクチャ, Machine Learning as a Service, 推論, エッジ・クラウド スライド: https://www.usenix.org/sites/default/files/conference/protected-files/opml19_slides_bhattacharjee.pdf Stratumというサーバレスの機械学基盤を提案。 モデルのデプロイ、スケジュール、データ入力ツールの管理を提供。 機械学習ベースの予測分析のアーキテクチャ。 その他のセッション傾向 キーワード: スケーラビリティ 監視・観測、診断 最適化・チューニング SYSML SYSMLの定義 公式ウェブサイトには、以下のように記載されている。 The Conference on Systems and Machine Learning (SysML) targets research at the intersection of systems and machine learning. The conference aims to elicit new connections amongst these fields, including identifying best practices and design principles for learning systems, as well as developing novel learning methods and theory tailored to practical machine learning workflows. 以上から、「システムと機械学習の両方に関係するトピックを扱う」と解釈して良さそうである。 SYSML'19 コミッティなど チェアたちの所属する組織の構成は以下の通り。 プログラムコミッティの所属する組織の構成は以下の通り。 チェア、コミッティともに、幅広い層組織からの参画が見受けられるが、大学関係者（教授等）が多い。 僅かな偏りではあるが、Stanford Univ.、Carnegie Mellon Univ.、UCBあたりの多さが目立つ。 コミッティに限れば、MITとGoogleが目立つ。 トピック トピックは以下の通り。 Parallel &amp; Distributed Learning Security and Privacy Video Analytics Hardware Hardware &amp; Debbuging Efficient Training &amp; Inference Programming Models 概ねセッション数に偏りは無いようだが、1個目のParallel &amp; Distributed Learningだけは他と比べて2倍のセッション数だった。 TicTac: Accelerating Distributed Deep Learning with Communication Scheduling タイトル: TicTac: Accelerating Distributed Deep Learning with Communication Scheduling キーワード: 深層学習, DNNの高速化, TensorFlow, PyTorch, 学習高速化, 推論高速化 スライド: https://www.sysml.cc/doc/2019/199.pdf パラメータ交換を高速化。その前提となる計算モデルを考慮して、パラメータ交換の順番を制御する。 TensorFlowを改造。パラメータサーバを改造。 モデル自体を変更することなく使用可能。 推論で37.7%、学習で19.2%の改善。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"OpML","slug":"Knowledge-Management/Machine-Learning/OpML","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/OpML/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"OpML","slug":"OpML","permalink":"https://dobachi.github.io/memo-blog/tags/OpML/"}]},{"title":"ideavimrc","slug":"ideavimrc","date":"2019-08-25T15:03:53.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/08/26/ideavimrc/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/26/ideavimrc/","excerpt":"","text":"参考 メモ 参考 IntelliJ(Android Studio)のVimプラグイン「IdeaVim」の使い方と設定 メモ IntelliJ(Android Studio)のVimプラグイン「IdeaVim」の使い方と設定 に.ideavimrcの書き方の例が記載されている。 ただし、リンクで書かれているGitHubレポジトリは存在しないようだた。 上記ブログを参考にしつつ、とりあえず初期バージョンを作成。→ https://github.com/dobachi/ideavimrc このあと色々と付け足す予定。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"Intellij","slug":"Knowledge-Management/Tools/Intellij","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/Intellij/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"},{"name":"Intellij","slug":"Intellij","permalink":"https://dobachi.github.io/memo-blog/tags/Intellij/"}]},{"title":"sort by timestamp with NERDTree","slug":"sort-by-timestamp-with-NERDTree","date":"2019-08-24T13:29:34.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/08/24/sort-by-timestamp-with-NERDTree/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/24/sort-by-timestamp-with-NERDTree/","excerpt":"","text":"参考 メモ 参考 Support sorting files and directories by modification time. #901 NERDTree.txt メモ vimでNERDTreeを使うとき、直近ファイルを参照したいことが多いため、デフォルトのソート順を変えることにした。 Support sorting files and directories by modification time. #901 を参照し、タイムスタンプでソートする機能が取り込まれていることがわかったので、 NERDTree.txt を参照して設定した。 具体的には、 NERDTreeSortOrder の節を参照。 vimrcには、 1let NERDTreeSortOrder=[&apos;\\/$&apos;, &apos;*&apos;, &apos;\\.swp$&apos;, &apos;\\.bak$&apos;, &apos;\\~$&apos;, &apos;[[-timestamp]]&apos;] を追記した。デフォルトのオーダに対し、 [[-timestamp]] を追加し、タイムスタンプ降順でソートするようにした。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"vim","slug":"Knowledge-Management/vim","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"},{"name":"NERDTree","slug":"NERDTree","permalink":"https://dobachi.github.io/memo-blog/tags/NERDTree/"}]},{"title":"Spark Docker Image","slug":"Spark-Docker-Image","date":"2019-08-24T11:51:44.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/08/24/Spark-Docker-Image/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/24/Spark-Docker-Image/","excerpt":"","text":"参考 メモ Spark公式のDockerfileを活用 Anaconda等も利用するとして？ 参考 Running Spark on Kubernetes Docker Images dockerhubのopenjdk DockerhubのAnacondaのDockerfile GitHub上のAlpine版のAnaconda公式Dockerfile GitHub上のDebianベースのOpenJDK Dockerfile メモ Spark公式のDockerfileを活用 Spark公式ドキュメントの Running Spark on Kubernetes 内にある Docker Images によると、 SparkにはDockerfileが含まれている。 以下のパッケージで確認。 123$ cat RELEASESpark 2.4.3 built for Hadoop 2.7.3Build flags: -B -Pmesos -Pyarn -Pkubernetes -Pflume -Psparkr -Pkafka-0-8 -Phadoop-2.7 -Phive -Phive-thriftserver -DzincPort=3036 Dockerfileは、パッケージ内の kubernetes/dockerfiles/spark/Dockerfile に存在する。 また、これを利用した bin/docker-image-tool.sh というスクリプトが存在し、 Dockerイメージをビルドして、プッシュできる。 今回はこのスクリプトを使ってみる。 ヘルプを見ると以下の通り。 1234567891011121314151617181920212223242526272829303132333435$ ./bin/docker-image-tool.sh --helpUsage: ./bin/docker-image-tool.sh [options] [command]Builds or pushes the built-in Spark Docker image.Commands: build Build image. Requires a repository address to be provided if the image will be pushed to a different registry. push Push a pre-built image to a registry. Requires a repository address to be provided.Options: -f file Dockerfile to build for JVM based Jobs. By default builds the Dockerfile shipped with Spark. -p file Dockerfile to build for PySpark Jobs. Builds Python dependencies and ships with Spark. -R file Dockerfile to build for SparkR Jobs. Builds R dependencies and ships with Spark. -r repo Repository address. -t tag Tag to apply to the built image, or to identify the image to be pushed. -m Use minikube&apos;s Docker daemon. -n Build docker image with --no-cache -b arg Build arg to build or push the image. For multiple build args, this option needs to be used separately for each build arg.Using minikube when building images will do so directly into minikube&apos;s Docker daemon.There is no need to push the images into minikube in that case, they&apos;ll be automaticallyavailable when running applications inside the minikube cluster.Check the following documentation for more information on using the minikube Docker daemon: https://kubernetes.io/docs/getting-started-guides/minikube/#reusing-the-docker-daemonExamples: - Build image in minikube with tag &quot;testing&quot; ./bin/docker-image-tool.sh -m -t testing build - Build and push image with tag &quot;v2.3.0&quot; to docker.io/myrepo ./bin/docker-image-tool.sh -r docker.io/myrepo -t v2.3.0 build ./bin/docker-image-tool.sh -r docker.io/myrepo -t v2.3.0 push つづいて最新版Sparkのバージョンをタグ付けて試す。 12$ sudo ./bin/docker-image-tool.sh -r docker.io/dobachi -t v2.4.3 build$ sudo ./bin/docker-image-tool.sh -r docker.io/dobachi -t v2.4.3 push これは成功。 早速起動して試してみる。 なお、Sparkのパッケージは、 /opt/spark 以下に含まれている。 1234567891011121314151617181920$ sudo docker run --rm -it dobachi/spark:v2.4.3 /bin/bash(snip)bash-4.4# /opt/spark/bin/spark-shell (snip)Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.4.3 /_/Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212) Type in expressions to have them evaluated.Type :help for more information.scala&gt; データを軽く読み込んでみる。 12345678910111213141516171819202122232425262728293031323334scala&gt; val path = &quot;/opt/spark/data/mllib/kmeans_data.txt&quot;scala&gt; val textFile = spark.read.textFile(path)scala&gt; textFile.firstres1: String = 0.0 0.0 0.0scala&gt; textFile.countres2: Long = 6scala&gt; val csvLike = spark.read.format(&quot;csv&quot;). option(&quot;sep&quot;, &quot; &quot;). option(&quot;inferSchema&quot;, &quot;true&quot;). option(&quot;header&quot;, &quot;false&quot;). load(path)scala&gt; csvLike.show+---+---+---+|_c0|_c1|_c2|+---+---+---+|0.0|0.0|0.0||0.1|0.1|0.1||0.2|0.2|0.2||9.0|9.0|9.0||9.1|9.1|9.1||9.2|9.2|9.2|+---+---+---+scala&gt; csvLike.where($&quot;_c0&quot; &gt; 5).show+---+---+---+|_c0|_c1|_c2|+---+---+---+|9.0|9.0|9.0||9.1|9.1|9.1||9.2|9.2|9.2|+---+---+---+ なお、PySparkが含まれているのは、違うレポジトリであり、 dobachi/spark-py を用いる必要がある。 1234567891011$ sudo docker run --rm -it dobachi/spark-py:v2.4.3 /bin/bash(snip)bash-4.4# /opt/spark/bin/pyspark Python 2.7.16 (default, May 6 2019, 19:35:26) [GCC 8.3.0] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.Could not open PYTHONSTARTUPIOError: [Errno 2] No such file or directory: &apos;/opt/spark/python/pyspark/shell.py&apos;&gt;&gt;&gt; この状態だと、OS標準付帯のPythonのみ使える状態に見えた。 実用を考えると、もう少し分析用途のPythonライブラリを整えた方が良さそう。 Anaconda等も利用するとして？ 上記の通り、Spark公式のDockerfileで作るDockerイメージには、Python関係の分析用のライブラリ、ツールが不足している。 そこでこれを補うために、Anaconda等を利用しようとするとして、パッと見でほんの少し議論なのは、 なにをベースにするかという点。 上記で紹介したSparkに含まれるDockerfileおよび dockerhubのopenjdk に記載の通り、当該DockerイメージはAlpine Linuxをベースとしている。 ただし、OpenJDKのDockerイメージ自体には、 GitHub上のDebianベースのOpenJDK Dockerfile がある。 AnacondaのDockerイメージは、 DockerhubのAnacondaのDockerfile の通り、Debianをベースとしている。 しかし、 GitHub上のAlpine版のAnaconda公式Dockerfile によるとAlpine Linux版もありそう。 どっちに合わせるか悩むところだが、Debianベースに置き換えでも良いか、という気持ちではある。 Debianベースのほうが慣れているという気持ちもあり。 また、Anacondaを合わせて導入するようにしたら、環境変数 PYSPARK_PYTHON も設定しておいたほうが良さそう。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Spark","slug":"Knowledge-Management/Spark","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"}]},{"title":"Databricks AutoML Toolkit","slug":"Databricks-AutoML-Toolkit","date":"2019-08-23T13:04:58.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/08/23/Databricks-AutoML-Toolkit/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/23/Databricks-AutoML-Toolkit/","excerpt":"","text":"参考 メモ ブログ？記事？ GitHub Readme（2019/08/23現在） 実装を確認 参考 Databricks launches AutoML Toolkit for model building and deployment GitHub databrickslabs/automl-toolkit メモ ブログ？記事？ Databricks launches AutoML Toolkit for model building and deployment の内容を確認する。 MLFlowなどをベースに開発された。 ハイパーパラメタータチューニング、バッチ推論、モデルサーチを自動化する。 TensorFlowやSageMakerなどと連携。 データサイエンティストとエンジニアが連携可能という点が他のAutoMLと異なる要素。 またコード実装能力が高い人、そうでない人混ざっているケースに対し、 高い抽象度で実装なく動かせると同時に、細かなカスタマイズも可能にする。 DatabricksのAutoMLは、Azureの機械学習サービスとも連携する。（パートナーシップあり） GitHub GitHub databrickslabs/automl-toolkit を軽く確認する。 Readme（2019/08/23現在） 「non-supported end-to-end supervised learning solution for automating」と定義されている。 以下の内容をautomationするようだ。 Feature clean-up（特徴選択？特徴の前処理？） Feature vectorization（特徴のデータのベクトル化） Model selection and training（モデル選択と学習） Hyper parameter optimization and selection（ハイパーパラメータ最適化と選択） Batch Prediction（バッチ処理での推論） Logging of model results and training runs (using MLFlow)（MLFlowを使ってロギング、学習実行） 対応している学習モデルは以下の通り。 Sparkの機械学習ライブラリを利用しているようなので、それに準ずることになりそう。 Decision Trees (Regressor and Classifier) Gradient Boosted Trees (Regressor and Classifier) Random Forest (Regressor and Classifier) Linear Regression Logistic Regression Multi-Layer Perceptron Classifier Support Vector Machines XGBoost (Regressor and Classifier) 開発言語はScala（100%） DBFS APIを前提としているように見えることから、Databricks Cloud前提か？ ハイレベルAPI（FamilyRunner）について載っている例から、ポイントを抜粋する。 12345678910111213141516171819202122232425// パッケージを見る限り、Databricksのラボとしての活動のようだ？import com.databricks.labs.automl.executor.config.ConfigurationGeneratorimport com.databricks.labs.automl.executor.FamilyRunnerval data = spark.table(&quot;ben_demo.adult_data&quot;)// MLFlowの設定値など、最低限の上書き設定を定義するval overrides = Map(&quot;labelCol&quot; -&gt; &quot;income&quot;,&quot;mlFlowExperimentName&quot; -&gt; &quot;My First AutoML Run&quot;,// Databrciks Cloud上で起動するDeltaの場所を指定&quot;mlFlowTrackingURI&quot; -&gt; &quot;https://&lt;my shard address&gt;&quot;,&quot;mlFlowAPIToken&quot; -&gt; dbutils.notebook.getContext().apiToken.get,&quot;mlFlowModelSaveDirectory&quot; -&gt; &quot;/ml/FirstAutoMLRun/&quot;,&quot;inferenceConfigSaveLocation&quot; -&gt; &quot;ml/FirstAutoMLRun/inference&quot;)// コンフィグを適切に？自動生成。合わせてどのモデルを使うかを指定している。val randomForestConfig = ConfigurationGenerator.generateConfigFromMap(&quot;RandomForest&quot;, &quot;classifier&quot;, overrides)val gbtConfig = ConfigurationGenerator.generateConfigFromMap(&quot;GBT&quot;, &quot;classifier&quot;, overrides)val logConfig = ConfigurationGenerator.generateConfigFromMap(&quot;LogisticRegression&quot;, &quot;classifier&quot;, overrides)// 生成されたコンフィグでランナーを生成し、実行val runner = FamilyRunner(data, Array(randomForestConfig, gbtConfig, logConfig)).execute() 上記実装では、以下のようなことが裏で行われる。 前処理 特徴データのベクタライズ 3種類のモデルについてパラメータチューニング 実装を確認 エントリポイントとしては、 com.databricks.labs.automl.executor.FamilyRunner としてみる。 com.databricks.labs.automl.executor.FamilyRunner executeメソッドが例として載っていたので確認。 当該メソッド全体は以下の通り。 com/databricks/labs/automl/executor/FamilyRunner.scala:115 123456789101112131415161718192021222324252627282930def execute(): FamilyFinalOutput = &#123; val outputBuffer = ArrayBuffer[FamilyOutput]() configs.foreach &#123; x =&gt; val mainConfiguration = ConfigurationGenerator.generateMainConfig(x) val runner: AutomationRunner = new AutomationRunner(data) .setMainConfig(mainConfiguration) val preppedData = runner.prepData() val preppedDataOverride = preppedData.copy(modelType = x.predictionType) val output = runner.executeTuning(preppedDataOverride) outputBuffer += new FamilyOutput(x.modelFamily, output.mlFlowOutput) &#123; override def modelReport: Array[GenericModelReturn] = output.modelReport override def generationReport: Array[GenerationalReport] = output.generationReport override def modelReportDataFrame: DataFrame = augmentDF(x.modelFamily, output.modelReportDataFrame) override def generationReportDataFrame: DataFrame = augmentDF(x.modelFamily, output.generationReportDataFrame) &#125; &#125; unifyFamilyOutput(outputBuffer.toArray)&#125; また、再掲ではあるが、当該メソッドは以下のように呼び出されている。 12// 生成されたコンフィグでランナーを生成し、実行val runner = FamilyRunner(data, Array(randomForestConfig, gbtConfig, logConfig)).execute() 第1引数はデータ、第2引数は採用するアルゴリズムごとのコンフィグを含んだArray。 では、executeメソッドを少し細かく見る。 まずループとなっている箇所があるが、ここはアルゴリズムごとのコンフィグをイテレートしている。 com/databricks/labs/automl/executor/FamilyRunner.scala:119 1configs.foreach &#123; x =&gt; つづいて、渡されたコンフィグを引数に取りながら、パラメータを生成する。 com/databricks/labs/automl/executor/FamilyRunner.scala:120 1val mainConfiguration = ConfigurationGenerator.generateMainConfig(x) 次は上記で生成されたパラメータを引数に取りながら、処理の自動実行用のAutomationRunnerインスタンスを生成する。 com/databricks/labs/automl/executor/FamilyRunner.scala:122 12val runner: AutomationRunner = new AutomationRunner(data) .setMainConfig(mainConfiguration) つづいてデータを前処理する。 com/databricks/labs/automl/executor/FamilyRunner.scala:125 123val preppedData = runner.prepData()val preppedDataOverride = preppedData.copy(modelType = x.predictionType) つづいて生成されたAutomationRunnerインスタンスを利用し、パラメータチューニングを実施する。 com/databricks/labs/automl/executor/FamilyRunner.scala:129 1val output = runner.executeTuning(preppedDataOverride) その後、出力内容が整理される。 レポートの類にmodelFamilyの情報を付与したり、など。 com/databricks/labs/automl/executor/FamilyRunner.scala:131 123456789outputBuffer += new FamilyOutput(x.modelFamily, output.mlFlowOutput) &#123; override def modelReport: Array[GenericModelReturn] = output.modelReport override def generationReport: Array[GenerationalReport] = output.generationReport override def modelReportDataFrame: DataFrame = augmentDF(x.modelFamily, output.modelReportDataFrame) override def generationReportDataFrame: DataFrame = augmentDF(x.modelFamily, output.generationReportDataFrame)&#125; 最後に、モデルごとのチューニングのループが完了したあとに、 それにより得られた複数の出力内容をまとめ上げる。 （Arrayをまとめたり、DataFrameをunionしたり、など） com/databricks/labs/automl/executor/FamilyRunner.scala:142 1unifyFamilyOutput(outputBuffer.toArray) com.databricks.labs.automl.AutomationRunner#executeTuning 指定されたモデルごとにうチューニングを行う部分を確認する。 当該メソッド内では、modelFamilyごとの処理の仕方が定義されている。 以下にRandomForestの場合を示す。 com/databricks/labs/automl/AutomationRunner.scala:1597 123456789101112131415161718192021 protected[automl] def executeTuning(payload: DataGeneration): TunerOutput = &#123; val genericResults = new ArrayBuffer[GenericModelReturn] val (resultArray, modelStats, modelSelection, dataframe) = _mainConfig.modelFamily match &#123; case &quot;RandomForest&quot; =&gt; val (results, stats, selection, data) = runRandomForest(payload) results.foreach &#123; x =&gt; genericResults += GenericModelReturn( hyperParams = extractPayload(x.modelHyperParams), model = x.model, score = x.score, metrics = x.evalMetrics, generation = x.generation ) &#125; (genericResults, stats, selection, data)(snip) なお、2019/8/25現在対応しているモデルは以下の通り。（caseになっていたのを抜粋） 12345678case &quot;RandomForest&quot; =&gt;case &quot;XGBoost&quot; =&gt;case &quot;GBT&quot; =&gt;case &quot;MLPC&quot; =&gt;case &quot;LinearRegression&quot; =&gt;case &quot;LogisticRegression&quot; =&gt;case &quot;SVM&quot; =&gt;case &quot;Trees&quot; =&gt; さて、モデルごとのチューニングの様子を確認する。 RandomForestを例にすると、実態は以下の runRandomForest メソッドに見える。 com/databricks/labs/automl/AutomationRunner.scala:1604 1val (results, stats, selection, data) = runRandomForest(payload) runRandomForest メソッドは、以下のようにペイロードを引数にとり、 その中でチューニングを実施する。 com/databricks/labs/automl/AutomationRunner.scala:46 12345 private def runRandomForest( payload: DataGeneration ): (Array[RandomForestModelsWithResults], DataFrame, String, DataFrame) = &#123;(snip) 特徴的なのは、非常にきめ細やかに初期設定している箇所。 これらの設定を上書きしてチューニングできる、はず。 com/databricks/labs/automl/AutomationRunner.scala:58 1234567 val initialize = new RandomForestTuner(cachedData, payload.modelType) .setLabelCol(_mainConfig.labelCol) .setFeaturesCol(_mainConfig.featuresCol) .setRandomForestNumericBoundaries(_mainConfig.numericBoundaries) .setRandomForestStringBoundaries(_mainConfig.stringBoundaries)(snip) 実際にチューニングしているのは、以下の箇所のように見える。 evolveWithScoringDF 内では、 RandomForestTuner クラスのメソッドが呼び出され、 ストラテジ（実装上は、バッチ方式か、continuous方式か）に従ったチューニングが実施される。 com/databricks/labs/automl/AutomationRunner.scala:169 1val (modelResultsRaw, modelStatsRaw) = initialize.evolveWithScoringDF()","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"AutoML","slug":"Knowledge-Management/Machine-Learning/AutoML","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/AutoML/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"AutoML","slug":"AutoML","permalink":"https://dobachi.github.io/memo-blog/tags/AutoML/"}]},{"title":"Manage the flow of hexo documentation with CircleCI","slug":"Manage-the-flow-of-hexo-documentation-with-CircleCI","date":"2019-08-23T12:18:21.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/08/23/Manage-the-flow-of-hexo-documentation-with-CircleCI/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/23/Manage-the-flow-of-hexo-documentation-with-CircleCI/","excerpt":"","text":"参考 メモ HexoでPandasレンダラーを使うために最新版PandocをインストールしたDockerイメージを用意 HexoをCircleCIでビルドするための設定ファイルの作成 CircleCI上でプロジェクトを作る GitHubへのPush用のSSH鍵を登録する 参考 LaTeXファイルをGithubにpushしたらCircle CIでPDF生成するようにした HexoのレンダリングエンジンとしてPandocを使う mathjax.html メモ HexoでPandasレンダラーを使うために最新版PandocをインストールしたDockerイメージを用意 ドキュメント等では、circleciのDockerイメージを使うことが記載されているが、 ここでは予め最新版Pandocをインストールしておきたかったので、circleciのDockerイメージをベースにしつつ、 インストーラをwget、dpkgでインストールしたDockerイメージを自作して利用した。 Dockerfileは以下の通り。 12345FROM circleci/node:8.10.0RUN wget -P /tmp https://github.com/jgm/pandoc/releases/download/2.7.3/pandoc-2.7.3-1-amd64.debRUN sudo dpkg -i /tmp/pandoc-2.7.3-1-amd64.deb 今回は、これを使って作ったDockerイメージを、予めDocker Hubに登録しておく。 ここでは hoge/fuga:1.0 と登録したものとする。 HexoをCircleCIでビルドするための設定ファイルの作成 上記で作ったDockerイメージを使う、以下のようなCircleCi設定ファイルを作り、 Hexoプロジェクトのトップディレクトリ以下 .circleci/config.yml に保存する。 1234567891011121314151617181920212223242526defaults: &amp;defaults docker: - image: hoge/fuga:1.0 environment: TZ: Asia/Tokyo working_directory: ~/workversion: 2jobs: build: &lt;&lt;: *defaults steps: - checkout - run: git submodule init - run: git submodule update - run: npm install --save - run: node_modules/.bin/hexo clean - run: sed -i -e &quot;s#http://cdn.mathjax.org/mathjax#https://cdn.mathjax.org/mathjax#&quot; node_modules/hexo-renderer-mathjax/mathjax.html - run: cat node_modules/hexo-renderer-mathjax/mathjax.html - run: node_modules/.bin/hexo generate - run: git config --global user.name &lt;your name&gt; - run: git config --global user.email &lt;your email address&gt; - run: node_modules/.bin/hexo deploy - persist_to_workspace: root: . paths: [ &apos;*&apos; ] 上記 config.yml は、今後の拡張性を踏まえて記載してあるので、やや冗長な表現になっている。 なお、 1- run: sed -i -e &quot;s#http://cdn.mathjax.org/mathjax#https://cdn.mathjax.org/mathjax#&quot; node_modules/hexo-renderer-mathjax/mathjax.html としている箇所があるが、これは現在の hexo-renderer-mathjax をインストールすると、httpsではなくhttpを使用するようになっており、 結果としてgithub.ioでサイト公開するとmathjax部分が意図通りに表示されないからである。 HexoのレンダリングエンジンとしてPandocを使う にもその旨説明がある。 mathjax.html を見る限り、GitHub上のmasterブランチでは修正済みのようだ。 CircleCI上でプロジェクトを作る GitHubアカウントを連携している場合、既存のレポジトリが見えるので、 プロジェクトを作成する。 GitHubへのPush用のSSH鍵を登録する LaTeXファイルをGithubにpushしたらCircle CIでPDF生成するようにした あたりを参考に、SSH鍵の登録を実施。 プロジェクトの設定から、「Checkout SSH keys」を開き、「Add user key」から鍵を生成→登録した。 GitHub側のアカウント設定から鍵が登録されたことが確認できる。 以上で手順終了。 ブログを追記して、プロジェクトにpushするとCircleCIがhexo generateし、hexo deployするようになる。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"},{"name":"CircleCI","slug":"CircleCI","permalink":"https://dobachi.github.io/memo-blog/tags/CircleCI/"}]},{"title":"Chi: a scalable and programmable control plane for distributed stream processing systems","slug":"Chi-a-scalable-and-programmable-control-plane-for-distributed-stream-processing-systems","date":"2019-08-14T06:55:23.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/08/14/Chi-a-scalable-and-programmable-control-plane-for-distributed-stream-processing-systems/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/14/Chi-a-scalable-and-programmable-control-plane-for-distributed-stream-processing-systems/","excerpt":"","text":"参考 メモ 2. MOTIVATION ワークロードを予測しがたい データの多様性 マルチテナントなコントロールポリシー 3. BACKGROUND 4. DESIGN 4.1 Overview 4.2 Control Mechanism 参考 Chi a scalable and programmable control plane for distributed stream processing systems VLDBのダウンロードリンク Orleans Trill TrillのGitHub メモ 2018/6にVLDBに掲載された論文。Microsoftが中心となって執筆？ 多数のパラメータがあり、調整可能性がある場合に、動的な設定を可能にするChiというControll Planeを提案。 昨今のストリーム処理エンジン（やアプリケーション？）は、様々なSLO（Service Level Objective）を目指し、 多数の調整用パラメータが存在する。 これをチューニングするのは玄人でも一筋縄ではない。 重要なのは、動的に設定できることに加え、常にモニタリングしフィードバックすること。 本論文では、非同期的にコントロールポリシーを適用するモデル（Control Plane）を提案。 ストリームデータを伝搬する機能を利用し、コントロールメッセージを伝搬する。 またコントロールメッセージを扱うためのリアクティブなプログラミングモデルを合わせて提案。これにより、ポリシーを実装するのにハイレベルAPIを利用すればよくなる。 グローバルな同期が不要であることが特徴。 過去の技術としては、Heron、Flink、Spark Streamingを例に挙げ、限定的な対応にとどまっている旨を指摘。 Chiは、 Orleans の上に構築されたFlare（★補足：Microsoft内部のシステムのようだ）という仕組み上に実装し、試した。 Trillも利用しているとのこと。 ★補足：GitHub等に実装が公開されていないか軽く確認したが存在しないようだ。 2. MOTIVATION メジャーなパブリッククラウド（★補足：ここではAzureのことを指しているのだと思われる）で生じるログを集めて分析したい。 サーバ台数は20万台、数十PB/日。 これはGoogleのログと同様の規模。 ワークロードを予測しがたい 生じるログはバラエティに富んでいる。これはそれを生成するプロセス、コンポーネントがバラエティ豊かだからである。 例えば、何か障害が起きるとそのとたんにバーストトラフィックが発生する。 デバッグモードを有効にするとログ量が増える。 特徴として、ひとつの種類のストリームの量も、バラエティ豊かであることが挙げられる。 例えば、あるストリームについては1分の間に数千万イベントまで到達するケースもあった。（つまり、バーストトラフィック） これはトラフィックが予測困難であることを示す。 データの多様性 Skewが存在し、多様である。 マルチテナントなコントロールポリシー ログにはError、Infoなどのレベルがあり、要件に応じて多様である。 またそれぞれ異なるSLOを求められることがある。 例えばInfoレベルはExactly Onceだが、それ以外はベストエフォート、など。 これをひとつのストリーム処理システムで実現しようとする。 またControl Policyは、リソース使用の最適化に用いられることもある。 ある処理の中間データがほかの処理で使われるために保持されたり、 データレイアウトやストレージの使い方を最適化したり…など。 これらの個別の先行研究は存在するが、まとめて管理しようとすると、 Control Planeに柔軟性とスケーラビリティが必要になる。 3. BACKGROUND ここでは、Naiad、StreamScope、Apache Flinkを例にとって説明。 いずれも、オペレータのDAG構造のモデル。 ひとつの例として、 LINQスタイルのクエリでWordCountを表現。 LINQ風の表現とオペレータのDAG表現の例 計算モデルの前提は以下の通り。 DAGを\\(\\displaystyle G( V,\\ E)\\)で表す。 \\(\\displaystyle E\\)はEdgeのセット、\\(\\displaystyle V\\)がOperatorのセット。 \\(\\displaystyle u\\ \\rightharpoonup v\\)はOperator \\(\\displaystyle u\\)から\\(\\displaystyle v\\)への有向Edgeを表す。 \\(\\displaystyle \\{\\cdot \\rightharpoonup v\\}\\)が\\(\\displaystyle v\\)への入力Edgeを表す。 \\(\\displaystyle \\{v\\rightharpoonup \\cdot \\}\\)が\\(\\displaystyle v\\)からの出力Edgeを表す。 Operator \\(\\displaystyle v\\ \\in V\\)はトリプル\\(\\displaystyle ( s_{v} \\ ,\\ f_{v} \\ ,\\ p_{v})\\)で表す。 この時、\\(\\displaystyle s_{v}\\)は\\(\\displaystyle v\\)のステート、\\(\\displaystyle f_{v}\\)は\\(\\displaystyle v\\)で実行される関数、\\(\\displaystyle p_{v}\\)はステートに含まれないプロパティを表す。 \\(\\displaystyle f_{v}\\)の例としては、 \\[\\begin{equation*} f:\\ s_{v} \\ ,\\ m_{e_{i} \\in \\{\\cdot \\ \\rightharpoonup \\ v\\}} \\rightharpoonup s&#39;_{v} ,\\ m&#39;_{e_{i} \\in \\{v\\ \\rightharpoonup \\ \\cdot \\}} \\end{equation*}\\] が挙げられていた。ここで\\(\\displaystyle m\\)はメッセージを表す。 つまり、\\(\\displaystyle m_{e_{i} \\in \\{\\cdot \\ \\rightharpoonup \\ v\\}}\\)は、入力Edge \\(\\displaystyle e_{i}\\)から入ってくるメッセージを表す。 また入力を受け取った時点でステート\\(\\displaystyle s_{v}\\)だったものを、出力時点で\\(\\displaystyle s&#39;_{v}\\)に変える関数を表す。 また、\\(\\displaystyle \\ m&#39;_{e_{i} \\in \\{v\\ \\rightharpoonup \\ \\cdot \\}}\\)は出力メッセージを表す。 4. DESIGN データ処理と同様のAPIを設け、データ処理のパイプライン（data-plane）を使う。 グローバルな同期を必要とせず、非同期的なControl Operationを可能にする。 4.1 Overview 前提は以下の通り。 Operator間のチャンネルはExactly Onceであること メッセージはFIFOで処理されること 一度に処理されるメッセージは1個ずつ 基盤となるSPEはOperatorに関する基本的なオペレーション（起動、停止、Kill）に対応すること Control Loopを通じて、Control Messageが伝搬される。 Control Loopの流れは以下の通り。 Phase 1: Controllerにより意思決定され、ユニークなIDが生成される Control Operationはシリアライズされる Phase 2: Source OperatorにOperationが渡される データフローに沿ってControl Messageが伝搬される 途中で情報が付与されることもある 各OperatorはControl Messageに沿って反応する Phase 3: SinkからControllerにフィードバックされる [LINQ風の表現とオペレータのDAG表現の例] のDAG表現を用いた例でいえば、 もともと Reducer \\(\\{R_1, R_2\\}\\) で処理していたのを \\(\\{R_1, R_2, R_3\\}\\) で処理するように変更する例が載っている。 Control Messageが \\(\\{R_1, R_2\\}\\) に届くと、ルーティング情報を更新しつつ、ステート情報をCheckpointする。 ステート情報は再分配され、 \\(R_3\\) にも分配される。 \\(R_3\\) は、Control Messageを受け取ると入力チャンネルをブロックし、 \\(\\{R_1, R_2\\}\\) からのステートがそろうのを待つ。 ステートがそろったら、マージし、新しい関数を動かし始める。 ControllerがすべてのControl Messageを受け取ったら、スケールアウトは完了である。 4.2 Control Mechanism 4.2.1 Graph Transitions through Meta Topology ★この辺を読んでいる ＜WIP＞","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"}]},{"title":"How to become a (Throughput) Billionaire The Stream Processing Engine PipeFabric","slug":"How-to-become-a-Throughput-Billionaire-The-Stream-Processing-Engine-PipeFabric","date":"2019-08-14T06:00:03.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/08/14/How-to-become-a-Throughput-Billionaire-The-Stream-Processing-Engine-PipeFabric/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/14/How-to-become-a-Throughput-Billionaire-The-Stream-Processing-Engine-PipeFabric/","excerpt":"","text":"参考 メモ 参考 How to become a (Throughput) Billionaire The Stream Processing Engine PipeFabric Reliable stream data processing for elastic distributed stream processing systems PipeFabricのGitHub メモ 2019年の論文 Reliable stream data processing for elastic distributed stream processing systems が引用されているため気になった。 ドイツのイルメナウ大学？で開発されているストリーム処理エンジン。 論文アブストラクトでは、「開発中」とされていたが、PipeFabricのGitHub を見ると、2019/8/14現在で最終更新が7か月前（2019/1）だった。 モダンなHWをどう生かすか？ということがテーマのように見えた。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Stream Processing","slug":"Clipping/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"}]},{"title":"On-the-fly Reconfiguration of Query Plans for Stateful Stream Processing Engines","slug":"On-the-fly-Reconfiguration-of-Query-Plans-for-Stateful-Stream-Processing-Engines","date":"2019-08-03T10:51:29.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/08/03/On-the-fly-Reconfiguration-of-Query-Plans-for-Stateful-Stream-Processing-Engines/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/03/On-the-fly-Reconfiguration-of-Query-Plans-for-Stateful-Stream-Processing-Engines/","excerpt":"","text":"参考 メモ 1 Introduction 2 Background 2.1 Data Stream Processing 2.2 Apache Flink 2.3 Fault Tolerance and Checkpointing in Apache Flink 3 Protocol Description 3.2 Migration Protocol Description 3.3 Modification Protocols Description 4 System Architecture 4.1 Vanilla Components Overview 4.2 Our Changes on the Coordinator Side 4.3 Our Changes on the Worker Side 4.4 Query Plan Modifications 5 Protocol Implementation 5.1 Operator Migration 5.2 Introduction of new Operators 5.3 Changing the Operator Function 6 Evaluation 6.3 Workloads 6.4 Migration Protocol Benchmark 6.5 Introducing new operators at runtime 6.6 Replacing the operator function at runtime 6.7 Discussion 7 Related work 参考 On-the-fly Reconfiguration of Query Plans for Stateful Stream Processing Engines メモ Bartnik, A., Del Monte, B., Rabl, T. &amp; Markl, V., らの論文。2019年。 多くのストリーム処理エンジンが、データ量の変動に対して対応するためにクエリの再起動を必要とし、 ステートの再構成（再度の分散化？）にコストを必要とする点に着目し、新たな手法を提案。 Apache Flinkへのアドオンとして対応し、外部のトリガを用いてオペレータの置換を行えることを示した。 Modificaton Coordinatorを導入し、RPC経由でメッセージ伝搬と同一の方法でModificationに関する情報を 各オペレータインスタンスに伝える。 またステートの保持にはCheckopintの仕組みを導入し、各オペレータインスタンスごとに ステートを永続化する。上流・下流オペレータはその状況を見ながら動作する。 多くの先行研究に対し、以下の点が特徴的であると言える。 並列度の変更だけではなく、データフロー変更、オペレータの追加に対応 ステートサイズが小さいケースに加え、大きい場合も考慮し実証 ステートサイズが小さい場合で数秒、ステートサイズが大きい倍で数十秒のオーダで オペレータインスタンスのマイグレーション、オペレータの追加などに対応 Exactly onceセマンティクスを考慮 1 Introduction ストリームデータの流量について。 ソーシャルメディアの昼夜での差のように予測可能なものもあるが、 スポーツや天気の影響のように予測不可能なものもある。 最近のSPE（Stream Processing Engine）は、実行中の設定変更に一部対応し始めているが、 基本的には設定変更のためには再実行が必要。 （ここではApache Flink、Apache Storm、Apache Sparkを例にとっている） そこで、提案手法では、実行中の設定変更を可能にする。 ネットワークバッファのサイズ変更 ステートフル/レスの両オペレーションのマイグレーション（オペレータをほかのノードに移す） 並列度の変更 新しいオペレーションの追加 UDFの変更 Apache Flinkに上記内容をプロトタイプし、動作を検証。 2 Background 2.1 Data Stream Processing 近年のSPEのターゲットは、大量データの並列分散処理。 アプローチは2種類：マイクロバッチ、tuple-at-a-time。 マイクロバッチはスループット重視。 tuple-at-a-timeは入力レコードに対して、より細かな粒度での処理を可能にするが、 実際のところ物理レベルではバッチ処理のメカニズムを採用している。 1 SPEでは、基本的にはデータフローの表現にDAGを利用。 Source、Processing、Sink、それをつなぐEdgeで構成される。 Processingノードはステートフル、ステートレスのどちらもあえりえる。 2.2 Apache Flink FlinkはParallelization Contract (PACT)プログラミングモデルを採用。 Flinkは投稿されたクエリをコンパイル（最適化含む）したうえで実行する。 Flinkはパイプライニングとバッファリングに依存する。 オペレータがデータを送信するときにはバッファを使用しバッファが一杯になったら送るようになっている。 これによりレイテンシを抑えつつ、スループットを稼ぐ。 Flinkはオペレータのフュージョンにも対応。 フュージョンされたオペレータはプッシュベースで通信する。 バックプレッシャもある。 2.3 Fault Tolerance and Checkpointing in Apache Flink チェックポイントは、状態情報を保存し、故障の際には復旧させる。 このおかげで、メッセージの送達保証の各セマンティクスを選択できるようになる。 また、セーブポイントのメカニズムを築くことになる。 これにより、計画的に停止、再起動させることができるようになる。 Flinkの場合、セーブポイントから再開する際に、並列度を変えるだけではなく、 オペレータを追加・削除可能。 定期的なチェックポイントにより、状態情報が保存される。 状態情報が小さい場合は性能影響は小さい。（逆もしかり） Flinkでは、プログラムエラーの際は実行を停止・再開し、最後にチェックポイントした 状態情報から再開する。 このとき、データソースはApache Kafkaのように再取得可能なものを想定している。 そうでないと、クエリの再開時にデータが一部失われることになる。 3 Protocol Description 3.1 System Model システムの前提 Operatorはそれぞれの並列度を持つ。 論理プランは、Source、Sink、Processing OperatorでDAGを構成する。 実際にSPE上で動作する物理プランをJobと呼ぶ。 Operatorは 単一の下流のOperatorにレコードを送る すべての下流のOperatorにブロードキャストする 何らかのパーティションルールにより送る のいずれかの動作をする。 CheckpointのためにMarkerも送る。 Markerは定期的か、ユーザ指定により送られる。 i は、i番目のCheckpointであることを示す。 内部通信経路でMarkerを受け取ったオペレータは、スナップショットを作成して下流のオペレータにMarkerを送る。 また、スナップショットは非同期的にバックグラウンドの永続化ストレージに保存される。 ここで、SPEはExactly Onceでの処理を保証するものとする。 SPEはControllerとWorkerで構成される。 3.2 Migration Protocol Description Modification Markerを送ることで、マイグレーションを開始する。 2 各Operatorは、eventualにMarkerを受けとり、マイグレーションに備えるが、 このとき枚グレート対象となるOperatorの上流・下流のOperatorにも影響することがポイント。 上流のOperatorは、バッファを駆使しつつ、Tupleの順序性を担保する。 3.3 Modification Protocols Description Operatorをマイグレートするだけでなく、Operatorの追加、更新が可能。 ただし、その場合はUDFの配布が必要となる。 3.3.1 Introduction of New Operators 上流OperatorはMarkerを受け取るとバッファリングし始め、その旨下流に通知する。 すべての上流Operatorのバッファリングが開始されたら、新しいOperatorのインスタンスが起動する。 ただし、実際に起動させる前に、あらかじめUDFを配布しておく。 下流Operatorは、新しく起動されたOperatorに接続を試みる。 4 System Architecture 試したFlinkは1.3.2。 ★ 4.1 Vanilla Components Overview まずクライアントがModification ControlメッセージをCoordinatorに送る。 CoordinatorはWorkerに当該情報をいきわたらせる。 このメッセージは、通常のデータとは異なり、RPCとして送られる。 ここではActorモデルに基づく。 Flinkの場合、2個のOperator間の通信はProducer/Consumer関係の下やり取りされる。 各Operatorのインスタンスは、上流からデータを取得するための通信路に責任を持つ。 システムアーキテクチャのイメージ 4.2 Our Changes on the Coordinator Side Modification Coordinatorは、Modificatoinに関する一切を取り仕切る。 バリデーションも含む。 例えば、現在走っているジョブに対して適用可能か？の面など。 Modificationの大まかな状態遷移は以下の通り。 Modificationのステート Modificationに関係し、Taskの大まかな状態遷移は以下の通り。 Taskのステート 4.3 Our Changes on the Worker Side オペレータ間の通信は、オペレータの関係に依存する。 例えば、同一マシンで動ているProducerインスタンスとConsumerインスタンスの場合はメモリを使ってデータをやり取りする。 一方、異なるマシンで動ている場合はネットワーク通信を挟む。 さて、Modificationが生じた場合、新しいConsumerが動き始めるまで、上流のインスタンスはバッファリングしないといけない。 提案手法では、ディスクへのスピル機能を含むバッファリングの仕組みを提案。それ専用のキューを設けることとした。 4.4 Query Plan Modifications あるOperatorがModificationを実現するには、上流と下流のOperatorの合わせた対応も必要。 そこでModification CoordinatorがModificationメッセージに、関連情報全体を載せ、RPCを使って各Operatorに伝搬する。 4.4.1 Upstream Operators Checkpointの間、上流から下流に向けて、Checkpointマーカーを直列に並べることで 故障耐性を実現する。 各オペレータは上流からのマーカーがそろうまでバッファリングを続ける。 もしオペレータをマイグレートしようとすると、このバッファもマイグレートする必要がある。 しかしこのバッファインの仕組みは、内部的な機能ではない（★要確認）ため、一定の手続きが必要。 Modificationメッセージには次のCheckpointのIDが含まれている。 このIDに該当するCheckpointが発動されたときには、マイグレート対象のオペレータの上流オペレータは CheckpointバリアのメッセージをModificationメッセージと一緒に送る。 このイベント情報は、上流オペレータがレコードをストレージにフラッシュしていることを下流に示すものとなる。 また、これを通じて、マイグレート対象となるオペレータと上流オペレータの間には仕掛中のレコードがないことを確認できる。 以上を踏まえると、Modificationを安全に進めるためには、Checkpointを待つことになる。 Checkpointインターバルは様々な要因で決まるが、例えばオペレータ数とステートの大きさに依存する。 ステートが大きく、Checkpointインターバルが大きい場合は、それだけModification開始を待たなくてはならない、ということである。 4.4.2 Target and Downstream Operators 下流のオペレータは、基本的には上流のオペレータの新しい情報を待つのみ。 5 Protocol Implementation 5.1 Operator Migration Modification Coordinatorがトリガーとなるメッセージをソースオペレータから発出。 対象オペレータに加え、上流オペレータも特定する。（上流オペレータは、レコードをディスクにスピルする） オペレータはcheckpointのマーカを待つ。 Checkpoint Coordinatorがマーカを発出し、マイグレート対象のオペレータの上流オペレータは 送信を止める。 各オペレータはPausing状態に移行するとともに、現在の状態情報をModification Coordinatorに送る。 さらに、下流のオペレータに新しいロケーションを伝える。 各オペレータがPaused状態に移行。 すべてのオペレータがPaused状態に移行したら、オペレータを再起動する。 その後、Modification Coordinatorが状態ロケーション？をアタッチし、タスクを実行開始する。 提案手法では、FlinkのCheckpointの仕組みを使用し、各オペレータの状態情報を取得し、アサインする。 5.2 Introduction of new Operators オペレータのModificationと同様の流れで、新しいオペレーションの挿入にも対応する。 上流のオペレータがスピルした後、新しいオペレータが挿入される。 デプロイのペイロードには、コンパイル済のコードが含まれる。 5.3 Changing the Operator Function Modification CoordinatorがModificationメッセージと一緒に、 新しいUDFを配布する。 Task Managerは非同期的にUFDを取得する。 またcallbackを用いて、グローバルCheckpointが完了したときに、新しいUDFを用いるようにする。 6 Evaluation ここから先は評価となるが、ここではポイントのみ紹介する。 6.3 Workloads 3種類のワークロード、 小さなステートの場合を確認するため、要素数をカウントするワークロード（SMQ） 大きなステートの場合を確認するため、Nexmarkベンチマーク 3 のクエリ8（NBQ8）。（オンラインオークションのワークロード） 上記SMQ、NBQ8についてオペレータのマイグレートを実施 2個めのワークロードではステートサイズが大きいので、インクリメンタルCheckpointを利用。 Flinkの場合は、埋め込みのRocksDB。 6.4 Migration Protocol Benchmark 6.4.1 Stateful Map Job Performance Drill Down レイテンシで見るとスパイクが生じるのは、ストリーム処理のジョブがリスタートするタイミング。 オペレータのMigration中、1度3500msecのレイテンシのスパイクが生じた。 またコミュニケーションオーバヘッドもあるようだ。 概ね、秒オーダ。 6.4.2 Nexmark Benchmark Performance Drill Down 概ね、100～200秒オーダ。 ステートサイズは全部で13.5GBで、そのうち2.7GBがステート用のバックエンドに格納され、再現された。 80個のジェネレータのスループットは、Migration発生時も大きくは変わらなかった。 6.5 Introducing new operators at runtime SMQのワークロードを用いた検証。 概ね、秒オーダ。 6.6 Replacing the operator function at runtime SMQのワークロードを用いた検証。 概ね、秒オーダ。 スループットへの影響は小さい。 6.7 Discussion Checkpointの同期は課題になりがち。 ステートのサイズが小さいときは高頻度で同期も可能かもしれないが、大きいときは頻度高くCheckpointすると、処理に影響が出る。 例えばステートサイズが小さい時には6秒以内にModificationを開始できたが、大きい時には60秒程度になった。 NBQ8の場合、従来のシャットダウンを伴うsavepointの仕組みと比べ、性能上の改善が見られた。 データソースが永続的であれば再取得することでExactly Onceを実現する。 実行中のジョブに対し、新しいオペレータを挿入することもできた。概ね10秒程度。 オペレータの関数を変更することもできた。概ね9秒程度。 これを突き詰めていくと、内部・外部状態に応じて挙動を変える、ということもできるようになるはず。 ★補足： とはいえ、そのような挙動変更は、最初からUDF内に組み込んでおくべきとも考える。（普通に条件文を内部に入れておけば？と） 条件分岐が問題になるほどシビアなレイテンシ要求があるユースケースで、ここで挙げられているようなストリーム処理エンジンを使うとは思えない。 ★補足終わり： 7 Related work ★補足：特に気になった関連研究を以下に並べる。 Schneider, S.; Hirzel, M.; Gedik, B.; Wu, K.: Auto-parallelizing stateful distributed streaming applications. ACM PACT, 2012. 並列度の変更に関する先行研究 本論文の提案手法では、大きなステートサイズの際のexactly onceを対象としている点が異なる Wu, Y.; Tan, K. L.: ChronoStream: Elastic stateful stream computation in the cloud. IEEE ICDE, 2015. 弾力性の実現に関する先行研究 ただし別の論文の指摘によると、同期に関する課題がある データフロー変更には対応していない。あくまでオペレータのマイグレーションのみ。 Heinze, T.; Pappalardo, V.; Jerzak, Z.; Fetzer, C.: Auto-scaling techniques for elastic data stream processing. In: IEEE ICDE Workshops. 2014. および、 Heinze, T.; Ji, Y.; Roediger, L.; Pappalardo, V.; Meister, A.; Jerzak, Z.; Fetzer, C.: FUGU: Elastic Data Stream Processing with Latency Constraints. IEEE Data Eng. Bull., 2015. オートスケールのタイミングを判断する。オンライン機械学習を利用。 簡単なマイグレーションのシナリオを想定。 Nasir, M.; Morales, G.; Kourtellis, N.; Serafini, M.: When Two Choices Are not Enough: Balancing at Scale in Distributed Stream Processing. CoRR, abs/1510.05714, 2015. 並列度の調整に関する先行研究 特にホットキーが存在する場合、そこにオペレータインスタンスを割り当てるように動く。 データフロー変更などには対応しない。 Mai, L.; Zeng, K.; Potharaju, R.; Xu, L.; Venkataraman, S.; Costa, P.; Kim, T.; Muthukrishnan, S.; Kuppa, V.; Dhulipalla, S.; Rao, S.: Chi: A Scalable and Programmable Control Plane for Distributed Stream Processing Systems. VLDB, 2018. 本論文で扱っているのに近い ただしステートサイズが大きなケースは対象としていない Carbone, P.; Ewen, S.; Fora, G.; Haridi, S.; Richter, S.; Tzoumas, K.: State Management in Apache Flink: Consistent Stateful Distributed Stream Processing. VLDB, 2017.↩︎ Del Monte, B.: Efficient Migration of Very Large Distributed State for Scalable Stream Processing. VLDB PhD Workshop, 2017.↩︎ Tucker, P.; Tufte, K.; Papadimos, V.; Maier, D.: NEXMark - A Benchmark for Queries over Data Streams. 2018.↩︎","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"}]},{"title":"Reliable stream data processing for elastic distributed stream processing systems","slug":"Reliable-stream-data-processing-for-elastic-distributed-stream-processing-systems","date":"2019-08-03T10:39:56.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/08/03/Reliable-stream-data-processing-for-elastic-distributed-stream-processing-systems/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/03/Reliable-stream-data-processing-for-elastic-distributed-stream-processing-systems/","excerpt":"","text":"参考 メモ 参考 Reliable stream data processing for elastic distributed stream processing systems メモ Reliable stream data processing for elastic distributed stream processing systems では、 弾力性を有するストリーム処理エンジン（Distributed Stream Processing System: DSPS）の さらなる課題について触れている。 動的にオペレータがスケールアップ、ダウンする中で、データのバックアップの一貫性を保つ故障耐性が必要がある チェックポイントへのロールバックが、直近のオートスケール調整を元に戻らせる可能性がある","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Stream Processing","slug":"Clipping/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"}]},{"title":"Transactions on Large-Scale Data- and Knowledge-Centered Systems XL","slug":"Transactions-on-Large-Scale-Data-and-Knowledge-Centered-Systems-XL","date":"2019-08-03T10:19:29.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/08/03/Transactions-on-Large-Scale-Data-and-Knowledge-Centered-Systems-XL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/08/03/Transactions-on-Large-Scale-Data-and-Knowledge-Centered-Systems-XL/","excerpt":"","text":"参考 メモ 参考 Transactions on Large-Scale Data- and Knowledge-Centered Systems XL DABS-Storm A Data-Aware Approach for Elastic Stream Processing メモ Transactions on Large-Scale Data- and Knowledge-Centered Systems XL の中に、 DABS-Storm A Data-Aware Approach for Elastic Stream Processing が含まれていて気になった。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Stream Processing","slug":"Clipping/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"}]},{"title":"vim modeline format in Markrown","slug":"vim-modeline-format-in-MarkDown","date":"2019-07-27T14:24:43.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/27/vim-modeline-format-in-MarkDown/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/27/vim-modeline-format-in-MarkDown/","excerpt":"","text":"参考 メモ 参考 Add vim modeline in markdown document メモ Add vim modeline in markdown document にvimのモードラインをMarkdownファイル内で記載する方法について言及あり。 結論としては、 1&lt;!-- vim: set ft=markdown: --&gt; のように記載する。 markdown: の末尾のコロンがポイント。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Vim","slug":"Clipping/Vim","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Vim/"}],"tags":[{"name":"Vim","slug":"Vim","permalink":"https://dobachi.github.io/memo-blog/tags/Vim/"}]},{"title":"vim modeline fileencoding","slug":"vim-modeline-fileencoding","date":"2019-07-27T14:19:43.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/27/vim-modeline-fileencoding/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/27/vim-modeline-fileencoding/","excerpt":"","text":"参考 メモ 参考 Vim の modeline で fileencoding を設定すべからず メモ Vim の modeline で fileencoding を設定すべからず に、モードラインでfencを設定してはいけない理由が記載されている。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Vim","slug":"Clipping/Vim","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Vim/"}],"tags":[{"name":"Vim","slug":"Vim","permalink":"https://dobachi.github.io/memo-blog/tags/Vim/"}]},{"title":"Apache Edgent","slug":"Apache-Edgent","date":"2019-07-26T12:56:26.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/07/26/Apache-Edgent/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/26/Apache-Edgent/","excerpt":"","text":"参考 メモ 所感 公式ドキュメント 概要を確認する Apache Edgentのケーパビリティ Getting Started GitHubで確認 開発言語 公式ウェブサイトで紹介されていたスライドシェアを確認 参考 Apache Edgent公式ウェブサイト Orientation in the fog: differences between stream processing in edge and cloud Apache Edgent Overview 公式GitHub Getting Started The Power of Apache Edgent KafkaProducer FiltersのJavadoc JIRA fluentbit公式ウェブサイト メモ 所感 JIRA や 公式GitHub を見る限り、ここ最近活動量が低下しているように見える。 最後のコミットが2019年4月だったり、ここ最近のコミット量が低めだったり、とか。 まじめに使うとしたら注意が必要。 ストリーム処理をエッジデバイスで…という動機は興味深い。 ただし、「エッジで動かすための特別な工夫」が何なのかを把握できておらず、 ほかのシンプルなアーキテクチャと何が違うのか、はよく考えた方がよさそう。 例えばもしエッジで「データロード」を主軸とするならば、fluentbitがコンパクトでよいのではないか、という考えもあるだろう。 一方でストリーム処理の実装という観点では、Javaの内部DSL的であるEdgentのAPIは一定の嬉しさがあるのかもしれない。 思想として、Edgentは分析のような処理までエッジで行おうとしているようだが、fluentbitはあくまで「Log Processor and Forwarder」 1 を担うものとしている点が異なると考えてよいだろう。（できる・できない、ではない） Edgentはエッジで動かす、という割にはJavaで実装されており、 それなりにリッチなデバイス（RaspberyPiなど）を動作環境として想定しているように感じる。 プアなデバイスがリッチなデバイスの先につながっており、リッチなデバイスを経由して ストリーム処理するイメージか。 ここではエッジデバイスも含めた、ストリーム処理を考える参考として調べる。 公式ドキュメント Apache Edgent公式ウェブサイト によると以下の通り。 つまり、ゲートウェイやエッジデバイスで実行されるランタイムのようだ。 Raspberry Pisやスマートホンが例として挙げられている。 Apache Edgent is a programming model and micro-kernel style runtime that can be embedded in gateways and small footprint edge devices enabling local, real-time, analytics on the continuous streams of data coming from equipment, vehicles, systems, appliances, devices and sensors of all kinds (for example, Raspberry Pis or smart phones). Working in conjunction with centralized analytic systems, Apache Edgent provides efficient and timely analytics across the whole IoT ecosystem: from the center to the edge. 概要を確認する Apache Edgent Overview を確認する。 以下、ポイントを記載する。 エッジからデータセンタにすべてのデータを転送するのはコストが高すぎる。 Apache Edgent（以降Edgent）はエッジでのデータ、イベントの分析を可能にする。 Edgentを使うと、エッジ上で異常検知などを実現できる。 例えばエンジンが通常より熱い、など。 これを用いると通常時はデータセンタにデータを送らず、異常時にはデータを送るようにする、などの制御が可能。 ユースケースの例： IoT エッジで分析することでNWトラフィックを減らす アプリケーションサーバに埋め込む エラー検知をエッジで行ってNWトラフィックを減らす サーバマシンあるいはマシンルーム ヘルスチェック。これもNWトラフィックを減らす。 動かせる場所： Java 8, including Raspberry Pi B and Pi2 B Java 7 Android ★補足： エッジデバイスとしてどの程度貧弱な環境で動かせるのか？はひとつポイントになるだろう。 ここではAndroidやRaspberry Piを挙げているので、エッジのマシンとしては強力な部類を想定しているように見える。 ★補足おわり： 逆に、エッジでは処理しきれないケースで、データセンタにデータを送ることもできる。 例えば… CPUやメモリを使う複雑な処理 大きなステート管理 複数のデータソースを混ぜあわせる など。 そのために、通信手段は複数に対応。 MQTT IBM Watson IoT Platform Apache Kafka カスタム・メッセージバス Apache Edgentのケーパビリティ The Power of Apache Edgent を確認する。 最初に載っていたサンプルは以下の通り。 12345678910111213141516public class ImpressiveEdgentExample &#123; public static void main(String[] args) &#123; DirectProvider provider = new DirectProvider(); Topology top = provider.newTopology(); IotDevice iotConnector = IotpDevice.quickstart(top, &quot;edgent-intro-device-2&quot;); // open https://quickstart.internetofthings.ibmcloud.com/#/device/edgent-intro-device-2 // ingest -&gt; transform -&gt; publish TStream&lt;Double&gt; readings = top.poll(new SimulatedTemperatureSensor(), 1, TimeUnit.SECONDS); TStream&lt;JsonObject&gt; events = readings.map(JsonFunctions.valueOfNumber(&quot;temp&quot;)); iotConnector.events(events, &quot;readingEvents&quot;, QoS.FIRE_AND_FORGET); provider.submit(top); &#125;&#125; IBM Watson IoT Platformに接続する。 1IotDevice iotConnector = IotpDevice.quickstart(top, &quot;edgent-intro-device-2&quot;); Connectors, Ingest and Sink イベント処理のアプリケーションは、外部からデータを読み込む（Ingest）のと、外部にデータを書き出す（Sink）が必要。 プリミティブなコネクタはすでに存在する。 もしMQTTを用いたいのだったら、上記サンプルの代わりに以下のようになる。 12MqttStreams iotConnector = new MqttStreams(top, &quot;ssl://myMqttServer:8883&quot;, &quot;my-device-client-id&quot;);iotConnector.publish(events, &quot;readingEvents&quot;, QoS.FIRE_AND_FORGET, false); 同じように、対Kafkaだったら、KafkaPdocuer、KafkaConsumerというクラスがそれぞれ提供されている。 ★補足： しかし、 KafkaProducer のJavadocを見ると、以下のようなコメントが記載されている。 The connector uses and includes components from the Kafka 0.8.2.2 release. It has been successfully tested against kafka_2.11-0.10.1.0 and kafka_2.11-0.9.0.0 server as well. For more information about Kafka see http://kafka.apache.org これを見る限り、使用しているKafkaのバージョンが古い。念のために、masterブランチを確認したところ、 以下の通り、1.1.0が用いられているように見える。ドキュメントが古いのか…。 connectors/kafka/pom.xml:56 12345678910 &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;!-- not transitive --&gt; &lt;groupId&gt;*&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; ★補足おわり： そのほかにも、JdbcStreamsというクラスもあるようだ。 またFileStreamsを使うと、ディレクトリ配下のファイルの監視もできる。 例としては以下のようなものが載っていた。 12345String watchedDir = &quot;/some/directory/path&quot;;List&lt;String&gt; csvFieldNames = ...TStream&lt;String&gt; pathnames = FileStreams.directoryWatcher(top, () -&gt; watchedDir, null);TStream&lt;String&gt; lines = FileStreams.textFileReader(pathnames);TStream&lt;JsonObject&gt; parsedLines = lines.map(line -&gt; Csv.toJson(Csv.parseCsv(line), csvFieldNames)); directoryWatcherというメソッドを使い、トポロジを構成するようだ。 これがどの程度のものなのかは要確認。 ★補足： directoryWatcherはDirectoryWatcherのインスタンスを引数にとり、sourceメソッドを使って、 TStreamインスタンスを返す。 org/apache/edgent/connectors/file/FileStreams.java:108 1234public static TStream&lt;String&gt; directoryWatcher(TopologyElement te, Supplier&lt;String&gt; directory, Comparator&lt;File&gt; comparator) &#123; return te.topology().source(() -&gt; new DirectoryWatcher(directory, comparator));&#125; DirectoryWatcherクラスでは、監視対象のディレクトリに追加されたファイルのファイル名を リストとして管理し、そのリストからイテレータを生成して用いる。 WatcherIteratorクラスは以下のようになっている。 ここで、pendingNamesがファイル名のリスト。 org/apache/edgent/connectors/file/runtime/DirectoryWatcher.java:201 123456789101112131415private class WatcherIterator implements Iterator&lt;String&gt; &#123; @Override public boolean hasNext() &#123; return true; &#125; @Override public String next() &#123; for (;;) &#123; String name = pendingNames.poll(); if (name != null) return name; このリストの管理はとても簡易なものである。 一応、比較のためのComparatorは渡せるようになっているが、もしリストに載せた後にファイルが削除されたとしても、 それはこのDirectoryWatcherでは関知しないようだ。 ★補足おわり： そのほか、コマンドで監視したり…。STDOUT、STDINと連携したり、という例も載っている。 More on Ingest 複数のソースをひとつのストリームにする例が載っていた。 また、Edgentにおいてストリームの型は自由度が高く、カスタムの型も使える。 カスタムの方を使う例が載っている。 「Simulated Sensors」を見る限り、データソースのシミュレーションもある。動作確認用か？ またEdgent自身ではセンサーライブラリを手掛けない。 Filtering Topologyでpollした後、以下のようにすることでフィルタを実現できる。 1readings = readings.filter(tuple -&gt; tuple &lt; 5d || tuple &gt; 30d); また、Filters#deadbandメソッドを用いると、より細かな（便利な？）フィルタを用いることができるようだ。 このあたり ★補足： FiltersのJavadoc の deadband メソッドの説明を見ると、図が掲載されている。 まず線が引かれているのが「deadband」であり、基本的にはその中に含まれたポイントはフィルタされる。 ただし、「最初のポイント」、「deadbandに入った『最初のポイント』」はフィルタされない。 これは、deadbandに入った瞬間の時刻をトレースする、などのユースケースにおいて有用と考える。 （deadbandに入った最初のポイントもフィルタしてしまうと、それが分からなくなってしますため） ★補足おわり： Split ストリームを分割可能。 Transforms mapメソッドでデータ変換可能。 特に分散処理でもないので、ふつうに任意の処理を書けばよいだけ。 ★補足：Sparkのようにシリアライゼーション周りで悩まされることは少なそうだ Windowing and aggregation ウィンドウにも対応。 直近10レコード、直近10秒、などの単位で集計可能。 このウィンドウは、ウィンドウがいっぱいになるまではaggregateされないようだ。 ★補足： 以下のようなイメージか。 ウィンドウのイメージ ★補足おわり： Misc PlumbingStreams#parallelを使うと、並列処理化できるようだ。 ストリームのタプルを分割して、マルチスレッドで処理する感じだろうか。★要確認 Getting Started Getting Started を確認する。 Apache Edgent and streaming analytics Edgentの基本要素は、ストリームである。 ストリームは、ローカルデバイスの制御という形で終わったり、外部出力されたりして終わる。 処理は関数のような形で表現される。Java8のlambdaみたいな感じ。 1reading -&gt; reading &lt; 50 || reading &gt; 80 基本的なフローは以下の通り。 123* プロバイダを定義* トポロジを定義し、処理グラフを定義* 処理を実行 複数のトポロジからなることもあり、ほかのアプリケーションから呼び出されて実行するというケースも想定される。 （あとは、Getting Startedにはフィルタの仕方など、基本的なAPIの使い方が紹介されている。） GitHubで確認 開発言語 公式GitHub によると、開発言語としてはJavaが90.9%だった。（2019/7/28現在） 開発言語の様子 Javaで作られているということは、やはりエッジといえど、それなりにリッチなものを想定していることがわかる。 公式ウェブサイトで紹介されていたスライドシェアを確認 Orientation in the fog: differences between stream processing in edge and cloud を確認する。 Industrial IoTについて。 そもそも産業界はコンサバである。 機器はPLCで制御される。 参考：Apache PLC4X Industrial IoTの現場で登場する要素：PLC、センサー、アクター イベントは区切られずに生成される。 時系列のイベントを取り出すためには、閾値、何らかのトリガ等々で区切る必要がある。 そこで、エッジでのストリーム処理。 ここからいくつかの例。 サイクル検知の例。シンプルな実装に始まり、後から「あ、実はジッターがあって…」などの 要件が足されていく。そして実装がきたなくなっていく…。 エラーロギングの例。2300bitのビットマスクを確認し、適切な例外を上げる。 複雑なアクションの例。あるビットが送られてくるまで待ちアクションを取り、別のビットを待ち…の連続。 クラウドとエッジにおけるストリーム処理の違いにも触れられている。 クラウドとエッジの違い この図でいう、「Fast」がエッジで「Easy」となっているのはよくわからない。 宣言的な表現へ。 CRUNCHの紹介。 https://github.com/pragmaticminds/crunch ラムダアーキテクチャをエッジで…？ Apache Edgent。 以下のランタイムを有する。 ストリーム処理 リアルタイム分析 クラウドとの通信 以上をエッジで行う。 エッジtoクラウドのソリューションもある。 しかし、これらは最初の障壁は低いが、ベンダ手動でロックイン。 そこでApache Edgentがある。 オープンソースでベンダーフリー。 ほかのApacheプロジェクトと連携。例えば、Apache PLC4X、IoTDBなど。 将来的には… さらにクラウド接続を充実させる。 CRUNCHとの連携 エッジインテグレーションパターン（？） ルーティング/ルールエンジンの充実 fluentbit公式ウェブサイト↩︎","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"},{"name":"Apache Edgent","slug":"Knowledge-Management/Stream-Processing/Apache-Edgent","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/Apache-Edgent/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Apache Edgent","slug":"Apache-Edgent","permalink":"https://dobachi.github.io/memo-blog/tags/Apache-Edgent/"}]},{"title":"AI and Compute","slug":"AI-and-Compute","date":"2019-07-23T05:13:12.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/07/23/AI-and-Compute/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/23/AI-and-Compute/","excerpt":"","text":"参考 メモ 導入 概要 時代 将来展望 付録 参考 AI and Compute メモ AI and Compute は、ムーアの法則を超えるペースで、計算量が増大していることを示すブログ。 以下にポイントを示す。 導入 2012年ころからAIの学習に用いられる計算量は3.5か月あたり2倍に増えてきた。 （ムーアの法則では18か月あたりに2倍） 学習に用いる計算量をペタフロップス/dayで表したグラフ に計算量増加のグラフが載っている。 AlexNetから始まり、AlphaGo Zeroまで。 30万倍になった。 概要 AIを進化させた3要素。 アルゴリズムの進化 データの改善 計算可能量の増大 ここでは計算可能量の増大に着目。 特にひとつのモデルを学習するのに用いられる計算量に着目。 概ね10倍/年ペース。 これにはそれ用のカスタムハードウェア（GPU、TPUなど）の導入に加え、 計算リソースを並列処理で使い切る手法の改善も寄与している。 時代 2012年より前 機械学習向けにGPUを使っていなかった 2012年〜2014年 1〜8個程度のGPUを使用 1〜2TFLOPS 2014〜2016年 10〜100GPUを使用 5〜10TFLOPS 2016〜2017年 TPUなどが登場 アルゴリズム上も並列度がとても高まった。 将来展望 多くのベンチャーがHWを開発。例えばFLOPSあたりの単価を下げる効果が期待される。 アルゴリズム側も進化を続けている。 一方で費用と物理制約の影響は大きい。 例えば、最も大きなモデルを学習するのに必要な計算リソースは100万ドル＝1億円である。 一方で、現在の多くの企業が支払っているのは、学習よりも推論側である。したがって、バランスを考えると 学習にもっと支払えるはず。 世の中のHW購入のバジェットは1兆ドルといわれており、その数字に照らし合わせるとまだまだ拡大の余地が残っている。 付録 計算方法についての補足が載っている。 FLOPSを直接計算可能な場合にはそうしたし、そうでない場合は使用したGPU数などから算出した、とのこと。 多くのケースで著者にも確認したそうだ。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"AI","slug":"Clipping/AI","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://dobachi.github.io/memo-blog/tags/AI/"},{"name":"Computing resource","slug":"Computing-resource","permalink":"https://dobachi.github.io/memo-blog/tags/Computing-resource/"},{"name":"Blog","slug":"Blog","permalink":"https://dobachi.github.io/memo-blog/tags/Blog/"},{"name":"Statistic","slug":"Statistic","permalink":"https://dobachi.github.io/memo-blog/tags/Statistic/"}]},{"title":"Distributed data stream processing and edge computing A survey on resource elasticity and future directions","slug":"Distributed-data-stream-processing-and-edge-computing-A-survey-on","date":"2019-07-09T14:38:13.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/07/09/Distributed-data-stream-processing-and-edge-computing-A-survey-on/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/09/Distributed-data-stream-processing-and-edge-computing-A-survey-on/","excerpt":"","text":"参考 メモ 1. Introduction 2. Background and architecture 2.1. Online data processing architecture 2.2. Data streams and models 2.3. Distributed data stream processing 2.4. Resource elasticity 3. Stream processing engines and tools 3.1. Early stream processing solutions 3.2. Current stream processing solutions 4. Managed cloud systems 4.1. Amazon Web Services (AWS) Kinesis 4.2. Google dataﬂow 4.3. Azure stream analytics 5. Elasticity in stream processing systems 5.1. Static techniques 5.2. Online techniques 5.3. Change and burst detection 6. Distributed and hybrid architecture 6.1. Lightweight virtualisation and containers 6.2. Application placement and reconﬁguration 7. Future directions 7.1. SDN and in-transit processing 7.2. Programming models for hybrid and highly distributed architecture 参考 Distributed data stream processing and edge computing: A survey on resource elasticity and future directions ウェーブレットと多重解像度処理 乱択データ構造の最新事情 －MinHash と HyperLogLog の最近の進歩－ Apache Edgent Twitter Heron論文 メモ Twitter Heronの論文を引用しているということでピックアップしてみた。 故障耐性を持ち、分散処理を実現するストリーム処理エンジンでは、処理の表現としてDAG構造を用いるものが多い印象。 一方で、宣言的なハイレベルのAPI（SQLなど）を提供するのは少数か？ （かつて、分散処理ではなかったり、故障耐性を持たなかったりした時代では、CQLが台頭していたが…） 全体的に、リソースアロケーションに関する研究では、Apache Stormのカスタムスケジューラとして実装して見せる論文が多く感じられた。 ただしエッジコンピューティングの前に、そもそもストリーム処理における弾力性（柔軟で安定したスケールアウト・イン）の実現自体がまだ枯れていない印象。 以下、一応、主観的に重要そうな個所に★マークを付与。 1. Introduction 大量のデータが生まれ始めた。 以下のような項目が要因の例として挙げられる。 計器化されたビジネス・プロセス ユーザアクティビティ ウェアラブルアシスタンス ウェブサイトトラッキング センサー 金融 会計 科学計算 ユースケースとしては、IoTなどが挙げられていた。 興味深い例としては以下。 123Rettig, L., Khayati, M., Cudré-Mauroux, P., Piórkowski, M., 2015. Online anomalydetection over big data streams. In: IEEE International Conference on Big Data (BigData 2015), IEEE, Santa Clara, USA, pp. 11131122 多くのツールがデータフローアプローチを取る。 つまりオペレータの有向グラフに変換して処理する。 一方でストリームデータを一定量ためてマイクロバッチするアプローチを取るツールもある。 スケーラビリティと故障耐性を狙った工夫である。 補足：これはSpark Streamingのことを意識した記述だと理解 さらにクラウド上での実行を想定したアプローチが登場。 ★ これにより、弾力性実現を狙う。 Google Dataflow (2015)の論文が引用されていた。 異なる実行モデルが混在しているケースを扱う技術として。 Apache Edgent, https://edgent.apache.org 2017. はインターネットのエッジで動作することを想定したフレームワークである。 過去の調査として、弾力性に注力していないものもあった。 123Zhao, X., Garg, S., Queiroz, C., Buyya, R., 2017. Software Architecture for Big Data andthe Cloud, Elsevier Morgan Kaufmann. Ch. A Taxonomy and Survey of StreamProcessing Systems. 本論文は以下の構成。 2章 ビッグデータエコシステム。オンラインデータ処理のアーキテクチャ。 3章 既存のエンジン。ストリームデータ処理の他のソリューション 4章 マネージドクラウドソリューション 5章 既存の技術がどのようにリソースの弾力性を実現しようとしているのか 6章 複数のインフラストラクチャを組み合わせる 7章 将来の話 2. Background and architecture 2.1. Online data processing architecture ここでは「オンライン」という単語を用いているが、以下のように定義して用いている。 12Similar to Boykin et al., hereafter use the term online to mean that “data areprocessed as they are being generated”. Fig. 1にストリーム処理アーキテクチャの典型的なコンポーネント関係図がある。（補足：当たり前の内容ではあるが参考になると思う） コンポーネント関係図 補足：ストレージとの関係性はもう少し深堀りしてもよさそう。最近だとDatabricks DeltaやApache Hudiのようなストレージレイヤのミドルウェアが生まれているため。 ★ まず「データソース」ではデータを送り出す（ローダ）、データフォーマット、MQTTのようなプロトコルに関する議論がある。 たいてい、送り出す機能はネットワークのエッジ側に置かれる。 コンポーネントそれぞれが独立して成長できるよう、MQ（ActiveMQ、RabbitMQ、Kestrel）、Pub/Subベースのソリューション（Kafka、DistributedLog）、 あるいはマネージドサービス（Amazon Kinesis Firehose、Azure IoT Hub）が挟まれることが多い。 補足：http://bookkeeper.apache.org/distributedlog/ 補足：DistributedLogは2017年にBookKeeperのサブプロジェクト化された。 https://bookkeeper.apache.org/docs/latest/api/distributedlog-api/ 大規模データを扱うための技術の代表例として、MapReduceが挙げられていた。 12Dean, J., Ghemawat, S. MapReduce: Simpliﬁed data processing on large clusters.Communications of the ACM 51 (1). 本論文で扱う技術は、ストリーム管理システムだったり、CEPと呼ばれる。 補足：ここではストリーム処理とCEPは分けて言及していないようだ。 ストリーム処理のアーキテクチャはデータストレージ機能を内包することがある。 目的は、さらなる処理のためだったり、結果を分析者等に渡すためだったりする。 ★ このとき用いられるストレージは、RDBMS、KVS、インメモリデータベース、NoSQLなどなど。パブリッククラウドにもいろいろな選択肢がある。 補足：ステート管理用のストレージを持つ場合もあるだろう。 ★ 2.2. Data streams and models 「データストリーム」の定義は様々。 不連続の信号、イベントログ、モニタリング情報、時系列データ、ビデオ、などなど。 2005年くらいの調査では、時系列データ、キャッシュレジスタ、回転扉あたりのデータモデルが対象だった。 一方最近ではSNS等から生じる非構造データ、準構造データが見受けられる。 データ構造としては、キー・バリュー形式を挙げており、キーがタイムスタンプである。 多くのケースで、ストリーム処理はオペレータのDAG構造を抽象化として用いる。 ★ 論理プランと物理プラン。 物理プラン側で並列度などを含め、クラスタのリソースにオペレータを割り当てることになる。 オペレータは選択性、ステートの軸でカテゴライズできる。 並列度に関するカテゴリもある。 ■並列度のカテゴリ パイプライン並列 アップストリームのオペレータは並列でタプルを処理できる タスク並列 グラフの中には同じタプルを並列で処理できる箇所がある データ並列 スプリッタでデータを分割し、マージャで処理されたデータを結合する 気になった論文 123Gedik, B., Özsema, H., Öztürk, O., 2016. Pipelined ﬁssion for stream programs withdynamic selectivity and partitioned state. J. Parallel Distrib. Comput. 96, 106120.http://dx.doi.org/10.1016/j.jpdc.2016.05.003. 2.3. Distributed data stream processing DBMSと対比し、DSMS（Data Stream Management System）と呼ぶことがある。 ★ DSMSは以下のようなオペレータを持つ。 join aggregation filter その他分析用の機能 初期のDSMSはSQLライクなインターフェースを提供していた。 CEPはイベント間の関係を明らかにする、などの用途に用いられた。 補足：ここからストリーム処理エンジンの世代説明。一般的な説明をするときに使えそう。 ★ 第1世代は、DBMSの拡張として用いられ、単一マシンで実行することを想定していた。（分散処理を考慮していなかった） 第2世代は、分散処理対応 ここでIBM System Sの登場。ゴールはスケーラビリティと効率の改善。 最近（第3世代）では、スケーラブルで故障耐性のある実行を目指している。 ただし、この手の技術は宣言的なインターフェースを持たず、ユーザがアプリケーションを書く必要がある。 UDFを用いることができる。 入力データを離散化（小さなバッチに区切り）し、マイクロバッチ処理を実行するものもあった。 ある程度まとめこまれたデータに対し、繰り返し処理を実行するものもある。 目的はスループット向上。ただし、低レイテンシの要求が厳しくない場合を対象とする。 第4世代は、エッジコンピューティングを含む。 この手の技術として挙げられていたのは以下の通り。 123Sajjad, H.P., Danniswara, K., Al-Shishtawy, A., Vlassov, V., 2016. SpanEdge: Towardsunifying stream processing over central and near-the-edge data centers. In: IEEE/ACM Symposium on Edge Computing (SEC), pp. 168178. SPEとして。 123Chan, S., 2016. Apache quarks, watson, and streaming analytics: Saving the world, onesmart sprinkler at a time. Bluemix Blog (June).URL 〈https://www.ibm.com/blogs/bluemix/2016/06/better-analytics-with-apache-quarks/〉 1234Pisani, F., Brunetta, J.R., do Rosario, V.M., Borin, E., 2017. Beyond the fog: Bringingcross-platform code execution to constrained iot devices. In: Proceedings of the 29thInternational Symposium on Computer Architecture and High PerformanceComputing (SBAC-PAD 2017), Campinas, Brazil, pp. 1724. クラウドとエッジ。 123Hirzel, M., Schneider, S., Gedik, B., An, S.P.L., 2017. extensible language for distributedstream processing. ACM Trans. Program. Lang. Syst. 39 (1), 5:15:39. http://dx.doi.org/10.1145/3039207, (URL 〈http://doi.acm.org/10.1145/3039207〉). 2.4. Resource elasticity クラウドの特徴の一つは、支払えばリソースを使えること。 もうひとつはリソース弾力性である。★ オートスケーリングも可能とする。 オートスケーリングに対応するには、スケールイン・アウトしたリソースに対応する仕組みも必要だが、 オートスケーリングのポリシーも必要。 オートスケーリングの仕組みは、主にバッチ処理についてビッグデータのワークロードに関し、 いくつか提案手法が存在する。★ 補足：クラウドベースのオートスケーリングについて以下の論文で触れられているらしい。 123Tolosana-Calasanz, R., çngel Ba-ares, J., Pham, C., Rana, O.F., 2016. Resourcemanagement for bursty streams on multi-tenancy cloud environments. FutureGener. Comput. Syst. 55, 444459. http://dx.doi.org/10.1016/j.future.2015.03.012. ここでのポイントは、「シビアな遅延があること」。 特にSPEでは、バッチ処理と比べてスケールイン・アウトの要件が厳しい。 例えば、処理を動かし続けるから、データロストしないでスケールイン・アウトさせるのが難しいし、 スケールイン・アウト後のオペレータの再配置が難しい。 補足：つまりバッチ処理におけるスケールアウト・インはある程度実現性があるが、ストリーム処理のスケールアウト・インはまだまだ研究段階？であると言えるのか ★ 3. Stream processing engines and tools SPEの歴史を振り返る。 3.1. Early stream processing solutions DBMSの拡張として登場。 2000年代。 ただし近年の技術と比べて、大規模データを対象としていない。 NiagaraCQ 2000年。 XMLデータに対し、定期的に繰り返し処理を実行する。 STREAM 2004年。 CQL。Continuous Query Lang. CQLをクエリプランにコンパイルする。 DAG構造で処理を表す。 実行時には、スケジューラがオペレータをリソースに割り当てる。 チェーンスケジューリングを用いる。 補足：このあたりではUDFについて言及されていない。 Aurora モニタリングアプリケーション向け。 STREAMと同様に、CQSを利用可能。 トレインスケジューリングを採用。 非直線性。 入力タプルをボックスに集め、それに対し処理を実行する。これによりI/Oオペレーションを減らす。 Medusa MedusaはAuroraをクエリ処理エンジンとして使いながら、分散処理対応させた。 UDFには対応していない。 3.2. Current stream processing solutions 2種類にわけられる。 オペレータグラフに基づきタプル単位で処理する（補足：こっちがいわゆるコンティニュアス・ストリーム処理） 入力データをある程度の塊に区切り、マイクロバッチ処理として処理する（補足：こっちがいわゆるマイクロバッチ処理によるストリーム処理） 3.2.1. Apache Storm アプリケーションはTopologyと呼ばれる。 データはチャンクとして取り込まれ、タスクを割り当てられたノードで処理される。 データはタプル群として扱われる。 Nimbusがマスタ機能であり、ZooKeeperと連携しながら、 タスクをスケジューリングする。 WorkerノードにはSupervisorがいて、（複数の）Workerプロセスを管理する。 Workerプロセス内では（複数の）Executorスレッドが起動され、SpoutかBoltの機能を実行する。 Spoutがデータソースからデータを取得し、Boltがタプルを処理する。 フィルタ、アグリゲーション、ジョイン、データベースクエリ、UDFなどを実行する。 並列度を制御するには、Topologyごとに各コンポーネントの並列度、 Executor数を指定する。 Workerノードを追加した際、新しいTopologyをローンチ可能。 また、Workerプロセス数やExecutorスレッド数を変更可能。 一方で、タスク数などを変更するときには、一度Topologyを止める必要がある。 ★ 特にステートを持つオペレータを用いているときは問題が複雑。 性能チューニングは、Executorの入出力キューの長さ、Workerプロセスのキューの長さを変更することで可能。 またTridentを用いたマイクロバッチ処理が可能になっているし、Summingbirdのような 抽象化層が生まれている。 3.2.2. Twitter heron 2015年の論文。 Stormの当時の欠点をいくつも改善しつつAPI互換性を実現した。 補足：なお、現在のApache Storm実装では、Heron論文で挙げられていた課題が多数解決されている。 Topologyはプロセスベースであり、デバッグしやすいさなどを重視。 ★ バックプレッシャ機能も有する。 Stormと同様にアプリケーションがDAGのTopologyで表現され、SpoutとBoltも存在。 各種コンテナサービスと連携する。 Aurora on Mesos、YARN、ECS。 Topologyがローンチされると、Topologyマスタ（TM）と複数のコンテナが立ち上がる。 ZooKeeperを用いてTMが単一であることを保証し、ほかのコンポーネントからTMを発見できるようにする。 TMはゲートウェイ機能にもなるし、スタンバイTMを用いることも可能。 各コンテナは、Heronインスタンス（HI）群、ストリームマネージャ（SM）、メトリクスマネージャ（MM）を起動する。 コンテナ間でSM同士がフルメッシュのNWを構成する。 Spout、BoltはHIが担うことになるが、HI自体はJVMである。これはStormと異なる点である。★ またHIは2スレッドで構成される。 1個目スレッドはユーザ定義の処理内容を実行し、2個目スレッドはほかのコンポーネントととの連絡を担う。 Heronトラッカ（HT)はTopology群の情報を得るための、クラスタ単位のゲートウェイである。 補足：詳しくは、 Twitter Heron論文 を参照。 3.2.3. Apache S4 2010年。 並列処理の管理にアクターモデルを用いる。 ★ プロセッシングエレメント（PE）がイベント交換と処理を担う。 分散型で、シンメトリックなアーキテクチャを採用。 プロセッシングノード（PN）は機能性に関し同一。 ZooKeeperがPNのコーディネーションのために用いられる。 アプリ開発者は、PEの処理内容を定義しないとならないが、 その際キーに基づく処理を作りこむことが多い。（キーのハッシュに基づく処理の割り当て） ほかにキーを用いないPEは、例えば入力データを扱うPEである。 3.2.4. Apache samza 2017年。 Apache Kafkaをメッセージング、YARNをデプロイメント、リソース管理、セキュリティ機能のために用いる。 データフローを定義して用いるが、コンシューマを定義する。 ネイティブではDAG構造による処理の定義に対応していない。 ★ Heronと同じくシングルスレッドモデル。 各コンテナはローカルのステート管理用のKVSを持つ。 KVSはほかのマシンに転送される。これにより故障耐性を有する。 3.2.5. Apache ﬂink 2015年。 DAG型のデータフローを定義する。 「ストリーム」は中間結果で、「トランスフォーメーション」が複数のストリームを入力とし、複数のストリームを出力とする。 並列度はストリームとオペレータの並列度に依存する。 ストリームはパーテイションに分割され、オペレータはサブタスクに分割される。 サブタスクは異なるスレッドに割り当てられる。 ジョブマスタ（JM）とタスクマネージャ（TM）。 JMがタスクのスケジューリング、チェックポイントなどに責任を持つ。 TMがサブタスクを実行する。 ワーカはJVMであり、サブタスクごとの独立したスレッドを実行する。 ★ スロットの概念があり、Stormと異なりメモリ管理機能を有する。 ★ 3.2.6. Spark streaming 2012年。 イミュータブルなデータセットの抽象表現であるResilient Distributed Datasets (RDDs) のDAGでアプリケーションを表現。 故障耐性向上のため、RDDのリネージを利用。 トラディショナルなストリーム処理エンジンでは、タプルの継続的な処理を担うオペレータのグラフによるモデルを採用しており、 故障耐性を担保するのが難しい。 そのようなエンジンでは、セクションのレプリケーション、もしくはアップストリームでのバックアップで実現する。 Spark Streamingでは故障耐性を実現するのに際し、RDDを使ったマイクロバッチ処理をベースとし、 RDDの故障耐性を利用することとした。 3.2.7. Other solutions System S（IBM Streams）はオペレータのDAGをアプリケーション構造に採用。 ★ 構造化、非構造化データの両方に対応。 SPL（ストリーム処理言語）に対応。 ストリーム処理コア（SPC）を有効活用するようSPLのコンパイルと最適化も実施する。 SPLについては以下の論文が気になる。 123Hirzel, M., Schneider, S., Gedik, B., An, S.P.L., 2017. extensible language for distributedstream processing. ACM Trans. Program. Lang. Syst. 39 (1), 5:1–5:39. http://dx.doi.org/10.1145/3039207, (URL： http://doi.acm.org/10.1145/3039207 ). ESC（以下の論文参照）もオペレータを中心としたDAGによるアプリケーション表現を採用。 並列処理の管理にはアクタモデルを採用。 ★ 123Satzger, B., Hummer, W., Leitner, P., Dustdar, S., 2011. Esc: Towards an elastic streamcomputing platform for the cloud. In: IEEE International Conference on CloudComputing (CLOUD), pp. 348–355. TimeStream（2013）もDAGによるアプリケーション表現を採用。 GoogleのMillWheel。2013年。 こちらもデータ変換等のグラフを抽象化してアプリケーションを作成する。 ホストの動的なセット上で動作することを想定しており、 リソース使用状況に応じて処理単位を分割する、などの挙動をとる。 Efficient Lightweight Flexible （ELF） ストリーム処理システム（2014年）は 非中央集権的アーキテクチャを採用。 各マシンがそれぞれウェブサーバからデータを取得し、それぞれがバッファツリーにデータを保持する。 各マシンで処理されたデータは、オーバレイ構成のDHTを用いて、Reduce処理される。 4. Managed cloud systems 4.1. Amazon Web Services (AWS) Kinesis ファイアホースを使うと、RedshiftやS3等に対し、ストリームデータを格納できる。 ★ ファイアホースはバッファリングを行う。 CloudWatchと連携すると、各サービスのメトリクスを集められる。 Amazon Kinesis Streamsはストリーム処理の仕組み。 ストリームデータはシャードに分けられて分散処理される。 シャードには固定のキャパシティが存在する。 単位時間あたりのオペレーション数やデータサイズである。 4.2. Google dataﬂow プログラミングモデルであり、マネージドサービスである。 ETLなどを含むバッチ処理とストリーム処理の両方を対象とする。 ★ ステップや変換を有向グラフにしたもので処理を定義する。 「PCollection」が入出力データの抽象表現である。 PCollectionには固定サイズのデータ、もしくはストリームデータをタイムスタンプで区切ったものが与えられる。 トリガを用いていつ出力するかを制御する。 SDKもある。Apache Beamプロジェクト配下でリリースされている。 Dataflowサービスは、ジョブを実行するためにGCE、GCSを管理する。 オートスケールや動的なリバランス機能を使って、オンザフライでリソースアロケーションを調整できる。 （論文当時）バッチ処理のオートスケールはリリースされていたが、ストリーム処理のオートスケールは実験段階だった。 補足：ステートフルなオペレータでも問題なくスケールアウト・インされるのか？要確認。 ★ 4.3. Azure stream analytics Azure Stream Analytics（ASA）のジョブ定義は、 入力、クエリ、出力で定義される。 Azure Event Hub、Azure IoT Hub、Blobサービスなどとの連携も可能。 T-SQLの亜種を用いた分析処理が可能。 書き出し（Sink）は色々と対応している。 Blob、Table、Azure SQL DB、Event Hub、Service Queueなど。 本論文執筆時点で、UDFには対応していない。 Streaming Units（SU）の単位で分散処理される。 なお、SUはCPU、メモリなどを包含した単位。 5. Elasticity in stream processing systems 弾力性は、モニタリング、分析、計画、実行（MAPE）プロセスで実現される。 弾力性の実現には互いに関係する2種類の課題が存在する。 アプリケーションワークロードに合致するITリソースを確保したり、解放したりする：elastic resource management 追加・解放されたリソースに合わせ、アプリケーションを調整する スケールアウト・イン計画に基づき、アプリケーション調整を行うのをelasticity actionsと呼ぶ。 水平、垂直の両方向のスケールの仕方が存在する。 オートスケールに対応するため、アプリケーションには調整が必要になることがある。 例えば、実行グラフの調整、中間クエリの並列度の調整、など。 ★ 水平分散はしばしば、処理グラフの適応（補足：つまり、グラフ形状の最適化）、オペレータの持つステートの出力（補足：つまりステートも引き継がないといけない）が必要になる。 ★ ウィンドウ化、もしくはシーケンス化されたオペレータを用いる場合、（ステート管理が必要になるため） 並列処理（のスケールアウト・インが）難しくなる。 ストリーム処理ではロングランなジョブを無停止で動かしたい、ということがあり、 ますますスケールアウト・インするときの扱いが難しい。 ★ 弾力性の実現アプローチには、staticとonlineの2種類がある。 staticでは並列度やオペレータの位置を（補足：あらかじめというニュアンスがある？）最適化する。そのためにグラフを最適化する。 onlineではリソースプールの変更、新しいリソースを活用するためのアプリケーションの動的変更を含む。 5.1. Static techniques 2008年あたりの論文によると、初期タスクアサインと中間処理の並列度の最適化アプローチでは、 水平スケールが可能だった。 R-Storm（2015年）ではStormのカスタムスケジューラを提案。 ★ CPU、メモリなどのリソースプールに基づき、スケジューリングする。 これは要は、ナップサック問題に帰着する。 ★ 当該論文では、従来手法が計算量大きすぎるとし、ここでは 「タスクを必要リソースのベクトル、ノードをリソースバジェットのベクトル」と見立て、 ベクトル間のユークリッド距離で最適解を求める手法を提案した。 タスク間の通信にはヒューリスティックスを利用。 SBON（Stream Based Overlay Network）（2006年）では、 レイテンシを考慮しながらネットワークを構成する。 コストスペースという多次元ユークリッド空間を定義し、 適応的な最適化手法を利用する。 Zhouらが提案した手法（2006年）では通信コストを最小化する。 ロードバランスも考慮。 クエリを分割し、極力アサインされるノードを減らす。 ほかにも、 AhmadやÇetintemel (2004年）の論文では ネットワークバンド幅を最小化するアルゴリズムが提案されている。 5.2. Online techniques ストリーム処理で弾力性を実現するには、以下の2個の要素が必要。★ リソース使用状況、サービスレベルのメトリクス（エンドツーエンドのレイテンシなど）などを 観測する仕組み スケーリングポリシー 多くのソリューションはアプリケーション（というより、オペレーション）をブラックボックスとして扱い、 メトリクスから最適化を行う。 SattlerやBeierはこれらの手法は信頼性向上にもつながる、としている（2013） その観点でいえば、タスクがボトルネックになった際に、オペレーショングラフのリライトを起こすべき、となる。 （補足：つまり、入力に対して、処理が追い付かなくなってきたとき、など） ステートフルなオペレータの最適化は非常に困難。 調整が走っている間、オペレータのステートは適切にパーティション化されていないといけない。 ★ （補足：しかもストリーム処理なのでレイテンシも気にされることになる） 補足：以下、関連論文の紹介が続く。 Fernandezらの論文（2013）では、オペレーションのステートをストリーム処理エンジンと統合するアプローチを提案した。 つまりステートはタプルの形式で定期的にチェックポイントされるようになり、 オペレータのスケールの最中、新旧のオペレータ間でリパーティションされるようになる。 ★ CPU使用状況がメトリクスとして管理され、複数のオペレータがボトルネックになっていることがわかるとスケールアウトを実行するようになっている。 T-Storm (Xu etら。2014年) = Traffic Aware Storm。 通信コストを減らすようなスケジュールをカスタムスケジューラを導入して実現。 Anielloらの論文（2013）でも同様のアプローチを提案。 オフラインで動作するスタティックなスケジューラと動的なスケジューラの両方を提案。 Lohrmannらの論文（2015）では、リソース使用を抑えつつ、与えられたエンドツーエンドの レイテンシを守るためのスケジューリングを実現する方法を提案。 ノードは均一であることを前提としている。 Rebalance（パーティション再アサイン）とResolveBottleneck（スケールアウト）の2種類の方法を使って実現。 ESCストリーム処理エンジンは、タスクスケジューリング、性能モニタ、リソースプール管理が協調して動作する。 またUDFを実行するプロセスは、ゲートウェイ機能（PEマネージャ）と実行機能（PEワーカ）の両方を内包。 マネージャはワーカのロードバランス機能を持っている。 リソース状況がひっ迫すると、PEマネージャごとPEワーカを止め、スケールアウトを実行する。 StreamCloud（2012）は、クエリをサブクエリに分け、独立した複数の計算クラスタに割り当てる。 ステートフルなオペレータに依存して分割度合いが決まる。 リソース管理の仕組みと、ロードバランスの機能を持つ。 Heinzeらの論文(2014)では、リソースの配置（再配置）を行う際に レイテンシのスパイクを考慮に入れる。 オペレータの配置は、「インクリメンタルなビン・パッケージング問題」とみなすことができる。 ここでは主にCPU使用率やキャパシティを対象とする。（メモリやネットワークも考慮はするようだ） 補足：ここからIBM Streams（System S?）の関連論文が数件続く。 Gedikらの論文(2014) では、IBM Streamsを対象として自動並列化を狙った。 ステートフルなオペレータも含むエラスティックな自動並列化を提案。 スプリッタにおけるブロッキング時間やスループットをメトリクスとし、並列度を決める。 データスプリッタでは、ステートレスの場合はラウンドロビンに従い、 そうでなければハッシュベースになる。 Tang and Gedik (2013) らの論文では、IBM Streamsを対象とし、 タスクやパイプラインの並列化を提案。 オペレータのパイプラインのスレッドを管理。スレッドを追加することでスループットを向上させる。 Gedik等の論文(2016) では、IBM Streamsを対象とし、パイプライン並列、データ並列の両方を実現する手法を提案。 チェーンのようにつながったデータフローグラフを分割する。 その際、オペレータがレプリケート可能かどうかを考慮し、並列化する。 Wu and Tanの論文 (2015) では、以下の点に関する検討を実施。 大きなサイズのステートを管理 ワークロードの変動 マルチテナントの実現 彼らはChronoStreamを提案。 弾力性とオペレータマイグレーションの実現のため、アプリケーションレベルのステートを 計算ノードレベルのステートに分割し、スライス化する。 スライス化されたステートは、チェックポイントされ、ほかのノードに複製される。 スライスごとに計算の進捗は管理される。（このあたりがSparkのDStreamと異なる） Xuらの論文（2016）では、Stela（Stream Processing Elasticity）を提案。 スケールアウト・インの後のスループットを最適化し、計算がストップするのを極力防ぐ。 Expected Throughput Percentage (ETP)をメトリクスとして使用。 オペレータの処理速度が変わった場合に最終的にどんなスループットになるのか？の値。 本手法はStormのスケジューラの拡張として提供されている。 Hidalgoらの論文では、オペレータの分割で並列化を実現する。 その際、オペレータの状態を管理するため以下の2種の手法を使用 ★。 short-termアルゴリズム トラフィックのピークを検知するために短期間のロードを観測 下限上限で管理 long-termアルゴリズム トラフィックパターンを把握するため長期間のロードを観測 マルコフチェーンモデルで管理 上記に基づき、当該オペレータが過負荷、安定、負荷不足なのかを予測する。 ★ ここ最近、コンテナや軽量な仮想化技術の利用に関する研究も盛んである。 Pahl and Lee らの論文(2015) では、コンテナをエッジ、クラウドコンピューティングの 弾力性を高めるための研究を実施。 Ottenwälderらの論文 (2013) では、オペレータ配置とマイグレーションを支援するため、 システムの特徴と移動体パターンの予測を用いる。 クラウドコンピューティングとフォグコンピューティングの両方を含む。 移動体は、近くのブローカに接続する。 これによりネットワークコストを低減し、エンドツーエンドのレイテンシも改善する。 オペレータのマイグレーション計画は動的に変わる。 その際タイムグラフモデルが用いられる。 以下に、ツール群の比較表を引用する。 ★ ツール比較表 5.3. Change and burst detection 入力データの変化、バーストを検知すること。（補足：というのが重要、ということ） それ自体は弾力性の実現ではないが、トリガとして関係することがある。 Zhu and Shashaの論文 (2003) では、移動ウェーブレットツリー・データ構造を用いて、 それらを検知する。ウィンドウの取り方は3種類。 ランドマーク（固定）、移動式、過去分を減衰。 参考：ウェーブレットと多重解像度処理 そのほか、Krishnamurthyらの論文(2003) では、Sketchを用いる。 参考：乱択データ構造の最新事情 －MinHash と HyperLogLog の最近の進歩－ 6. Distributed and hybrid architecture 旧来の多くの分散ストリーム処理の技術は、クラスタを対象としていた。 最近の研究では、エッジを考慮したものが登場している。 インターネットのエッジであるマイクロデータセンタは、しばしば「Cloudlets」と呼ばれる。 データ転送量の低減、エンドツーエンドのレイテンシ改善、クラウドからのオフロードが目的。 ただし、多くの手法はまだ模索段階。 6.1. Lightweight virtualisation and containers 軽量な仮想化、もしくはコンテナを利用する、というのがしばしば挙がる。 Yanguiらの論文(2016)は、クラウド・フォグコンピューティングのPaaSを提案。 Cloud Foundaryを拡張。 Morabito and Beijarの論文 (2016) は、エッジ・コンピューテーション・プラットフォームを 提案した。リソースの弾力性を実現するため、コンテナを利用する。 シングルボードのコンピュータをゲートウェイとして利用する。 データ圧縮などに利用する。 Petroloらの論文（2016）も、同様にゲートウェイデザインを提案。 ワイヤレスセンサーネットワーク向け。 Hochreinerらの論文 (2016) では、エラスティックなストリーム処理の仕組みとして、VIennaエコシステムを提案。 プロセッシンググラフを書くためのグラフィカルなユーザインターフェースも供える。 6.2. Application placement and reconﬁguration モバイルクラウドとヘテロジーニアスメモリに関するGaiらの論文（2016) では、 ハイブリッドシナリオでのタスクスケジューリングについて触れられている。 Benoitらの論文 (2013)、Roh等の論文（2017）では、ヘテロジーニアス・ハードウェアやハイブリッドアーキテクチャにおける コンピュータリソース使用に関するスケジューリングについて取り扱っている。 Cardelliniらの論文 (2016)では、 Optimal Distributed Stream Processing Problem (ODP)向けに ヘテロジーニアスなリソースを考慮した integer programming formulation を提案。 Stormを拡張し、ODPベースのスケジューラを採用。 ネットワークレイテンシを推測。 地理分散されたVMに、ストリーム処理のオペレータを配置する問題は、NP困難（ないし、NP完全）である。（2016、2017） しかし、コストアウェアなヒューリスティックは提案されている。(Gu et al., 2016; Chen et al.,） SpanEdge（Sajjad et al. (2016) ）は、中央とエッジのデータセンタを使うストリーム処理ソリューション。 マスタ・ワーカ構成のアーキテクチャであり、hubワーカ（中央）とspokeワーカ（エッジ）を持つ。 スケジューラはローカルタスクを各エッジ上のワーカで実行しようとする。 ★ Mehdipourらの論文 (2016) は、クラウドとフォグの間の通信を最小化するように動く。 IoTデバイスからのデータを処理する。 Shenらの論文(2015) は、CiscoのConnected Streaming Analytics（CSA）を 利用する。データセンタとエッジの両方を利用する。 ★ CSAが継続的クエリのためのクエリ言語を提供する。 Geelytics（Chengらの論文 2016）は、IoT環境のためのシステム。複数の地理分散されたデータProducer、結果Consumer、計算リソースを活用する。 それらはクラウドもしくはネットワークエッジのいずれかに置かれる。 マスタ・ワーカアーキテクチャであり、Pub/Subモデルのサービスを採用。 アプリはオペレータのDAG。 スコープ限定されたタスクが特徴的。 各タスクのスコープの粒度をユーザが指定可能。 データProducerの地理情報に基づきresultタスクを配置する。 7. Future directions 大きな組織におけるビッグデータソリューションは、 バッチとオンラインの両方を有することになる。 ★ また複数のデータセンタにまたがることもあり、それらの中で弾力的にリソースを使用できることが理想的である。 ここでは将来の展望を述べる。 7.1. SDN and in-transit processing SDN等を通じて、ネットワーク経路の途中で計算を行う。 このアプローチは、セキュリティとリソース管理の課題をはらむ。 例えば、IoTデバイスからデータセンタの経路の途中で計算させることは、攻撃可能な個所を増やすことにつながる。 また状況に合わせたリソース配置も困難だ。 既存の多くの研究では、複数のオペレータの配置においては、 レイテンシやバンド幅などのネットワーク・メトリクスに着目する。 一方で、ネットワークがプログラム可能であることは考慮されていない。 コグニティブ・アシスタンスのユースケースでは、データセンターで機械学習モデルが学習され、 その後エッジ側で推論される。 ★ このときのひとつの課題は、結果的に生じるコンセプト変動である。 7.2. Programming models for hybrid and highly distributed architecture Boykinら（2014）、Google Cloud Dataﬂow（2015)のように、ハイレベルの抽象化を実現したものがある。 これらのソリューションは単一のクラスタやデータセンタでの利用に限られているが、 一部これをエッジコンピューティングまで拡張する取り組みが生じている。 そのような条件下で、リソース使用を効率的に管理する手法に関する研究が多く行われるだろう。 Apache Beam（2016）のプロジェクトでは、ユニファイドなSDKを提供。 Apache SparkやApache Flinkに対応。 一方で、エッジ・フォグ・データセンタコンピューティングを横断するユニファイドなSDKはない。 Apache Edgent （2017) がインターネット・エッジでの計算・分析を可能にする。 ★ 参考：Apache Edgent","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"}]},{"title":"Windows Tools","slug":"Windows-Tools","date":"2019-07-08T14:17:03.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/08/Windows-Tools/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/08/Windows-Tools/","excerpt":"","text":"SurfEasy WindowsとAndroidで共通して使えるVPNソリューション 自動でVPN有効化してくれる 公衆無線LANに接続するときに安心 Lenovo関係で知った Dashlane パスワード、ID、支払い方法等の管理ソリューション WindowsとAndroidで共通して使える Lenovo関係で知った VMWare Workstation Pro Vagrantのvagrant-vmware-desktopプラグインで用いるのはWorkstation Proであることに注意。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Windows Tools","slug":"Clipping/Windows-Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Windows-Tools/"}],"tags":[{"name":"Windows Tools","slug":"Windows-Tools","permalink":"https://dobachi.github.io/memo-blog/tags/Windows-Tools/"},{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/tags/Clipping/"}]},{"title":"WSLからVagrantを使う","slug":"Vagrant-with-WSL","date":"2019-07-08T13:42:22.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/08/Vagrant-with-WSL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/08/Vagrant-with-WSL/","excerpt":"","text":"参考 メモ 参考 Windows10 WSL から Vagrant を起動する （公式）Vagrant and Windows Subsystem for Linux メモ 基本的には、 （公式）Vagrant and Windows Subsystem for Linux の通り進める。 まずWindowsとWSLの両方に、Vagrantをインストールする。 両方のバージョンが一致するように気を付ける。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"Vagrant","slug":"Knowledge-Management/WSL/Vagrant","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/Vagrant/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"},{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"Vagrant","slug":"Vagrant","permalink":"https://dobachi.github.io/memo-blog/tags/Vagrant/"}]},{"title":"ZooKeeperをスーパバイザでセルフヒーリングする","slug":"ZooKeeper-supervision","date":"2019-07-08T13:35:59.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/08/ZooKeeper-supervision/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/08/ZooKeeper-supervision/","excerpt":"","text":"参考 メモ 参考 zookeeperAdmin.html#sc_supervision メモ Heron公式ドキュメントを参照していて改めて気づいたのだが、 ZooKeeperはSupervisor機能で自浄作用を持たせた方がよいだろう。 公式ドキュメントの zookeeperAdmin.html#sc_supervision では、 daemontoolsやSMFをスーパバイザの例として挙げていた。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"ZooKeeper","slug":"Knowledge-Management/ZooKeeper","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/ZooKeeper/"}],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://dobachi.github.io/memo-blog/tags/ZooKeeper/"},{"name":"Supervision","slug":"Supervision","permalink":"https://dobachi.github.io/memo-blog/tags/Supervision/"}]},{"title":"Hexoでふっとノートを用いる","slug":"Hexo-footnote","date":"2019-07-08T05:03:19.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/07/08/Hexo-footnote/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/08/Hexo-footnote/","excerpt":"","text":"参考 参考 hexo-renderer-markdown-it ＃メモ 最初はhexo-footnoteを見つけたのだが、開発停止されているようなので、 そこで言及されていた hexo-renderer-markdown-it を利用してみることにした。 院すとーすして、以下のようなコンフィグレーションを _config.yml に追加。 1234567891011121314151617181920markdown: render: html: true xhtmlOut: false breaks: false linkify: false typographer: false #quotes: &apos;“”‘’&apos; plugins: - markdown-it-abbr - markdown-it-footnote - markdown-it-ins - markdown-it-sub - markdown-it-sup anchors: level: 2 collisionSuffix: &apos;v&apos; permalink: true permalinkClass: header-anchor permalinkSymbol: ¶ これでフットノートが使えるようになった。 しかし、アンカーが使えないのだがまだ詳しく調査していない。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"},{"name":"Hexo Plugin","slug":"Hexo-Plugin","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo-Plugin/"}]},{"title":"後で調べるリスト","slug":"List-to-research-later","date":"2019-07-08T04:14:37.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/07/08/List-to-research-later/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/08/List-to-research-later/","excerpt":"","text":"sulrm Heronを調べる中で引っかかった話 Apache Aurora Heronを調べる中で引っかかった話","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"List","slug":"Clipping/List","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/List/"},{"name":"Research","slug":"Clipping/List/Research","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/List/Research/"}],"tags":[{"name":"Research later","slug":"Research-later","permalink":"https://dobachi.github.io/memo-blog/tags/Research-later/"}]},{"title":"ライティングに関する記事","slug":"Lighting-using-flashes","date":"2019-07-07T14:37:41.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/07/07/Lighting-using-flashes/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/07/Lighting-using-flashes/","excerpt":"","text":"参照 メモ スピードライトを使ったテクニック 参照 スピードライトだけで海外のような写真 メモ スピードライトを使ったテクニック スピードライトだけで海外のような写真 に記載あり。作例も載っているのでわかりやすい。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Camera","slug":"Clipping/Camera","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Camera/"},{"name":"Lighting","slug":"Clipping/Camera/Lighting","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Camera/Lighting/"}],"tags":[{"name":"Camera","slug":"Camera","permalink":"https://dobachi.github.io/memo-blog/tags/Camera/"},{"name":"Lighthing","slug":"Lighthing","permalink":"https://dobachi.github.io/memo-blog/tags/Lighthing/"}]},{"title":"重大事故の時にすべきこと","slug":"What-to-do-when-you-encountered-an-incident","date":"2019-07-07T11:58:06.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/07/What-to-do-when-you-encountered-an-incident/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/07/What-to-do-when-you-encountered-an-incident/","excerpt":"","text":"参考 メモ 参考 重大事故の時にどうするか？ メモ 重大事故の時にどうするか？ は、ソフトバンクのZコーポレーションのミヤサカさんが書かれた記事。 重大事故の時に何をすべきか？が書かれたブログ。 リーダ目線での具体的な行動指針が書かれている。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Management","slug":"Clipping/Management","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Management/"}],"tags":[{"name":"Incident","slug":"Incident","permalink":"https://dobachi.github.io/memo-blog/tags/Incident/"},{"name":"Management","slug":"Management","permalink":"https://dobachi.github.io/memo-blog/tags/Management/"}]},{"title":"Twitter Heronの論文","slug":"Twitter-Heron-Paper","date":"2019-07-03T12:59:46.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/07/03/Twitter-Heron-Paper/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/07/03/Twitter-Heron-Paper/","excerpt":"","text":"参考 論文の要点メモ 1. Introduction 2. Related Work 3. Motivation for Heron 3.1 Storm Background 3.2 Storm Worker Architecture: Limitations 3.3 Issues with the Storm Nimbus 3.4 Lack of Backpressure 3.5 Efficiency 4. Design Alternatives 5. Heron 5.1 Data Model and API 5.2 Architecture overview 5.3 Topology Master 5.4 Stream Manager 5.5 Heron Instance 5.6 Metrics Manager 5.7 Startup Sequence and Failure Scenarios 5.8 Architecture Features: Summary 6. Heron in Production 6.1 Heron Tracker 6.2 Heron UI 6.3 Heron Viz 6.4 Heron@Twitter 7. Empirical Evaluation 7.3 Word Count Topology 7.4 RTAC Topology 8. Conclusions and Future work 最近のHeronはどうか？のメモ READMEによる「Update」 公式ドキュメントを覗いてみる Heron's Design Goals 商用サポートはあるのか？ 参考 Twitter Heronの論文要旨 Trident Apache incubation Heron Apache Heronのコミット状況 Apache Heronのコントリビュータ Sanjeev Kulkarni objmagic Ning Wang slurm 公式ドキュメント Storm vs. Heron Introduction to Apache Heron by Streamlio Streamlioのサポートサービス Streamlioのプロセッシングエンジンに関する説明 Pulsar functions 論文の要点メモ 昔のメモをコピペ。 なお、 Storm vs. Heron のブログに記載の通り、 本論文当時のStormは古く、現在のStormでは解消されている（可能性のある）課題が取り扱われていることに注意が必要。 1. Introduction Twitterでのリアルタイム・ストリーミングのユースケースの例。 RTAC（リアルタイム・アクティブユーザ・カウント） 図1にRTACのトポロジーイメージが載っている リアルタイム・エンゲージメント（ツイートや広告） 大きな課題のひとつは、デバッガビリティ。性能劣化の原因を素早く探りたい。 論理単位と物理的なプロセスを結びつけてシンプルにしたかった。 クラスタ管理の仕組み（TwitterではAuroraを使用）と組み合わせたかった。 計算リソースの効率も上げたかった。 一方で既存のアプリを書き換えたくないので、Storm API、Summingbird APIに対応させたい。 2. Related Work 挙げられていたのは以下のような技術。 Apache Samza MillWheel(2013) Photon(2013) DataTorrent Stormy(2012) S4 Spark Streaming またトラディショナルなデータベースに組み込まれたストリーム処理エンジンも挙げられていた。 Microsoft StreamInsight IBM Inforsphere Streams Oracle continuous query 要件 OSS 性能高い スケーラブル Storm APIとの互換性あり 3. Motivation for Heron 3.1 Storm Background 特筆なし。Stormの論理構造の説明があるだけ。 3.2 Storm Worker Architecture: Limitations Stormはプロセス内で複数のタスクを実行するし、それらのタスクは複数のトポロジーに属している。 これにより性能劣化などが発生しても、その原因を見極めるのが難しく、結果としてトポロジー「再起動」という 手段を取らざるを得なくなる。 ログが混ざって調査しづらい。 マシンリソースのリザーブが一律である点が辛い。 余分なリソースを確保してしまう。特に複雑なトポロジーを用いることになるハイレベルAPIで利用する場合。 大きなヒープを用いることもデバッグしづらさを助長する。 ワーカ内では、グローバルな送信スレッド、受信スレッド、コンピューティングスレッドがある。 その間で内外の通信を行う。これは無駄が多い。 3.3 Issues with the Storm Nimbus Stormのスケジューラは、ワーカにおいてリソース使用分割をきちんと実施しない。 Storm on YARNを導入しても十分ではなかった。 ZooKeeperがワーカ数の制約になる。 NimbusはSPOF。 Nimbusがとまると新しいトポロジーを投入できないし、走っているトポロジーを止められない。 エラーが起きても気づけないし、自動的にリカバーできない。 3.4 Lack of Backpressure Stormにはバックプレッシャ機能がなかった。 3.5 Efficiency 効率の悪さ（マシンリソースを使い切らない）が課題。 イメージで言えば…100コアのクラスタがあるとき、Utilization 90%以上のコアが30個で動いていほしいところ、Utilizaiton 30%のコアが90個になる、という具合。 4. Design Alternatives Heronの他に選択肢がなかったのか、というと、なかった、ということが書かれている。 ポイントはStorm互換APIを持ち、Stormの欠点を克服するプロダクトがなかった、ということ。 5. Heron 5.1 Data Model and API データモデルとAPIはStormと同様。 トポロジーは論理プラン、実際に実行されるパーティション化されたスパウトやボルトが 物理プランに相当すると考えて良い。 5.2 Architecture overview ジェネラルなスケジューラであるAuroraを使用し、トポロジーをデプロイする。 Nimbusからの離脱。 以下のようなコンポーネントで成り立つ。 トポロジマスタ（TM）（スタンバイTMを起動することも可能） ストリームマネージャ（SM） メトリクスマネージャ（MM） ヘロンインスタンス（HI）（要はスパウトとボルト） コンテナは単独の物理ノードとして起動する。 なお、Twitterではcgroupsでコンテナを起動する。 メタデータはZooKeeperに保存する。 なお、HIはJavaで実装されている。 通信はProtocol Buffersで。 5.3 Topology Master YARNでいうAMに近い。 ZooKeeperのエファメラルノードの機能を使い、複数のTMがマスタになるのを防ぎ、TMを探すのを助ける。 5.4 Stream Manager HIは、そのローカルのSMを通じて通信する。 通信経路は O(k^2) である。このとき、HIの数 n は、 kよりもずっと大きいことが多いので、効率的に通信できるはずである。 補足：ひとつのマシンにひとつのSM、複数のコンテナ（つまりHI等）があるモデルを仮定。 バックプレッシャの方式には種類がある。 バックプレッシャについて。 複数のステージで構成されるケースにおいて、後段のステージの処理時間が長引いていると、 タプルを送るのが つまって バッファがあふれる可能性が生じる。 そこでそれを調整する機能が必要。 TCPバックプレッシャ。 HIの処理時間が長くなると、HIの受信バッファが埋まり始め、合わせて SMの送信バッファも埋まり始める。 これによりSMは つまり 始めているのを検知し、それを上流に伝搬する。 この仕組みは実装は簡単だが、実際にはワークしない。 HI間の論理通信経路は、SM間の物理通信経路上で、 オーバーレイ構成されるためである。 スパウトバックプレッシャについて。 TCPバックプレッシャと組み合わせて用いられる。 SMがHIの処理遅延を検知すると、 スパウトのデータ読み込みを止める。 続いてスタートバックプレッシャのメッセージを他のSMに送る。これにより読み込みが抑制される。 処理遅延が解消されると、ストップバックプレッシャのメッセージを送る。 欠点は、過剰に読み込みを抑止すること、メッセージングのオーバヘッドがあること。利点は、トポロジによらず素早く対処可能なこと。 その他、ステージ・バイ・ステージバックプレッシャについて。 これはトポロジはステージの連続からなる。 バックプレッシャを伝搬させることで必要分だけ読み込み抑制する。 Heronでは、スパウトバックプレッシャ方式を用いた。 ソケットをアプリケーションレベルのバッファに対応させ、 ハイ・ウォータマークとロー・ウォータマークを定義。 ハイ・ウォータマークを超えるとバックプレッシャが発動し、 ロー・ウォータマークを下回るまでバックプレッシャが続く。 5.5 Heron Instance デザインにはいくつかの選択肢がある。 5.5.1 Single-threaded approach HIはJVMであり、単独のタスクを実行する。 これによりデバッグしやすくなる。 しかし、シングルスレッドアプローチは「ユーザコードが様々な理由によりブロックする可能性がある」という欠点がある。 ブロックする理由は様々だが、例えば… スリープのシステムコールを実行 ファイルやソケットI/Oのため読み書きのシステムコールを実行 スレッド同期を実行 これらのブロックは特にメトリクスの取り扱いにおいて問題になった。 つまり、仮にブロックされてしまうともし問題が起きていたとしてもメトリクスの伝搬が遅くなることがあり、 ユーザは信用を置けなくなってしまうからだ。 5.5.2 Two-threaded approach ゲートウェイスレオッドとタスク実行スレッドの2種類で構成する。 ゲートウェイスレッドは、HIの通信管理を担う。 TCP接続の管理など。 タスク実行スレッドはユーザコードを実行する。 スパウトか、ボルトかによって実行されるメソッドが異なる。 スパウトであれば nextTuple を実行してデータをフェッチするし、 ボルトであれば execute メソッドを実行してデータを処理する。 処理されたデータはゲートウェイスレッドに渡され、ローカルのSMに渡される。 なお、その他にも送られたタプルの数などのメトリクスが取得される。 ★補足：このデザインは汎用的なので、他のプロダクトにも利用できそう。 スレッド間はいくつかのキューで結ばれる。 data-in、data-out、metrics-outである。 重要なのは、data-inとdata-outは長さが決まっており、このキューがいっぱいになるとバックプレッシャ機能が有効になる仕組みになっていること。 問題は、NWがいっぱいなときにdata-outキューが溜まった状態になることだった。 これにより、生存オブジェクトがメモリ内に大量に残り、GCによる回収ができない。 このとき、もしdata-outキューからの送信よりも、先に受信が発動すると、新しいオブジェクトが生成されることになり、 GCを発動するが回収可能なオブジェクトが少ないため、さらなる性能劣化を引き起こす。 これを軽減するため、data-out、data-inキューの長さを状況に応じて増減することにした。 5.6 Metrics Manager メトリクスマネージャは、コンテナごとに1個。 5.7 Startup Sequence and Failure Scenarios トポロジがサブミットされてから実際に処理が開始されるまでの流れの説明。 スケジューラがリソースをアロケート TMが起動し、ZooKeeperにエファメラルノードを登録 SMがZooKeeperからTMを確認し、SMと接続する。ハートビートを送り始める。 SMの接続が完了すると、TMはトポロジのコンポーネントをアサインする SMはTMから物理プランを取得し、SM同士がつながる HIが立ち上がり、ローカルSMを見つけ、物理プランをダウンロードし実行開始。 故障時の対応のためTMは物理プランをZooKeeperに保存する。 いくつか故障シナリオが想定されている。 TMが故障した場合、ZooKeeper上の情報を使って復旧可能である。復旧後、SMは新しいTMを見つける。 SMが故障した場合、復旧したSMは物理プランを取得する。他のSMも新しいSMの物理プランを取得する。 HIが故障した場合、再起動してローカルSMにつなぎにいく。その後物理プランを取得し処理を再開する。 5.8 Architecture Features: Summary まとめが記載されているのみ。 6. Heron in Production プロダクションで利用するため、いくつかの周辺機能を有している。 6.1 Heron Tracker ヘロントラッカーはZooKeeperを利用し、トポロジのローンチ、既存トポロジの停止、 物理プランの変更を追従する。 また同様にトポロジマスタを把握し、メトリクス等を取得する。 6.2 Heron UI ヘロントラッカーAPIを利用し、UIを提供する。 論文上にはUIの例が載っている。 トポロジのDAG、コンテナ、コンポーネント、メトリクスを把握できる。 6.3 Heron Viz メトリクスの可視化。トラッカーAPIを利用し、新しいトポロジがローンチされたことを検知し、可視化する。 ヘルスメトリクス、リソースメトリクス、コンポーネントメトリクス、ストリームマネージャメトリクス。 ヘルスメトリクスではラグや失敗などを表示する。 リソースメトリクスでは予約されたCPU、実際に使用されたCPU、同様にメモリに関する情報を扱う。またGCなども。 コンポーネントメトリクスはスパウトではエミットされたタプル数などのコンポーネントごとに固有のメトリクスを扱う。 エンドツーエンドのレイテンシも扱う。 ストリームマネージャメトリクスは、インスタンスに送受信されたタプル数やバックプレッシャ機能に関するメトリクスを扱う。 6.4 Heron@Twitter TwitterではStormではなくHeronがデファクトスタンダードである。 ユースケースは多岐にわたるが、データ加工、フィルタリング、結合、コンテンツのアグリゲーションなどなど。 機械学習も含む。例えば、回帰、アソシエーション、クラスタリングなど。 3倍ほどのリソース使用効率を得られた。 7. Empirical Evaluation 本論文用に組まれた動作確認。 StormとHeronの比較。 Ackあり・なしの両方。 なお、計測はデータ処理が安定してから開始するようにした。 そのため、Stormでは0mq層 1 でほとんどドロップが起きていないときに計測することを意味し、 Heronではバックプレッシャが発動しておらず転送キューが溜まっていない状態での計測を意味する。 Ackありのケースについて、タプルのドロップは、 Storm：0mqでのドロップもしくはタイムアウト Heron：タイムアウト を要因とするものを想定する。 7.3 Word Count Topology スパウトで高々175k単語のランダムな単語群を生成する。 それをボルトに渡し、メモリマップに保持する。 これは単純な処理なので、オーバーヘッドを計測するのに適している。 結果は、スループットで10倍〜14倍、レイテンシで5〜15倍の改善が見られた。 Heronのエンドツーエンドでのレイテンシにおけるボトルネックは、 SMのバッファでバッチ化されることであり、これは概ね数十ms程度の影響がある。 CPUコアの使用量は、2〜3倍小さくなった。 （補足：無駄にリソース確保せずに、きちんと各コアを使い切っている、というのも影響しているようだ） Ackが有効、無効で同様の傾向。 7.4 RTAC Topology Ack有効の場合、Stormで6Mタプル/min出すのに360コア必要だった。レイテンシは70ms。 対してHeronでは、36コアでよく、レイテンシも24msだった。 Ack無効の場合も同様の傾向。必要なCPUコア数に関し、10倍（つまり1/10のコアで良い）の改善が得られた。 8. Conclusions and Future work Exactly oneceセマンティクスは論文執筆時点では対応されていない。 論文中では、 Trident が引用されていた。 最近のHeronはどうか？のメモ 2019/7/8現在になりどうなったかを軽く確認。 Apache incubation Heron が公式レポジトリである。 Apache Heronのコミット状況 を見る限り、2019/7/8現在も活発に活動されている。 Apache Heronのコントリビュータ を見る限り以下の様子。 コアの開発者はApache Pulsarの人でもある Sanjeev Kulkarni や objmagic ただし最近は Ning Wang のように見える。彼はもともと2013年あたりまでGoogleでYouTubeに携わっていたようだ。 READMEによる「Update」 2019/7/8時点のREADMEによると、Mesos in AWS、Mesos/Aurora in AWS、ローカル（ラップトップ）の上でネイティブ動作するようになった。 またApache REEFを用いてApache YARN上で動作するように試みている。 slurm にも対応しようとしているとのこと。 公式ドキュメントを覗いてみる 公式ドキュメント を確認し、最近の様子を探る。 Python APIがある UIがかなり進化している スケジューラとしては、k8s、k8s with Helm、 メトリクス監視の仕組みには、Prometheus、Graphite、Scribeが挙げられている Heron's Design Goals 2019/7/8現在、以下のようなゴールを掲げている。 1億件/minをさばける 小さなエンドツーエンド・レイテンシ スケールによらず予測可能な挙動。またトラフィックのスパイクやパイプラインの輻輳が生じても予測可能な挙動。 Simple administration, including: シンプルな運用 共有インフラにデプロイ可能 強力なモニタリングの仕組み 細やかに設定可能 デバッグしやすい 商用サポートはあるのか？ 2019/7/8現在、Heronの商用サポートがあるのか？ →なさそうに見える。 Introduction to Apache Heron by Streamlio の通り、StreamlioがよくHeronの説明をしているように見える。 またこのスライドではユースケースとして、 Ads Monitoring Product Safety Real Time Trends Real Time Machine Learning Real Time Business Intelligence あたりを挙げている。参考までに。 また顧客（？）としては、 Twitter Google Stanford University Machine Zone Inidiana University Microsoft Industrial.io を挙げている。 ただし、 Streamlioのサポートサービス を見る限り、Apache Pulsarを対象としているがApache Heronが対象に入っているようには見えない。 また Streamlioのプロセッシングエンジンに関する説明 を見ると、Apache Heronに言及しているが、あくまでStreamlioがApache Heronの 開発に携わっていた経験がStreamlioのストリーム処理エンジンの開発に生かされている件について触れられているだけである。 現在は、 Pulsar functions が彼らのコアか。 0mqの記述が読み取れることから、0.8系Stormを比較対象としたように見える。↩︎","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"},{"name":"Twitter Heron","slug":"Knowledge-Management/Stream-Processing/Twitter-Heron","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/Twitter-Heron/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"Twitter Heron","slug":"Twitter-Heron","permalink":"https://dobachi.github.io/memo-blog/tags/Twitter-Heron/"},{"name":"Twitter","slug":"Twitter","permalink":"https://dobachi.github.io/memo-blog/tags/Twitter/"}]},{"title":"Apache Kafkaビルド時のエラー","slug":"Error-during-building-Apache-Kafka","date":"2019-06-28T01:13:33.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/06/28/Error-during-building-Apache-Kafka/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/28/Error-during-building-Apache-Kafka/","excerpt":"","text":"参考 メモ 参考 Re: Kafka Trunk Build Failure with Gradle 5.0 メモ Re: Kafka Trunk Build Failure with Gradle 5.0 に記載のように Gradle 4系でエラーが出るようなので、いったん5系をインストールして実行し直した。 （現環境のUbuntuではaptインストールでのバージョンが4系だったので、 SDKMANを使って5系をインストールした） Gradleのバージョンを上げたところ問題なく実行できた。 以下の通り。 1$ gradle clients:test --tests org.apache.kafka.clients.producer.KafkaProducerTest ここではGradleを使って、clientsサブプロジェクト以下の org.apache.kafka.clients.producer.KafkaProducerTest を実行する例である。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"Kafkaコンソールプロデューサを起点とした確認","slug":"Kafka-Console-Producer","date":"2019-06-26T12:43:58.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/06/26/Kafka-Console-Producer/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/26/Kafka-Console-Producer/","excerpt":"","text":"参考 メモ ConsoleProducer KafkaProducer コンストラクタを確認する 参考 Confluentのトランザクションに関する説明 メモ 2019/6/23時点でのtrunkで再確認した内容。 ほぼ昔のメモのままコピペ・・・。 ConsoleProducer エントリポイントとして、Kafkaのコンソールプロデューサを選ぶ。 bin/kafka-console-producer.sh:20 1exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleProducer &quot;$@&quot; コンソールプロデューサの実態は、KafkaProducerである。 kafka/tools/ConsoleProducer.scala:45 1val producer = new KafkaProducer[Array[Byte], Array[Byte]](producerProps(config)) なお、 ConsoleProducerにはsendメソッドが定義されており、 以下のように標準入力からデータを読み出してはメッセージを送信する、を繰り返す。 kafka/tools/ConsoleProducer.scala:54 12345do &#123; record = reader.readMessage() if (record != null) send(producer, record, config.sync)&#125; while (record != null) kafka/tools/ConsoleProducer.scala:70 1234567private def send(producer: KafkaProducer[Array[Byte], Array[Byte]], record: ProducerRecord[Array[Byte], Array[Byte]], sync: Boolean): Unit = &#123; if (sync) producer.send(record).get() else producer.send(record, new ErrorLoggingCallback(record.topic, record.key, record.value, false))&#125; KafkaProducer それでは実態であるKafkaProducerを確認する。 コンストラクタを確認する まずクライアントのIDが定義される。 org/apache/kafka/clients/producer/KafkaProducer.java:332 1234String clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);if (clientId.length() &lt;= 0) clientId = &quot;producer-&quot; + PRODUCER_CLIENT_ID_SEQUENCE.getAndIncrement();this.clientId = clientId; 続いてトランザクションIDが定義される。 org/apache/kafka/clients/producer/KafkaProducer.java:337 12String transactionalId = userProvidedConfigs.containsKey(ProducerConfig.TRANSACTIONAL_ID_CONFIG) ? (String) userProvidedConfigs.get(ProducerConfig.TRANSACTIONAL_ID_CONFIG) : null; Kafkaのトランザクションについては、 Confluentのトランザクションに関する説明 を参照。 その後、ログ設定、メトリクス設定、キー・バリューのシリアライザ設定。 その後、各種設定値を定義する。 特徴的なところとしては、メッセージを送信するときにメッセージをいったんまとめこむ accumulator も このときに定義される。 org/apache/kafka/clients/producer/KafkaProducer.java:396 123456789101112this.accumulator = new RecordAccumulator(logContext, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), this.compressionType, lingerMs(config), retryBackoffMs, deliveryTimeoutMs, metrics, PRODUCER_METRIC_GROUP_NAME, time, apiVersions, transactionManager, new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME)); 他には Sendor も挙げられる。 例えば、上記で示したaccumulatorも、Sendorのコンストラクタに渡され、内部で利用される。 org/apache/kafka/clients/producer/KafkaProducer.java:422 1this.sender = newSender(logContext, kafkaClient, this.metadata); org/apache/kafka/clients/producer/KafkaProducer.java:463 1234567891011121314return new Sender(logContext, client, metadata, this.accumulator, maxInflightRequests == 1, producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG), acks, retries, metricsRegistry.senderMetrics, time, requestTimeoutMs, producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG), this.transactionManager, apiVersions); その他にはトランザクション・マネージャなども。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"GradleプロジェクトをIntellijでインポート","slug":"Import-gradle-projects","date":"2019-06-24T13:55:09.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/06/24/Import-gradle-projects/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/24/Import-gradle-projects/","excerpt":"","text":"参考 メモ 参考 ./grdlew idea 叩かなくていいよ。 メモ 昔は gradlew idea とやってIdea用のプロジェクトファイルを生成してから インポートしていたけれども、最近のIntellijは「ディレクトリを指定」してインポートすると、 Gradleプロジェクトを検知して、ウィザードを開いてくれる。 例えばApache KafkaなどのGradleプロジェクトをインポートするときには、 メニューからインポートを選択し、ディレクトリをインポートすると良い。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"Intellij","slug":"Knowledge-Management/Tools/Intellij","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/Intellij/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"Intellij","slug":"Intellij","permalink":"https://dobachi.github.io/memo-blog/tags/Intellij/"},{"name":"Apache Kafka","slug":"Apache-Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Apache-Kafka/"},{"name":"Gradle","slug":"Gradle","permalink":"https://dobachi.github.io/memo-blog/tags/Gradle/"}]},{"title":"Kafkaコンソールコンシューマを起点とした確認","slug":"Kafka-Console-Consumer","date":"2019-06-23T14:03:33.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/06/23/Kafka-Console-Consumer/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/23/Kafka-Console-Consumer/","excerpt":"","text":"参考 メモ ConsoleConsumer KafkaConsumer トピックアサイン コンシューマ故障の検知 参考 メモ 2019/6/23時点でのtrunkで再確認した内容。 ほぼ昔のメモのままコピペ・・・。 ConsoleConsumer コンシューマの実装を確認するにあたっては、コンソール・コンシューマが良いのではないか。 bin/kafka-console-consumer.sh:21 1exec $(dirname $0)/kafka-run-class.sh kafka.tools.ConsoleConsumer &quot;$@&quot; から、 kafka.tools.ConsoleConsumer クラスあたりを確認すると良いだろう。 mainを確認すると、ConsumerConfigのインスタンスを生成し、runメソッドに渡すことがわかる。 core/src/main/scala/kafka/tools/ConsoleConsumer.scala:54 123val conf = new ConsumerConfig(args)try &#123; run(conf) ここで、ConsumerConfigはコンソールからパラメータを受け取るためのクラス。 実態はrunメソッドである。 まず大事な箇所として、 KafkaConsumer クラスのインスタンスを生成している箇所がある。 ★ core/src/main/scala/kafka/tools/ConsoleConsumer.scala:67 1val consumer = new KafkaConsumer(consumerProps(conf), new ByteArrayDeserializer, new ByteArrayDeserializer) この KafkaConsumer クラスがコンシューマの実態である。 なお、コンソール・コンシューマでは、これをラップした便利クラス ConsumerWrapper が定義されており、 そちらを通じて主に使う。 またシャットダウンフックを定義している。 core/src/main/scala/kafka/tools/ConsoleConsumer.scala:75 1addShutdownHook(consumerWrapper, conf) core/src/main/scala/kafka/tools/ConsoleConsumer.scala:87 12345678910111213def addShutdownHook(consumer: ConsumerWrapper, conf: ConsumerConfig) &#123; Runtime.getRuntime.addShutdownHook(new Thread() &#123; override def run() &#123; consumer.wakeup() shutdownLatch.await() if (conf.enableSystestEventsLogging) &#123; System.out.println(&quot;shutdown_complete&quot;) &#125; &#125; &#125;)&#125; これはグレースフルにシャットダウンするには大切。 ★ またデータ処理の実態は、以下の process メソッドである。 core/src/main/scala/kafka/tools/ConsoleConsumer.scala:77 1try process(conf.maxMessages, conf.formatter, consumerWrapper, System.out, conf.skipMessageOnError) core/src/main/scala/kafka/tools/ConsoleConsumer.scala:101 1234def process(maxMessages: Integer, formatter: MessageFormatter, consumer: ConsumerWrapper, output: PrintStream, skipMessageOnError: Boolean) &#123;(snip) 処理するメッセージ最大数、出力ストリームに渡す形を整形するメッセージフォーマッタ、コンシューマのラッパ、出力ストリーム、 エラーのときに呼び飛ばすかどうかのフラグが渡される。 少し処理の中身を確認する。 まず以下のようにメッセージを取得する。 core/src/main/scala/kafka/tools/ConsoleConsumer.scala:104 1234 val msg: ConsumerRecord[Array[Byte], Array[Byte]] = try &#123; consumer.receive()(snip) 上記の receive は、先程の通り、ラッパーのreceiveメソッドである。 receiveメソッドは以下のようにコンシューマのポール機能を使い、 メッセージを取得する。 core/src/main/scala/kafka/tools/ConsoleConsumer.scala:437 123456789def receive(): ConsumerRecord[Array[Byte], Array[Byte]] = &#123; if (!recordIter.hasNext) &#123; recordIter = consumer.poll(Duration.ofMillis(timeoutMs)).iterator if (!recordIter.hasNext) throw new TimeoutException() &#125; recordIter.next&#125; メッセージを取得するためのイテレータが得られるので、nextメソッドで取得する。 →イテレータのnextを使って１件ずつ取り出していることがわかる。 ★ 続いて、取り出されたメッセージを ConsumerRecord 化してフォーマッタに渡す。 core/src/main/scala/kafka/tools/ConsoleConsumer.scala:118 12formatter.writeTo(new ConsumerRecord(msg.topic, msg.partition, msg.offset, msg.timestamp, msg.timestampType, 0, 0, 0, msg.key, msg.value, msg.headers), output) これにより出力が確定する。 KafkaConsumer つづいて、KafkaConsumerクラスを確認する。 トピックアサイン ここではトピックへの登録を確認する。 ★ コンシューマグループを使って自動で行うアサインメントを使用する場合。リバランスも行われる。 clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java:936 123 public void subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) &#123;(snip) 上記はトピックのコレクションを渡すメソッドだが、パターンを指定する方法もある。 clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java:1008 123 public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) &#123;(snip) なお、何らかの理由（詳しくはJavadoc参照）により、リバランスが必要になったら、 第2引数に渡されているリスナがまず呼び出される。 コンシューマ・グループを利用せず、手動で行うアサインメントの場合は以下の通り。 clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java:1083 1public void assign(Collection&lt;TopicPartition&gt; partitions) &#123; いずれの場合も内部的には SubscriptionState が管理のために用いられる。 タイプは以下の通り。 clients/src/main/java/org/apache/kafka/clients/consumer/internals/SubscriptionState.java:72 123private enum SubscriptionType &#123; NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED&#125; またコンシューマの管理を行う ConsumerCoordinator には、 updatePatternSubscription メソッドがある。 org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java:192 1234567public void updatePatternSubscription(Cluster cluster) &#123; final Set&lt;String&gt; topicsToSubscribe = cluster.topics().stream() .filter(subscriptions::matchesSubscribedPattern) .collect(Collectors.toSet()); if (subscriptions.subscribeFromPattern(topicsToSubscribe)) metadata.requestUpdateForNewTopics();&#125; matchesSubscribedPattern を用いて、現在クラスタが抱えているトピックの中から、 サブスクライブ対象のトピックをフィルタして取得し、SubscriptionState#subscribeFromPattern メソッドを呼ぶ。 これにより、当該コンシューマの購読するトピックが更新される。 この更新はいくつかのタイミングで発動するが、例えば KafkaConsumer#poll(java.time.Duration) の 中で呼ばれる updateAssignmentMetadataIfNeeded メソッドを通じて呼び出される。 コンシューマ故障の検知 基本的にはハートビート（ heartbeat.interval.ms で設定）が session.timeout.ms を 超えて届かなくなると、故障したとみなされる。 その結果、当該クライアント（この場合はコンシューマ）がグループから外され、 リバランスが起きる。 なお、ハートビートはコーディネータに対して送られる。 コーディネータは以下のように定義されている。 org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java:128 1private Node coordinator = null; org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java:692 1234AbstractCoordinator.this.coordinator = new Node( coordinatorConnectionId, findCoordinatorResponse.data().host(), findCoordinatorResponse.data().port()); AbstractCoordinator#sendFindCoordinatorRequest メソッドが呼ばれる契機は複数あるが、 例えば、コンシューマがポールするときなどにコーディネータが更新される。 なお、コーディネータにクライアントを登録する際、 セッションタイムアウトの値も渡され、対応される。 予め定められた group.min.session.timeout.ms や group.max.session.timeout.ms を満たす セッションタイムアウトが用いられる。 セッションタイムアウトの値は、例えば以下のように設定される。 kafka/coordinator/group/GroupCoordinator.scala:146 1doJoinGroup(group, memberId, groupInstanceId, clientId, clientHost, rebalanceTimeoutMs, sessionTimeoutMs, protocolType, protocols, responseCallback) この値は、最終的に MemberMetadata に渡されて用いられる。 例えばハートビートのデッドラインの計算などに用いられることになる。 kafka/coordinator/group/MemberMetadata.scala:56 123456789private[group] class MemberMetadata(var memberId: String, val groupId: String, val groupInstanceId: Option[String], val clientId: String, val clientHost: String, val rebalanceTimeoutMs: Int, val sessionTimeoutMs: Int, val protocolType: String, var supportedProtocols: List[(String, Array[Byte])]) &#123;","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"}]},{"title":"The 8 Requirements of Real-Time Stream Processing","slug":"The-8-Requirements-of-Real-Time-Stream-Processing","date":"2019-06-22T14:35:41.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/06/22/The-8-Requirements-of-Real-Time-Stream-Processing/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/22/The-8-Requirements-of-Real-Time-Stream-Processing/","excerpt":"","text":"参照 メモ 1. Introduction 2. Eight rules for stream processing Rule 1: Keep the Data Moving Rule 2: Query using SQL on Streams (StreamSQL) Rule 3: Handle Stream Imperfections (Delayed, Missing and Out-of-Order Data) Rule 4: Generate Predictable Outcomes Rule 5: Integrate Stored and Streaming Data Rule 6: Guarantee Data Safety and Availability Rule 7: Partition and Scale Applications Automatically Rule 8: Process and Respond Instantaneously 3. SOFTWARE TECHNOLOGIES for STREAM PROCESSING 3.3 Tabular results 参照 The 8 Requirements of Real-Time Stream Processing ACMページ MillWheel メモ MillWheel でも触れられているストリーム処理について書かれた論文。 2005年。 時代は古いが当時の考察は現在でも有用と考える。 1. Introduction ウォール街のデータ処理量は大きい。 2005年時点で122,000msg/sec。年率2倍で増大。 しかも処理レイテンシが秒では遅い。 金融に限らず異常検知、センサーデータ活用などのユースケースでは同様の課題感だろう。 「メインメモリDBMS」、「ルールベースエンジン」などがこの手のユースケースのために再注目されたが、 その後いわゆる「ストリーム処理エンジン」も登場。 2. Eight rules for stream processing Rule 1: Keep the Data Moving ストレージに書き込むなどしてレイテンシを悪化させてはならない。 ポーリングもレイテンシを悪化させる。 そこでイベント駆動のアプローチを用いることがよく考えられる。 Rule 2: Query using SQL on Streams (StreamSQL) ハイレベルAPIを用いることで、開発サイクルを加速し、保守性を上げる。 StreamSQL：ストリーム処理固有の機能を有するSQL リッチなウィンドウ関数、マージなど。 Rule 3: Handle Stream Imperfections (Delayed, Missing and Out-of-Order Data) 例えば複数のストリームからなるデータを処理する際、 あるストリームからのデータが到着しないとき、タイムアウトして結果を出す必要がある。 同じようにout-of-orderデータについてもウィンドウに応じてタイムアウトしないといけない。 Rule 4: Generate Predictable Outcomes 結果がdeterministicであり、再現性があること。 計算そのものだけではなく、対故障性の観点でも重要。 Rule 5: Integrate Stored and Streaming Data 過去データに基づき、何らかのパターンなどを見つける処理はよくある。 そのためステート管理の仕組み（データストア）は重要。 例えば金融のトレーディングで、あるパターンを見つける、など。 他にも異常検知も。 過去データを使いながら、実時間のデータにキャッチアップするケースもあるだろう。 このようにヒストリカルデータとライブデータをシームレスにつなぐ機能は重要。 Rule 6: Guarantee Data Safety and Availability HAは悩ましい課題。 故障が発生してフェールオーバするとして、バックアップハードウェアを 立ち上げて処理可能な状態にするような待ち時間は許容できない。 そこでタンデム構成を取ることは現実的にありえる。 Rule 7: Partition and Scale Applications Automatically ローレベルの実装を経ずに、分散処理できること。 またマルチスレッド処理可能なこと。 Rule 8: Process and Respond Instantaneously 数万メッセージ/secの量を処理する。 さらにレイテンシはマイクロ秒オーダから、ミリ秒オーダ。 そのためには極力コンポーネント間をまたぐ処理をしないこと。 3. SOFTWARE TECHNOLOGIES for STREAM PROCESSING 基本的なアーキテクチャは、DBMS、ルールベースエンジン、ストリーム処理エンジン。 DBMSはデータをストアしてから処理する。データを動かしつづけるわけではない。 またSPEはSQLスタイルでのストリームデータの処理を可能とする。 SPEとルールベースエンジンは、ブロックおよびタイムアウトを実現しやすい。 DBMSはそのあたりはアプリケーションに委ねられる。 仮にトリガを用いたとしても…。 いずれのアーキテクチャにおいても、予測可能な結果を得るためには、 deterministicな処理を実現しないといけない（例えばタイムスタンプの並びを利用、など） SPEとルールベースエンジンはそれを実現しやすいが、DBMSのACID特性は あくまでトラディショナルなデータベースのためのもであり、ストリームデータ処理のための ものではない。 状態を保存するデータストアとしてDBMSを見たとき、サーバ・クライアントモデルのDBMSは 主にレイテンシの面で不十分である。 唯一受け入れられるとしたら、アプリケーション内に埋め込めるDBMSの場合である。 一方ルールベースエンジンはストリームデータを扱うのは得意だが、 大規模な状態管理と状態に対する柔軟なクエリが苦手である。 3.3 Tabular results このあたりにまとめ表が載っている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"Stonebraker","slug":"Stonebraker","permalink":"https://dobachi.github.io/memo-blog/tags/Stonebraker/"}]},{"title":"MillWheel Paper","slug":"MillWheel-Paper","date":"2019-06-14T06:38:59.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/06/14/MillWheel-Paper/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/14/MillWheel-Paper/","excerpt":"","text":"参考 メモ 1. Introduction 2. Motivation and requirements 3. System overview 4. Core concept 5. API 6. Fault tolerance 7. System Implementation 8. Evaluation 9. Related work 参考 Googleの公開サイト 「MillWheel」から学ぶストリーミング処理の基礎 The 8 Requirements of Real-Time Stream Processing メモ 昔のメモをここに転記。 1. Introduction 故障耐性、ステート永続化、スケーラビリティが重要。 Googleでは多数の分散処理システムを運用しているので、何かしらおかしなことが置き続けている。 MillWheelはMapReduceと同様にフレームワーク。ただし、ストリーム処理やローレイテンシでの処理のため。 Spark StreamingとSonoraは、チェックポイント機能持つが、実装のためのオペレータが限られている。 S4は故障耐性が不十分。 Stormはexactly onceで動作するが、Tridentは厳密な順序保証が必要。 MapReduce的な考えをストリーム処理に持ち込んでも、妥協した柔軟性しか得られていない。 ストリーミングSQLは簡潔な手段をもたらしているが、ステートの抽象化、複雑なアプリケーションの実装しやすさという意味では、 オペレーショナルなフローに基づくアプローチのほうが有利。 分散処理の専門家でなくても複雑なストリーム処理を実装できること 実行可能性 2. Motivation and requirements GoogleのZeitgeistを例にした動機の説明。 永続化のためのストレージ Low Watermark（遅れデータへの対応） 重複への対応（Exactly Onceの実現） 3. System overview Zeitgeistの例で言えば、検索クエリが入力データであり、データ流量がスパイクしたり、凹んだりしたことが出力データ。 データ形式：key、value、timestamp 4. Core concept キーごとに計算され、並列処理化される。 また、キーがアグリゲーションや比較の軸になる。 Zeitgeistの例では、例えば検索クエリそのものをキーとすることが考えられる。 また、キー抽出のための関数を定義して用いる。 ストリーム：データの流れ。 コンピューテーションは複数のストリームを出力することもある。 ウィンドウ修正するときなどに用いられるステートを保持する。 そのための抽象化の仕組みがある。 キーごとのステート。 Low Watermarkの仕組みがあり、Wall timeが進むごとにWatermarkが進む。 時間経過とともにWatermarkを越したデータに対し、計算が行われる。 タイマー機能あり。 Low Watermarkもタイマーでキックされる、と考えて良い。 5. API 計算APIは、キーごとのステートをフェッチ、加工し、必要に応じてレコード生成し、タイマーを セットする。 Injector：MillWheelに外部データを入力する。 injector low watermarkも生成可能。 なお、low watermarkを逸脱するデータが生じた場合は、 ユーザアプリでは捨てるか、それとも現在の集計に組み込むか決められる。 6. Fault tolerance 到達保証の考え方としては、ユーザアプリで冪等性を考慮しなくて良いようにする、という点が挙げられる。 基本的にAckがなければデータが再送される設計に基づいているが、 受信者がAckを返す直前に何らかの理由で故障した場合、データが重複して処理される可能性がある。 そこでユニークIDを付与し、重複デーかどうかを確かめられるようにする。 判定にはブルームフィルタも利用する。 ID管理の仕組みにはガベージコレクションの仕組みもある。 チェックポイントの仕組みもある。 バックエンドストレージにはBigtableなどを想定。 なお、性能を重視しチェックポイントを無効化することもできるが、 そうすると何らかの故障が生じて、データ送信元へのAckが遅れることがある。 パイプラインが並列化すると、それだけシステム内のどこかで故障が生じる可能性は上がる。 そこで、滞留するデータについては部分的にチェックポイントすることで、 計算コストとエンドツーエンドレイテンシのバランスを保つ。 ★ 永続化されたステートの一貫性を保つため、アトミックなオペレーションでラップする。 ただし、マシンのフェールオーバ時などにゾンビ化したWriterが存在する可能性を考慮し、 シークエンサートークンを用いるようにする。 つまり、新たに現れたWriterは古いシークエンサートークンを無効化してから、 動作を開始するようにしている。 7. System Implementation MillWheelはステート管理のためにデータストア（BigTableなど）を使う。 故障発生時にはデータストアからメモリ情報を再現する。 low watermarkのジャーナルはデータストアに保存される。 感想：このあたりデータストアの性能は、最終的なパフォーマンスに大きく影響しそうだ。 ★ 8. Evaluation 単純なシャッフル処理では、レイテンシの中央値は数ミリ秒。95パーセンタイルで30ミリ秒。 Exactly onceなどを有効化すると中央値は33.7ミリ秒。95パーセンタイルで93.8ミリ秒。 CPU数をスケールアウトしても、レイテンシに著しい劣化は見られない。（逆に言うと、99パーセンタイルではある程度の劣化が見られる） low watermarkは処理のステージをまたぐと、実時間に対してラグが生じる。 このあたりは活発に改善を進められているところ。 ステート管理などにストレージ（BitTableなど）を使う。 これによりリード・ライトがたくさん発生する。 ワーカにおけるキャッシュは有効。 実際のユースケースは、広告関係。 そのほか、Google Street Viewでのパノラマ画像の生成など。 ★ 9. Related work ストリーム処理システムが必要とするものは、以下の論文に記載されている。 The 8 Requirements of Real-Time Stream Processing Spark Streamingに対しては、MillWheelの方がgeneralであると主張。 RDDへの依存性がユーザに制約をもたらす、とも。 またチェックポイントの間隔はMillWheelの方が粒度が細かい。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Stream Processing","slug":"Knowledge-Management/Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/"},{"name":"MillWheel","slug":"Knowledge-Management/Stream-Processing/MillWheel","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Stream-Processing/MillWheel/"}],"tags":[{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Stream-Processing/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"MillWheel","slug":"MillWheel","permalink":"https://dobachi.github.io/memo-blog/tags/MillWheel/"},{"name":"Google","slug":"Google","permalink":"https://dobachi.github.io/memo-blog/tags/Google/"}]},{"title":"System monitor on Ubuntu18","slug":"System-monitor-on-Ubuntu18","date":"2019-06-12T14:11:32.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/06/12/System-monitor-on-Ubuntu18/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/12/System-monitor-on-Ubuntu18/","excerpt":"","text":"参考 メモ 参考 Ubuntu 18.04 LTSをインストールした直後に行う設定 &amp; インストールするソフト メモ Ubuntu 18.04 LTSをインストールした直後に行う設定 &amp; インストールするソフト に記載されているとおり、 GNOME Shell ExtensionsからSystem-monitorをインストールすると、タスクバー上にシステムモニターを表示できる。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"Gnome","slug":"Home-server/Ubuntu/Gnome","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/Gnome/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"}]},{"title":"Alt Tab on Ubuntu18","slug":"Alt-Tab-on-Ubuntu18","date":"2019-06-11T11:08:03.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/06/11/Alt-Tab-on-Ubuntu18/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/11/Alt-Tab-on-Ubuntu18/","excerpt":"","text":"参考 メモ 参考 Ubuntu 18.04 LTSをインストールした直後に行う設定 &amp; インストールするソフト メモ Ubuntu 18.04 LTSをインストールした直後に行う設定 &amp; インストールするソフト に諸々記載されている。 設定から変えられる。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"},{"name":"GNOME","slug":"GNOME","permalink":"https://dobachi.github.io/memo-blog/tags/GNOME/"}]},{"title":"Docker DesktopをWSLから利用している場合のマウントについて","slug":"Mount-volums-on-WSL","date":"2019-06-09T14:59:02.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/06/09/Mount-volums-on-WSL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/09/Mount-volums-on-WSL/","excerpt":"","text":"参考 メモ 参考 Docker for WindowsをWSLから使う時のVolumeの扱い方 メモ Docker for WindowsをWSLから使う時のVolumeの扱い方 に起債されている通りだが、 うっかり -v /tmp/hoge:/hoge とかやると、Docker Desktopで使用しているDocker起動用の仮想マシン中の /tmp/hoge をマウントすることになる。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"Docker","slug":"Knowledge-Management/WSL/Docker","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/Docker/"}],"tags":[{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"}]},{"title":"MLflow","slug":"MLflow","date":"2019-06-05T15:33:39.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/06/06/MLflow/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/06/MLflow/","excerpt":"","text":"参考 メモ クイックスタートを試す クイックスタートのプロジェクト チュートリアルの確認 パッケージング Dockerでパッケージング モデルサーブ Runファイルが保存される場所 トラッキングサーバを試す mlflowコマンドを確認する start_runについて uiの実装 mlflow-exampleプロジェクトを眺めてみる MLproject conda.yaml train.py 【ボツ】Dockeでクイックスタートを実行 参考 公式ドキュメント 公式クイックスタート 公式チュートリアル 公式GitHub MLflowによる機械学習モデルのライフサイクルの管理 Dockerでパッケージング where-runs-are-recorded mlflow-example メモ クイックスタートを試す 公式クイックスタート の通り、簡単な例を試す。 手元のAnaconda環境で仮想環境を構築し、実行する。 123$ /opt/Anaconda/default/bin/conda create -n mlflow ipython jupyter$ source activate mlflow$ pip install mlflow サンプルコードを実装。実験のために、もともと載っていた内容を修正。 12345678910111213141516171819202122232425$ mkdir -p Sources/mlflow_quickstart$ cd Sources/mlflow_quickstart$ cat &lt;&lt; EOF &gt; quickstart.py &gt; import os&gt; from mlflow import log_metric, log_param, log_artifact&gt; &gt; if __name__ == &quot;__main__&quot;:&gt; # Log a parameter (key-value pair)&gt; log_param(&quot;param1&quot;, 5)&gt; &gt; # Log a metric; metrics can be updated throughout the run&gt; log_metric(&quot;foo&quot;, 1)&gt; log_metric(&quot;foo&quot;, 2)&gt; log_metric(&quot;foo&quot;, 3)&gt; &gt; # Log an artifact (output file)&gt; with open(&quot;output.txt&quot;, &quot;w&quot;) as f:&gt; f.write(&quot;Hello world! 1\\n&quot;)&gt; with open(&quot;output.txt&quot;, &quot;w&quot;) as f:&gt; f.write(&quot;Hello world! 2\\n&quot;)&gt; log_artifact(&quot;output.txt&quot;)&gt; with open(&quot;output.txt&quot;, &quot;w&quot;) as f:&gt; f.write(&quot;Hello world! 3\\n&quot;)&gt; log_artifact(&quot;output.txt&quot;)&gt; EOF 以下のようなファイルが生成される。 12$ lsmlruns output.txt quickstart.py つづいてUIを試す。 1$ mlflow ui ブラウザで、 http://localhost:5000/ にアクセスするとウェブUIを見られる。 メトリクスは複数回記録すると履歴となって時系列データとして見えるようだ。 一方、アーティファクトは複数回出力しても１回分（最後の１回？）分しか記録されない？ なお、複数回実行すると、時系列データとして登録される。 試行錯誤の履歴が残るようになる。 クイックスタートのプロジェクト 公式クイックスタート には、MLflowプロジェクトとして取り回す方法の例として、 GitHubに登録されたサンプルプロジェクトをロードして実行する例が載っている。 1$ mlflow run https://github.com/mlflow/mlflow-example.git -P alpha=5 GitHubからロードされたプロジェクトが実行される。 なお、ローカルファイルシステムのmlrunsには今回実行したときの履歴が保存される。 チュートリアルの確認 公式チュートリアル を見ながら考える。 examples/sklearn_elasticnet_wine/train.py が取り上げられているのでまずは確認する。 27行目辺りからメイン関数。 最初にCSVファイルを読み込んでPandas DFを作る。 examples/sklearn_elasticnet_wine/train.py:32 12wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;wine-quality.csv&quot;)data = pd.read_csv(wine_path) 入力ファイルを適当に分割したあと、MLflowのセッション（と呼べばよいのだろうか）を起動する。 examples/sklearn_elasticnet_wine/train.py:47 1with mlflow.start_run(): これにより、MLflowが動作に必要とするスタックなどが初期化される。 詳しくは mlflow/tracking/fluent.py:71 あたりの run メソッドの実装を参照。 セッション開始後、モデルを定義し学習を実行する。 その後推論結果を取得し、メトリクスを計算する。 このあたりは通常のアプリと同じ実装。 123456lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)lr.fit(train_x, train_y)predicted_qualities = lr.predict(test_x)(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities) その後パラメータ、メトリクス、モデルを記録する。 1234567mlflow.log_param(&quot;alpha&quot;, alpha)mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)mlflow.log_metric(&quot;rmse&quot;, rmse)mlflow.log_metric(&quot;r2&quot;, r2)mlflow.log_metric(&quot;mae&quot;, mae)mlflow.sklearn.log_model(lr, &quot;model&quot;) このあたりが、MLflowのトラッキング機能を利用している箇所。 試しにパラメータを色々渡して実行してみる。 以下のようなシェルスクリプトを用意した。 1234567for alpha in 0 1 0.5do for l1_ratio in 1 0.5 0.2 0 do python ~/Sources/mlflow/examples/sklearn_elasticnet_wine/train.py $&#123;alpha&#125; $&#123;l1_ratio&#125; donedone その後、mlflow ui コマンドでUIを表示すると、先程試行した実験の結果がわかる。 メトリクスでソートもできるので、 モデルを試行錯誤しながら最低なモデル、パラメータを探すこともできる。 パッケージング チュートリアルでは、以下のようなMLprojectファイル、conda.yamlが紹介されている。 MLproject 12345678910name: tutorialconda_env: conda.yamlentry_points: main: parameters: alpha: float l1_ratio: &#123;type: float, default: 0.1&#125; command: &quot;python train.py &#123;alpha&#125; &#123;l1_ratio&#125;&quot; このファイルはプロジェクトの定義を表すものであり、 依存関係と実装へのエントリーポイントを表す。 見てのとおり、パラメータ定義やコマンド定義が記載されている。 conda.yaml 12345678name: tutorialchannels: - defaultsdependencies: - python=3.6 - scikit-learn=0.19.1 - pip: - mlflow&gt;=1.0 このファイルは環境の依存関係を表す。 このプロジェクトを実行するには、以下のようにする。 1$ mlflow run ~/Sources/mlflow/examples/sklearn_elasticnet_wine -P alpha=0.5 これを実行すると、conda環境を構築し、その上でアプリを実行する。 もちろん、run情報が残る。 Dockerでパッケージング Dockerでパッケージング を見ると、Dockerをビルドしてその中でアプリを動かす例も載っている。 最初にDockerイメージをビルドしておく。 12$ cd ~/Sources/mlflow/examples/docker$ docker build -t mlflow-docker-example -f Dockerfile . つづいて、プロジェクトを実行する。 1$ sudo mlflow run ~/Sources/mlflow/examples/docker -P alpha=0.5 これでconda環境で実行するのと同じように、Docker環境で実行される。 ちなみにMLprojectファイルは以下の通り。 1234567891011name: docker-exampledocker_env: image: mlflow-docker-exampleentry_points: main: parameters: alpha: float l1_ratio: &#123;type: float, default: 0.1&#125; command: &quot;python train.py --alpha &#123;alpha&#125; --l1-ratio &#123;l1_ratio&#125;&quot; モデルサーブ チュートリアルで学習したモデルをサーブできる。 mlflow ui でモデルの情報を確認すると、アーティファクト内にモデルが格納されていることがわかるので、 それを対象としてサーブする。 1$ mlflow models serve -m /home/dobachi/Sources/mlflow_tutorials/mlruns/0/a87ee8c6c6f04f5c822a32e3ecae830e/artifacts/model -p 1234 サーブされたモデルを使った推論は、REST APIで可能。 1$ curl -X POST -H &quot;Content-Type:application/json; format=pandas-split&quot; --data &apos;&#123;&quot;columns&quot;:[&quot;fixed acidity&quot;,&quot;volatile acidity&quot;,&quot;citric acid&quot;,&quot;residual sugar&quot;,&quot;chlorides&quot;,&quot;free sulfur dioxide&quot;,&quot;total sulfur dioxide&quot;,&quot;density&quot;,&quot;pH&quot;,&quot;sulphates&quot;,&quot;alcohol&quot;],&quot;data&quot;:[[7,0.27,0.36,20.7,0.045,45,170,1.001,3,0.45,8.8]]&#125;&apos; http://127.0.0.1:1234/invocations Runファイルが保存される場所 where-runs-are-recorded によると、以下の通り。 Local file path (specified as file:/my/local/dir), where data is just directly stored locally. →ローカルファイルシステム Database encoded as +://:@:/. Mlflow supports the dialects mysql, mssql, sqlite, and postgresql. For more details, see SQLAlchemy database uri. →データベース（SQLAlchemyが対応しているもの） HTTP server (specified as https://my-server:5000), which is a server hosting an MLFlow tracking server. →MLflowトラッキングサーバ Databricks workspace (specified as databricks or as databricks://, a Databricks CLI profile. トラッキングサーバを試す まずはトラッキングサーバを起動しておく。 1$ mlflow server --backend-store-uri /tmp/hoge --default-artifact-root /tmp/fuga --host 0.0.0.0 つづいて、学習を実行する。 1$ MLFLOW_TRACKING_URI=http://localhost:5000 mlflow run ~/Sources/mlflow/examples/sklearn_elasticnet_wine -P alpha=0.5 前の例と違い、環境変数MLFLOW_TRACKING_URIが利用され、上記で起動したトラッキングサーバが指定されていることがわかる。 改めてブラウザで、http://localhost:5000にアクセスすると、先程実行した学習の履歴を確認できる。 mlflowコマンドを確認する mlflowコマンドの実態は以下のようなPythonスクリプトである。 12345678import reimport sysfrom mlflow.cli import cliif __name__ == &apos;__main__&apos;: sys.argv[0] = re.sub(r&apos;(-script\\.pyw?|\\.exe)?$&apos;, &apos;&apos;, sys.argv[0]) sys.exit(cli()) mlflow.cliは、clickを使って実装されている。 コマンドとしては、 run ui server あたりに加え、以下のようなものが定義されていた。 mlflow/cli.py:260 1234567cli.add_command(mlflow.models.cli.commands)cli.add_command(mlflow.sagemaker.cli.commands)cli.add_command(mlflow.experiments.commands)cli.add_command(mlflow.store.cli.commands)cli.add_command(mlflow.azureml.cli.commands)cli.add_command(mlflow.runs.commands)cli.add_command(mlflow.db.commands) start_runについて mlflowを使うときには、以下のようにstart_runメソッドを呼び出す。 123with mlflow.start_run(): lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42) lr.fit(train_x, train_y) start_runメソッドの実態は以下のように定義されている。 mlflow/init.py:54 1start_run = mlflow.tracking.fluent.start_run なお、start_runの戻り値は mlflow.ActiveRun とのこと。 mlflow/tracking/fluent.py:155 12_active_run_stack.append(ActiveRun(active_run_obj))return _active_run_stack[-1] mlflow/tracking/fluent.py:61 12345678910111213class ActiveRun(Run): # pylint: disable=W0223 &quot;&quot;&quot;Wrapper around :py:class:`mlflow.entities.Run` to enable using Python ``with`` syntax.&quot;&quot;&quot; def __init__(self, run): Run.__init__(self, run.info, run.data) def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): status = RunStatus.FINISHED if exc_type is None else RunStatus.FAILED end_run(RunStatus.to_string(status)) return exc_type is None ActiveRunクラスは上記のような実装なので、withステートメントで用いる際には、終了処理をするようになっている。 ステータスを変更し、アクティブな実行（ActiveRun）を終了させる。 上記の構造から見るに、アクティブな実行（ActiveRun）はネスト構造？が可能なように見える。 uiの実装 cli.pyのuiコマンドを確認すると、どうやら手元で気軽に確認するたのコマンドのようだった。 mlflow/cli.py:158 123456def ui(backend_store_uri, default_artifact_root, port): &quot;&quot;&quot; Launch the MLflow tracking UI for local viewing of run results. To launch a production server, use the &quot;mlflow server&quot; command instead. The UI will be visible at http://localhost:5000 by default. 実際には簡易設定で、serverを起動しているようだ。 mlflow/cli.py:184 1_run_server(backend_store_uri, default_artifact_root, &quot;127.0.0.1&quot;, port, 1, None, []) mlflow/server/init.py:51 12345678910111213141516171819def _run_server(file_store_path, default_artifact_root, host, port, workers, static_prefix, gunicorn_opts): &quot;&quot;&quot; Run the MLflow server, wrapping it in gunicorn :param static_prefix: If set, the index.html asset will be served from the path static_prefix. If left None, the index.html asset will be served from the root path. :return: None &quot;&quot;&quot; env_map = &#123;&#125; if file_store_path: env_map[BACKEND_STORE_URI_ENV_VAR] = file_store_path if default_artifact_root: env_map[ARTIFACT_ROOT_ENV_VAR] = default_artifact_root if static_prefix: env_map[STATIC_PREFIX_ENV_VAR] = static_prefix bind_address = &quot;%s:%s&quot; % (host, port) opts = shlex.split(gunicorn_opts) if gunicorn_opts else [] exec_cmd([&quot;gunicorn&quot;] + opts + [&quot;-b&quot;, bind_address, &quot;-w&quot;, &quot;%s&quot; % workers, &quot;mlflow.server:app&quot;], env=env_map, stream_output=True) なお、serverのヘルプを見ると以下の通り。 123456789101112131415161718192021222324252627282930313233343536Usage: mlflow server [OPTIONS] Run the MLflow tracking server. The server which listen on http://localhost:5000 by default, and only accept connections from the local machine. To let the server accept connections from other machines, you will need to pass --host 0.0.0.0 to listen on all network interfaces (or a specific interface address).Options: --backend-store-uri PATH URI to which to persist experiment and run data. Acceptable URIs are SQLAlchemy-compatible database connection strings (e.g. &apos;sqlite:///path/to/file.db&apos;) or local filesystem URIs (e.g. &apos;file:///absolute/path/to/directory&apos;). By default, data will be logged to the ./mlruns directory. --default-artifact-root URI Local or S3 URI to store artifacts, for new experiments. Note that this flag does not impact already-created experiments. Default: Within file store, if a file:/ URI is provided. If a sql backend is used, then this option is required. -h, --host HOST The network address to listen on (default: 127.0.0.1). Use 0.0.0.0 to bind to all addresses if you want to access the tracking server from other machines. -p, --port INTEGER The port to listen on (default: 5000). -w, --workers INTEGER Number of gunicorn worker processes to handle requests (default: 4). --static-prefix TEXT A prefix which will be prepended to the path of all static paths. --gunicorn-opts TEXT Additional command line options forwarded to gunicorn processes. --help Show this message and exit. なお、上記ヘルプを見ると、runデータを保存するのはデフォルトではローカルファイルシステムだが、 SQLAlchemyでアクセス可能なRDBMSでも良いようだ。 サーバとして、gunicornを使っているようだ。 多数のリクエストをさばくため、複数のワーカを使うこともできるようだ。 mlflow/server/init.py:68 12exec_cmd([&quot;gunicorn&quot;] + opts + [&quot;-b&quot;, bind_address, &quot;-w&quot;, &quot;%s&quot; % workers, &quot;mlflow.server:app&quot;], env=env_map, stream_output=True) 上記の通り、 mlflow.server:app が実態なので確認する。 このアプリケーションはFlaskが用いられている。 いったん、 / の定義から。 mlflow/server/init.py:45 1234# Serve the index.html for the React App for all other routes.@app.route(_add_static_prefix(&apos;/&apos;))def serve(): return send_from_directory(STATIC_DIR, &apos;index.html&apos;) mlflow/server/init.py:16 1234REL_STATIC_DIR = &quot;js/build&quot;app = Flask(__name__, static_folder=REL_STATIC_DIR)STATIC_DIR = os.path.join(app.root_path, REL_STATIC_DIR) 以上のように、 mlflow/server/js 以下にアプリが存在するようだが、 そのREADME.mdを見ると、当該アプリは https://github.com/facebook/create-react-app を 使って開発されたように見える。 mlflow-exampleプロジェクトを眺めてみる mlflow-example には、mlflow公式のサンプルプロジェクトが存在する。 この中身を軽く眺めてみる。 MLproject ファイルの内容は以下の通り。 12345678910name: tutorialconda_env: conda.yamlentry_points: main: parameters: alpha: float l1_ratio: &#123;type: float, default: 0.1&#125; command: &quot;python train.py &#123;alpha&#125; &#123;l1_ratio&#125;&quot; プロジェクト名は tutorial であり、condaによる環境構成情報は別途 conda.yaml に定義されていることがわかる。 エントリポイントには複数を定義可能だが、ここでは1個のみ（mainのみ）定義されている。 パラメータは2個（alpha、l1_ratio）与えられている。 それらのパラメータは、実行コマンド定義内でコマンドライン引数として渡されることになっている。 なお、実行されるPythonスクリプト内では以下のように、コマンドライン引数を処理している。 train.py:44 12alpha = float(sys.argv[1]) if len(sys.argv) &gt; 1 else 0.5l1_ratio = float(sys.argv[2]) if len(sys.argv) &gt; 2 else 0.5 conda.yaml 本ファイルには、condaを使ってインストールするライブラリが指定されている。 123456789name: tutorialchannels: - defaultsdependencies: - numpy=1.14.3 - pandas=0.22.0 - scikit-learn=0.19.1 - pip: - mlflow numpy、pandas、scikit-learnあたりの基本的なライブラリをcondaで導入し、 最後にpipでmlflowを導入していることがわかる。 またチャンネルの設定もできるようであるが、ここではデフォルトのみ使用することになっている。 train.py ここでは、ハイライトを確認する。 最初に入力データの読み出しと分割等。 train.py:31 123456789101112# Read the wine-quality csv file (make sure you&apos;re running this from the root of MLflow!)wine_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), &quot;wine-quality.csv&quot;)data = pd.read_csv(wine_path)# Split the data into training and test sets. (0.75, 0.25) split.train, test = train_test_split(data)# The predicted column is &quot;quality&quot; which is a scalar from [3, 9]train_x = train.drop([&quot;quality&quot;], axis=1)test_x = test.drop([&quot;quality&quot;], axis=1)train_y = train[[&quot;quality&quot;]]test_y = test[[&quot;quality&quot;]] MLflowのセッションを開始 train.py:47 1with mlflow.start_run(): モデルを定義し、学習。その後テストデータを用いて予測値を算出し、メトリクスを計算する。 train.py:48 123456lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)lr.fit(train_x, train_y)predicted_qualities = lr.predict(test_x)(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities) 上記のメトリクスは標準出力にも出されるようにもなっている。 手元で試した例では、以下のような感じ。 1234Elasticnet model (alpha=5.000000, l1_ratio=0.100000): RMSE: 0.8594260117338262 MAE: 0.6480675144220314 R2: 0.046025292604596424 最後に、メトリクスを記録し、モデルを保存する。 train.py:60 1234567mlflow.log_param(&quot;alpha&quot;, alpha)mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)mlflow.log_metric(&quot;rmse&quot;, rmse)mlflow.log_metric(&quot;r2&quot;, r2)mlflow.log_metric(&quot;mae&quot;, mae)mlflow.sklearn.log_model(lr, &quot;model&quot;) 【ボツ】Dockeでクイックスタートを実行 ※以下の手順は、UIを表示させようとしたところでエラーになっている。まだ原因分析していない。 -&gt; おおかたバインドするアドレスが127.0.0.1になっており、外部から参照できなくなっているのでは、と。-&gt; mlflow ui の実装をぱっと見る限り、そうっぽい。 公式GitHub からCloneし、Dockerイメージをビルドする。 これを実行環境とする。 123$ cd Sources$ git clone https://github.com/mlflow/mlflow.git$ cd mlflow/examples/docker なお、実行時に便利なようにDockerfileを以下のように修正し、ipython等をインストールしておく。 1234diff --git a/examples/docker/Dockerfile b/examples/docker/Dockerfileindex e436f49..686e0e2 100644--- a/examples/docker/Dockerfile+++ b/examples/docker/Dockerfile@@ -1,5 +1,7 @@ FROM continuumio/miniconda:4.5.4+RUN conda install ipython jupyter+ RUN pip install mlflow&gt;=1.0 \\ &amp;&amp; pip install azure-storage==0.36.0 \\ &amp;&amp; pip install numpy==1.14.3 ではビルドする。 1$ sudo -i docker build -t &quot;dobachi/mlflow:latest&quot; `pwd` チュートリアルのサンプルアプリを作成する。 123456789101112131415161718$ cat &lt;&lt; EOF &gt; tutorial.py&gt; import os&gt; from mlflow import log_metric, log_param, log_artifact&gt;&gt; if __name__ == &quot;__main__&quot;:&gt; # Log a parameter (key-value pair)&gt; log_param(&quot;param1&quot;, 5)&gt;&gt; # Log a metric; metrics can be updated throughout the run&gt; log_metric(&quot;foo&quot;, 1)&gt; log_metric(&quot;foo&quot;, 2)&gt; log_metric(&quot;foo&quot;, 3)&gt;&gt; # Log an artifact (output file)&gt; with open(&quot;output.txt&quot;, &quot;w&quot;) as f:&gt; f.write(&quot;Hello world!&quot;)&gt; log_artifact(&quot;output.txt&quot;)&gt; EOF 起動する。 このとき、サンプルアプリも /apps 以下にマウントするようにする。 1$ sudo -i docker run -v `pwd`:/apps --rm -it dobachi/mlflow:latest /bin/bash 先程作成したアプリを実行する。 12# cd apps# python tutorial.py 結果は以下の通り。 12# cat output.txt Hello world! ここで mlflow ui を起動して見ようと持ったが、ウェブフロントエンドに繋がらなかった。 デバッグを試みる。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"MLflow","slug":"Knowledge-Management/Machine-Learning/MLflow","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/MLflow/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"ML Model Management","slug":"ML-Model-Management","permalink":"https://dobachi.github.io/memo-blog/tags/ML-Model-Management/"},{"name":"MLflow","slug":"MLflow","permalink":"https://dobachi.github.io/memo-blog/tags/MLflow/"},{"name":"Machine Learning Lifecycle","slug":"Machine-Learning-Lifecycle","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning-Lifecycle/"}]},{"title":"HBase on Docker for test","slug":"HBase-on-Docker-for-test","date":"2019-06-02T15:31:51.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/06/03/HBase-on-Docker-for-test/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/06/03/HBase-on-Docker-for-test/","excerpt":"","text":"参考 メモ 参考 HariSekhon/Dockerfiles dajobe/hbase-docker メモ 最初は、 dajobe/hbase-docker を試そうとしたが、後から HariSekhon/Dockerfiles を見つけた。 この中の hbase ディレクトリ以下にある手順に従ったところ、無事に起動した。 HariSekhon/Dockerfiles は、ほかにもビッグデータ系のプロダクトのDockerイメージを配布しているようだ。 ただ、Dockerfileを見ると割と作りこまれているようなので、動作確認にはよいがまじめな環境としてはきちんと見直してから使う方がよさそう。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"HBase","slug":"Knowledge-Management/HBase","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/HBase/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"},{"name":"HBase","slug":"HBase","permalink":"https://dobachi.github.io/memo-blog/tags/HBase/"}]},{"title":"Spark Summit 2019","slug":"Spark-Summit-2019","date":"2019-05-16T16:21:34.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/05/17/Spark-Summit-2019/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/05/17/Spark-Summit-2019/","excerpt":"","text":"セッションメモ Accelerate Your Apache Spark with Intel Optane DC Persistent Memory Building Robust Production Data Pipelines with Databricks Delta Deploying Enterprise Scale Deep Learning in Actuarial Modeling at Nationwide Improving Apache Spark's Reliability with DataSourceV2 Lessons Learned Using Apache Spark for Self-Service Data Prep in SaaS World Using Spark Mllib Models in a Production Training and Serving Platform: Experiences and Extensions ★ A Journey to Building an Autonomous Streaming Data PlatformScaling to Trillion Events Monthly at Nvidia Apache Spark on K8S Best Practice and Performance in the Cloud Deep Learning on Apache Spark at CERN’s Large Hadron Collider with Intel Technologies ★ Elastify Cloud-Native Spark Application with Persistent Memory Geospatial Analytics at Scale with Deep Learning and Apache Spark Improving Call Center Operations and Marketing ROI with Real-Time AI/ML Streams Large-Scale Malicious Domain Detection with Spark AI Migrating to Apache Spark at Netflix Apache Arrow-Based Unified Data Sharing and Transferring Format Among CPU and Accelerators Apache Spark and Sights at Speed: Streaming, Feature Management, and Execution Data-Driven Transformation: Leveraging Big Data at Showtime with Apache Spark In-Memory Storage Evolution in Apache Spark SparkML: Easy ML Productization for Real-Time Bidding Best Practices for Hyperparameter Tuning with MLflow Scaling Apache Spark on Kubernetes at Lyft A \"Real-Time\" Architecture for Machine Learning Execution with MLeap ★ Managing Apache Spark Workload and Automatic Optimizing Optimizing Delta/Parquet Data Lakes for Apache Spark Creating an Omni-Channel Customer Experience with ML, Apache Spark, and Azure Databricks Optimizing Performance and Computing Resource Efficiency of In-Memory Big Data Analytics with Disaggregated Persistent Memory Self-Service Apache Spark Structured Streaming Applications and Analytics Building Resilient and Scalable Data Pipelines by Decoupling Compute and Storage Automating Real-Time Data Pipelines into Databricks Delta Reimagining Devon Energy’s Data Estate with a Unified Approach to Integrations, Analytics, and Machine Learning Data Prep for Data Science in MinutesA Real World Use Case Study of Telematics A Deep Dive into Query Execution Engine of Spark SQL Balancing Automation and Explanation in Machine Learning Enabling Data Scientists to bring their Models to Market TensorFlow Extended: An End-to-End Machine Learning Platform for TensorFlow Continuous Applications at Scale of 100 Teams with Databricks Delta and Structured Streaming How Australia’s National Health Services Directory Improved Data Quality, Reliability, and Integrity with Databricks Delta and Structured Streaming セッションメモ Accelerate Your Apache Spark with Intel Optane DC Persistent Memory Accelerate Your Apache Spark with Intel Optane DC Persistent Memory p.12あたりにアーキイメージがある。 キャッシュを中心としたイメージ。 DCPMMを用いる。 p.14 Optimized Analytics Package（OAP）はDCMPPを用いてSpark SQLを加速する。 p.16あたりからキャシュデザインに関する課題感や考え方が記載されている。 例えば… 手動によるメモリ管理はフラグメンテーションを生じさせる libvmemcacheのリンクドリストによりロック待ちを起因としたボトルネックが生じる そこでリングバッファを利用し、できるだけロック機会を減らすことにした p.26あたりからSpark SQLを題材とした検証結果が載っている。 DRAMに乗り切るサイズであえてやってみた例、乗り切らさないサイズでやってみた例。 p.30あたりからSpark MLlibのK-meansを題材とした検証結果が載っている。 ★ DCPMMのデータに関する階層の定義がある。 DRAM+Diskの場合と比べ、間にOptain DC Persistent Memoryが入ることで性能向上を狙う。 K-meansのワークロードでは最初にHDFSからデータをロードしたあと、 エグゼキュータのローカルディスクを含むストレージを使用してイテレーティブに計算する。 このときDCPMMを用いることで大きなサイズのデータを扱うときでも、ディスクにスピルさせることなく動作可能になり、処理時間の縮小になった。（つまり、性能向上効果はデータサイズによる） ★ Building Robust Production Data Pipelines with Databricks Delta Building Robust Production Data Pipelines with Databricks Delta p.5 データレイクがあっても、信頼するデータがない（データを信頼できない）ことで 多くのデータサイエンスや機械学習が失敗している。 p.7 なぜデータが信頼できなくなるのかというと、 失敗したジョブが破損したデータを生成する スキーマを強制するすべがなく、一貫性が崩れたり、 データの品質が悪くなる。 追記・読み込み、バッチとストリーム処理の混在が許されない （補足：これだけデータの信頼性の話とは異なるような気がした） p.10 鍵となる特徴が載っている。 ACID（トランザクション）、スキーマ強制、バッチとストリーム処理の混在、タイムトラベル（スナップショット） Deploying Enterprise Scale Deep Learning in Actuarial Modeling at Nationwide Deploying Enterprise Scale Deep Learning in Actuarial Modeling at Nationwide ユースケース。 自動車保険を中心としたサービスを提供するNationwide社の事例。 p.7 ★ CDOを筆頭とし、Enterprise Data Officeを構成。 EDOは完全なデータと分析サービスをもとにソリューションを提供し、ビジネスを強化する。 p.8 Analytical Lab。 Databricksのサービスを用いて分析パイプラインを構成？ p.11 Enterprise Analytics Officeというコンセプトの下、データから叡智を取り出す方法論を提供。 p.17 Sparkを用いている理由は、並列化可能な内容がたくさんあるから。例えば以下の通り。 * データ変換がたくさん * スコアリングも並列化可能 * ハイパーパラメータチューニングなども並列化可能 Improving Apache Spark's Reliability with DataSourceV2 Improving Apache Spark's Reliability with DataSourceV2 Neflixの事例。 S3が結果整合性。 Hiveがファイルのリスティングに依存。たまに失敗する。 Netflixの規模だと、「たまに」というのが毎日になる。 p.7 2016年当時の工夫。 p.10〜 DataFrameのDataSourceに関連するつらみ。 DataFrameWriterでは振る舞いが規定されていない。 p.17 Icebergの採用（2019） Hive等で課題となっていたテーブルの物理構造を見直した。 くわしくは、 Strata NY 2018でのIcebergの説明 参照。 p.20 DataSourcedV2が登場したが、一部の課題は引き継がれた。 書き込みのバリデーションがない SaveModeの挙動がデータソースまかせ p.21' DSv2での活動紹介 Lessons Learned Using Apache Spark for Self-Service Data Prep in SaaS World Lessons Learned Using Apache Spark for Self-Service Data Prep in SaaS World ユースケース。 workday における事例。 p.13 「Prism」分析のワークフロー。 p.14 分析前の前処理はSparkで実装される。 ウェブフロントエンドを通じて処理を定義する。 インタラクティブな処理とバッチ処理の両方に対応。 p.25 Sparkのデータ型にさらにデータ型を追加。 StructType、StructFieldを利用して実装。 p.28〜 lessons learnedをいくつか。 ネストされたSQL、self joinやself unionなどの二重オペレータ、ブロードキャストJoinの効率の悪さ、 Caseセンシブなグループ化 Using Spark Mllib Models in a Production Training and Serving Platform: Experiences and Extensions ★ Using Spark Mllib Models in a Production Training and Serving Platform: Experiences and Extensions ユースケース。 Uberの機械学習基盤 Michelangelo についてのセッション。 UberではSpark MLlibをベースとしつつも、独自の改造を施して用いている。 p.2〜 Spark MLlibのパイプラインに関する説明。 パイプラインを用いることで、一貫性をもたせることができる。 データ変換 特徴抽出と前処理 機械学習モデルによる推論 推論後の処理 p.8〜 パイプラインにより複雑さを隠蔽できる。 ★ 例：異なるワークフローを含むことによる複雑さ、異なるユーザニーズを含むことによる複雑さ。 p.10〜 達成したいこと。 まずは性能と一貫性の観点。 Spark SQLをベースとした高い性能 リアルタイムサービング（P99レイテンシ &lt; 10ms、高いスループット） バッチとリアルタイム両対応 柔軟性とVelocityの観点。 モデル定義：ライブラリやフレームワークの柔軟性 Michelangeloの構造 Sparkアップグレード プロトコルバッファによるモデル表現の改善の観点。 MLeap、PMML、PFA、Spark PipelineModel。 Spark PiplelineModelを採用したかったのだが以下の観点が難点だった。 モデルロードが遅い サービングAPIがオンラインサービング向けには遅い p.15〜 改善活動。例えば、SparkのAPIを使ってメタデータをロードしていたところを シンプルなJava APIでロードするようにする、など細かな改善を実施。 Parquet関連では、ParquetのAPIを直接利用するように変えた。 A Journey to Building an Autonomous Streaming Data PlatformScaling to Trillion Events Monthly at Nvidia A Journey to Building an Autonomous Streaming Data PlatformScaling to Trillion Events Monthly at Nvidia NVIDIAの考えるアーキテクチャの変遷。 V1からV2へ、という形式で解説。 p.10〜 V1アーキテクチャ。 KafkaやS3を中心としたデータフロー。 p.12にワークフローイメージ。 p.13 V1で学んだこと。データ量が増えた、セルフサービス化が必要、など。 ★ 状況が変わったこともある。 p.14〜 V2アーキテクチャ。 セルフサービス化のために、NV Data Bots、NV Data Appsを導入。 p.23 NV Data Appsによる機械化で課題になったのは、 スキーマ管理。 Elastic Beatsではネストされたスキーマも用いれるが、そうするとスキーマの推論が難しい。 p.29 V2でのワークフロー。 ワークフローの大部分が機械化された。 ★ p.32 スケーラビリティの課題を解くため、Databricks Deltaを導入。 PrestoからSpark + Deltaに移行。 Apache Spark on K8S Best Practice and Performance in the Cloud Apache Spark on K8S Best Practice and Performance in the Cloud TencentにおけるSpark on k8sに関するナレッジの紹介。 p.6 「Sparkling」について。アーキテクチャなど。 p.18〜 YARNとk8sでの性能比較結果など。 TeraSortでの動作確認では、YARNおほうがスループットが高い結果となり、 データサイズを大きくしたところk8sは処理が途中で失敗した。 ディスクへの負荷により、Evictされたため、tmpfsを使用するようにして試す例も記載されている。 p.23〜 spark-sql-perfによるベンチマーク結果。 そのままでは圧倒的に性能が悪い。これはYARNではデフォルトでyarn.local.dirにしていされた 複数のディスクを使用するのに対し、on k8sでは一つのディレクトリがマウントされるから。 これを複数ディスクを使うようにした結果も記載されている。YARNには届かないが随分改善された。 Deep Learning on Apache Spark at CERN’s Large Hadron Collider with Intel Technologies ★ Deep Learning on Apache Spark at CERN’s Large Hadron Collider with Intel Technologies p.6 CERNで扱うデータ規模。 PB/secのデータを生成することになる。 p.9〜 Sparkを実行するのにYARNとk8sの両方を使っている。 JupyterからSparkにつなぎ、その裏で様々なデータソース（データストア）にアクセスする。 p.13 物理データにアクセスする際には、Hadoop APIからxrootdプロトコルでアクセス。 p.18 Apache Spark、Analytics Zoo、BigDLを活用。 Analytics Zooを利用することでTensorFlow、BigDL、Sparkを結びつける。 また、このあとワークフロー（データフロー）に沿って個別に解説。 Kerasなどを用いてモデル開発し、その後BigDLでスケールアウトしながら分散学習。 BigDLは、よくスケールする。 p.31 推論はストリーム処理ベースで行う。 Kafka経由でデータをフィードし、サーブされたモデルをSparkで読み込んで推論。 また、FPGAも活用する。 Elastify Cloud-Native Spark Application with Persistent Memory Elastify Cloud-Native Spark Application with Persistent Memory Tencentにおけるアーキテクチャの説明。 p.4 データ規模など。 100PB+ p.5 TencentにおけるBig Data / AI基盤 p.6 MemVerge：メモリ・コンバージド基盤（MCI） p.7〜 MapReduceの時代は、データを移動するのではなくプログラムを移動する。 その後ネットワークは高速化（高速なネットワークをDC内に導入する企業の増加） SSD導入の割合は増加。 p.10 ただし未だにDRAM、SSD、HDD/TAPEの階層は残る。 そこでIntel Optain DC Persistent Memoryが登場。（DCPMM） p.14 MemVergeによるPMEMセントリックなデータプラットフォーム。 クラスタワイドにPMEMを共有。 p.17〜 Sparkのシャッフルとブロックマネージャについて。 現状の課題をうけ、シャッフルを再検討。 p.20 エグゼキュータでのシャッフル処理に関し、データの取扱を改善。 ストレージと計算を分離し、さらにプラガブルにした。 さらにストレージ層に、Persistent Memoryを置けるようにした。 Geospatial Analytics at Scale with Deep Learning and Apache Spark Geospatial Analytics at Scale with Deep Learning and Apache Spark p.7 新たなチャレンジ 立地なデータの増加（ドローンの導入など） トラディショナルなツールがスケーラビリティない パイプラインの構成 p.10 Apache Spark : glue of Big Data p.13〜 Sparkにおける画像の取扱。 Spark2.3でImageSchemaの登場。 Spark Join 画像をXMLと結合する。 イメージのチップ作成。 p.18 Deep Learningフレームワーク。 Spark Deep Learning Pipelines p.22 Magellan Improving Call Center Operations and Marketing ROI with Real-Time AI/ML Streams Improving Call Center Operations and Marketing ROI with Real-Time AI/ML Streams スライドが公開されていなかった（2019/05/18時点） Large-Scale Malicious Domain Detection with Spark AI Large-Scale Malicious Domain Detection with Spark AI DDoS、暗号マイニングマルウェア。 Tencentの事例。 p.17 シーケンスを使って検知する。 p.19〜 Domain2Vec、Domainクラスタリング p.24 Word2Vecを使って、ドメインからベクトルを生成する。 （ドメインのつながりを文字列結合してから実施？） p.28あたり LSHをつかった例も載っている。 Migrating to Apache Spark at Netflix Migrating to Apache Spark at Netflix 現在Netflixでは90%以上のジョブがSparkで構成されている。 p.11〜 アップストリームへの追従 複数バージョンを並行利用するようにしている p.17〜 各バージョンで安定性の課題があり、徐々に解消されていった p.22 メモリ管理はNetflixにおいても課題だった。 教育よって是正していた面もある。 p.23〜 Sparkを使うに当たってのベストプラクティスを列挙 設定方針、心構え、ルールなど。 Apache Arrow-Based Unified Data Sharing and Transferring Format Among CPU and Accelerators Apache Arrow-Based Unified Data Sharing and Transferring Format Among CPU and Accelerators p.6 FPGAでオフロード。 p.8〜 オフロードによるオーバーヘッドは無視できない。 Apache Spark and Sights at Speed: Streaming, Feature Management, and Execution Apache Spark and Sights at Speed: Streaming, Feature Management, and Execution スライドが公開されていなかった（2019/05/18時点） Data-Driven Transformation: Leveraging Big Data at Showtime with Apache Spark Data-Driven Transformation: Leveraging Big Data at Showtime with Apache Spark ユースケース。 SHOWTIME の事例。 SHOWTIMEはスタンドアローンのストリーミングサービス。 そのため、顧客のインタラクションデータが集まる。 p.6 ビジネス上生じる疑問の例。 疑問は、答えられる量を上回って増加。 簡単な質問だったら通常のレポートで答えられるが、 複雑な質問に答えるには時間と専門的なスキルが必要。 p.10 データストラテジーチーム。 ★ データと分析を民主化する、購買者の振る舞いを理解し予測する、データ駆動のプログラミングやスケジューリングに対応する。 p.12 ★ 数千のメトリクスや振る舞いを観測する。 ユーザとシリーズの関係性をトラックする。 p.14 機械学習を用いてユーザの振る舞いをモデル化する。 p.20〜 Airflowでパイプラインを最適化。 AirflowとDatabricksを組み合わせる。 ★ p.29 Databricks Deltaも利用している。 ★ In-Memory Storage Evolution in Apache Spark In-Memory Storage Evolution in Apache Spark スライドが公開されていなかった（2019/05/18時点） SparkML: Easy ML Productization for Real-Time Bidding SparkML: Easy ML Productization for Real-Time Bidding リアルタイムでの広告へのBid。 動機：機械学習を用いることで、マーケットをスマートにしたい。 p.6 スケールに関する数値。例：300億件/secのBid意思決定。 p.7 ゴール。 p.8〜 9年前からHadoopを使い、4年ほど前にSparkにTry。 p.12 SparkのMLlibにおけるパイプラインに対し拡張を加えた。 「RowModel」を扱う機能を追加。 p.13 StringIndexerを用いたカテゴリ値の変換が遅いので、独自に開発。 p.14 リソースボトルネックが、キャンペーンによって変わる。そしてうまくリソースを使い切れない。 そこでジョブを並列で立ち上げることにした。 p.15〜 モデルデプロイ（最新モデルへの切り替え）が難しい。 モデルを生成したあと、部分的にデプロイしたあと全体に反映させる。（A/Bテスト後の切り替え） レイテンシの制約が厳しかった。 ★ 補足：モデルを頻繁にデプロイするケースにおいてのレイテンシ保証は難しそうだ ★ p.18 「セルフチューニング」のためのパイプラン ★ Best Practices for Hyperparameter Tuning with MLflow Best Practices for Hyperparameter Tuning with MLflow p.4〜 ハイパーパラメータチューニングについて。 p.6 チューニングにおける挑戦★ p.9 データサイエンスにおけるチューニングのフロー ★ p.10 AutoMLは、ハイパーパラメータチューニングを含む。 p.13〜 チューニング方法の例 ★ マニュアルサーチ グリッドサーチ ランダムサーチ ポピュレーションベースのアルゴリズム ベイジアンアルゴリズム ハイパーパラメータをlossとして最適化？ p.37 様々なツールのハイパーパラメータチューニング手段のまとめ ★ p.40〜 MLflowの機能概要 p.42 単一のモデルではなく、パイプライン全体をチューニングすること。 ★ p.49 ハイパーパラメータチューニングの並列化。 Hyperopt、Apache Spark、MLflowインテグレーション ★ Scaling Apache Spark on Kubernetes at Lyft Scaling Apache Spark on Kubernetes at Lyft p.6 Liftにおけるバッチ処理アーキテクチャの進化。 ★ p.7 アーキテクチャ図。 Druidも含まれている。 p.8 初期のバッチ処理アーキテクチャ図。 p.9〜 SQLでは複雑になりすぎる処理が存在する。またPythonも用いたい。 例：PythonからGeoに関するライブラリを呼び出して用いたい。 Sparkを中心としたアーキテクチャ。 p.19 残ったチャレンジ Spark on k8sは若い 単一クラスタのスケール限界 コントロールプレーン ポッドChurnやIPアロケーションのスロットリング p.22 最新アーキテクチャ★ Spark本体、ヒストリサーバ、Sparkのオペレータ、ジョブ管を含め、すべてk8s上に構成。 他にもゲートウェイ機能をk8s上に実現。 p.23〜 複数クラスタによるアーキテクチャ。 クラスタごとにラベルをつけて、ゲートウェイ経由で使い分ける。 バックエンドのストレージ（ここではS3？）、スキーマ等は共通化。 クラスタ間でローテーション。 複数のネームスペース。 ジョブ管から見たとき、ポッドを共有するアーキテクチャもありえる。 p.28 DDLをDMLから分離する。 Spark Thrift Server経由でDDLを実行。 p.29 プライオリティとプリエンプションはWIP。 p.32 Sparkジョブのコンフィグをオーバレイ構成にする。 ★ p.36 モニタリングのツールキット。★ p.38 Terraformを使った構成。Jinjaテンプレートでパラメータを扱うようだ。 p.39 今後の課題 ★ サーバレス、セルフサービス可能にする、など。 Spark3系への対応もある。 p.40 Sparkは様々なワークロードに適用可能 k8sを使うことで複数バージョンへの対応や依存関係への対応がやりやすくなる マルチクラスタ構成のメッシュアーキテクチャにすることでSpark on k8sはスケールさせられる A \"Real-Time\" Architecture for Machine Learning Execution with MLeap ★ A \"Real-Time\" Architecture for Machine Learning Execution with MLeap p.2 機械学習におけるリアルタイムのユースケースはごく一部。 p.5 1999年から異常検知に取り組んできた企業。 p.7 データフローのイメージ図。 MLeap。 p.10 リアルタイムのアーキテクチャのオーバービュー。 ヒストリカルデータストアからHDFSにデータを取り込み、教師あり機械学習モデルを作成する。 モデルはモデル管理用のデータストアに格納され、推論のシステムに渡される。 モデルを作るところはMLeapのパイプラインで構成される。 p.12 並列処理は、メッセージバス（Kafkaなど）によって実現される。 推論結果の生データをログに書き出すこと。 p.13 モデル管理のフロー。 学習したモデルをデプロイするときには、ブルー・グリーンデプロイメントのように実施する。 p.16〜 メトリクスについて。 平均と分散。 MLeapにより、レイテンシの99パーセンタイルが改善。 Managing Apache Spark Workload and Automatic Optimizing Managing Apache Spark Workload and Automatic Optimizing Optimizing Delta/Parquet Data Lakes for Apache Spark スライドが公開されていなかった。 Creating an Omni-Channel Customer Experience with ML, Apache Spark, and Azure Databricks Creating an Omni-Channel Customer Experience with ML, Apache Spark, and Azure Databricks Optimizing Performance and Computing Resource Efficiency of In-Memory Big Data Analytics with Disaggregated Persistent Memory Optimizing Performance and Computing Resource Efficiency of In-Memory Big Data Analytics with Disaggregated Persistent Memory Self-Service Apache Spark Structured Streaming Applications and Analytics Self-Service Apache Spark Structured Streaming Applications and Analytics Building Resilient and Scalable Data Pipelines by Decoupling Compute and Storage Building Resilient and Scalable Data Pipelines by Decoupling Compute and Storage Building Resilient and Scalable Data Pipelines by Decoupling Compute and Storage: https://www.slideshare.net/databricks/building-resilient-and-scalable-data-pipelines-by-decoupling-compute-and-storage Automating Real-Time Data Pipelines into Databricks Delta Automating Real-Time Data Pipelines into Databricks Delta Automating Real-Time Data Pipelines into Databricks Delta: https://databricks.com/session/automating-real-time-data-pipelines-into-databricks-delta 資料が公開されていなかった。 Reimagining Devon Energy’s Data Estate with a Unified Approach to Integrations, Analytics, and Machine Learning Reimagining Devon Energy’s Data Estate with a Unified Approach to Integrations, Analytics, and Machine Learning Reimagining Devon Energy’s Data Estate with a Unified Approach to Integrations, Analytics, and Machine Learning: https://www.slideshare.net/databricks/reimagining-devon-energys-data-estate-with-a-unified-approach-to-integrations-analytics-and-machine-learning Data Prep for Data Science in MinutesA Real World Use Case Study of Telematics Data Prep for Data Science in MinutesA Real World Use Case Study of Telematics Data Prep for Data Science in MinutesA Real World Use Case Study of Telematics: https://www.slideshare.net/databricks/data-prep-for-data-science-in-minutesa-real-world-use-case-study-of-telematics A Deep Dive into Query Execution Engine of Spark SQL A Deep Dive into Query Execution Engine of Spark SQL A Deep Dive into Query Execution Engine of Spark SQL: https://www.slideshare.net/databricks/a-deep-dive-into-query-execution-engine-of-spark-sql Balancing Automation and Explanation in Machine Learning Balancing Automation and Explanation in Machine Learning Balancing Automation and Explanation in Machine Learning: https://www.slideshare.net/databricks/balancing-automation-and-explanation-in-machine-learning Enabling Data Scientists to bring their Models to Market Enabling Data Scientists to bring their Models to Market Enabling Data Scientists to bring their Models to Market: https://databricks.com/session/enabling-data-scientists-to-bring-their-models-to-market Nikeの事例？ 資料が公開されていなかった。 TensorFlow Extended: An End-to-End Machine Learning Platform for TensorFlow TensorFlow Extended: An End-to-End Machine Learning Platform for TensorFlow TensorFlow Extended: An End-to-End Machine Learning Platform for TensorFlow: https://www.slideshare.net/databricks/tensorflow-extended-an-endtoend-machine-learning-platform-for-tensorflow Continuous Applications at Scale of 100 Teams with Databricks Delta and Structured Streaming Continuous Applications at Scale of 100 Teams with Databricks Delta and Structured Streaming Continuous Applications at Scale of 100 Teams with Databricks Delta and Structured Streaming: https://www.slideshare.net/databricks/continuous-applications-at-scale-of-100-teams-with-databricks-delta-and-structured-streaming How Australia’s National Health Services Directory Improved Data Quality, Reliability, and Integrity with Databricks Delta and Structured Streaming How Australia’s National Health Services Directory Improved Data Quality, Reliability, and Integrity with Databricks Delta and Structured Streaming How Australia’s National Health Services Directory Improved Data Quality, Reliability, and Integrity with Databricks Delta and Structured Streaming: https://www.slideshare.net/databricks/how-australias-national-health-services-directory-improved-data-quality-reliability-and-integrity-with-databricks-delta-and-structured-streaming","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Spark","slug":"Knowledge-Management/Spark","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Spark/"}],"tags":[{"name":"Apache Spark","slug":"Apache-Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Apache-Spark/"},{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"}]},{"title":"Alluxio Security","slug":"Alluxio-Security","date":"2019-05-10T07:07:48.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/05/10/Alluxio-Security/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/05/10/Alluxio-Security/","excerpt":"","text":"参考 メモ ユーザ認証 Kerberos認証はサポートされていない SIMPLEモードの認証 CUSTOMモードの認証 監査ログ 参考 Alluxio公式ドキュメントのセキュリティに関する説明 公式ドキュメントのAuthenticationの説明 Kerberosがサポートされていない？ AlluxioのJAAS実装では、login時に必ずTrueを返す？ commitでも特に認証をしていないように見える？ メモ Alluxio公式ドキュメントのセキュリティに関する説明 を眺めてみる。 提供機能は以下の通りか。 認証 認可 POSIXパーミッション相当の認可機能を提供する Impersonation システムユーザなどが複数のユーザを演じることが可能 監査ログの出力 ユーザ認証 基本的には、2019/05/10現在ではSIMPLEモードが用いられるようである。 alluxio.security.login.username プロパティで指定されたユーザ名か、そうでなければOS上のユーザ名がログインユーザ名として用いられる。 公式ドキュメントのAuthenticationの説明 によると、 JAAS (Java Authentication and Authorization Service) is used to determine who is currently executing the process. とのこと。設定をどうするのか、という点はあまり記載されていない。 Kerberos認証はサポートされていない なお、 Kerberosがサポートされていない？ を見る限り、2019/05/10現在でKerberosがサポートされていない。 SIMPLEモードの認証 また、 AlluxioのJAAS実装では、login時に必ずTrueを返す？ を見ると、2019/05/10現在ではユーザ・パスワード認証がloginメソッド内で定義されていないように見える。 また、念のために commitでも特に認証をしていないように見える？ を見ても、commitメソッド内でも何かしら認証しているように見えない？ ただ、 mSubject.getPrincipalsしている箇所 があるので、そこを確認したほうが良さそう。 関連する実装を確認する。 そこでまずは、 alluxio.security.LoginUser#login メソッドを確認する。 当該メソッドではSubjectインスタンスを生成し、 alluxio.security.LoginUser#createLoginContext メソッド 内部で javax.security.auth.login.LoginContext クラスのインスタンス生成に用いている。 alluxio/security/LoginUser.java:80 123456789Subject subject = new Subject();try &#123; // Use the class loader of User.class to construct the LoginContext. LoginContext uses this // class loader to dynamically instantiate login modules. This enables // Subject#getPrincipals to use reflection to search for User.class instances. LoginContext loginContext = createLoginContext(authType, subject, User.class.getClassLoader(), new LoginModuleConfiguration(), conf); loginContext.login(); ちなみに、 alluxio.security.authentication.AuthType を確認している中で、 SIMPLEモードのときはクライアント側、サーバ側ともにVerify処理をしない旨のコメントを見つけた。 alluxio/security/authentication/AuthType.java:26 123456/** * User is aware in Alluxio. On the client side, the login username is determined by the * &quot;alluxio.security.login.username&quot; property, or the OS user upon failure. * On the server side, the verification of client user is disabled. */SIMPLE, 現時点でわかったことをまとめると、SIMPLEモード時はプロパティで渡された情報や、そうでなければ OSユーザから得られた情報を用いてユーザ名を用いるようになっている。 CUSTOMモードの認証 一方、CUSTOMモード時はサーバ側で任意のVersify処理を実行する旨のコメントが記載されていた。 1234567/** * User is aware in Alluxio. On the client side, the login username is determined by the * &quot;alluxio.security.login.username&quot; property, or the OS user upon failure. * On the server side, the user is verified by a Custom authentication provider * (Specified by property &quot;alluxio.security.authentication.custom.provider.class&quot;). */CUSTOM, ユーザをVerifyしたいときは、CUSTOMモードを使い、サーバ側で確認せよ、ということか。 ただ、CUSTOMモードは Alluxio公式ドキュメントのセキュリティに関する説明 において、 CUSTOM Authentication is enabled. Alluxio file system can know the user accessing it, and use customized AuthenticationProvider to verify the user is the one he/she claims. Experimental. This mode is only used in tests currently. と記載されており、「Experimental」であることから、積極的には使いづらい状況に見える。（2019/05/10現在） 関連事項として、 alluxio.security.authentication.AuthenticationProvider を見ると、 alluxio.security.authentication.custom.provider.class プロパティで渡された クラス名を用いて CustomAuthenticationProvider をインスタンス生成するようにしているように見える。 alluxio/security/authentication/AuthenticationProvider.java:44 123456789switch (authType) &#123; case SIMPLE: return new SimpleAuthenticationProvider(); case CUSTOM: String customProviderName = conf.get(PropertyKey.SECURITY_AUTHENTICATION_CUSTOM_PROVIDER_CLASS); return new CustomAuthenticationProvider(customProviderName); default: throw new AuthenticationException(&quot;Unsupported AuthType: &quot; + authType.getAuthName()); なお、テストには以下のような実装が見られ、CUSTOMモードの使い方を想像できる。 alluxio.security.authentication.PlainSaslServerCallbackHandlerTest.NameMatchAuthenticationProvider alluxio.security.authentication.GrpcSecurityTest.ExactlyMatchAuthenticationProvider alluxio.server.auth.MasterClientAuthenticationIntegrationTest.NameMatchAuthenticationProvider alluxio.security.authentication.CustomAuthenticationProviderTest.MockAuthenticationProvider ExactlyMatchAuthenticationProviderを用いたテストは以下の通り。 alluxio/security/authentication/GrpcSecurityTest.java:78 12345678910111213public void testCustomAuthentication() throws Exception &#123; mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_TYPE, AuthType.CUSTOM.getAuthName()); mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_CUSTOM_PROVIDER_CLASS, ExactlyMatchAuthenticationProvider.class.getName()); GrpcServer server = createServer(AuthType.CUSTOM); server.start(); GrpcChannelBuilder channelBuilder = GrpcChannelBuilder.newBuilder(getServerConnectAddress(server), mConfiguration); channelBuilder.setCredentials(ExactlyMatchAuthenticationProvider.USERNAME, ExactlyMatchAuthenticationProvider.PASSWORD, null).build(); server.shutdown();&#125; alluxio/security/authentication/GrpcSecurityTest.java:93 12345678910111213public void testCustomAuthenticationFails() throws Exception &#123; mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_TYPE, AuthType.CUSTOM.getAuthName()); mConfiguration.set(PropertyKey.SECURITY_AUTHENTICATION_CUSTOM_PROVIDER_CLASS, ExactlyMatchAuthenticationProvider.class.getName()); GrpcServer server = createServer(AuthType.CUSTOM); server.start(); GrpcChannelBuilder channelBuilder = GrpcChannelBuilder.newBuilder(getServerConnectAddress(server), mConfiguration); mThrown.expect(UnauthenticatedException.class); channelBuilder.setCredentials(&quot;fail&quot;, &quot;fail&quot;, null).build(); server.shutdown();&#125; 参考までに、SIMPLEモードで用いられる alluxio.security.authentication.plain.SimpleAuthenticationProvider では、 実際に以下のように何もしないauthenticateメソッドが定義されている。 123public void authenticate(String user, String password) throws AuthenticationException &#123; // no-op authentication&#125; 監査ログ Alluxio公式ドキュメントのセキュリティに関する説明 の「AUDITING」にログのエントリが記載されている。 2019/05/10時点では、以下の通り。 succeeded: True if the command has succeeded. To succeed, it must also have been allowed. allowed: True if the command has been allowed. Note that a command can still fail even if it has been allowed. ugi: User group information, including username, primary group, and authentication type. ip: Client IP address. cmd: Command issued by the user. src: Path of the source file or directory. dst: Path of the destination file or directory. If not applicable, the value is null. perm: User:group:mask or null if not applicable. HDFSにおける監査ログ相当の内容が出力されるようだ。 これが、下層にあるストレージによらずに出力されるとしたら、 Alluxioによる抽象化層でAuditログを取る、という方針も悪くないか？ ただ、そのときにはどのパス（URI？）に、どのストレージをマウントしたか、という情報もセットで保存しておく必要があるだろう。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Alluxio","slug":"Knowledge-Management/Alluxio","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Alluxio/"}],"tags":[{"name":"Alluxio","slug":"Alluxio","permalink":"https://dobachi.github.io/memo-blog/tags/Alluxio/"},{"name":"Security","slug":"Security","permalink":"https://dobachi.github.io/memo-blog/tags/Security/"}]},{"title":"Storage Layer ? Storage Engine ?","slug":"Storage-Layer-Storage-Engine","date":"2019-05-10T05:36:50.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/05/10/Storage-Layer-Storage-Engine/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/05/10/Storage-Layer-Storage-Engine/","excerpt":"","text":"参考 データベースエンジン あるいは ストレージエンジン Apache Parquet Delta Lake Apache Hudi Alluxio AWS Aurora Apache HBase Apache Kudu メモ 動機 どういうユースケースを想定するか？ どういう特徴を持つストレージを探るか？ データベースエンジン あるいは ストレージエンジン Apache Parquet Delta Lake Apache Hudi Alluxio AWS Auroraのバックエンドストレージ（もしくはストレージエンジン） Apache HBase Apache Kudu 参考 データベースエンジン あるいは ストレージエンジン jp.wikipediaのデータベースエンジン MySQLの代替ストレージエンジン InnoDBイントロダクション Apache Parquet Parquetの公式ウェブサイト Delta Lake Delta Lake公式ウェブサイト Apache Hudi Apache Hudiの公式ウェブサイト Hudiのイメージを表す図 Alluxio Alluxio公式ウェブサイト AWS Aurora kumagiさんの解説記事 ACMライブラリの論文 Apache HBase HBaseの公式ウェブサイト Apache Kudu Apache Kuduの公式ウェブサイト DBテックショーでのKudu説明 メモ 動機 単にPut、Getするだけではなく、例えばデータを内部的に構造化したり、トランザクションに対応したりする機能を持つ ストレージ機能のことをなんと呼べば良いのかを考えてみる。 どういうユースケースを想定するか？ データがストリームとして届く できる限り鮮度の高い状態で扱う 主に分析および分析結果を用いたビジネス 大規模なデータを取り扱う 大規模なデータを一度の分析で取り扱う 大規模なデータの中から、一部を一度の分析で取り扱う どういう特徴を持つストレージを探るか？ 必須 Put、Getなど（あるいは、それ相当）の基本的なAPIを有する 補足：POSIXでなくてもよい？ 大規模データを扱える。スケーラビリティを有する。 大規模の種類には、サイズが大きいこと、件数が多いことの両方の特徴がありえる データを高効率で圧縮できることは重要そう クエリエンジンと連係する あると望ましい トランザクションに対応 ストリーム処理の出力先として用いる場合、Exactly Onceセマンティクスを 達成するためには出力先ストレージ側でトランザクション対応していることで ストリーム処理アプリケーションをシンプルにできる、はず ストリームデータを効率的に永続化し、オンデマンドの分析向けにバックエンドで変換。 あるいは分析向けにストア可能 高頻度での書き込み + 一括での読み出し サービスレス 複雑なサービスを運用せずに済むなら越したことはない。 読み出しについて、プッシュダウンフィルタへの対応 更にデータ構造によっては、基本的な集計関数に対応していたら便利ではある 更新のあるデータについて、過去のデータも取り出せる データベースエンジン あるいは ストレージエンジン jp.wikipediaのデータベースエンジン によると、 データベース管理システム (DBMS)がデータベースに対しデータを 挿入、抽出、更新および削除(CRUD参照)するために使用する基礎となる ソフトウェア部品 とある。 MySQLの場合には、 MySQLの代替ストレージエンジン に載っているようなものが該当する。 なお、デフォルトはInnoDB。 InnoDBイントロダクション に「表 14.1 InnoDB ストレージエンジンの機能」という項目がある。 インデックス、キャッシュ、トランザクション、バックアップ・リカバリ、レプリケーション、 圧縮、地理空間情報の取扱、暗号化対応などが大まかに挙げられる。 Apache Parquet Parquetの公式ウェブサイト によると以下のように定義されている。 Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. ストレージフォーマット単体でも、バッチ処理向けにはある程度有用であると考えられる。 Delta Lake Delta Lake公式ウェブサイト には、以下のような定義が記載されている。 Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads. またウェブサイトには、「Key Features」が挙げられている。2019/05/10時点では以下の通り。 ACID Transactions Scalable Metadata Handling Time Travel (data versioning) Open Format Unified Batch and Streaming Source and Sink Schema Enforcement Schema Evolution 100% Compatible with Apache Spark API データベースの「データベースエンジン」、「ストレージエンジン」とは異なる特徴を有することから、 定義上も「Storage Layer」と読んでいるのだろうか。 Apache Hudi Apache Hudiの公式ウェブサイト によると、 Hudi (pronounced “Hoodie”) ingests &amp; manages storage of large analytical datasets over DFS (HDFS or cloud stores) and provides three logical views for query access. とのこと。 また特徴としては、2019/05/10現在 Read Optimized View Incremental View Near-Real time Table が挙げられていた。 なお、機能をイメージするには、 Hudiのイメージを表す図 がちょうどよい。 Apache Hudiでは、「XXなYYである」のような定義は挙げられていない。 Alluxio ストレージそのものというより、ストレージの抽象化や透過的アクセスの仕組みとして考えられる。 Alluxio公式ウェブサイト には以下のように定義されている。 Data orchestration for analytics and machine learning in the cloud Alluxio公式ウェブサイト には、「KEY TECHNICAL FEATURES」というのが記載されている。 気になったものを列挙すると以下の通り。 Compute Flexible APIs Intelligent data caching and tiering Storage Built-in data policies Plug and play under stores Transparent unified namespace for file system and object stores Enterprise Security Monitoring and management AWS Auroraのバックエンドストレージ（もしくはストレージエンジン） kumagiさんの解説記事 あたりが入り口としてとてもわかり易い。 また、元ネタは、 ACMライブラリの論文 あたりか。 語弊を恐れずにいえば、上記でも触れられていたとおり、「Redo-logリプレイ機能付き分散ストレージ」という 名前が妥当だろうか。 Apache HBase HBaseの公式ウェブサイト によると、 Apache HBase is the Hadoop database, a distributed, scalable, big data store. のように定義されている。 また、挙げられていた特徴は以下のとおり。（2019/05/12現在） Linear and modular scalability. Strictly consistent reads and writes. Automatic and configurable sharding of tables Automatic failover support between RegionServers. Convenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables. Easy to use Java API for client access. Block cache and Bloom Filters for real-time queries. Query predicate push down via server side Filters Thrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options Extensible jruby-based (JIRB) shell Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX いわゆるスケールアウト可能なKVSだが、これを分析向けのストレージとして用いる選択肢もありえる。 ただし、いわゆるカラムナフォーマットでデータを扱うわけではない点と、 サービスを運用する必要がある点は懸念すべき点である。 Apache Kudu Apache Kuduの公式ウェブサイト を見ると、以下のように定義されている。 A new addition to the open source Apache Hadoop ecosystem, Apache Kudu completes Hadoop's storage layer to enable fast analytics on fast data. またKuduの利点は以下のように記載されている。 Fast processing of OLAP workloads. Integration with MapReduce, Spark and other Hadoop ecosystem components. Tight integration with Apache Impala, making it a good, mutable alternative to using HDFS with Apache Parquet. Strong but flexible consistency model, allowing you to choose consistency requirements on a per-request basis, including the option for strict-serializable consistency. Strong performance for running sequential and random workloads simultaneously. Easy to administer and manage. High availability. Tablet Servers and Masters use the Raft Consensus Algorithm, which ensures that as long as more than half the total number of replicas is available, the tablet is available for reads and writes. For instance, if 2 out of 3 replicas or 3 out of 5 replicas are available, the tablet is available. Reads can be serviced by read-only follower tablets, even in the event of a leader tablet failure. Structured data model. クエリエンジンImpalaと連係することを想定されている。 また、「ストレージレイヤ」という呼び方が、Delta Lake同様に用いられている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"}],"tags":[{"name":"Storage Layer","slug":"Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/tags/Storage-Layer/"},{"name":"Storage Engine","slug":"Storage-Engine","permalink":"https://dobachi.github.io/memo-blog/tags/Storage-Engine/"}]},{"title":"Delta Lake","slug":"Delta-Lake","date":"2019-05-05T14:27:14.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/05/05/Delta-Lake/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/05/05/Delta-Lake/","excerpt":"","text":"参考 メモ 公式ドキュメントのうち気になったところのメモ（2019/5時点のメモ） Sparkのユーザから見たときにはデータソースとして使えばよいようになっている スキーマの管理 ストリームとバッチの両対応 Databrciksドキュメントの気になったところのメモ（2019/5時点のメモ） データリテンション 最適化 クイックスタートから実装を追ってみる（2019/5時点でのメモ） ひとまずRelationを調べて見る Data Source V1とV2 設定の類 チェックポイントを調査してみる スナップショットについて 動作確認しながら実装確認（v0.5.0） クイックスタートの書き込み クイックスタートの読み込み クイックスタートの更新時 クイックスタートのストリームデータ読み書き スキーマバリデーション 試しに例外を出してみる 自動でのカラム追加（スキーマ変更）を試す overwriteモード時のスキーマ上書き パーティション Delta Table 条件でレコード削除 更新 upsert（マージ） マージの例 タイムトラベル org.apache.spark.sql.delta.OptimisticTransactionImpl#commit プロトコルバージョン アイソレーションレベル sbtでテストコードを使って確認 コンパクション バキューム 読まれているファイルの削除 コンフィグ ログのクリーンアップ ParquetからDeltaテーブルへの変換 動作確認 Symlink Format Manifest 類似技術 参考 公式ドキュメント 公式ドキュメント（クイックスタート） ストリームへの対応についての説明 公式クイックスタート DatabricksのDelta Lakeの説明 DataFrameを使った上書き データ・リテンション VACUUM 最適化 Delta LakeのOptimistic Concurrency Controlに関する記述 Apache Hudi 公式ドキュメントのCompact filesの説明 公式ドキュメントのパーティション説明 公式ドキュメントのスキーマバリデーション 公式ドキュメントのスキーママージ 公式ドキュメントのスキーマ上書き 公式ドキュメントのマージの例 公式ドキュメントのマージを使った上書き Slowly changing data (SCD) Type 2 operation Write change data into a Delta table Upsert from streaming queries using foreachBatch org.apache.spark.sql.streaming.DataStreamWriter#foreachBatch 公式ドキュメントのVacuum Presto and Athena to Delta Lake Integration Presto Athena連係の制約 メモ 公式ドキュメントのうち気になったところのメモ（2019/5時点のメモ） 以下、ポイントの記載。あくまでドキュメント上の話。 Sparkのユーザから見たときにはデータソースとして使えばよいようになっている SparkにはData Sourceの仕組みがあるが、その１種として使えるようになっている。 したがって、DatasetやDataFrameで定義したデータを読み書きするデータソースの１種類として考えて使えば自然に使えるようになっている。 スキーマの管理 通常のSpark APIでは、DataFrameを出力するときに過去のデータのスキーマを考慮したりしないが、 Delta Lakeを用いると過去のスキーマを考慮した振る舞いをさせることができる。 例えば、「スキーマを更新させない（意図しない更新が発生しているときにはエラーを吐かせるなど）」、 「既存のスキーマを利用して部分的な更新」などが可能。 またカラムの追加など、言ってみたら、スキーマの自動更新みたいなことも可能。 ストリームとバッチの両対応 ストリームへの対応についての説明 に記載があるが、Spark Structured Streamingの読み書き宛先として利用可能。 ストリーム処理では「Exactly Once」セマンティクスの保証が大変だったりするが、 そのあたりをDelta Lake層で考慮してくれる、などの利点が挙げられる。 Dalta Lake自身が差分管理の仕組みを持っているので、その仕組みを使って読み書きさせるのだろう、という想像。 なお、入力元としてDelta Lakeを想定した場合、「レコードの削除」、「レコードの更新」が発生することが 考えうる。それを入力元としてどう扱うか？を設定するパラメータがある。 ignoreDeletes: 過去レコードの削除を下流に伝搬しない ignoreChanges: 上記に加え、更新されたレコードを含むファイルの内容全体を下流に伝搬させる 出力先としてDelta Lakeを想定した場合、トランザクションの仕組みを用いられるところが特徴となる。 つまり複数のストリーム処理アプリケーションが同じテーブルに出力するときにも適切にハンドルできるようになり、 出力先も含めたExactly Onceを実現可能になる。 Databrciksドキュメントの気になったところのメモ（2019/5時点のメモ） 全体的な注意点として、Databricksのドキュメントなので、Databricksクラウド特有の話が含まれている可能性があることが挙げられる。 データリテンション データ・リテンション に記述あり。 デフォルトでは30日間のコミットログが保持される、とのこと。 VACUUM句を用いて縮めることができるようだ。 VACUUMについては、VACUUMを参照のこと。 最適化 最適化 に記載がある。コンパクションやZ-Orderingなど。 Databricksクラウド上でSQL文で発行するようだ。 クイックスタートから実装を追ってみる（2019/5時点でのメモ） 公式クイックスタート を参照しながら実装を確認してみる。 上記によると、まずはSBTでは以下の依存関係を追加することになっている。 1libraryDependencies += &quot;io.delta&quot; %% &quot;delta-core&quot; % &quot;0.1.0&quot; Create a tableの章 には、バッチ処理での書き込みについて以下のような例が記載されていた。 12345678import org.apache.spark.sql.SparkSession;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;SparkSession spark = ... // create SparkSessionDataset&lt;Row&gt; data = data = spark.range(0, 5);data.write().format(\"delta\").save(\"/tmp/delta-table\"); SparkのData Sourcesの仕組みとして扱えるようになっている。 テストコードで言えば、 org/apache/spark/sql/delta/DeltaSuiteOSS.scala:24 あたりを眺めるとよいのではないか。 バッチ方式では、以下のようなテストが実装されている。 123456789101112test(\"append then read\") &#123; val tempDir = Utils.createTempDir() Seq(1).toDF().write.format(\"delta\").save(tempDir.toString) Seq(2, 3).toDF().write.format(\"delta\").mode(\"append\").save(tempDir.toString) def data: DataFrame = spark.read.format(\"delta\").load(tempDir.toString) checkAnswer(data, Row(1) :: Row(2) :: Row(3) :: Nil) // append more Seq(4, 5, 6).toDF().write.format(\"delta\").mode(\"append\").save(tempDir.toString) checkAnswer(data.toDF(), Row(1) :: Row(2) :: Row(3) :: Row(4) :: Row(5) :: Row(6) :: Nil)&#125; ストリーム方式では以下のようなテストが実装されている。 1234567891011121314151617181920212223242526272829303132333435test(\"append mode\") &#123; failAfter(streamingTimeout) &#123; withTempDirs &#123; (outputDir, checkpointDir) =&gt; val inputData = MemoryStream[Int] val df = inputData.toDF() val query = df.writeStream .option(\"checkpointLocation\", checkpointDir.getCanonicalPath) .format(\"delta\") .start(outputDir.getCanonicalPath) val log = DeltaLog.forTable(spark, outputDir.getCanonicalPath) try &#123; inputData.addData(1) query.processAllAvailable() val outputDf = spark.read.format(\"delta\").load(outputDir.getCanonicalPath) checkDatasetUnorderly(outputDf.as[Int], 1) assert(log.update().transactions.head == (query.id.toString -&gt; 0L)) inputData.addData(2) query.processAllAvailable() checkDatasetUnorderly(outputDf.as[Int], 1, 2) assert(log.update().transactions.head == (query.id.toString -&gt; 1L)) inputData.addData(3) query.processAllAvailable() checkDatasetUnorderly(outputDf.as[Int], 1, 2, 3) assert(log.update().transactions.head == (query.id.toString -&gt; 2L)) &#125; finally &#123; query.stop() &#125; &#125; &#125;&#125; ひとまずRelationを調べて見る いったんData Source V1だと仮定 1 して、createRelationメソッドを探したところ、 org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation あたりを眺めると、 実態としては org/apache/spark/sql/delta/sources/DeltaDataSource.scala:148 1deltaLog.createRelation() が戻り値を返しているようだ。 このメソッドは、以下のように内部的にはorg.apache.spark.sql.execution.datasources.HadoopFsRelationクラスを 用いている。というより、うまいことほぼ流用している。 以下のような実装。 org/apache/spark/sql/delta/DeltaLog.scala:600 123456789101112131415161718new HadoopFsRelation( fileIndex, partitionSchema = snapshotToUse.metadata.partitionSchema, dataSchema = snapshotToUse.metadata.schema, bucketSpec = None, snapshotToUse.fileFormat, snapshotToUse.metadata.format.options)(spark) with InsertableRelation &#123; def insert(data: DataFrame, overwrite: Boolean): Unit = &#123; val mode = if (overwrite) SaveMode.Overwrite else SaveMode.Append WriteIntoDelta( deltaLog = DeltaLog.this, mode = mode, new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty, configuration = Map.empty, data = data).run(spark) &#125;&#125; insertメソッドでDelta Lake独自の書き込み方式を実行する処理を定義している。 なお、runの中でorg.apache.spark.sql.delta.DeltaOperations内で定義されたWriteオペレーションを実行するように なっているのだが、そこではorg.apache.spark.sql.delta.OptimisticTransactionを用いるようになっている。 要は、Optimistic Concurrency Controlの考え方を応用した実装になっているのではないかと想像。 2 ※ Delta LakeのOptimistic Concurrency Controlに関する記述 にその旨記載されているようだ。 また、上記の通り、データはDeltaLogクラスを経由して管理される。 実際にデータが書き込まれると思われる org.apache.spark.sql.delta.commands.WriteIntoDelta#write を眺めてよう。 最初にメタデータ更新？ org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:87 1updateMetadata(txn, data, partitionColumns, configuration, isOverwriteOperation) updateMetadataメソッドでは、内部的に以下の処理を実施。 スキーマをマージ 「パーティションカラム」のチェック（パーティションカラムに指定された名前を持つ「複数のカラム」がないかどうか、など） replaceWhereオプション（ DataFrameを使った上書き 参照）によるフィルタ構成 ★2019/5時点では、ここで調査が止まっていた。これ以降に続く実装調査については、 動作確認しながら実装確認 を参照 Data Source V1とV2 org.apache.spark.sql.delta.sources.DeltaDataSource あたりがData Source V1向けの実装のエントリポイントか。 これを見る限り、V1で実装されているように見える…？ 設定の類 org.apache.spark.sql.delta.sources.DeltaSQLConf あたりが設定か。 spark.databricks.delta.$keyで指定可能なようだ。 チェックポイントを調査してみる チェックポイントは、その名の通り、将来のリプレイ時にショートカットするための機能。 スナップショットからチェックポイントを作成する。 エントリポイントは、 org.apache.spark.sql.delta.Checkpoints だろうか。 ひとまず、 org.apache.spark.sql.delta.Checkpoints#checkpoint メソッドを見つけた。 org/apache/spark/sql/delta/Checkpoints.scala:118 1234567def checkpoint(): Unit = recordDeltaOperation(this, &quot;delta.checkpoint&quot;) &#123; val checkpointMetaData = checkpoint(snapshot) val json = JsonUtils.toJson(checkpointMetaData) store.write(LAST_CHECKPOINT, Iterator(json), overwrite = true) doLogCleanup()&#125; 実態は、org.apache.spark.sql.delta.Checkpoints$#writeCheckpoint メソッドか。 org/apache/spark/sql/delta/Checkpoints.scala:126 123protected def checkpoint(snapshotToCheckpoint: Snapshot): CheckpointMetaData = &#123; Checkpoints.writeCheckpoint(spark, this, snapshotToCheckpoint)&#125; ちなみに、 org.apache.spark.sql.delta.Checkpoints$#writeCheckpoint メソッド内ではややトリッキーな方法でチェックポイントの書き出しを行っている。 org.apache.spark.sql.delta.Checkpoints#checkpoint() メソッドが呼び出されるのは、 以下のようにOptimistic Transactionのポストコミット処理中である。 org/apache/spark/sql/delta/OptimisticTransaction.scala:294 1234567891011protected def postCommit(commitVersion: Long, committActions: Seq[Action]): Unit = &#123; committed = true if (commitVersion != 0 &amp;&amp; commitVersion % deltaLog.checkpointInterval == 0) &#123; try &#123; deltaLog.checkpoint() &#125; catch &#123; case e: IllegalStateException =&gt; logWarning(&quot;Failed to checkpoint table state.&quot;, e) &#125; &#125;&#125; 呼び出されるタイミングは予め設定されたインターバルのパラメータに依存する。 ということは、数回に1回はチェックポイント書き出しが行われるため、そのあたりでパフォーマンス上の影響があるのではないか、と想像される。 スナップショットについて 関連事項として、スナップショットも調査。 スナップショットが作られるタイミングの 一例 としては、org.apache.spark.sql.delta.DeltaLog#updateInternalメソッドが 呼ばれるタイミングが挙げられる。 updateInternalメソッドは org.apache.spark.sql.delta.DeltaLog#update メソッド内で呼ばれる。 updateメソッドが呼ばれるタイミングはいくつかあるが、例えばトランザクション開始時に呼ばれる流れも存在する。 つまり、トランザクションを開始する前にはいったんスナップショットが定義されることがわかった。 その他にも、 org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation メソッドの処理から実行されるケースもある。 このように、いくつかのタイミングでスナップショットが定義（最新化？）されるようになっている。 動作確認しながら実装確認（v0.5.0） クイックスタートの書き込み 公式ドキュメント（クイックスタート） を見る限り、 シェルで利用する分には --package などでDelta Lakeのパッケージを指定すれば良い。 1$ &lt;path to spark home&gt;/bin/spark-shell --packages io.delta:delta-core_2.11:0.5.0 クイックスタートの例を実行する。 12val data = spark.range(0, 5)data.write.format(\"delta\").save(\"/tmp/delta-table\") 実際に出力されたParquetファイルは以下の通り。 12345678$ ls -R /tmp/delta-table//tmp/delta-table/:_delta_log part-00003-93af5943-2745-42e8-9ac6-c001f257f3a8-c000.snappy.parquet part-00007-8ed33d7c-5634-4739-afbb-471961bec689-c000.snappy.parquetpart-00000-26d26a0d-ad19-44ac-aa78-046d1709e28b-c000.snappy.parquet part-00004-11001300-1797-4a69-9155-876319eb2d00-c000.snappy.parquetpart-00001-6e4655ff-555e-441d-bdc9-68176e630936-c000.snappy.parquet part-00006-94de7a9e-4dbd-4b50-b33c-949ae38dc676-c000.snappy.parquet/tmp/delta-table/_delta_log:00000000000000000000.json これをデバッガをアタッチして、動作状況を覗いてみることにする。 1$ &lt;path to spark home&gt;/bin/spark-shell --packages io.delta:delta-core_2.11:0.5.0 --driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 Intellijなどでデバッガをアタッチする。 机上調査があっているか確認するため、 org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation に ブレイクポイントを設定して動作確認。 src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:119 123456val path = parameters.getOrElse(\"path\", &#123; throw DeltaErrors.pathNotSpecifiedException&#125;)val partitionColumns = parameters.get(DeltaSourceUtils.PARTITIONING_COLUMNS_KEY) .map(DeltaDataSource.decodePartitioningColumns) .getOrElse(Nil) パスとパーティション指定するカラムを確認。 上記の例では、 /tmp/delta-table と Nil が戻り値になる。 src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:126 1val deltaLog = DeltaLog.forTable(sqlContext.sparkSession, path) DeltaLogのインスタンスを受け取る。 つづいて、 org.apache.spark.sql.delta.commands.WriteIntoDelta#run メソッドが呼び出される。 src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:63 12345678override def run(sparkSession: SparkSession): Seq[Row] = &#123; deltaLog.withNewTransaction &#123; txn =&gt; val actions = write(txn, sparkSession) val operation = DeltaOperations.Write(mode, Option(partitionColumns), options.replaceWhere) txn.commit(actions, operation) &#125; Seq.empty&#125; withNewTransaction 内で、 org.apache.spark.sql.delta.commands.WriteIntoDelta#writeが呼び出される。 つまり、ここでトランザクションが開始され、下記の記載の通り、最終的にコミットされることになる。 いったん withNewTransaction の中で行われる処理に着目する。 まずはメタデータ（スキーマ？など？）が更新される。 src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:85 1updateMetadata(txn, data, partitionColumns, configuration, isOverwriteOperation, rearrangeOnly) つづいて、\"reaplaceWhere\" 機能（パーティション列に対し、述語に一致するデータのみを置き換える機能）の フィルタを算出する。 1234567891011val replaceWhere = options.replaceWhereval partitionFilters = if (replaceWhere.isDefined) &#123; val predicates = parsePartitionPredicates(sparkSession, replaceWhere.get) if (mode == SaveMode.Overwrite) &#123; verifyPartitionPredicates( sparkSession, txn.metadata.partitionColumns, predicates) &#125; Some(predicates)&#125; else &#123; None&#125; つづいて、 org.apache.spark.sql.delta.files.TransactionalWrite#writeFiles メソッドが呼び出される。 src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:105 1val newFiles = txn.writeFiles(data, Some(options)) 内部的には、 org.apache.spark.sql.execution.datasources.FileFormatWriter#write を用いて 与えられた data （＝DataFrame つまり Dataset）を書き出す処理（物理プラン）を実行する。 ここでのポイントは、割と直接的にSparkのDataSourcesの機能を利用しているところだ。 実装が簡素になる代わりに、Sparkに強く依存していることがわかる。 また、newFilesには出力PATHに作られるファイル群の情報（実際にはcase classのインスタンス）が含まれる。 ここで org.apache.spark.sql.delta.commands.WriteIntoDelta#write の呼び出し元、 org.apache.spark.sql.delta.commands.WriteIntoDelta#run に戻る。 writeが呼ばれたあとは、オペレーション情報がインスタンス化される。 src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:66 1val operation = DeltaOperations.Write(mode, Option(partitionColumns), options.replaceWhere) 上記のアクションとオペレーション情報を合わせて、以下のようにコミットされる。 src/main/scala/org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:67 1txn.commit(actions, operation) クイックスタートの読み込み 公式ドキュメント（クイックスタート） には以下のような簡単な例が載っている。 12val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")df.show() デバッガをアタッチしながら動作を確認しよう。 まず最初の 1val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\") により、SparkのDataSourceの仕組みに基づいて、リレーションが生成される。 org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation メソッドあたりを読み解く。 src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:149 12val maybeTimeTravel = DeltaTableUtils.extractIfPathContainsTimeTravel(sqlContext.sparkSession, maybePath) 最初にタイムトラベル対象かどうかを判定する。タイムトラベル自体は別途。 src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:159 123456789val hadoopPath = new Path(path)val rootPath = DeltaTableUtils.findDeltaTableRoot(sqlContext.sparkSession, hadoopPath) .getOrElse &#123; val fs = hadoopPath.getFileSystem(sqlContext.sparkSession.sessionState.newHadoopConf()) if (!fs.exists(hadoopPath)) &#123; throw DeltaErrors.pathNotExistsException(path) &#125; hadoopPath &#125; つづいて、Hadoopの機能を利用し、下回りのデータストアのファイルシステムを取得する。 このあたりでHadoopの機能を利用しているあたりが、HadoopやSparkを前提としたシステムであることがわかる。 また、一貫性を考えると、通常のHadoopやSparkを利用するときと同様に、 S3で一貫性を担保する仕組み（例えばs3a、s3ガードなど）を利用したほうが良いだろう。 src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:169 1val deltaLog = DeltaLog.forTable(sqlContext.sparkSession, rootPath) つづいて、DeltaLogインスタンスが生成される。 これにより、ログファイルに対する操作を開始できるようになる。（ここでは読み取りだが、書き込みも対応可能になる） src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:171 1234567891011 val partitionFilters = if (rootPath != hadoopPath) &#123;(snip) if (files.count() == 0) &#123; throw DeltaErrors.pathNotExistsException(path) &#125; filters &#125; else &#123; Nil &#125; パーティションの読み込みかどうかを検知。 ただし、Delta LakeではパーティションのPATHを直接指定するのは推奨されておらず、where句を使うのが推奨されている。 src/main/scala/org/apache/spark/sql/delta/sources/DeltaDataSource.scala:210 1deltaLog.createRelation(partitionFilters, timeTravelByParams.orElse(timeTravelByPath)) 最後に、実際にリレーションを生成し、戻り値とする。 なお、 org.apache.spark.sql.delta.DeltaLog#createDataFrame メソッドは以下の通り。 src/main/scala/org/apache/spark/sql/delta/DeltaLog.scala:630 1234567891011121314151617181920212223242526 def createRelation( partitionFilters: Seq[Expression] = Nil, timeTravel: Option[DeltaTimeTravelSpec] = None): BaseRelation = &#123;(snip) new HadoopFsRelation( fileIndex, partitionSchema = snapshotToUse.metadata.partitionSchema, dataSchema = snapshotToUse.metadata.schema, bucketSpec = None, snapshotToUse.fileFormat, snapshotToUse.metadata.format.options)(spark) with InsertableRelation &#123; def insert(data: DataFrame, overwrite: Boolean): Unit = &#123; val mode = if (overwrite) SaveMode.Overwrite else SaveMode.Append WriteIntoDelta( deltaLog = DeltaLog.this, mode = mode, new DeltaOptions(Map.empty[String, String], spark.sessionState.conf), partitionColumns = Seq.empty, configuration = Map.empty, data = data).run(spark) &#125; &#125; &#125;&#125; TahoeLogFileIndex なお、生成されたDataFrameのプランを確認すると以下のように表示される。 123scala&gt; df.explain== Physical Plan ==*(1) FileScan parquet [id#778L] Batched: true, Format: Parquet, Location: TahoeLogFileIndex[file:/tmp/delta-table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;id:bigint&gt; TahoeLogFileIndex はDelta Lakeが実装しているFileIndexの一種。 クラス階層は以下の通り。 TahoeFileIndex (org.apache.spark.sql.delta.files) TahoeBatchFileIndex (org.apache.spark.sql.delta.files) TahoeLogFileIndex (org.apache.spark.sql.delta.files) 親クラスの TahoeFileIndex は、以下の通り FileIndexを継承している。 org/apache/spark/sql/delta/files/TahoeFileIndex.scala:35 1234abstract class TahoeFileIndex( val spark: SparkSession, val deltaLog: DeltaLog, val path: Path) extends FileIndex &#123; TahoeFileIndex のJavaDocには、 A [[FileIndex]] that generates the list of files managed by the Tahoe protocol. とあり、「Tahoeプロトコル」なるものを通じて、SparkのFileIndex機能を実現する。 例えば、showメソッドを呼び出すと、 1scala&gt; df.show() 実行の途中で、 org.apache.spark.sql.delta.files.TahoeFileIndex#listFiles メソッドが呼び出される。 以下、listFiles内での動作を軽く確認する。 org/apache/spark/sql/delta/files/TahoeFileIndex.scala:56 1matchingFiles(partitionFilters, dataFilters).groupBy(_.partitionValues).map &#123; まず、 org.apache.spark.sql.delta.files.TahoeFileIndex#matchingFiles メソッドが呼ばれ、 AddFile インスタンスのシーケンスが返される。 org/apache/spark/sql/delta/files/TahoeFileIndex.scala:129 1234567override def matchingFiles( partitionFilters: Seq[Expression], dataFilters: Seq[Expression], keepStats: Boolean = false): Seq[AddFile] = &#123; getSnapshot(stalenessAcceptable = false).filesForScan( projection = Nil, this.partitionFilters ++ partitionFilters ++ dataFilters, keepStats).files&#125; 上記の通り、戻り値は Seq[AddFile] である。 そこで matchingFiles メソッドの戻り値をグループ化した後は、mapメソッドにより、 以下のような処理が実行される。 org/apache/spark/sql/delta/files/TahoeFileIndex.scala:57 1234567891011121314151617case (partitionValues, files) =&gt; val rowValues: Array[Any] = partitionSchema.map &#123; p =&gt; Cast(Literal(partitionValues(p.name)), p.dataType, Option(timeZone)).eval() &#125;.toArray val fileStats = files.map &#123; f =&gt; new FileStatus( /* length */ f.size, /* isDir */ false, /* blockReplication */ 0, /* blockSize */ 1, /* modificationTime */ f.modificationTime, absolutePath(f.path)) &#125;.toArray PartitionDirectory(new GenericInternalRow(rowValues), fileStats) 参考までに、このとき、 files には以下のような値が入っている。 1234567files = &#123;WrappedArray$ofRef@19711&#125; &quot;WrappedArray$ofRef&quot; size = 6 0 = &#123;AddFile@19694&#125; &quot;AddFile(part-00004-11001300-1797-4a69-9155-876319eb2d00-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot; 1 = &#123;AddFile@19719&#125; &quot;AddFile(part-00003-93af5943-2745-42e8-9ac6-c001f257f3a8-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot; 2 = &#123;AddFile@19720&#125; &quot;AddFile(part-00000-26d26a0d-ad19-44ac-aa78-046d1709e28b-c000.snappy.parquet,Map(),262,1582472443000,false,null,null)&quot; 3 = &#123;AddFile@19721&#125; &quot;AddFile(part-00006-94de7a9e-4dbd-4b50-b33c-949ae38dc676-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot; 4 = &#123;AddFile@19722&#125; &quot;AddFile(part-00001-6e4655ff-555e-441d-bdc9-68176e630936-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot; 5 = &#123;AddFile@19723&#125; &quot;AddFile(part-00007-8ed33d7c-5634-4739-afbb-471961bec689-c000.snappy.parquet,Map(),429,1582472443000,false,null,null)&quot; ここまでが org.apache.spark.sql.execution.datasources.FileIndex#listFiles メソッドの内容である。 （参考）TahoeFileIndexがどこから呼ばれるか ここではSpark v2.4.2を確認する。 org.apache.spark.sql.Dataset#show メソッドでは、内部的に org.apache.spark.sql.Dataset#showString メソッドを呼び出す。 org/apache/spark/sql/Dataset.scala:744 12345def show(numRows: Int, truncate: Boolean): Unit = if (truncate) &#123; println(showString(numRows, truncate = 20))&#125; else &#123; println(showString(numRows, truncate = 0))&#125; org.apache.spark.sql.Dataset#showString メソッド内で、 org.apache.spark.sql.Dataset#getRows メソッドが呼ばれる。 org/apache/spark/sql/Dataset.scala:285 123456789 private[sql] def showString( _numRows: Int, truncate: Int = 20, vertical: Boolean = false): String = &#123; val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1) // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data. val tmpRows = getRows(numRows, truncate)(snip) org.apache.spark.sql.Dataset#getRows メソッド内では、 org.apache.spark.sql.Dataset#take メソッドが呼ばれる。 org/apache/spark/sql/Dataset.scala:241 1234567 private[sql] def getRows((snip) val data = newDf.select(castCols: _*).take(numRows + 1) (snip) org.apache.spark.sql.Dataset#take メソッドでは org.apache.spark.sql.Dataset#head(int) メソッドが呼ばれる。 org/apache/spark/sql/Dataset.scala:2758 1def take(n: Int): Array[T] = head(n) org.apache.spark.sql.Dataset#head メソッド内で、 org.apache.spark.sql.Dataset#withAction を用いて、 org.apache.spark.sql.Dataset#collectFromPlan メソッドが呼ばれる。 org/apache/spark/sql/Dataset.scala:2544 1def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan) org.apache.spark.sql.Dataset#collectFromPlan メソッド内で org.apache.spark.sql.execution.SparkPlan#executeCollect メソッドが呼ばれる。 org/apache/spark/sql/Dataset.scala:3379 12345678910private def collectFromPlan(plan: SparkPlan): Array[T] = &#123; // This projection writes output to a `InternalRow`, which means applying this projection is not // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe. val objProj = GenerateSafeProjection.generate(deserializer :: Nil) plan.executeCollect().map &#123; row =&gt; // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type // parameter of its `get` method, so it's safe to use null here. objProj(row).get(0, null).asInstanceOf[T] &#125;&#125; なお、 org.apache.spark.sql.execution.CollectLimitExec のcase classインスタンス化の中で、 org.apache.spark.sql.execution.CollectLimitExec#executeCollect が以下のように定義されている。 org/apache/spark/sql/execution/limit.scala:35 123456case class CollectLimitExec(limit: Int, child: SparkPlan) extends UnaryExecNode &#123; override def output: Seq[Attribute] = child.output override def outputPartitioning: Partitioning = SinglePartition override def executeCollect(): Array[InternalRow] = child.executeTake(limit)(snip) そこで、 org.apache.spark.sql.execution.SparkPlan#executeTake メソッドに着目していく。 当該メソッド内で、 org.apache.spark.sql.execution.SparkPlan#getByteArrayRdd メソッドが呼ばれ、 childRDD が生成される。 org/apache/spark/sql/execution/SparkPlan.scala:334 123456def executeTake(n: Int): Array[InternalRow] = &#123; if (n == 0) &#123; return new Array[InternalRow](0) &#125; val childRDD = getByteArrayRdd(n).map(_._2) org.apache.spark.sql.execution.SparkPlan#getByteArrayRdd メソッドの実装は以下の通りで、 内部的に org.apache.spark.sql.execution.SparkPlan#execute メソッドが呼ばれる。 12345 private def getByteArrayRdd(n: Int = -1): RDD[(Long, Array[Byte])] = &#123; execute().mapPartitionsInternal &#123; iter =&gt; var count = 0(snip) org.apache.spark.sql.execution.SparkPlan#execute メソッド内で org.apache.spark.sql.execution.SparkPlan#executeQuery を利用し、 その内部でorg.apache.spark.sql.execution.SparkPlan#doExecute メソッドが呼ばれる。 org/apache/spark/sql/execution/SparkPlan.scala:127 123456final def execute(): RDD[InternalRow] = executeQuery &#123; if (isCanonicalizedPlan) &#123; throw new IllegalStateException(\"A canonicalized plan is not supposed to be executed.\") &#125; doExecute()&#125; 参考までに、org.apache.spark.sql.execution.SparkPlan#executeQuery 内では、 org.apache.spark.rdd.RDDOperationScope#withScope を使ってクエリが実行される。 org/apache/spark/sql/execution/SparkPlan.scala:151 1234567protected final def executeQuery[T](query: =&gt; T): T = &#123; RDDOperationScope.withScope(sparkContext, nodeName, false, true) &#123; prepare() waitForSubqueries() query &#125;&#125; つづいて、 doExecute メソッドの確認に戻る。 org.apache.spark.sql.execution.WholeStageCodegenExec#doExecute メソッド内で、 org.apache.spark.sql.execution.CodegenSupport#inputRDDs メソッドが呼ばれる。 12345678910 override def doExecute(): RDD[InternalRow] = &#123;(snip) val durationMs = longMetric(\"pipelineTime\") val rdds = child.asInstanceOf[CodegenSupport].inputRDDs() assert(rdds.size &lt;= 2, \"Up to two input RDDs can be supported\")(snip) inputRDDs メソッド内でinputRDDが呼び出される。 org/apache/spark/sql/execution/DataSourceScanExec.scala:326 123override def inputRDDs(): Seq[RDD[InternalRow]] = &#123; inputRDD :: Nil&#125; inputRDD にRDDインスタンスをバインドする際、その中で org.apache.spark.sql.execution.FileSourceScanExec#createNonBucketedReadRDD メソッドが呼ばれ、 そこに渡される readFile メソッドが実行される。 org/apache/spark/sql/execution/DataSourceScanExec.scala:305 1234567891011121314151617181920private lazy val inputRDD: RDD[InternalRow] = &#123; // Update metrics for taking effect in both code generation node and normal node. updateDriverMetrics() val readFile: (PartitionedFile) =&gt; Iterator[InternalRow] = relation.fileFormat.buildReaderWithPartitionValues( sparkSession = relation.sparkSession, dataSchema = relation.dataSchema, partitionSchema = relation.partitionSchema, requiredSchema = requiredSchema, filters = pushedDownFilters, options = relation.options, hadoopConf = relation.sparkSession.sessionState.newHadoopConfWithOptions(relation.options)) relation.bucketSpec match &#123; case Some(bucketing) if relation.sparkSession.sessionState.conf.bucketingEnabled =&gt; createBucketedReadRDD(bucketing, readFile, selectedPartitions, relation) case _ =&gt; createNonBucketedReadRDD(readFile, selectedPartitions, relation) &#125;&#125; readFile メソッドを一緒に org.apache.spark.sql.execution.FileSourceScanExec#createNonBucketedReadRDD メソッドに渡されている org.apache.spark.sql.execution.FileSourceScanExec#selectedPartitions は以下のように定義される。 Seq[PartitionDirectory] のインスタンスを selectedPartitions にバインドする際に、 org.apache.spark.sql.execution.datasources.FileIndex#listFiles メソッドが呼び出される。 org/apache/spark/sql/execution/DataSourceScanExec.scala:190 12345678@transient private lazy val selectedPartitions: Seq[PartitionDirectory] = &#123; val optimizerMetadataTimeNs = relation.location.metadataOpsTimeNs.getOrElse(0L) val startTime = System.nanoTime() val ret = relation.location.listFiles(partitionFilters, dataFilters) val timeTakenMs = ((System.nanoTime() - startTime) + optimizerMetadataTimeNs) / 1000 / 1000 metadataTime = timeTakenMs ret&#125; このとき、具象クラス側の listFiles メソッドが呼ばれることになるが、 TahoeLogFileIndex クラスの場合は、親クラスのメソッド org.apache.spark.sql.delta.files.TahoeFileIndex#listFiles が呼ばれる。 当該メソッドの内容は、上記で示したとおり。 クイックスタートの更新時 公式ドキュメント（クイックスタート） には 以下のような例が載っている。 12val data = spark.range(5, 10)data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\") 上記を実行した際のデータ保存ディレクトリの構成は以下の通り。 123456789101112$ ls -R /tmp/delta-table//tmp/delta-table/:_delta_log part-00004-11001300-1797-4a69-9155-876319eb2d00-c000.snappy.parquetpart-00000-191c798b-3202-4fdf-9447-891f19953a37-c000.snappy.parquet part-00004-62735ceb-255f-4349-b043-d14d798f653a-c000.snappy.parquetpart-00000-26d26a0d-ad19-44ac-aa78-046d1709e28b-c000.snappy.parquet part-00006-94de7a9e-4dbd-4b50-b33c-949ae38dc676-c000.snappy.parquetpart-00001-6e4655ff-555e-441d-bdc9-68176e630936-c000.snappy.parquet part-00006-f5cb90a2-06bd-46ec-af61-9490bdd4321c-c000.snappy.parquetpart-00001-b3bcd196-dc8b-43b8-ad43-f73ecac35ccb-c000.snappy.parquet part-00007-6431fca3-bf2c-4ad7-a42c-a2e18feb3ed7-c000.snappy.parquetpart-00003-7fed5d2a-0ba9-4dcf-bc8d-ad9c729884e3-c000.snappy.parquet part-00007-8ed33d7c-5634-4739-afbb-471961bec689-c000.snappy.parquetpart-00003-93af5943-2745-42e8-9ac6-c001f257f3a8-c000.snappy.parquet/tmp/delta-table/_delta_log:00000000000000000000.json 00000000000000000001.json 00000000000000000001.json の内容は以下の通り。 1234567891011121314151617181920212223&#123; \"commitInfo\": &#123; \"timestamp\": 1582681285873, \"operation\": \"WRITE\", \"operationParameters\": &#123; \"mode\": \"Overwrite\", \"partitionBy\": \"[]\" &#125;, \"readVersion\": 0, \"isBlindAppend\": false &#125;&#125;&#123; \"add\": &#123; \"path\": \"part-00000-191c798b-3202-4fdf-9447-891f19953a37-c000.snappy.parquet\", \"partitionValues\": &#123;&#125;, \"size\": 262, \"modificationTime\": 1582681285000, \"dataChange\": true &#125;&#125;(snip) 確かに上書きモードで書き込まれたことが確かめられる。 このモードは、例えば org.apache.spark.sql.delta.commands.WriteIntoDelta で用いられる。 上記の 1data.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\") が呼ばれるときに、内部的に org.apache.spark.sql.delta.commands.WriteIntoDelta#run メソッドが呼ばれ、 以下のように、 mode が渡される。 org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:63 12345678override def run(sparkSession: SparkSession): Seq[Row] = &#123; deltaLog.withNewTransaction &#123; txn =&gt; val actions = write(txn, sparkSession) val operation = DeltaOperations.Write(mode, Option(partitionColumns), options.replaceWhere) txn.commit(actions, operation) &#125; Seq.empty&#125; なお、overwriteモードを指定しないと、 mode には ErrorIfExists が渡される。 モードの詳細は、enum org.apache.spark.sql.SaveMode （Spark SQL）を参照。 （例えば、他にも append や Ignore あたりも指定可能そうではある） クイックスタートのストリームデータ読み書き 公式ドキュメント（クイックスタート） には以下の例が載っていた。 12scala&gt; val streamingDf = spark.readStream.format(\"rate\").load()scala&gt; val stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\") 別のターミナルを開き、Sparkシェルを開き、以下でデータを読み込む。 1scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\") 裏でストリームデータを書き込んでいるので、適当に間をおいてcountを実行するとレコード数が増えていることが確かめられる。 12345scala&gt; df.countres1: Long = 1139scala&gt; df.countres2: Long = 1158 12345678910111213141516171819202122232425scala&gt; df.groupBy(\"id\").count.sort($\"count\".desc).show+----+-----+| id|count|+----+-----+| 964| 1|| 29| 1|| 26| 1|| 474| 1|| 831| 1||1042| 1|| 167| 1|| 112| 1|| 299| 1|| 155| 1|| 385| 1|| 736| 1|| 113| 1||1055| 1|| 502| 1||1480| 1|| 656| 1|| 287| 1|| 0| 1|| 348| 1|+----+-----+ 止めるには、ストリーム処理を実行しているターミナルで以下を実行。 1scala&gt; stream.stop() 上記だと面白くないので、次に実行する処理内容を少しだけ修正。10で割った余りとするようにした。 1scala&gt; val stream = streamingDf.select($\"value\" % 10 as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\") 結果は以下の通り。先程書き込んだデータも含まれているので、 件数が1個のデータと、いま書き込んでいる10で割った余りのデータが混在しているように見えるはず。 1234567891011121314151617181920212223242526scala&gt; df.groupBy(\"id\").count.sort($\"count\".desc).show+----+-----+| id|count|+----+-----+| 0| 19|| 9| 19|| 1| 19|| 2| 19|| 3| 18|| 6| 18|| 5| 18|| 8| 18|| 7| 18|| 4| 18||1055| 1||1547| 1|| 26| 1||1224| 1|| 502| 1||1480| 1|| 237| 1|| 588| 1|| 602| 1|| 347| 1|+----+-----+only showing top 20 rows つづいてドキュメントに載っているストリームとしての読み取りを試す。 1scala&gt; val stream2 = spark.readStream.format(\"delta\").load(\"/tmp/delta-table\").writeStream.format(\"console\").start() 以下のような表示がコンソールに出るはず。 123456789-------------------------------------------Batch: 7-------------------------------------------+---+| id|+---+| 5|| 6|+---+ それぞれ止めておく。 1scala&gt; stream.stop() 1scala&gt; stream2.stop() さて、まず書き込み時の動作を確認する。 上記の例で言うと、ストリーム処理を定義し、スタートすると、 1scala&gt; val stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", \"/tmp/checkpoint\").start(\"/tmp/delta-table\") 以下の org.apache.spark.sql.delta.sources.DeltaDataSource#createSink メソッドが呼ばれる。 org/apache/spark/sql/delta/sources/DeltaDataSource.scala:99 1234567891011121314override def createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink = &#123; val path = parameters.getOrElse(\"path\", &#123; throw DeltaErrors.pathNotSpecifiedException &#125;) if (outputMode != OutputMode.Append &amp;&amp; outputMode != OutputMode.Complete) &#123; throw DeltaErrors.outputModeNotSupportedException(getClass.getName, outputMode) &#125; val deltaOptions = new DeltaOptions(parameters, sqlContext.sparkSession.sessionState.conf) new DeltaSink(sqlContext, new Path(path), partitionColumns, outputMode, deltaOptions)&#125; 上記の通り、実態は org.apache.spark.sql.delta.sources.DeltaSink クラスである。 当該クラスはSpark Structured Streamingの org.apache.spark.sql.execution.streaming.Sink トレートを継承（ミックスイン）している。 org.apache.spark.sql.delta.sources.DeltaSink#addBatch メソッドが、実際にシンクにバッチを追加する処理を規定している。 org/apache/spark/sql/delta/sources/DeltaSink.scala:50 123456789 override def addBatch(batchId: Long, data: DataFrame): Unit = deltaLog.withNewTransaction &#123; txn =&gt; val queryId = sqlContext.sparkContext.getLocalProperty(StreamExecution.QUERY_ID_KEY) assert(queryId != null) if (SchemaUtils.typeExistsRecursively(data.schema)(_.isInstanceOf[NullType])) &#123; throw DeltaErrors.streamWriteNullTypeException &#125;(snip) 上記のように、 org.apache.spark.sql.delta.DeltaLog#withNewTransaction メソッドを用いてトランザクションが開始される。 また引数には、バッチのユニークなIDと書き込み対象のDataFrameが含まれる。 このまま、軽く org.apache.spark.sql.delta.sources.DeltaSink#addBatch の内容を確認する。 以下、順に記載。 org/apache/spark/sql/delta/sources/DeltaSink.scala:63 123456val selfScan = data.queryExecution.analyzed.collectFirst &#123; case DeltaTable(index) if index.deltaLog.isSameLogAs(txn.deltaLog) =&gt; true&#125;.nonEmptyif (selfScan) &#123; txn.readWholeTable()&#125; この書き込み時に、同時に読み込みをしているかどうかを確認。 （トランザクション制御の関係で・・・） org/apache/spark/sql/delta/sources/DeltaSink.scala:71 123456updateMetadata( txn, data, partitionColumns, configuration = Map.empty, outputMode == OutputMode.Complete()) メタデータ更新。 org/apache/spark/sql/delta/sources/DeltaSink.scala:78 12345val currentVersion = txn.txnVersion(queryId)if (currentVersion &gt;= batchId) &#123; logInfo(s\"Skipping already complete epoch $batchId, in query $queryId\") return&#125; バッチが重なっていないかどうかを確認。 org/apache/spark/sql/delta/sources/DeltaSink.scala:84 123456val deletedFiles = outputMode match &#123; case o if o == OutputMode.Complete() =&gt; deltaLog.assertRemovable() txn.filterFiles().map(_.remove) case _ =&gt; Nil&#125; 削除対象ファイルを確認。 なお、ここで記載している例においてはモードがAppendなので、値がNilになる。 org/apache/spark/sql/delta/sources/DeltaSink.scala:90 1val newFiles = txn.writeFiles(data, Some(options)) つづいて、ファイルを書き込み。 ちなみに、書き込まれたファイル情報が戻り値として得られる。 内容は以下のような感じ。 12345678newFiles = &#123;ArrayBuffer@20585&#125; &quot;ArrayBuffer&quot; size = 7 0 = &#123;AddFile@20592&#125; &quot;AddFile(part-00000-80d99ff7-f0cd-48f7-80a1-30e7f60be763-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 1 = &#123;AddFile@20593&#125; &quot;AddFile(part-00001-72553d89-8181-4550-b961-11463562768c-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 2 = &#123;AddFile@20594&#125; &quot;AddFile(part-00002-3f95aa5d-03fd-40c7-ae36-c65f20d5c7e0-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 3 = &#123;AddFile@20595&#125; &quot;AddFile(part-00003-ad9c1b95-868c-475f-a090-d73127bbff2b-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 4 = &#123;AddFile@20596&#125; &quot;AddFile(part-00004-2d1b9266-6325-4f72-aa37-46b065d574a0-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 5 = &#123;AddFile@20597&#125; &quot;AddFile(part-00005-aa35596d-3eb6-4046-977f-761d6f3e97f5-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 6 = &#123;AddFile@20598&#125; &quot;AddFile(part-00006-5d370239-de8d-45fc-9836-6c154808291a-c000.snappy.parquet,Map(),429,1582991406000,true,null,null)&quot; 実際に更新されたファイルを確認すると以下の通り。 123456789$ ls -lt /tmp/delta-table/ | head合計 8180-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00000-80d99ff7-f0cd-48f7-80a1-30e7f60be763-c000.snappy.parquet-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00003-ad9c1b95-868c-475f-a090-d73127bbff2b-c000.snappy.parquet-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00006-5d370239-de8d-45fc-9836-6c154808291a-c000.snappy.parquet-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00004-2d1b9266-6325-4f72-aa37-46b065d574a0-c000.snappy.parquet-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00005-aa35596d-3eb6-4046-977f-761d6f3e97f5-c000.snappy.parquet-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00001-72553d89-8181-4550-b961-11463562768c-c000.snappy.parquet-rw-r--r-- 1 dobachi dobachi 429 3月 1 00:50 part-00002-3f95aa5d-03fd-40c7-ae36-c65f20d5c7e0-c000.snappy.parquet では実装確認に戻る。 org/apache/spark/sql/delta/sources/DeltaSink.scala:91 123val setTxn = SetTransaction(queryId, batchId, Some(deltaLog.clock.getTimeMillis())) :: Nilval info = DeltaOperations.StreamingUpdate(outputMode, queryId, batchId)txn.commit(setTxn ++ newFiles ++ deletedFiles, info) トランザクションをコミットすると、 _delta_log 以下のファイルも更新される。 次は、読み込みの動作を確認する。 読み込み時は、 org.apache.spark.sql.delta.sources.DeltaSource#getBatch メソッドが呼ばれ、 バッチが取得される。 org/apache/spark/sql/delta/sources/DeltaSource.scala:273 123override def getBatch(start: Option[Offset], end: Offset): DataFrame = &#123;(snip) ここでは org.apache.spark.sql.delta.sources.DeltaSource#getBatch メソッド内の処理を確認する。 org/apache/spark/sql/delta/sources/DeltaSource.scala:274 12val endOffset = DeltaSourceOffset(tableId, end)previousOffset = endOffset // For recovery 最初に、当該バッチの終わりの位置を示すオフセットを算出する。 org/apache/spark/sql/delta/sources/DeltaSource.scala:276 1234567891011121314151617val changes = if (start.isEmpty) &#123; if (endOffset.isStartingVersion) &#123; getChanges(endOffset.reservoirVersion, -1L, isStartingVersion = true) &#125; else &#123; assert(endOffset.reservoirVersion &gt; 0, s\"invalid reservoirVersion in endOffset: $endOffset\") // Load from snapshot `endOffset.reservoirVersion - 1L` so that `index` in `endOffset` // is still valid. getChanges(endOffset.reservoirVersion - 1L, -1L, isStartingVersion = true) &#125;&#125; else &#123; val startOffset = DeltaSourceOffset(tableId, start.get) if (!startOffset.isStartingVersion) &#123; // unpersist `snapshot` because it won't be used any more. cleanUpSnapshotResources() &#125; getChanges(startOffset.reservoirVersion, startOffset.index, startOffset.isStartingVersion)&#125; 上記の通り、 org.apache.spark.sql.delta.sources.DeltaSource#getChanges メソッドを使って org.apache.spark.sql.delta.sources.IndexedFile インスタンスのイテレータを受け取る。 特に初回のバッチでは変数 start が None であり、さらにスナップショットから開始するようになる。 なお、初回では無い場合は、以下が実行される。 org/apache/spark/sql/delta/sources/DeltaSource.scala:285 12345678&#125; else &#123; val startOffset = DeltaSourceOffset(tableId, start.get) if (!startOffset.isStartingVersion) &#123; // unpersist `snapshot` because it won't be used any more. cleanUpSnapshotResources() &#125; getChanges(startOffset.reservoirVersion, startOffset.index, startOffset.isStartingVersion)&#125; 与えられたオフセット情報を元に org.apache.spark.sql.delta.sources.DeltaSourceOffset をインスタンス化し、それを用いてイテレータを生成する。 org/apache/spark/sql/delta/sources/DeltaSource.scala:294 123456val addFilesIter = changes.takeWhile &#123; case IndexedFile(version, index, _, _) =&gt; version &lt; endOffset.reservoirVersion || (version == endOffset.reservoirVersion &amp;&amp; index &lt;= endOffset.index)&#125;.collect &#123; case i: IndexedFile if i.add != null =&gt; i.add &#125;val addFiles = addFilesIter.filter(a =&gt; excludeRegex.forall(_.findFirstIn(a.path).isEmpty)).toSeq 上記で得られたイテレータをベースに、 AddFile インスタンスのシーケンスを得る。 12logDebug(s\"start: $start end: $end $&#123;addFiles.toList&#125;\")deltaLog.createDataFrame(deltaLog.snapshot, addFiles, isStreaming = true) org.apache.spark.sql.delta.DeltaLog#createDataFrame メソッドを使って、今回のバッチ向けのDataFrameを得る。 （参考）DeltaSinkのaddBatchメソッドがどこから呼ばれるか。 Sparkの org.apache.spark.sql.execution.streaming.MicroBatchExecution#runBatch メソッド内で呼ばれる。 org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala:534 12345678910reportTimeTaken(\"addBatch\") &#123; SQLExecution.withNewExecutionId(sparkSessionToRunBatch, lastExecution) &#123; sink match &#123; case s: Sink =&gt; s.addBatch(currentBatchId, nextBatch) case _: StreamWriteSupport =&gt; // This doesn't accumulate any data - it just forces execution of the microbatch writer. nextBatch.collect() &#125; &#125;&#125; なお、ここで渡されている nextBatch は、その直前で以下のように生成される。 org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala:531 12val nextBatch = new Dataset(sparkSessionToRunBatch, lastExecution, RowEncoder(lastExecution.analyzed.schema)) なお、ほぼ自明であるが、上記例において nextBatch で渡されるバッチのクエリプランは以下のとおりである。 123*(1) Project [value#684L AS id#4L]+- *(1) Project [timestamp#683, value#684L] +- *(1) ScanV2 rate[timestamp#683, value#684L] org.apache.spark.sql.execution.streaming.MicroBatchExecution#runBatch メソッド自体は、 org.apache.spark.sql.execution.streaming.MicroBatchExecution#runActivatedStream メソッド内で呼ばれる。 すなわち、 org.apache.spark.sql.execution.streaming.StreamExecution クラス内で管理される、ストリーム処理を実行する仕組みの中で、 繰り返し呼ばれることになる。 スキーマバリデーション 公式ドキュメントのスキーマバリデーション には書き込みの際のスキーマ確認のアルゴリズムが載っている。 原則はカラムや型が一致することを求める。違っていたら例外が上がる。 また大文字小文字だけが異なるカラムを用ってはいけない。 試しに例外を出してみる 試しにスキーマの異なるレコードを追加しようと試みてみた。 ここでは、保存されているDelta Lakeテーブルには存在しない列を追加したデータを書き込もうとしてみる。 123456789scala&gt; // テーブルの準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // いったん別のDFとして読み込み定義scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")scala&gt; // 新しいカラム付きのレコードを定義し、書き込みscala&gt; val newRecords = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\", rand() * 10 % 10 as \"tmp\" cast \"int\")scala&gt; newRecords.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta-table\") 以下のようなエラーが生じた。 1234567891011121314151617181920212223242526scala&gt; newRecords.write.format(&quot;delta&quot;).mode(&quot;append&quot;).save(&quot;/tmp/delta-table&quot;)org.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table.To enable schema migration, please set:&apos;.option(&quot;mergeSchema&quot;, &quot;true&quot;)&apos;.Table schema:root-- id: long (nullable = true)-- rand: integer (nullable = true)Data schema:root-- id: long (nullable = true)-- rand: integer (nullable = true)-- tmp: integer (nullable = true)If Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLEcommand for changing the schema. ; at org.apache.spark.sql.delta.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:851) at org.apache.spark.sql.delta.schema.ImplicitMetadataOperation$class.updateMetadata(ImplicitMetadataOperation.scala:122) at org.apache.spark.sql.delta.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:45) at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:85)(snip) 上記エラーは、 org.apache.spark.sql.delta.MetadataMismatchErrorBuilder#addSchemaMismatch メソッド内で 生成されたものである。 org/apache/spark/sql/delta/DeltaErrors.scala:810 1234567891011121314def addSchemaMismatch(original: StructType, data: StructType): Unit = &#123; bits ++= s\"\"\"A schema mismatch detected when writing to the Delta table. |To enable schema migration, please set: |'.option(\"$&#123;DeltaOptions.MERGE_SCHEMA_OPTION&#125;\", \"true\")'. | |Table schema: |$&#123;DeltaErrors.formatSchema(original)&#125; | |Data schema: |$&#123;DeltaErrors.formatSchema(data)&#125; \"\"\".stripMargin :: Nil mentionedOption = true&#125; 上記の通り、Before / Afterでスキーマを表示してくれるようになっている。 このメソッドは、 org.apache.spark.sql.delta.schema.ImplicitMetadataOperation#updateMetadata メソッド内で 呼ばれる。 org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:113 123if (isNewSchema) &#123; errorBuilder.addSchemaMismatch(txn.metadata.schema, dataSchema)&#125; ここで isNewSchema は現在のスキーマと新たに渡されたデータの スキーマが一致しているかどうかを示す変数。 org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:57 12345678910val dataSchema = data.schema.asNullableval mergedSchema = if (isOverwriteMode &amp;&amp; canOverwriteSchema) &#123; dataSchema&#125; else &#123; SchemaUtils.mergeSchemas(txn.metadata.schema, dataSchema)&#125;val normalizedPartitionCols = normalizePartitionColumns(data.sparkSession, partitionColumns, dataSchema)// Merged schema will contain additional columns at the enddef isNewSchema: Boolean = txn.metadata.schema != mergedSchema 上記の通り、overwriteモードでスキーマのオーバライトが可能なときは、新しいデータのスキーマが有効。 そうでない場合は、 org.apache.spark.sql.delta.schema.SchemaUtils$#mergeSchemas メソッドを使って 改めてスキーマが確定する。 自動でのカラム追加（スキーマ変更）を試す 公式ドキュメントのスキーママージ に記載があるとおり試す。 上記の例外が出た例に対し、以下のexpressionは成功する。 123456789scala&gt; // テーブルの準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // いったん別のDFとして読み込み定義scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")scala&gt; // 新しいカラム付きのレコードを定義し、書き込みscala&gt; val newRecords = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\", rand() * 10 % 10 as \"tmp\" cast \"int\")scala&gt; newRecords.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/tmp/delta-table\") org.apache.spark.sql.delta.schema.ImplicitMetadataOperation#updateMetadata メソッドの内容を確認する。 org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:57 123456val dataSchema = data.schema.asNullableval mergedSchema = if (isOverwriteMode &amp;&amp; canOverwriteSchema) &#123; dataSchema&#125; else &#123; SchemaUtils.mergeSchemas(txn.metadata.schema, dataSchema)&#125; 上記expressionを実行した際、まず dataSchema には、以下のような値が含まれている。 つまり、新しく渡されたレコードのスキーマである。 1234dataSchema = &#123;StructType@16480&#125; &quot;StructType&quot; size = 3 0 = &#123;StructField@19860&#125; &quot;StructField(id,LongType,true)&quot; 1 = &#123;StructField@19861&#125; &quot;StructField(rand,IntegerType,true)&quot; 2 = &#123;StructField@19855&#125; &quot;StructField(tmp,IntegerType,true)&quot; 一方、 txn.metadata.schema には以下のような値が含まれている。 つまり、書き込み先のテーブルのスキーマである。 123result = &#123;StructType@19866&#125; &quot;StructType&quot; size = 2 0 = &#123;StructField@19871&#125; &quot;StructField(id,LongType,true)&quot; 1 = &#123;StructField@19872&#125; &quot;StructField(rand,IntegerType,true)&quot; org.apache.spark.sql.delta.schema.SchemaUtils$#mergeSchemas メソッドによりマージされたスキーマ mergedSchema は、 1234result = &#123;StructType@16487&#125; &quot;StructType&quot; size = 3 0 = &#123;StructField@19853&#125; &quot;StructField(id,LongType,true)&quot; 1 = &#123;StructField@19854&#125; &quot;StructField(rand,IntegerType,true)&quot; 2 = &#123;StructField@19855&#125; &quot;StructField(tmp,IntegerType,true)&quot; となる。 したがって、 isNewSchema メソッドの戻り値は true となる。 1def isNewSchema: Boolean = txn.metadata.schema != mergedSchema 実際には overwriteモードかどうか 新しいパーティショニングが必要かどうか スキーマが変わるかどうか に依存して処理が分かれるが、今回のケースでは以下のようにメタデータが更新される。 org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:103 1234567&#125; else if (isNewSchema &amp;&amp; canMergeSchema &amp;&amp; !isNewPartitioning) &#123; logInfo(s\"New merged schema: $&#123;mergedSchema.treeString&#125;\") recordDeltaEvent(txn.deltaLog, \"delta.ddl.mergeSchema\") if (rearrangeOnly) &#123; throw DeltaErrors.unexpectedDataChangeException(\"Change the Delta table schema\") &#125; txn.updateMetadata(txn.metadata.copy(schemaString = mergedSchema.json)) overwriteモード時のスキーマ上書き 公式ドキュメントのスキーマ上書き に記載の通り、overwriteモードのとき、デフォルトではスキーマを更新しない。 したがって以下のexpressionは例外を生じる。 123456789scala&gt; // テーブルの準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // いったん別のDFとして読み込み定義scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")scala&gt; // 新しいカラム付きのレコードを定義し、書き込みscala&gt; val newRecords = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\", rand() * 10 % 10 as \"tmp\" cast \"int\")scala&gt; newRecords.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\") option(\"overwriteSchema\", \"true\") をつけると、overwriteモード時にスキーマの異なるデータで上書きするとき、例外を生じなくなる。 123456789scala&gt; // テーブルの準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // いったん別のDFとして読み込み定義scala&gt; val df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")scala&gt; // 新しいカラム付きのレコードを定義し、書き込みscala&gt; val newRecords = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\", rand() * 10 % 10 as \"tmp\" cast \"int\")scala&gt; newRecords.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(\"/tmp/delta-table\") つまり、 overwriteモード スキーマ上書き可能 新しいスキーマ という条件の下で、以下のようにメタデータが更新される。 org/apache/spark/sql/delta/schema/ImplicitMetadataOperation.scala:91 123456789101112&#125; else if (isOverwriteMode &amp;&amp; canOverwriteSchema &amp;&amp; (isNewSchema || isNewPartitioning)) &#123; // Can define new partitioning in overwrite mode val newMetadata = txn.metadata.copy( schemaString = dataSchema.json, partitionColumns = normalizedPartitionCols ) recordDeltaEvent(txn.deltaLog, \"delta.ddl.overwriteSchema\") if (rearrangeOnly) &#123; throw DeltaErrors.unexpectedDataChangeException(\"Overwrite the Delta table schema or \" + \"change the partition schema\") &#125; txn.updateMetadata(newMetadata) パーティション 公式ドキュメントのパーティション説明 には、以下のような例が載っている。 1scala&gt; df.write.format(\"delta\").partitionBy(\"date\").save(\"/delta/events\") 特にパーティションを指定せずに実行すると以下のようになる。 1234567scala&gt; val id = spark.range(0, 100000)id: org.apache.spark.sql.Dataset[Long] = [id: bigint]scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")data: org.apache.spark.sql.DataFrame = [id: bigint, rand: double]scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\") 12345678910$ ls -1 /tmp/delta-table/_delta_logpart-00000-6861eaa0-a0dc-4365-872b-b0c110fe1462-c000.snappy.parquetpart-00001-11725618-2e8e-4a6b-ad6c-5e32f668cb90-c000.snappy.parquetpart-00002-8d69aea0-a2f0-46c0-b5a3-dd892a182307-c000.snappy.parquetpart-00003-ff70d70b-0252-497e-93db-2cd715db8ab6-c000.snappy.parquetpart-00004-46e681ef-2521-4642-ae8c-63fc3c7c9817-c000.snappy.parquetpart-00005-10aec3fc-9538-408c-b520-894ccf529663-c000.snappy.parquetpart-00006-62bf3268-79f7-47ea-8400-b3f7566914cb-c000.snappy.parquetpart-00007-8683ce7e-9fe6-4b64-be97-bd158cda551f-c000.snappy.parquet Parquetファイルが出力ディレクトリに直接置かれる。 つづいてパーティションカラムを指定して書き込み。 1scala&gt; data.write.format(\"delta\").partitionBy(\"rand\").save(\"/tmp/delta-table-partitioned\") 12345678910111213141516171819202122232425262728$ ls -1 -R /tmp/delta-table-partitioned//tmp/delta-table-partitioned/:_delta_log'rand=0''rand=1''rand=2''rand=3''rand=4''rand=5''rand=6''rand=7''rand=8''rand=9'/tmp/delta-table-partitioned/_delta_log:00000000000000000000.json'/tmp/delta-table-partitioned/rand=0':part-00000-336b194c-8e44-4e37-ac8f-114fb0e813c7.c000.snappy.parquetpart-00001-d49f42cd-f0ca-4a5c-87a6-4f7647aecd52.c000.snappy.parquetpart-00002-2a5e96ed-c56b-4b6b-9471-6ca5830d512e.c000.snappy.parquetpart-00003-4e8d330d-9da7-40f5-91d2-cecf27347769.c000.snappy.parquetpart-00004-8f6b1d40-ea8c-4533-840d-ae688f729b50.c000.snappy.parquetpart-00005-f65d497c-2dc2-48b3-9252-e18dc077c5aa.c000.snappy.parquetpart-00006-eed688bb-104e-4940-a87a-e239294d4975.c000.snappy.parquetpart-00007-b6366ae1-fbbb-4789-910e-d896f62cee68.c000.snappy.parquet(snip) 先程指定したカラムの値に基づき、パーティション化されていることがわかる。 なお、メタデータ（ /tmp/delta-table-partitioned/_delta_log/00000000000000000000.json 内でも以下のようにパーティションカラムが指定されたことが示されてる。 123&#123;\"commitInfo\":&#123;\"timestamp\":1583070456680,\"operation\":\"WRITE\",\"operationParameters\":&#123;\"mode\":\"ErrorIfExists\",\"partitionBy\":\"[\\\"rand\\\"]\"&#125;,\"isBlindAppend\":true&#125;&#125;(snip) Delta Table 公式ドキュメント（クイックスタート） にも記載あるが、データをテーブル形式で操作できる。 上記で掲載されている例は以下の通り。 123456789101112131415161718192021222324252627import io.delta.tables._import org.apache.spark.sql.functions._val deltaTable = DeltaTable.forPath(\"/tmp/delta-table\")// Update every even value by adding 100 to itdeltaTable.update( condition = expr(\"id % 2 == 0\"), set = Map(\"id\" -&gt; expr(\"id + 100\")))// Delete every even valuedeltaTable.delete(condition = expr(\"id % 2 == 0\"))// Upsert (merge) new dataval newData = spark.range(0, 20).toDFdeltaTable.as(\"oldData\") .merge( newData.as(\"newData\"), \"oldData.id = newData.id\") .whenMatched .update(Map(\"id\" -&gt; col(\"newData.id\"))) .whenNotMatched .insert(Map(\"id\" -&gt; col(\"newData.id\"))) .execute()deltaTable.toDF.show() なお、 io.delta.tables.DeltaTable クラスの実態は、Dataset（DataFrame）とそれを操作するためのAPIの塊である。 例えば上記の io.delta.tables.DeltaTable#forPath メソッドは以下のように定義されている。 12345678def forPath(sparkSession: SparkSession, path: String): DeltaTable = &#123; if (DeltaTableUtils.isDeltaTable(sparkSession, new Path(path))) &#123; new DeltaTable(sparkSession.read.format(\"delta\").load(path), DeltaLog.forTable(sparkSession, path)) &#125; else &#123; throw DeltaErrors.notADeltaTableException(DeltaTableIdentifier(path = Some(path))) &#125;&#125; 条件でレコード削除 123456789101112131415161718192021222324scala&gt; // データ準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // テーブルとして読み込み定義scala&gt; import io.delta.tables._scala&gt; import org.apache.spark.sql.functions._scala&gt; val deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")scala&gt; // 条件に基づきレコードを削除scala&gt; deltaTable.delete(\"rand &gt; 5\")scala&gt; deltaTable.delete($\"rand\" &gt; 6)scala&gt; deltaTable.toDF.show+-----+----+| id|rand|+-----+----+|62500| 0||62501| 5||62503| 4||62504| 3||62505| 3||62506| 3||62507| 2|(snip) なお、 io.delta.tables.DeltaTable#delete メソッドの実態は、 io.delta.tables.execution.DeltaTableOperations#executeDelete メソッドである。 コメントで、 // current DELETE does not support subquery, // and the reason why perform checking here is that // we want to have more meaningful exception messages, // instead of having some random msg generated by executePlan(). と記載されており、バージョン0.5.0の段階ではサブクエリを使った削除には対応していないようだ。 またドキュメントによると、 delete removes the data from the latest version of the Delta table but does not remove it from the physical storage until the old versions are explicitly vacuumed. See vacuum for more details. とあり、不要になったファイルはバキュームで削除されるとのこと。 更新 12345678910scala&gt; // データ準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // テーブルとして読み込み定義scala&gt; import io.delta.tables._scala&gt; import org.apache.spark.sql.functions._scala&gt; val deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")scala&gt; // 条件に基づきレコードを更新scala&gt; deltaTable.updateExpr(\"rand = 0\", Map(\"rand\" -&gt; \"-1\")) 元のデータが 12345678910111213141516scala&gt; deltaTable.toDF.show+---+----+| id|rand|+---+----+| 0| 4|| 1| 6|| 2| 0|| 3| 5|| 4| 1|| 5| 6|| 6| 7|| 7| 0|| 8| 0|| 9| 5|(snip) だとすると、 123456789101112131415scala&gt; deltaTable.toDF.show+---+----+| id|rand|+---+----+| 0| 4|| 1| 6|| 2| -1|| 3| 5|| 4| 1|| 5| 6|| 6| 7|| 7| -1|| 8| -1|(snip) のようになる。 io.delta.tables.DeltaTable#updateExpr メソッドの実態は、 io.delta.tables.execution.DeltaTableOperations#executeUpdate メソッドである。 upsert（マージ） 12345678910111213141516171819202122232425262728scala&gt; // データ準備scala&gt; val id = spark.range(0, 100000)scala&gt; val data = id.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; data.write.format(\"delta\").save(\"/tmp/delta-table\")scala&gt; // テーブルとして読み込み定義scala&gt; import io.delta.tables._scala&gt; import org.apache.spark.sql.functions._scala&gt; val deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-table\")scala&gt; // upsert用のデータ作成scala&gt; val id2 = spark.range(0, 200000)scala&gt; val data2 = id2.select($\"id\", rand() * 10 % 10 as \"rand\" cast \"int\")scala&gt; val dataForUpsert = data2.sample(false, 0.5)scala&gt; // データをマージscala&gt; :pastedeltaTable .as(\"before\") .merge( dataForUpsert.as(\"updates\"), \"before.id = updates.id\") .whenMatched .updateExpr( Map(\"rand\" -&gt; \"updates.rand\")) .whenNotMatched .insertExpr( Map( \"id\" -&gt; \"updates.id\", \"rand\" -&gt; \"updates.rand\")) .execute() まず元データには、99999より大きな値はない。 1234567&gt; deltaTable.toDF.filter($\"id\" &gt; 99997).show+-----+----+| id|rand|+-----+----+|99998| 4||99999| 0|+-----+----+ 元データの先頭は以下の通り。 123456789101112131415161718scala&gt; deltaTable.toDF.show+---+----+| id|rand|+---+----+| 0| 1|| 1| 3|| 2| 5|| 3| 8|| 4| 3|| 5| 0|| 6| 5|| 7| 6|| 8| 4|| 9| 9|| 10| 4|| 11| 4|(snip) upsert用のデータは以下の通り。 1234567891011scala&gt; dataForUpsert.show+---+----+| id|rand|+---+----+| 0| 7|| 2| 6|| 5| 2|| 8| 9|| 10| 9|(snip) 99999よりも大きな id も存在する。 1234567891011scala&gt; dataForUpsert.filter($\"id\" &gt; 99997).show+------+----+| id|rand|+------+----+| 99999| 0||100005| 3||100006| 9||100008| 7||100009| 1|(snip) upseart（マージ）を実行すると、 123456789101112131415scala&gt; deltaTable.toDF.orderBy($\"id\").show+---+----+| id|rand|+---+----+| 0| 7|| 1| 3|| 2| 6|| 3| 8|| 4| 3|| 5| 2|| 6| 5|| 7| 6|| 8| 9|(snip) 上記のように、既存のレコードについては値が更新された。 3 また、 1234567891011scala&gt; deltaTable.toDF.filter($\"id\" &gt; 99997).show+------+----+| id|rand|+------+----+|100544| 5||100795| 1||101090| 5||101499| 1||101963| 7|(snip) のように、99997よりも大きな id のレコードも存在することがわかる。 io.delta.tables.DeltaTable#merge 上記の例に載っていた io.delta.tables.DeltaTable#merge を確認する。 io/delta/tables/DeltaTable.scala:501 123def merge(source: DataFrame, condition: Column): DeltaMergeBuilder = &#123; DeltaMergeBuilder(this, source, condition)&#125; io.delta.tables.DeltaMergeBuilder はマージのロジックを構成するためのクラス。 whenMatched whenNotMatched メソッドが提供されており、それぞれマージの条件が合致した場合、合致しなかった場合に実行する処理を指定可能。 なお、それぞれメソッドの引数にString型の変数を渡すことができる。 その場合、マージの条件に 加えて さらに条件を加えられる。 なお、 whenNotMatched に引数を与えた場合は、 マージ条件に合致しなかった 引数で与えられた条件に合致した が成り立つ場合に有効である。 whenMatched の場合は戻り値は io.delta.tables.DeltaMergeMatchedActionBuilder である。 io.delta.tables.DeltaMergeMatchedActionBuilder クラスには update メソッド、 udpateExpr、 updateAll、 delete メソッドがあり、 条件に合致したときに実行する処理を定義できる。 例えば、 update メソッドは以下の通り。 io/delta/tables/DeltaMergeBuilder.scala:298 123def update(set: Map[String, Column]): DeltaMergeBuilder = &#123; addUpdateClause(set)&#125; io/delta/tables/DeltaMergeBuilder.scala:365 1234567891011121314private def addUpdateClause(set: Map[String, Column]): DeltaMergeBuilder = &#123; if (set.isEmpty &amp;&amp; matchCondition.isEmpty) &#123; // Nothing to update = no need to add an update clause mergeBuilder &#125; else &#123; val setActions = set.toSeq val updateActions = MergeIntoClause.toActions( colNames = setActions.map(x =&gt; UnresolvedAttribute.quotedString(x._1)), exprs = setActions.map(x =&gt; x._2.expr), isEmptySeqEqualToStar = false) val updateClause = MergeIntoUpdateClause(matchCondition.map(_.expr), updateActions) mergeBuilder.withClause(updateClause) &#125;&#125; 戻り値は上記の通り、 io.delta.tables.DeltaMergeBuilder になる。 当該クラスは、メンバに whenClauses を持ち、指定された更新用の式（のセット）を保持する。 io/delta/tables/DeltaMergeBuilder.scala:244 1234private[delta] def withClause(clause: MergeIntoClause): DeltaMergeBuilder = &#123; new DeltaMergeBuilder( this.targetTable, this.source, this.onCondition, this.whenClauses :+ clause)&#125; なお、最後に execute メソッドを確認する。 io/delta/tables/DeltaMergeBuilder.scala:225 123456789101112def execute(): Unit = &#123; val sparkSession = targetTable.toDF.sparkSession val resolvedMergeInto = MergeInto.resolveReferences(mergePlan)(tryResolveReferences(sparkSession) _) if (!resolvedMergeInto.resolved) &#123; throw DeltaErrors.analysisException(\"Failed to resolve\\n\", plan = Some(resolvedMergeInto)) &#125; // Preprocess the actions and verify val mergeIntoCommand = PreprocessTableMerge(sparkSession.sessionState.conf)(resolvedMergeInto) sparkSession.sessionState.analyzer.checkAnalysis(mergeIntoCommand) mergeIntoCommand.run(sparkSession)&#125; 最初に、DataFrameの変換前後の実行プラン、マージの際の条件等から マージの実行プランを作成する。 ★要確認 参考）値がユニークではないカラムを使ってのマージ 値がユニークではないカラムを使ってマージしようとすると、以下のようなエラーを生じる。 12345678910111213java.lang.UnsupportedOperationException: Cannot perform MERGE as multiple source rows matched and attempted to update the sametarget row in the Delta table. By SQL semantics of merge, when multiple source rows matchon the same target row, the update operation is ambiguous as it is unclear which sourceshould be used to update the matching target row.You can preprocess the source table to eliminate the possibility of multiple matches.Please refer tohttps://docs.delta.io/latest/delta/delta-update.html#upsert-into-a-table-using-merge at org.apache.spark.sql.delta.DeltaErrors$.multipleSourceRowMatchingTargetRowInMergeException(DeltaErrors.scala:444) at org.apache.spark.sql.delta.commands.MergeIntoCommand.org$apache$spark$sql$delta$commands$MergeIntoCommand$$findTouchedFiles(MergeIntoCommand.scala:225) at org.apache.spark.sql.delta.commands.MergeIntoCommand$$anonfun$run$1$$anonfun$apply$1$$anonfun$1.apply(MergeIntoCommand.scala:132)(snip) つまり、上記の例では dataForUpsert の中に、万が一重複する id をもつレコードが含まれていると、例外を生じることになる。 これは、重複する id について、どの値を採用したらよいか断定できなくなるため。 実装上は、 org.apache.spark.sql.delta.commands.MergeIntoCommand#findTouchedFiles メソッド内で重複の確認が行われる。 以下の通り。 org/apache/spark/sql/delta/commands/MergeIntoCommand.scala:223 1234val matchedRowCounts = collectTouchedFiles.groupBy(ROW_ID_COL).agg(sum(\"one\").as(\"count\"))if (matchedRowCounts.filter(\"count &gt; 1\").count() != 0) &#123; throw DeltaErrors.multipleSourceRowMatchingTargetRowInMergeException(spark)&#125; 対象のカラムでgroupByして件数を数えていることがわかる。 最も容易に再現する方法は、 val dataForUpsert = data2.sample(false, 0.5) の第1引数をtrueにすれば、おそらく再現する。 マージの例 公式ドキュメントのマージの例 に有用な例が載っている。 公式ドキュメントのマージを使った上書き ログなどを収集する際、データソース側で重複させることがある。Delta Lakeのテーブルにマージしながら入力することで、 レコード重複を排除しながらテーブルの内容を更新できる さらに、新しいデータをマージする際、マージすべきデータの範囲がわかっていれば（例：最近7日間のログなど）、 それを使ってスキャン・書き換えの範囲を絞りこめる。 Slowly changing data (SCD) Type 2 operation Dimensionalなデータを断続的にゆっくりと更新するワークロード 新しい値で更新するときに、古い値を残しておく Write change data into a Delta table 外部テーブルの変更をDelta Lakeに取り込む DataFrame内にキー、タイムスタンプ、新しい値、削除フラグを持つ「変更内容」を表すレコードを保持。 上記DataFrameには、あるキーに関する変更を複数含む可能性があるため、キーごとに一度アグリゲートし、 グループごとに最も新しい変更内容を採用する。 DeltaTableのmerge機能を利用してupsert（や削除）する。 Upsert from streaming queries using foreachBatch org.apache.spark.sql.streaming.DataStreamWriter#foreachBatch を用いて、ストリームの各マイクロバッチに対する、 Delta Lakeのマージ処理を行う。 CDCの方法と組み合わせ、重複排除（ユニークID利用、マイクロバッチのIDが使える？）との組み合わせも可能 タイムトラベル Deltaの持つヒストリは、上記の DeltaTable を利用して見られる。（その他にも、メタデータを直接確認する、という手もあるはある） 12345678910111213scala&gt; val deltaTable = DeltaTable.forPath(\"/tmp/delta-table\")scala&gt; deltaTable.history.show+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+|version| timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+| 13|2020-02-26 23:19:17| null| null| MERGE|[predicate -&gt; (ol...|null| null| null| 12| null| false|| 12|2020-02-26 23:16:13| null| null| DELETE|[predicate -&gt; [\"(...|null| null| null| 11| null| false|| 11|2020-02-26 23:16:00| null| null| UPDATE|[predicate -&gt; ((i...|null| null| null| 10| null| false|| 10|2020-02-26 23:12:48| null| null| WRITE|[mode -&gt; Append, ...|null| null| null| 9| null| true|| 9|2020-02-26 23:12:33| null| null| WRITE|[mode -&gt; Overwrit...|null| null| null| 8| null| false|| 8|2020-02-26 23:11:48| null| null| WRITE|[mode -&gt; Append, ...|null| null| null| 7| null| true|(snip) このうち、readVersionを指定し、ロードすることもできる。 1scala&gt; spark.read.format(\"delta\").option(\"versionAsOf\", 8).load(\"/tmp/delta-table\").show ここで指定した versionAsOf の値は、 org.apache.spark.sql.delta.sources.DeltaDataSource 内で用いられる。 org.apache.spark.sql.delta.sources.DeltaDataSource#createRelation メソッド内に以下のような実装があり、 ここでタイムトラベルの値が読み込まれる。 org/apache/spark/sql/delta/sources/DeltaDataSource.scala:153 1val timeTravelByParams = getTimeTravelVersion(parameters) その他にもタイムスタンプで指定する方法もある。 公式ドキュメント の例： 1df1 = spark.read.format(\"delta\").option(\"timestampAsOf\", timestamp_string).load(\"/delta/events\") なお、 公式ドキュメント には以下のような記載が見られた。 This sample code writes out the data in df, validates that it all falls within the specified partitions, and performs an atomic replacement. これは、データ本体を書き出してから、メタデータを新規作成することでアトミックに更新することを示していると想像される。 org.apache.spark.sql.delta.OptimisticTransactionImpl#commit メソッドあたりを確認したら良さそう。 org.apache.spark.sql.delta.OptimisticTransactionImpl#commit org.apache.spark.sql.delta.OptimisticTransactionImpl#commit メソッドは、 例えば org.apache.spark.sql.delta.commands.WriteIntoDelta#run メソッド内で呼ばれる。 org/apache/spark/sql/delta/commands/WriteIntoDelta.scala:63 12345678override def run(sparkSession: SparkSession): Seq[Row] = &#123; deltaLog.withNewTransaction &#123; txn =&gt; val actions = write(txn, sparkSession) val operation = DeltaOperations.Write(mode, Option(partitionColumns), options.replaceWhere) txn.commit(actions, operation) &#125; Seq.empty&#125; Delta Logを書き出した後に、メタデータを更新し書き出したDelta Logを有効化する。 書き込み衝突検知なども行われる。 org/apache/spark/sql/delta/OptimisticTransaction.scala:250 123456 def commit(actions: Seq[Action], op: DeltaOperations.Operation): Long = recordDeltaOperation( deltaLog, \"delta.commit\") &#123; val version = try &#123;(snip) commitメソッドは上記の通り、 Action と Operation を引数に取る。 Action: Delta Tableに対する変更内容を表す。Actionのシーケンスがテーブル更新の内容を表す。 Operation: Delta Tableに対する操作を表す。必ずしもテーブル内容の更新を示すわけではない。 なお、Operationの子クラスは以下の通り。 ManualUpdate$ in DeltaOperations$ (org.apache.spark.sql.delta) UnsetTableProperties in DeltaOperations$ (org.apache.spark.sql.delta) Write in DeltaOperations$ (org.apache.spark.sql.delta) Convert in DeltaOperations$ (org.apache.spark.sql.delta) UpgradeProtocol in DeltaOperations$ (org.apache.spark.sql.delta) ComputeStats in DeltaOperations$ (org.apache.spark.sql.delta) SetTableProperties in DeltaOperations$ (org.apache.spark.sql.delta) FileNotificationRetention$ in DeltaOperations$ (org.apache.spark.sql.delta) Update in DeltaOperations$ (org.apache.spark.sql.delta) ReplaceTable in DeltaOperations$ (org.apache.spark.sql.delta) Truncate in DeltaOperations$ (org.apache.spark.sql.delta) Fsck in DeltaOperations$ (org.apache.spark.sql.delta) CreateTable in DeltaOperations$ (org.apache.spark.sql.delta) Merge in DeltaOperations$ (org.apache.spark.sql.delta) Optimize in DeltaOperations$ (org.apache.spark.sql.delta) ReplaceColumns in DeltaOperations$ (org.apache.spark.sql.delta) UpdateColumnMetadata in DeltaOperations$ (org.apache.spark.sql.delta) StreamingUpdate in DeltaOperations$ (org.apache.spark.sql.delta) Delete in DeltaOperations$ (org.apache.spark.sql.delta) ChangeColumn in DeltaOperations$ (org.apache.spark.sql.delta) ResetZCubeInfo in DeltaOperations$ (org.apache.spark.sql.delta) UpdateSchema in DeltaOperations$ (org.apache.spark.sql.delta) AddColumns in DeltaOperations$ (org.apache.spark.sql.delta) では commit メソッドの内容確認に戻る。 org/apache/spark/sql/delta/OptimisticTransaction.scala:253 123val version = try &#123; // Try to commit at the next version. var finalActions = prepareCommit(actions, op) 最初に、 org.apache.spark.sql.delta.OptimisticTransactionImpl#prepareCommit メソッドを利用し準備する。 例えば、 メタデータ更新が一度に複数予定されていないか 最初の書き込みか？必要に応じて出力先ディレクトリを作り、初期のプロトコルバージョンを決める、など。 不要なファイルの削除 ここで「プロトコルバージョン」と言っているのは、クライアントが書き込みする際に使用するプロトコルのバージョンである。 つまり、テーブルに後方互換性を崩すような変更があった際、最小プロトコルバージョンを規定することで、 古すぎるクライアントのアクセスを許さないような仕組みが、Delta Lakeには備わっている。 では commit メソッドの内容確認に戻る。 org/apache/spark/sql/delta/OptimisticTransaction.scala:257 12345678910// Find the isolation level to use for this commitval noDataChanged = actions.collect &#123; case f: FileAction =&gt; f.dataChange &#125;.forall(_ == false)val isolationLevelToUse = if (noDataChanged) &#123; // If no data has changed (i.e. its is only being rearranged), then SnapshotIsolation // provides Serializable guarantee. Hence, allow reduced conflict detection by using // SnapshotIsolation of what the table isolation level is. SnapshotIsolation&#125; else &#123; Serializable&#125; 書き込みファイル衝突時のアイソレーションレベルの確認。 なお、上記では実際には データ変更なし： SnapshotIsolation データ変更あり： Serializable とされている。詳しくは #アイソレーションレベル を参照。 (WIP) プロトコルバージョン あとで書く。 アイソレーションレベル Delta Lakeは楽観ロックの仕組みであるが、3種類のアイソレーションレベルが用いられることになっている。（使い分けられることになっている） DeltaTableのコンストラクタに、 delta ソースとして読み込んだDataFrameが渡されていることがわかる。 * org.apache.spark.sql.delta.Serializable * org.apache.spark.sql.delta.WriteSerializable * org.apache.spark.sql.delta.SnapshotIsolation org.apache.spark.sql.delta.OptimisticTransactionImpl#commit メソッド内では 以下のようにデータ変更があるかどうかでアイソレーションレベルを使い分けるようになっている。 src/main/scala/org/apache/spark/sql/delta/OptimisticTransaction.scala:259 12345678val isolationLevelToUse = if (noDataChanged) &#123; // If no data has changed (i.e. its is only being rearranged), then SnapshotIsolation // provides Serializable guarantee. Hence, allow reduced conflict detection by using // SnapshotIsolation of what the table isolation level is. SnapshotIsolation&#125; else &#123; Serializable&#125; また、実際に上記アイソレーションレベルの設定が用いられるのは、 org.apache.spark.sql.delta.OptimisticTransactionImpl#doCommit メソッドである。 src/main/scala/org/apache/spark/sql/delta/OptimisticTransaction.scala:293 1val commitVersion = doCommit(snapshot.version + 1, finalActions, 0, isolationLevelToUse) org.apache.spark.sql.delta.OptimisticTransactionImpl#doCommit メソッドでは、 tryを用いてデルタログのストアファイルに書き込む。 12345678910111213private def doCommit( attemptVersion: Long, actions: Seq[Action], attemptNumber: Int, isolationLevel: IsolationLevel): Long = deltaLog.lockInterruptibly &#123; try &#123; logDebug( s\"Attempting to commit version $attemptVersion with $&#123;actions.size&#125; actions with \" + s\"$isolationLevel isolation level\") deltaLog.store.write( deltaFile(deltaLog.logPath, attemptVersion), actions.map(_.json).toIterator) ここで java.nio.file.FileAlreadyExistsException が発生すると、先程のアイソレーションレベルに 基づいた処理が行われることになる。 これは、楽観ロックであるため、いざ書き込もうとしたら「あ、デルタログのストアファイルが、誰かに書き込まれている…」ということが ありえるからある。 1234&#125; catch &#123; case e: java.nio.file.FileAlreadyExistsException =&gt; checkAndRetry(attemptVersion, actions, attemptNumber, isolationLevel)&#125; 上記の org.apache.spark.sql.delta.OptimisticTransactionImpl#checkAndRetry メソッドの内容を確認する。 org/apache/spark/sql/delta/OptimisticTransaction.scala:442 123456789101112protected def checkAndRetry( checkVersion: Long, actions: Seq[Action], attemptNumber: Int, commitIsolationLevel: IsolationLevel): Long = recordDeltaOperation( deltaLog, \"delta.commit.retry\", tags = Map(TAG_LOG_STORE_CLASS -&gt; deltaLog.store.getClass.getName)) &#123; import _spark.implicits._ val nextAttemptVersion = getNextAttemptVersion(checkVersion) 最初に現在の最新のDeltaログのバージョン確認。 これをもとに、チェックするべきバージョン、つまりトランザクションを始めてから、 更新された内容をトラックする。 以下の通り。 org/apache/spark/sql/delta/OptimisticTransaction.scala:453 123456 (checkVersion until nextAttemptVersion).foreach &#123; version =&gt; // Actions of a commit which went in before ours val winningCommitActions = deltaLog.store.read(deltaFile(deltaLog.logPath, version)).map(Action.fromJson)(snip) 上記で、トランザクション開始後のアクションを確認できる。 org/apache/spark/sql/delta/OptimisticTransaction.scala:460 123456val metadataUpdates = winningCommitActions.collect &#123; case a: Metadata =&gt; a &#125;val removedFiles = winningCommitActions.collect &#123; case a: RemoveFile =&gt; a &#125;val txns = winningCommitActions.collect &#123; case a: SetTransaction =&gt; a &#125;val protocol = winningCommitActions.collect &#123; case a: Protocol =&gt; a &#125;val commitInfo = winningCommitActions.collectFirst &#123; case a: CommitInfo =&gt; a &#125;.map( ci =&gt; ci.copy(version = Some(version))) この後の処理のため、アクションから各種情報を取り出す。 org/apache/spark/sql/delta/OptimisticTransaction.scala:467 123456789val blindAppendAddedFiles = mutable.ArrayBuffer[AddFile]()val changedDataAddedFiles = mutable.ArrayBuffer[AddFile]()val isBlindAppendOption = commitInfo.flatMap(_.isBlindAppend)if (isBlindAppendOption.getOrElse(false)) &#123; blindAppendAddedFiles ++= winningCommitActions.collect &#123; case a: AddFile =&gt; a &#125;&#125; else &#123; changedDataAddedFiles ++= winningCommitActions.collect &#123; case a: AddFile =&gt; a &#125;&#125; まず、変更のあったファイルを確認する。 このとき、既存ファイルに関係なく書き込まれた（blind append）ファイルか、 そうでないかを仕分けながら確認する。 一応、後々アイソレーションレベルに基づいてファイルを処理する際に、 ここで得られた情報が用いられる。 4 org/apache/spark/sql/delta/OptimisticTransaction.scala:479 12345678910if (protocol.nonEmpty) &#123; protocol.foreach &#123; p =&gt; deltaLog.protocolRead(p) deltaLog.protocolWrite(p) &#125; actions.foreach &#123; case Protocol(_, _) =&gt; throw new ProtocolChangedException(commitInfo) case _ =&gt; &#125;&#125; プロトコル変更がないかどうかを確認する。 org/apache/spark/sql/delta/OptimisticTransaction.scala:491 123if (metadataUpdates.nonEmpty) &#123; throw new MetadataChangedException(commitInfo)&#125; メタデータに変更があったら例外。 つまり、トランザクション開始後、例えばスキーマ変更があったら例外になる、ということ。 スキーマ上書きを有効化した上でカラム追加を伴うような書き込みを行った際にメタデータが変わるため、 他のトランザクションからそのような書き込みがあった場合は、例外になることになる。 src/main/scala/org/apache/spark/sql/delta/OptimisticTransaction.scala:496 12345val addedFilesToCheckForConflicts = commitIsolationLevel match &#123; case Serializable =&gt; changedDataAddedFiles ++ blindAppendAddedFiles case WriteSerializable =&gt; changedDataAddedFiles // don't conflict with blind appends case SnapshotIsolation =&gt; Seq.empty&#125; アイソレーションレベルに基づいて、競合確認する対象ファイルを決める。 なお、前述の通り、実際にはデータの変更のありなしでアイソレーションレベルが変わるようになっている。 具体的には、 データ変更あり：Serializable データ変更なし：SnapshotIsolation となっており、いまの実装ではWriteSerializableは用いられないようにみえる。 org.apache.spark.sql.delta.IsolationLevel にもそのような旨の記載がある。 org/apache/spark/sql/delta/isolationLevels.scala:83 12/** All the valid isolation levels that can be specified as the table isolation level */val validTableIsolationLevels = Set[IsolationLevel](Serializable, WriteSerializable) では、データの変更あり・なしは、どうやって設定されるのかというと、 dataChange というオプションで指定することになる。 データ変更なしの書き込みはいつ使われるのか、というと、 例えば「たくさん作られたDelta Lakeのファイルをまとめる（Compactする）とき」である。 説明が 公式ドキュメントのCompact filesの説明 にある。 ドキュメントの例では以下のようにオプションが指定されている。 123456789101112val path = \"...\"val numFiles = 16spark.read .format(\"delta\") .load(path) .repartition(numFiles) .write .option(\"dataChange\", \"false\") .format(\"delta\") .mode(\"overwrite\") .save(path) 例外処理の内容確認に戻る。 org/apache/spark/sql/delta/OptimisticTransaction.scala:496 12345val addedFilesToCheckForConflicts = commitIsolationLevel match &#123; case Serializable =&gt; changedDataAddedFiles ++ blindAppendAddedFiles case WriteSerializable =&gt; changedDataAddedFiles // don't conflict with blind appends case SnapshotIsolation =&gt; Seq.empty&#125; Delta Lakeバージョン0.5.0では、今回のトランザクションで書き込みがある場合は既存ファイルに影響ない書き込みを含め、すべて確認対象とする。 そうでない場合は確認対象は空である。 org/apache/spark/sql/delta/OptimisticTransaction.scala:501 1234567val predicatesMatchingAddedFiles = ExpressionSet(readPredicates).iterator.flatMap &#123; p =&gt; val conflictingFile = DeltaLog.filterFileList( metadata.partitionColumns, addedFilesToCheckForConflicts.toDF(), p :: Nil).as[AddFile].take(1) conflictingFile.headOption.map(f =&gt; getPrettyPartitionMessage(f.partitionValues))&#125;.take(1).toArray トランザクション中に、別のトランザクションによりファイルが追加された場合、関係するパーティション情報を取得。 org/apache/spark/sql/delta/OptimisticTransaction.scala:509 12345678910111213141516if (predicatesMatchingAddedFiles.nonEmpty) &#123; val isWriteSerializable = commitIsolationLevel == WriteSerializable val onlyAddFiles = winningCommitActions.collect &#123; case f: FileAction =&gt; f &#125;.forall(_.isInstanceOf[AddFile]) val retryMsg = if (isWriteSerializable &amp;&amp; onlyAddFiles &amp;&amp; isBlindAppendOption.isEmpty) &#123; // This transaction was made by an older version which did not set `isBlindAppend` flag. // So even if it looks like an append, we don't know for sure if it was a blind append // or not. So we suggest them to upgrade all there workloads to latest version. Some( \"Upgrading all your concurrent writers to use the latest Delta Lake may \" + \"avoid this error. Please upgrade and then retry this operation again.\") &#125; else None throw new ConcurrentAppendException(commitInfo, predicatesMatchingAddedFiles.head, retryMsg)&#125; 当該パーティション情報がから出ない場合は、例外 org.apache.spark.sql.delta.ConcurrentAppendException#ConcurrentAppendException が生じる。 つづいて、上記でないケースのうち、削除が行われたケース。 org/apache/spark/sql/delta/OptimisticTransaction.scala:527 1234567val readFilePaths = readFiles.map(f =&gt; f.path -&gt; f.partitionValues).toMapval deleteReadOverlap = removedFiles.find(r =&gt; readFilePaths.contains(r.path))if (deleteReadOverlap.nonEmpty) &#123; val filePath = deleteReadOverlap.get.path val partition = getPrettyPartitionMessage(readFilePaths(filePath)) throw new ConcurrentDeleteReadException(commitInfo, s\"$filePath in $partition\")&#125; 読もうとしたファイルが他のトランザクションにより削除された倍は、 例外 org.apache.spark.sql.delta.ConcurrentDeleteReadException#ConcurrentDeleteReadException を生じる。 もしくは、同じファイルを複数回消そうとしている場合、 org/apache/spark/sql/delta/OptimisticTransaction.scala:536 12345val txnDeletes = actions.collect &#123; case r: RemoveFile =&gt; r &#125;.map(_.path).toSetval deleteOverlap = removedFiles.map(_.path).toSet intersect txnDeletesif (deleteOverlap.nonEmpty) &#123; throw new ConcurrentDeleteDeleteException(commitInfo, deleteOverlap.head)&#125; 例外 org.apache.spark.sql.delta.ConcurrentDeleteDeleteException#ConcurrentDeleteDeleteException が生じる。 その他、何らかトランザクション上重複した場合… 1234val txnOverlap = txns.map(_.appId).toSet intersect readTxn.toSetif (txnOverlap.nonEmpty) &#123; throw new ConcurrentTransactionException(commitInfo)&#125; 例外 org.apache.spark.sql.delta.ConcurrentTransactionException#ConcurrentTransactionException が生じる。 以上に引っかからなかった場合は、例外を生じない。 トランザクション開始後、すべてのバージョンについて競合チェックが行われた後、 問題ない場合は、 org/apache/spark/sql/delta/OptimisticTransaction.scala:549 12logInfo(s\"No logical conflicts with deltas [$checkVersion, $nextAttemptVersion), retrying.\")doCommit(nextAttemptVersion, actions, attemptNumber + 1, commitIsolationLevel) org.apache.spark.sql.delta.OptimisticTransactionImpl#doCommit メソッドが再実行される。 つまり、リトライである。 こうなるケースは何かというと、コンパクションを行った際、他のトランザクションで削除などが行われなかったケースなどだと想像。 sbtでテストコードを使って確認 上記実装の内容を確認するため、SBTのコンフィグを以下のように修正し、デバッガをアタッチして確認する。 1234567891011diff --git a/build.sbt b/build.sbtindex af4ab71..444fe55 100644--- a/build.sbt+++ b/build.sbt@@ -63,6 +63,7 @@ scalacOptions ++= Seq( ) javaOptions += \"-Xmx1024m\"+javaOptions += \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005\" fork in Test := true sbtを実行 1$ ./build/sbt sbtで以下のテストを実行する。 1&gt; testOnly org.apache.spark.sql.delta.OptimisticTransactionSuite -- -z &quot;block concurrent commit on full table scan&quot; テストの内容は以下の通り。 tx1でスキャンしようとしたとき、別のtx2で先に削除が行われたケース。 12345678910111213141516test(\"block concurrent commit on full table scan\") &#123; withLog(addA_P1 :: addD_P2 :: Nil) &#123; log =&gt; val tx1 = log.startTransaction() // TX1 full table scan tx1.filterFiles() tx1.filterFiles(('part === 1).expr :: Nil) val tx2 = log.startTransaction() tx2.filterFiles() tx2.commit(addC_P2 :: addD_P2.remove :: Nil, ManualUpdate) intercept[ConcurrentAppendException] &#123; tx1.commit(addE_P3 :: addF_P3 :: Nil, ManualUpdate) &#125; &#125;&#125; したがって、ここでは removedFiles に値が含まれることになる。 12removedFiles = &#123;ArrayBuffer@12893&#125; &quot;ArrayBuffer&quot; size = 1 0 = &#123;RemoveFile@15705&#125; &quot;RemoveFile(part=2/d,Some(1583466515581),true)&quot; またtx2のアクションは正確には追加と削除であるから、 winningCommitActions の内容は以下の通りとなる。 1234winningCommitActions = &#123;ArrayBuffer@12887&#125; &quot;ArrayBuffer&quot; size = 3 0 = &#123;CommitInfo@16006&#125; &quot;CommitInfo(None,2020-03-05 19:48:35.582,None,None,Manual Update,Map(),None,None,None,Some(0),None,Some(false))&quot; 1 = &#123;AddFile@15816&#125; &quot;AddFile(part=2/c,Map(part -&gt; 2),1,1,true,null,null)&quot; 2 = &#123;RemoveFile@15705&#125; &quot;RemoveFile(part=2/d,Some(1583466515581),true)&quot; なお、今回は既存ファイルへの変更になるため、Blind Appendではない。 結果として、 changedDataAddedFiles には以下のような値が含まれることになる。 12changedDataAddedFiles = &#123;ArrayBuffer@15736&#125; &quot;ArrayBuffer&quot; size = 1 0 = &#123;AddFile@15816&#125; &quot;AddFile(part=2/c,Map(part -&gt; 2),1,1,true,null,null)&quot; addedFilesToCheckForConflicts も同様。 addedFilesToCheckForConflicts には以下の通り、 AddFile インスンタスが含まれる。 つまりtx2で追加しているファイルの情報が含まれる。 12addedFilesToCheckForConflicts = &#123;ArrayBuffer@15866&#125; &quot;ArrayBuffer&quot; size = 1 0 = &#123;AddFile@15816&#125; &quot;AddFile(part=2/c,Map(part -&gt; 2),1,1,true,null,null)&quot; predicatesMatchingAddedFiles は以下の通り、ファイル追加に関係するパーティション情報が含まれる。 12predicatesMatchingAddedFiles = &#123;String[1]@15911&#125; 0 = &quot;partition [part=2]&quot; このことから、 org/apache/spark/sql/delta/OptimisticTransaction.scala:509 1if (predicatesMatchingAddedFiles.nonEmpty) &#123; がfalseになり、例外 org.apache.spark.sql.delta.ConcurrentAppendException#ConcurrentAppendException が生じることになる。 つまり、以下の箇所。 org/apache/spark/sql/delta/OptimisticTransaction.scala:509 12345678910111213141516if (predicatesMatchingAddedFiles.nonEmpty) &#123; val isWriteSerializable = commitIsolationLevel == WriteSerializable val onlyAddFiles = winningCommitActions.collect &#123; case f: FileAction =&gt; f &#125;.forall(_.isInstanceOf[AddFile]) val retryMsg = if (isWriteSerializable &amp;&amp; onlyAddFiles &amp;&amp; isBlindAppendOption.isEmpty) &#123; // This transaction was made by an older version which did not set `isBlindAppend` flag. // So even if it looks like an append, we don't know for sure if it was a blind append // or not. So we suggest them to upgrade all there workloads to latest version. Some( \"Upgrading all your concurrent writers to use the latest Delta Lake may \" + \"avoid this error. Please upgrade and then retry this operation again.\") &#125; else None throw new ConcurrentAppendException(commitInfo, predicatesMatchingAddedFiles.head, retryMsg)&#125; コンパクション 公式ドキュメントのCompact filesの説明 にある通り、Delta Lakeで細かくデータを書き込むとファイルがたくさんできる。 これは性能に悪影響を及ぼす。 そこで、コンパクション（まとめこみ）を行うことがベストプラクティスとして紹介されている…。 なお、コンパクションでは古いファイルは消されないので、バキュームすることってことも書かれている。 バキューム 公式ドキュメントのVacuum に記載の通り、Deltaテーブルから参照されていないファイルを削除する。 リテンション時間はデフォルト7日間。 io.delta.tables.DeltaTable#vacuum メソッドの実態は、 io/delta/tables/DeltaTable.scala:90 123def vacuum(retentionHours: Double): DataFrame = &#123; executeVacuum(deltaLog, Some(retentionHours))&#125; の通り、 io.delta.tables.execution.DeltaTableOperations#executeVacuum メソッドである。 io/delta/tables/execution/DeltaTableOperations.scala:106 123456protected def executeVacuum( deltaLog: DeltaLog, retentionHours: Option[Double]): DataFrame = &#123; VacuumCommand.gc(sparkSession, deltaLog, false, retentionHours) sparkSession.emptyDataFrame&#125; なお、 org.apache.spark.sql.delta.commands.VacuumCommand#gc メソッドの内容は意外と複雑。 というのも、バキューム対象となるのは、「Deltaテーブルで参照されて いない ファイル」となるので、 すべてのファイル（やディレクトリ、そしてディレクトリ内のファイルも）をリストアップし、 その後消してはいけないファイルを除外できるようにして消すようになっている。 読まれているファイルの削除 公式ドキュメントのVacuum の中で、警告が書かれている。 We do not recommend that you set a retention interval shorter than 7 days, because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table. If VACUUM cleans up active files, concurrent readers can fail or, worse, tables can be corrupted when VACUUM deletes files that have not yet been committed. これは、バキューム対象から外す期間（リテンション時間）を不用意に短くしすぎると、 バキューム対象となったファイルを何かしらのトランザクションが読み込んでいる可能性があるからだ、という主旨の内容である。 実装から見ても、 org.apache.spark.sql.delta.actions.FileAction トレートに基づくフィルタリングはあるが、 当該トレートの子クラスは AddFile RemoveFile であり、読み込みは含まれない。 そのため、仕様上、何らかのトランザクションが読み込んでいるファイルを消すことがあり得る、ということと考えられる。 ★要確認 なお、うっかりミスを防止するためのチェック機能はあり、 org.apache.spark.sql.delta.commands.VacuumCommand#checkRetentionPeriodSafety メソッドがその実態である。 このメソッド内では、DeltaLogごとに定義されている org.apache.spark.sql.delta.DeltaLog#tombstoneRetentionMillis で指定されるよりも、短い期間をリテンション時間として 指定しているかどうかを確認し、警告を出すようになっている。 つまり、 1deltaTable.vacuum(100) // vacuum files not required by versions more than 100 hours old のようにリテンション時間を指定しながらバキュームを実行する際に、DeltaLog自身が保持している tombstoneRetentionMillis よりも短い期間を閾値として バキュームを実行しようとすると警告が生じる。なお、このチェックをオフにすることもできる。 コンフィグ org.apache.spark.sql.delta.DeltaConfigs あたりにDelta Lakeのコンフィグと説明がある。 例えば、 org.apache.spark.sql.delta.DeltaConfigs$#LOG_RETENTION であれば、 org.apache.spark.sql.delta.MetadataCleanup トレート内で利用されている。 ログのクリーンアップ コンフィグ にてリテンションを決めるパラメータを例として上げた。 具体的には、以下のように、ログのリテンション時間を決めるパラメータとして利用されている。 org/apache/spark/sql/delta/MetadataCleanup.scala:38 1234def deltaRetentionMillis: Long = &#123; val interval = DeltaConfigs.LOG_RETENTION.fromMetaData(metadata) DeltaConfigs.getMilliSeconds(interval)&#125; 上記の org.apache.spark.sql.delta.MetadataCleanup#deltaRetentionMillis メソッドは、 org.apache.spark.sql.delta.MetadataCleanup#cleanUpExpiredLogs メソッド内で利用される。 このメソッドは古くなったデルタログやチェックポイントを削除する。 このメソッド自体は単純で、以下のような実装である。 org/apache/spark/sql/delta/MetadataCleanup.scala:50 1234567891011121314private[delta] def cleanUpExpiredLogs(): Unit = &#123; recordDeltaOperation(this, \"delta.log.cleanup\") &#123; val fileCutOffTime = truncateDay(clock.getTimeMillis() - deltaRetentionMillis).getTime val formattedDate = fileCutOffTime.toGMTString logInfo(s\"Starting the deletion of log files older than $formattedDate\") var numDeleted = 0 listExpiredDeltaLogs(fileCutOffTime.getTime).map(_.getPath).foreach &#123; path =&gt; // recursive = false if (fs.delete(path, false)) numDeleted += 1 &#125; logInfo(s\"Deleted $numDeleted log files older than $formattedDate\") &#125;&#125; ポイントはいくつかある。 org.apache.spark.sql.delta.MetadataCleanup#listExpiredDeltaLogs メソッドは、チェックポイントファイル、デルタファイルの両方について、 期限切れになっているファイルを返すイテレータを戻す。 上記イテレータに対し、mapとforeachでループさせ、ファイルを消す（fs.delete(path, false)） 最終的に消された件数をログに書き出す なお、このクリーンアップは、チェックポイントのタイミングで実施される。 org/apache/spark/sql/delta/Checkpoints.scala:119 1234567def checkpoint(): Unit = recordDeltaOperation(this, \"delta.checkpoint\") &#123; val checkpointMetaData = checkpoint(snapshot) val json = JsonUtils.toJson(checkpointMetaData) store.write(LAST_CHECKPOINT, Iterator(json), overwrite = true) doLogCleanup()&#125; チェックポイントが実行されるのは、別途 チェックポイントを調査してみる で説明したとおり、 トランザクションがコミットされるタイミングなどである。（正確にはコミット後の処理postCommit処理の中で行われる） ParquetからDeltaテーブルへの変換 スキーマ指定する方法としない方法がある。 1234567import io.delta.tables._// Convert unpartitioned parquet table at path '/path/to/table'val deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/path/to/table`\")// Convert partitioned parquet table at path '/path/to/table' and partitioned by integer column named 'part'val partitionedDeltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/path/to/table`\", \"part int\") スキーマを指定しないAPIは以下の通り。 io/delta/tables/DeltaTable.scala:599 123456def convertToDelta( spark: SparkSession, identifier: String): DeltaTable = &#123; val tableId: TableIdentifier = spark.sessionState.sqlParser.parseTableIdentifier(identifier) DeltaConvert.executeConvert(spark, tableId, None, None)&#125; 上記の通り、実態は io.delta.tables.execution.DeltaConvertBase#executeConvert メソッドであり、 その中で用いられている org.apache.spark.sql.delta.commands.ConvertToDeltaCommandBase#run メソッドである。 io/delta/tables/execution/DeltaConvert.scala:26 1234567891011trait DeltaConvertBase &#123; def executeConvert( spark: SparkSession, tableIdentifier: TableIdentifier, partitionSchema: Option[StructType], deltaPath: Option[String]): DeltaTable = &#123; val cvt = ConvertToDeltaCommand(tableIdentifier, partitionSchema, deltaPath) cvt.run(spark) DeltaTable.forPath(spark, tableIdentifier.table) &#125;&#125; また、上記 run メソッド内の実態は org.apache.spark.sql.delta.commands.ConvertToDeltaCommandBase#performConvert メソッドである。 org/apache/spark/sql/delta/commands/ConvertToDeltaCommand.scala:76 1234567override def run(spark: SparkSession): Seq[Row] = &#123; val convertProperties = getConvertProperties(spark, tableIdentifier) (snip) performConvert(spark, txn, convertProperties)&#125; run メソッド内の performConvert メソッドを利用し、実際に元データからDelta Lakeのデータ構造を作りつつ、 その後の io.delta.tables.DeltaTable$#forPath(org.apache.spark.sql.SparkSession, java.lang.String) メソッドを呼び出すことで、 データが正しく出力されたことを確認している。 1DeltaTable.forPath(spark, tableIdentifier.table) もし出力された内容に何か問題あるようであれば、 forPath メソッドの途中で例外が生じるはず。 ただ、この仕組みだと変換自体は、たとえ失敗したとしても動いてしまう（アトミックな動作ではない）ところが気になった。 ★気になった点 動作確認 まずは、サンプルとしてSparkに含まれているParquetファイルを読み込む。 123456789scala&gt; val originDFPath = \"/opt/spark/default/examples/src/main/resources/users.parquet\"scala&gt; val originDF = spark.read.format(\"parquet\").load(originDFPath)scala&gt; originDF.show+------+--------------+----------------+| name|favorite_color|favorite_numbers|+------+--------------+----------------+|Alyssa| null| [3, 9, 15, 20]|| Ben| red| []|+------+--------------+----------------+ では、このParquetファイルを変換する。 io.delta.tables.DeltaTable#convertToDelta メソッドの引数で与えるPATHは、 Parqeutテーブルを含む「ディレクトリ」を期待するので、最初にParquetファイルを 適当なディレクトリにコピーしておく。 12$ mkdir /tmp/origin$ cp /opt/spark/default/examples/src/main/resources/users.parquet /tmp/origin 上記ディレクトリを指定して、変換する。 12scala&gt; import io.delta.tables._scala&gt; val deltaTable = DeltaTable.convertToDelta(spark, s\"parquet.`/tmp/origin`\") ディレクトリ以下は以下のようになった。 123456dobachi@thk:/mnt/c/Users/dobachi/Sources.win/memo-blog-text$ ls -R /tmp/origin//tmp/origin/:_delta_log users.parquet/tmp/origin/_delta_log:00000000000000000000.checkpoint.parquet 00000000000000000000.json _last_checkpoint なお、メタデータは以下の通り。 1234567891011121314151617181920212223242526272829303132333435363738394041$ cat /tmp/origin/_delta_log/00000000000000000000.json | jq&#123; \"commitInfo\": &#123; \"timestamp\": 1584541495383, \"operation\": \"CONVERT\", \"operationParameters\": &#123; \"numFiles\": 1, \"partitionedBy\": \"[]\", \"collectStats\": false &#125; &#125;&#125;&#123; \"protocol\": &#123; \"minReaderVersion\": 1, \"minWriterVersion\": 2 &#125;&#125;&#123; \"metaData\": &#123; \"id\": \"40bd74eb-8005-4aaa-a455-fbbb37b22bb7\", \"format\": &#123; \"provider\": \"parquet\", \"options\": &#123;&#125; &#125;, \"schemaString\": \"&#123;\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[&#123;\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":&#123;&#125;&#125;,&#123;\\\"name\\\":\\\"favorite_color\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":&#123;&#125;&#125;,&#123;\\\"name\\\":\\\"favorite_numbers\\\",\\\"type\\\":&#123;\\\"type\\\":\\\"array\\\",\\\"elementType\\\":\\\"integer\\\",\\\"containsNull\\\":true&#125;,\\\"nullable\\\":true,\\\"metadata\\\":&#123;&#125;&#125;]&#125;\", \"partitionColumns\": [], \"configuration\": &#123;&#125;, \"createdTime\": 1584541495356 &#125;&#125;&#123; \"add\": &#123; \"path\": \"users.parquet\", \"partitionValues\": &#123;&#125;, \"size\": 615, \"modificationTime\": 1584541479000, \"dataChange\": true &#125;&#125; Symlink Format Manifest Presto and Athena to Delta Lake Integration によると、Presto等で利用可能なマニフェストを出力できる。 ここでは予め /tmp/delta-table に作成しておいたDelta Tableを読み込み、マニフェストを出力する。 123scala&gt; import io.delta.tables._scala&gt; val deltaTable = DeltaTable.forPath(\"/tmp/delta-table\")scala&gt; deltaTable.generate(\"symlink_format_manifest\") 以下のようなディレクトリ、ファイルが出力される。 12345678910$ ls -1_delta_log_symlink_format_manifestderby.logmetastore_dbpart-00000-e4518073-8ff1-4c2e-b765-922114a06c08-c000.snappy.parquetpart-00000-f692adf2-c015-4b0b-8db9-8004a69ac80b-c000.snappy.parquetpart-00001-7c3dd52d-f763-4835-9e97-9c6805ceff36-c000.snappy.parquetpart-00001-d13867d8-c685-4a56-b0cd-6541009222a5-c000.snappy.parquet(snip) _delta_log および part-000... というディレクトリ、ファイルはもともとDelta Lakeとして 存在していたものである。 これに対し、 _symlink_format_manifest Deltaテーブルが含むファイル群を示すマニフェストを含むディレクトリ derby.log HiveメタストアDBのログ（Derbyのログ） metastore_db Hiveメタストア が出力されたと言える。 マニフェストの内容は以下の通り。 123456$ cat _symlink_format_manifest/manifestfile:/tmp/delta-table/part-00002-5f9ef6f0-da56-442d-8232-13937e00a54e-c000.snappy.parquetfile:/tmp/delta-table/part-00007-5522221f-20c2-4d70-aec8-3a990933b50e-c000.snappy.parquetfile:/tmp/delta-table/part-00003-572f44fd-9c36-409a-bbc8-8f23f869e3f1-c000.snappy.parquetfile:/tmp/delta-table/part-00000-f692adf2-c015-4b0b-8db9-8004a69ac80b-c000.snappy.parquet(snip) 上記の例では、パーティション化されていないDeltaテーブルを扱った。 この場合、マニフェストは１個である。 アトミックな書き込みが可能。 Snapshot consistency が実現できる。 一方、 Presto Athena連係の制約 によるとパーティション化されている場合、マニフェストもパーティション化される。 そのため、全パーティションを通じて一貫性を保つことができない。（部分的な更新が生じうる） それでは、実装を確認する。 エントリポイントは、公式ドキュメントの例にも載っている io.delta.tables.DeltaTable#generate メソッド。 io/delta/tables/DeltaTable.scala:151 1234def generate(mode: String): Unit = &#123; val path = deltaLog.dataPath.toString executeGenerate(s\"delta.`$path`\", mode)&#125; これの定義を辿っていくと、 org.apache.spark.sql.delta.commands.DeltaGenerateCommand#run メソッドにたどり着く。 org/apache/spark/sql/delta/commands/DeltaGenerateCommand.scala:48 1234567891011121314 override def run(sparkSession: SparkSession): Seq[Row] = &#123; if (!modeNameToGenerationFunc.contains(modeName)) &#123; throw DeltaErrors.unsupportedGenerateModeException(modeName) &#125; val tablePath = getPath(sparkSession, tableId) val deltaLog = DeltaLog.forTable(sparkSession, tablePath) if (deltaLog.snapshot.version &lt; 0) &#123; throw new AnalysisException(s\"Delta table not found at $tablePath.\") &#125; val generationFunc = modeNameToGenerationFunc(modeName) generationFunc(sparkSession, deltaLog) Seq.empty &#125;&#125; 上記の通り、Delta LakeのディレクトリからDeltaLogを再現し、 それを引数としてマニフェストを生成するメソッドを呼び出す。 変数にバインドするなどしているが、実態は org.apache.spark.sql.delta.hooks.GenerateSymlinkManifestImpl#generateFullManifest メソッドである。 org/apache/spark/sql/delta/commands/DeltaGenerateCommand.scala:63 123456object DeltaGenerateCommand &#123; val modeNameToGenerationFunc = CaseInsensitiveMap( Map[String, (SparkSession, DeltaLog) =&gt; Unit]( \"symlink_format_manifest\" -&gt; GenerateSymlinkManifest.generateFullManifest ))&#125; そこで当該メソッドの内容を軽く確認する。 org/apache/spark/sql/delta/hooks/GenerateSymlinkManifest.scala:154 123456789101112131415161718 def generateFullManifest( spark: SparkSession, deltaLog: DeltaLog): Unit = recordManifestGeneration(deltaLog, full = true) &#123; val snapshot = deltaLog.update(stalenessAcceptable = false) val partitionCols = snapshot.metadata.partitionColumns val manifestRootDirPath = new Path(deltaLog.dataPath, MANIFEST_LOCATION).toString val hadoopConf = new SerializableConfiguration(spark.sessionState.newHadoopConf()) // Update manifest files of the current partitions val newManifestPartitionRelativePaths = writeManifestFiles( deltaLog.dataPath, manifestRootDirPath, snapshot.allFiles, partitionCols, hadoopConf)(snip) ポイントは、 org.apache.spark.sql.delta.hooks.GenerateSymlinkManifestImpl#writeManifestFiles メソッドである。 これが実際にマニフェストを書き出すメソッド。 類似技術 Apache Hudi https://hudi.apache.org/ Ice https://iceberg.apache.org/ その後の確認でやはりv1利用のように見えた。（2020/2時点）↩︎ その後の調査（2020/2時点）で改めて楽観ロックの仕組みで実現されていることを確認↩︎ マージ後は id 順に並んでいない。明示的なソートが必要のようだ。↩︎ ただし、バージョン0.5.0では、実際のところ使われているアイソレーションレベルが限られているので、ここでの仕分けはあまり意味がないかもしれない。↩︎","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Storage Layer","slug":"Knowledge-Management/Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/"},{"name":"Delta Lake","slug":"Knowledge-Management/Storage-Layer/Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Storage-Layer/Delta-Lake/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"Delta Lake","slug":"Delta-Lake","permalink":"https://dobachi.github.io/memo-blog/tags/Delta-Lake/"},{"name":"Storage Layer","slug":"Storage-Layer","permalink":"https://dobachi.github.io/memo-blog/tags/Storage-Layer/"},{"name":"Parquet","slug":"Parquet","permalink":"https://dobachi.github.io/memo-blog/tags/Parquet/"}]},{"title":"Dask of Python","slug":"Dask-of-Python","date":"2019-04-23T14:12:05.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/04/23/Dask-of-Python/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/23/Dask-of-Python/","excerpt":"","text":"参考 メモ ダミー変数化での支障について 参考 Dask使ってみた Python Dask で 並列 DataFrame 処理 【初めての大規模データ②】Daskでの並列分散処理 時間のかかる前処理をDaskで高速化 メモ ダミー変数化での支障について 【初めての大規模データ②】Daskでの並列分散処理 に記載あった点が気になった。 以下、引用。 1234dask.dataframeは行方向にしかデータを分割できないため、ダミー変数を作成する際には1列分のデータを取得するために、すべてのデータを読み込まなければならず、メモリエラーを起こす危険性があります。そこで、大規模データの処理を行う際には、dask.dataframeを一度dask.arrayに1列ずつに分割した形で変換し、それから1列分のデータのみを再度dask.dataframeに変換し、get_dummiesしてやるのが良いと思います。※私はこの縛りに気づき、daskを使うのを諦めました... 上記ブログでは、少なくともダミー変数化の部分は素のPythonで実装したようだ。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Processing Engine","slug":"Knowledge-Management/Data-Processing-Engine","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Analysis/"},{"name":"Dask","slug":"Dask","permalink":"https://dobachi.github.io/memo-blog/tags/Dask/"},{"name":"Data Processing Engine","slug":"Data-Processing-Engine","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Processing-Engine/"}]},{"title":"Spark Summit NA 2019","slug":"Spark-Summit-NA-2019","date":"2019-04-22T12:20:43.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/22/Spark-Summit-NA-2019/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/22/Spark-Summit-NA-2019/","excerpt":"","text":"参考 メモ 参考 Spark Summit NA 2019のスケジュール 個人的に気になったセッション メモ 気になるセッションを 個人的に気になったセッション に示す。 ただし、個別のDeep Diveネタは除く。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Spark","slug":"Knowledge-Management/Spark","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Spark/"},{"name":"Spark Summit","slug":"Knowledge-Management/Spark/Spark-Summit","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Spark/Spark-Summit/"}],"tags":[{"name":"Spark Summit","slug":"Spark-Summit","permalink":"https://dobachi.github.io/memo-blog/tags/Spark-Summit/"}]},{"title":"RAPIDS","slug":"RAPIDS","date":"2019-04-16T13:40:07.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/04/16/RAPIDS/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/16/RAPIDS/","excerpt":"","text":"参考 メモ 概要 一言で言うと…？ コンポーネント群 処理性能に関する雰囲気 関係者 分散の仕組みはDaskベース？ cuDF バージョン0.7での予定 RAPIDS on Databricks Cloud Condaでの動作確認 前提 動作確認 Dockerで動作確認 参考）Dockerで動作確認する際の試行錯誤 前提 動作確認 サンプルノートブックの確認 参考）Condaで動作確認の試行錯誤メモ インスタンス種類を変えてみる 参考 RAPIDSの公式ウェブサイト RAPIDSの公式ドキュメント RAPIDSの公式Getting Started コンポーネントの関係図 性能比較のグラフ RAPIDS0.6に関する公式ブログ記事 RAPIDS on Databricks Cloudのブログ RAPIDS_PCA_demo_avro_read.ipynb Dask_with_cuDF_and_XGBoost.ipynb RMMの定義 cuDFのAPI定義 メモ 概要 一言で言うと…？ PandasライクなAPI、scikit learnライクなAPIなどを提供するライブラリ群。 CUDAをレバレッジし、GPUを活用しやくしている。 コンポーネント群 公式の コンポーネントの関係図 がわかりやすい。 前処理から、データサイエンス（と、公式で言われている）まで一貫して手助けするものである、というのが思想のようだ。 処理性能に関する雰囲気 公式の 性能比較のグラフ がわかりやすい。 2桁台数の「CPUノード」との比較。 DGX-2 1台、もしくはDGX-1 5台。 計算（ロード、特徴エンジニアリング、変換、学習）時間の比較。 関係者 公式ウェブサイトでは、「Contributors」、「Adopters」、「Open Source」というカテゴリで整理されていた。 Contributorsでは、AnacondaやNVIDIAなどのイメージ通りの企業から、Walmartまであった。 Adoptersでは、Databricksが入っている。 分散の仕組みはDaskベース？ Sparkと組み合わせての動作はまだ対応していないようだが、Daskで分散（なのか、現時点でのシングルマシンでのマルチGPU対応だけなのか）処理できるようだ。 RAPIDS0.6に関する公式ブログ記事 によると、Dask-CUDA、Dask-cuDFに対応の様子。 また、Dask-cuMLでは、k-NNと線形回帰に関し、マルチGPU動作を可能にしたようだ。 Dask_with_cuDF_and_XGBoost.ipynb を見ると、DaskでcuDFを使う方法が例示されていた。 Daskのクラスタを定義し、 1234from dask_cuda import LocalCUDAClustercluster = LocalCUDACluster()client = Client(cluster) RMM（RAPIDS Memory Manager）を初期化する。 123456from librmm_cffi import librmm_config as rmm_cfgrmm_cfg.use_pool_allocator = True#rmm_cfg.initial_pool_size = 2&lt;&lt;30 # set to 2GiB. Default is 1/2 total GPU memoryimport cudfreturn cudf._gdf.rmm_initialize() また、 RMMの定義 にRAPIDS Memory ManagerのAPI定義が記載されている。 cuDFのAPI定義 にもDaskでの動作について書かれている。 「Multi-GPU with Dask-cuDF」の項目。 cuDF cuDFのAPI定義 に仕様が記載されている。 現状できることなどの記載がある。 線形なオペレーション、集約処理、グルーピング、結合あたりの基本的な操作対応している。 バージョン0.7での予定 RAPIDS0.6に関する公式ブログ記事 には一部0.7に関する記述もあった。 個人的に気になったのは、Parquetリーダが改善されてGPUを使うようになること、 デバッグメッセージが改善されること、など。 RAPIDS on Databricks Cloud RAPIDS on Databricks Cloudのブログ を見ると、Databricks CloudでRAPIDSが動くことが書かれている。 ただ、 RAPIDS_PCA_demo_avro_read.ipynb を見たら、SparkでロードしたデータをそのままPandasのDataFrameに変換しているようだった。 Condaでの動作確認 前提 今回AWS環境を使った。 p3.2xlargeインスタンスを利用。 予めCUDA9.2をインストールしておいた。 またDockerで試す場合には、nvidia-dockerをインストールしておく。 動作確認 以下の通り仮想環境を構築し、パッケージをインストールする。 123$ conda create -n rapids python=3.6 python$ conda install -c nvidia -c rapidsai -c pytorch -c numba -c conda-forge \\ cudf=0.6 cuml=0.6 python=3.6 その後、GitHubからノートブックをクローン。 1$ git clone https://github.com/rapidsai/notebooks.git ノートブックのディレクトリでJupyterを起動。 1$ jupyter notebook --ip=0.0.0.0 --browser=&quot;&quot; つづいて、notebooks/cuml/linear_regression_demo.ipynbを試した。 なお、%%timeマジックを使っている箇所の変数をバインドできなかったので、 適当にセルをコピーして実行した。 確かに学習が早くなったことは実感できた。 scikit-learn 123456%%timeskols = skLinearRegression(fit_intercept=True, normalize=True)skols.fit(X_train, y_train)CPU times: user 21.5 s, sys: 5.33 s, total: 26.9 sWall time: 7.51 s cudf + cuml 1234567%%timecuols = cuLinearRegression(fit_intercept=True, normalize=True, algorithm=&apos;eig&apos;)cuols.fit(X_cudf, y_cudf)CPU times: user 1.18 s, sys: 350 ms, total: 1.53 sWall time: 2.72 s Dockerで動作確認 つづいてDockerで試す。 123$ sudo docker pull rapidsai/rapidsai:cuda9.2-runtime-centos7$ sudo docker run --runtime=nvidia --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 \\ rapidsai/rapidsai:cuda9.2-runtime-centos7 Dockerコンテナが起動したら、以下の通り、Jupyter Labを起動する。 1# bash utils/start-jupyter.sh 線形回帰のサンプルノートブックを試したが、大丈夫だった。 参考）Dockerで動作確認する際の試行錯誤 RAPIDSの公式Getting Started を参考に動かしてみる。 前提 今回は、AWS環境を使った。 予め、CUDAとnvidia-dockerをインストールしておいた。 今回は、CUDA10系を使ったので、以下のようにDockerイメージを取得。 1$ sudo docker pull rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04 動作確認。このとき、runtimeにnvidiaを指定する。 123456789101112131415161718$ sudo docker run --runtime=nvidia --rm nvidia/cuda:10.1-base nvidia-smiWed Apr 17 13:28:02 2019+-----------------------------------------------------------------------------+| NVIDIA-SMI 418.39 Driver Version: 418.39 CUDA Version: 10.1 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla M60 Off | 00000000:00:1E.0 Off | 0 || N/A 31C P0 42W / 150W | 0MiB / 7618MiB | 81% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 以下のDockerfileを作り、動作確認する。 123456789101112131415161718# FROM defines the base image# FROM nvidia/cuda:10.0FROM rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04# RUN executes a shell command# You can chain multiple commands together with &amp;&amp;# A \\ is used to split long lines to help with readability# This particular instruction installs the source files# for deviceQuery by installing the CUDA samples via aptRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ cuda-samples-$CUDA_PKG_VERSION &amp;&amp; \\ rm -rf /var/lib/apt/lists/*# set the working directoryWORKDIR /usr/local/cuda/samples/1_Utilities/deviceQueryRUN make# CMD defines the default command to be run in the container# CMD is overridden by supplying a command + arguments to# `docker run`, e.g. `nvcc --version` or `bash`CMD ./deviceQuery 123456789101112131415161718192021222324252627282930313233343536373839404142$ sudo nvidia-docker build -t device-query .$ sudo nvidia-docker run --rm -ti device-query$ sudo nvidia-docker run --rm -ti device-query./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking)Detected 1 CUDA Capable device(s)Device 0: &quot;Tesla M60&quot; CUDA Driver Version / Runtime Version 10.1 / 10.0 CUDA Capability Major/Minor version number: 5.2 Total amount of global memory: 7619 MBytes (7988903936 bytes) (16) Multiprocessors, (128) CUDA Cores/MP: 2048 CUDA Cores GPU Max Clock rate: 1178 MHz (1.18 GHz) Memory Clock rate: 2505 Mhz Memory Bus Width: 256-bit L2 Cache Size: 2097152 bytes Maximum Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096) Maximum Layered 1D Texture Size, (num) layers 1D=(16384), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(16384, 16384), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Compute Preemption: No Supports Cooperative Kernel Launch: No Supports MultiDevice Co-op Kernel Launch: No Device PCI Domain ID / Bus ID / location ID: 0 / 0 / 30 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.0, NumDevs = 1Result = PASS 動作確認 コンテナを起動。 12$ sudo docker run --runtime=nvidia --rm -it -p 8888:8888 -p 8787:8787 -p 8786:8786 \\ rapidsai/rapidsai:cuda10.0-runtime-ubuntu18.04 12345(rapids) root@5d41281699e3:/rapids/notebooks# nvcc -Vnvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2018 NVIDIA CorporationBuilt on Sat_Aug_25_21:08:01_CDT_2018Cuda compilation tools, release 10.0, V10.0.130 ノートブックも起動してみる。（JupyterLabだった） 1# bash utils/start-jupyter.sh サンプルノートブックの確認 回帰分析の内容を確認してみる。 cuml/linear_regression_demo.ipynb 冒頭部分でライブラリをロードしている。 12345678910import numpy as npimport pandas as pdimport cudfimport osfrom cuml import LinearRegression as cuLinearRegressionfrom sklearn.linear_model import LinearRegression as skLinearRegressionfrom sklearn.datasets import make_regression# Select a particular GPU to run the notebook os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;2&quot; cudf、cumlあたりがRAPIDSのライブラリか。 途中、cudfを使うあたりで以下のエラーが発生。 1terminate called after throwing an instance of &apos;cudf::cuda_error&apos; what(): CUDA error encountered at: /rapids/cudf/cpp/src/bitmask/valid_ops.cu:170: 48 cudaErrorNoKernelImageForDevice no kernel image is available for execution on the device 参考）Condaで動作確認の試行錯誤メモ 切り分けも兼ねて、Condaで環境構築し、試してみる。 12$ conda create -n rapids python=3.6 python$ conda install -c nvidia/label/cuda10.0 -c rapidsai/label/cuda10.0 -c numba -c conda-forge -c defaults cudf エラー。 1PackageNotFoundError: Dependencies missing in current linux-64 channels: Condaのバージョンが古かったようなので、conda update condaしてから再度実行。→成功 cumlを同様にインストールしておく。 1$ conda install -c nvidia -c rapidsai -c conda-forge -c pytorch -c defaults cuml Jupyterノートブックを起動し、cuml/linear_regression_demoを試す。 cudaをインポートするところで以下のようなエラー。 1OSError: cannot load library &apos;/home/centos/.conda/envs/rapids/lib/librmm.so&apos;: libcudart.so.10.0: cannot open shared object file: No such file or directory そこで、 https://github.com/rapidsai/cudf/issues/496 を参照し、cudatoolkitのインストールを試す。 状況が変化し、今度は、libcublas.so.9.2が必要と言われた。 1ImportError: libcublas.so.9.2: cannot open shared object file: No such file or directory CUDA9系を指定されているように見える。 しかし実際にインストールしたのはCUDA10系。 1/home/centos/.conda/pkgs/cudatoolkit-10.0.130-0/lib/libcublas.so.10.0 ここでCUDA9系で試すことにする。 改めてCUDA10系をアンインストールし、CUDA9系をインストール後に以下を実行。 12$ conda install -c nvidia -c rapidsai -c pytorch -c numba -c conda-forge \\ cudf=0.6 cuml=0.6 python=3.6 その後、少なくともライブラリインポートはうまくいった。 余談だが、手元のJupyterノートブック環境では、%%timeマジックを使ったときに、 そのとき定義した変数がバインドされなかった。 （Dockerで試したときはうまく行ったような気がするが…） cudfをつかうところで以下のエラー発生。改めてcudatoolkitをインストールする。 12NvvmSupportError: libNVVM cannot be found. Do `conda install cudatoolkit`:library nvvm not found その後再度実行。 改めて以下のエラーを発生。 1what(): CUDA error encountered at: /conda/envs/gdf/conda-bld/libcudf_1553535868363/work/cpp/src/bitmask/valid_ops.cu:170: 48 cudaErrorNoKernelImageForDevice no kernel image is available for execution on the device インスタンス種類を変えてみる 基本的なことに気がついた。 g3.4xlargeではなく、p3.2xlargeに変更してみた。 うまくいった。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Processing Engine","slug":"Knowledge-Management/Data-Processing-Engine","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Processing-Engine/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Analysis/"},{"name":"Dask","slug":"Dask","permalink":"https://dobachi.github.io/memo-blog/tags/Dask/"},{"name":"Data Processing Engine","slug":"Data-Processing-Engine","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Processing-Engine/"},{"name":"PAPIDS","slug":"PAPIDS","permalink":"https://dobachi.github.io/memo-blog/tags/PAPIDS/"}]},{"title":"alexisbcook/hello-seaborn","slug":"alexisbcook-hello-seaborn","date":"2019-04-14T13:53:56.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/14/alexisbcook-hello-seaborn/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/14/alexisbcook-hello-seaborn/","excerpt":"","text":"参考 メモ 基本的な使い方 参考 Kaggleのalexisbcook hello-seaborn Kaggleのlearn data-visualization seabornの公式 seabornのギャラリー seabornのAPI Hello, Seaborn Data Visualization: from Non-Coder to Coder メモ seabornの公式 から、 seabornのギャラリー の公式ウェブサイトを見ると、 さまざまな例が載っている。 seabornのAPI を見ると、各種APIが載っている。 例えば、seaborn.barplotを見ると、 棒グラフの使い方が記載されている。 基本的な使い方 基本的には、 1import seaborn as sns して、 1sns.lineplot(data=fifa_data) のように、プロットの種類ごとに定義されたAPIを呼び出すだけ。 もし、グラフの見た目などを変えたいのであれば、matplotlibをインポートし、 設定すれば良い。 1plt.figure(figsize=(16,6)) や、 1plt.xticks(rotation=90) など。 その他、Data Visualization: from Non-Coder to Coderを見ると、 基本的な使い方を理解できるようになっている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Visualization","slug":"Knowledge-Management/Machine-Learning/Visualization","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/"},{"name":"Seaborn","slug":"Knowledge-Management/Machine-Learning/Visualization/Seaborn","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Visualization/Seaborn/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Visualization","slug":"Visualization","permalink":"https://dobachi.github.io/memo-blog/tags/Visualization/"},{"name":"Seaborn","slug":"Seaborn","permalink":"https://dobachi.github.io/memo-blog/tags/Seaborn/"}]},{"title":"dansbecker/data-leakage","slug":"dansbecker-data-leakage","date":"2019-04-14T13:03:51.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/14/dansbecker-data-leakage/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/14/dansbecker-data-leakage/","excerpt":"","text":"参考 メモ Leaky Predictor どう対処するのか？ Leaky Validation Strategy どう対処するのか？ クレジットカードのデータの例 参考 Kaggleのdansbecker data-leakage メモ Leaky Predictor 例では、肺炎発症と抗生物質摂取のケースが挙げられていた。 肺炎が発症したあとで、抗生物質を摂取する。 抗生物質を摂取したかどうかは、肺炎発症の前後で値が変わる。 値が変わることを考慮しないと、抗生物質を摂取しない人は肺炎にならない、というモデルが出来上がる可能性がある。 どう対処するのか？ 汎用的な対処方法はなく、データや要件に強く依存する。 Leaky Predictorを発見する コツ は、強い相関のある特徴同士に着目する、 とても高いaccuracyを得られたときに気をつける、など。 Leaky Validation Strategy 例では、train-testスプリットの前に前処理を行おうケースが挙げられていた。 バリデーション対象には、前処理も含めないといけない。 そうでないと、バリデーションで高いモデル性能が得られたとしても、 実際の判定処理で期待したモデル性能が得られない可能性がある。 どう対処するのか？ パイプラインを組むときに、例えばクロスバリデーションの処理内に、 前処理を入れるようにする、など。 クレジットカードのデータの例 クレジットカードの使用がアプリケーションで認められたかどうか、を判定する例。 ここでは、クレジットカードの使用量の特徴が、Leaky Predictorとして挙げられていた。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model","slug":"Knowledge-Management/Machine-Learning/Model","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/"},{"name":"Data Leakage","slug":"Knowledge-Management/Machine-Learning/Model/Data-Leakage","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Data-Leakage/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Model","slug":"Model","permalink":"https://dobachi.github.io/memo-blog/tags/Model/"},{"name":"Data Leakage","slug":"Data-Leakage","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Leakage/"}]},{"title":"dansbecker/cross-validation","slug":"dansbecker-cross-validation","date":"2019-04-13T14:57:21.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/13/dansbecker-cross-validation/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/13/dansbecker-cross-validation/","excerpt":"","text":"参考 メモ いつ使うか？ 参考 Kaggleのdansbecker cross-validation メモ いつ使うか？ 端的には、データ量が少ないときの効果が大きい。 逆に、データ量が大きいときは、用いなくてもよいかもしれない。 結果論で言えば、クロスバリデーションして結果が書く回で変わらなかったとき、 それはtrain-testスプリットで十分とも言える。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model","slug":"Knowledge-Management/Machine-Learning/Model","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/"},{"name":"Cross Validation","slug":"Knowledge-Management/Machine-Learning/Model/Cross-Validation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Cross-Validation/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Model","slug":"Model","permalink":"https://dobachi.github.io/memo-blog/tags/Model/"},{"name":"Cross Validation","slug":"Cross-Validation","permalink":"https://dobachi.github.io/memo-blog/tags/Cross-Validation/"}]},{"title":"dansbecker/partial-dependence-plots","slug":"dansbecker-partial-dependence-plots","date":"2019-04-13T13:33:24.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/13/dansbecker-partial-dependence-plots/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/13/dansbecker-partial-dependence-plots/","excerpt":"","text":"参考 メモ PDPはモデルが学習されたあとに計算可能 PDPの計算方法 タイタニックの例 「考察」に関する議論 参考 Kaggleのdansbecker partial-dependence-plots 機械学習モデルを解釈する方法 Permutation Importance / Partial Dependence Plot メモ PDPはモデルが学習されたあとに計算可能 ただし、様々なモデルに適用可能。 PDPの計算方法 以下のような感じ。 12345my_plots = plot_partial_dependence(my_model, features=[0, 1, 2], # column numbers of plots we want to show X=X, # raw predictors data. feature_names=[&apos;Distance&apos;, &apos;Landsize&apos;, &apos;BuildingArea&apos;], # labels on graphs grid_resolution=10) # number of values to plot on x axis 注意点として挙げられていたのは、grid_resolutionを細かくしたときに、 乱れたグラフが見られたとしても、その細かな挙動に対して文脈を考えすぎること。 どうしてもランダム性があるので、細かな挙動にいちいち気にしているとミスリードになる。 なお、partial_dependenceという関数を用いると、グラフを出力するのではなく、数値データそのものを得られる。 1234my_plots2 = partial_dependence(my_model, target_variables=[0, 1, 2], # column numbers of plots we want to show X=X, # raw predictors data. grid_resolution=10) # number of values to plot on x axis なお、微妙にオプションが異なることに注意…。 タイタニックの例 PDPを見ることで、年齢や支払い料金と生存結果の関係を解釈する例が記載されていた。 「考察」に関する議論 PDPで得られた結果を考察すること自体について、議論があるようだ。 意味のある・なし、という点において。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model","slug":"Knowledge-Management/Machine-Learning/Model","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/"},{"name":"Partial Dependency Plot","slug":"Knowledge-Management/Machine-Learning/Model/Partial-Dependency-Plot","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/Partial-Dependency-Plot/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Model","slug":"Model","permalink":"https://dobachi.github.io/memo-blog/tags/Model/"},{"name":"Partial Dependency Plot","slug":"Partial-Dependency-Plot","permalink":"https://dobachi.github.io/memo-blog/tags/Partial-Dependency-Plot/"}]},{"title":"dansbecker/xgboost","slug":"dansbecker-xgboost","date":"2019-04-12T12:48:11.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/12/dansbecker-xgboost/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/12/dansbecker-xgboost/","excerpt":"","text":"参考 メモ チューニングパラメータ 参考 Kaggleのdansbecker xgboost XGBoostの主な特徴と理論の概要 XGBoostの概要 この説明がわかりやすい メモ XGBoost自体については、XGBoostの概要がわかりやすい。 チューニングパラメータ n_estimators 大きさは100〜1000の間の値を用いることが多い early_stopping_rounds n_estimators を大きめにしておいて、本パラメータで学習を止めるのが良さそう learning_rate 加減しながら・・・。例では0.05あたりを使っていた。 n_jobs 並列処理（コア数の指定）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model","slug":"Knowledge-Management/Machine-Learning/Model","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/"},{"name":"XGBoost","slug":"Knowledge-Management/Machine-Learning/Model/XGBoost","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model/XGBoost/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Model","slug":"Model","permalink":"https://dobachi.github.io/memo-blog/tags/Model/"},{"name":"XGBoost","slug":"XGBoost","permalink":"https://dobachi.github.io/memo-blog/tags/XGBoost/"}]},{"title":"dansbecker/using-categorical-data-with-one-hot-encoding","slug":"dansbecker-using-categorical-data-with-one-hot-encoding","date":"2019-04-12T07:10:22.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/12/dansbecker-using-categorical-data-with-one-hot-encoding/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/12/dansbecker-using-categorical-data-with-one-hot-encoding/","excerpt":"","text":"参考 メモ カテゴリ値の判定にカージナリティを利用 カラムのdtype確認 Pandasのget_dummies 学習データとテストデータの列の並びを揃える 参考 Kaggleのusing-categorical-data-with-one-hot-encoding メモ one hot encodingについて。 Kaggleのラーニングコースを読んでみた。 カテゴリ値の判定にカージナリティを利用 コメントにもやや恣意的な…と書かれてはいたが、 カージナリティが低く、dtypeが objectであるカラムをカテゴリ値としたようだ。 12345low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if candidate_train_predictors[cname].nunique() &lt; 10 and candidate_train_predictors[cname].dtype == &quot;object&quot;]numeric_cols = [cname for cname in candidate_train_predictors.columns if candidate_train_predictors[cname].dtype in [&apos;int64&apos;, &apos;float64&apos;]] カラムのdtype確認 こんな感じで確かめられる。 1train_predictors.dtypes.sample(10) 結果の例 123456LandContour objectScreenPorch int64BsmtFinSF2 int64SaleType objectBedroomAbvGr int64(snip) Pandasのget_dummies 1one_hot_encoded_training_predictors = pd.get_dummies(train_predictors) 学習データとテストデータの列の並びを揃える 学習データとテストデータをそれぞれone hot encodingすると、 それぞれの列の並びが揃わないことがある。 scikit-learnは、列の並びに敏感である。 そこで、DataFrame#alignを用いて並びを揃える。 12345one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, join=&apos;left&apos;, axis=1) なお、joinオプションはSQLにおけるJOINのルールと同等だと考えれば良い。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Preparation","slug":"Knowledge-Management/Machine-Learning/Preparation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Preparation/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Preparation","slug":"Preparation","permalink":"https://dobachi.github.io/memo-blog/tags/Preparation/"}]},{"title":"dansbecker/handling-missing-values","slug":"dansbecker-handling-missing-values","date":"2019-04-12T06:15:31.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/12/dansbecker-handling-missing-values/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/12/dansbecker-handling-missing-values/","excerpt":"","text":"参考 メモ 欠落のあるカラムの排除 欠落を埋める 欠落を埋めつつ、欠落していたことを真理値として保持 参考 Kaggleのhandling-missing-values scikit-learnのimputeの説明 メモ Kaggleのhandling-missing-values の内容から気になった箇所をメモ。 欠落のあるカラムの排除 最もシンプルな方法として、欠落のあるカラムを排除する方法が挙げられていた。 1234cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]reduced_X_train = X_train.drop(cols_with_missing, axis=1)reduced_X_test = X_test.drop(cols_with_missing, axis=1) X_train[col].isnull().any()のところで、カラム内の値のいずれかがNULL値かどうかを確認している。 欠落を埋める 欠落を埋める。 12345from sklearn.impute import SimpleImputermy_imputer = SimpleImputer()imputed_X_train = my_imputer.fit_transform(X_train)imputed_X_test = my_imputer.transform(X_test) なお、SimpleImputer#fit_transformメソッドの戻り値は、numpy.ndarrayである。 また、scikit-learnのimputeの説明を見る限り、カテゴリ値にも対応しているようだ。 欠落を埋めつつ、欠落していたことを真理値として保持 12345678imputed_X_train_plus = X_train.copy()imputed_X_test_plus = X_test.copy()cols_with_missing = (col for col in X_train.columns if X_train[col].isnull().any())for col in cols_with_missing: imputed_X_train_plus[col + &apos;_was_missing&apos;] = imputed_X_train_plus[col].isnull() imputed_X_test_plus[col + &apos;_was_missing&apos;] = imputed_X_test_plus[col].isnull()","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Preparation","slug":"Knowledge-Management/Machine-Learning/Preparation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Preparation/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"},{"name":"Preparation","slug":"Preparation","permalink":"https://dobachi.github.io/memo-blog/tags/Preparation/"}]},{"title":"Kaggle入門","slug":"Introductoin-Kaggle","date":"2019-04-09T15:01:41.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/04/10/Introductoin-Kaggle/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/10/Introductoin-Kaggle/","excerpt":"","text":"参考 メモ エントリポイント 参考 Kaggle Learn (Machine Learning) Kaggleを始める人に役に立つ記事 Faster Data Science Education メモ エントリポイント Kaggleの環境やお作法？になれる目的で、Faster Data Science Educationを見始めた。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Kaggle","slug":"Knowledge-Management/Machine-Learning/Kaggle","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Kaggle/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://dobachi.github.io/memo-blog/tags/Kaggle/"}]},{"title":"Word2Vec with Python","slug":"Word2Vec-with-Python","date":"2019-04-08T13:12:44.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/08/Word2Vec-with-Python/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/08/Word2Vec-with-Python/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Init git log of an existing repository","slug":"Init-git-log-of-an-existing-repository","date":"2019-04-07T14:10:13.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/04/07/Init-git-log-of-an-existing-repository/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/07/Init-git-log-of-an-existing-repository/","excerpt":"","text":"参考 メモ 参考 Word2Vecの進化形Doc2Vecで文章と文章の類似度を算出する メモ ある文書群をクラスタリングする際、TF-IDFの代わりにWord2Vecを使っていることにした。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Word2Vec","slug":"Knowledge-Management/Machine-Learning/Word2Vec","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Word2Vec/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Word2Vec","slug":"Word2Vec","permalink":"https://dobachi.github.io/memo-blog/tags/Word2Vec/"}]},{"title":"Singularization with Python","slug":"Singularization-with-Python","date":"2019-04-06T13:53:33.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/04/06/Singularization-with-Python/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/06/Singularization-with-Python/","excerpt":"","text":"参考 メモ 参考 Inflector Inflectorする(単数形を複数形に変換したり..) メモ condaコマンドではインストールできなかったので、pipコマンドでインストールした。 使い方は、公式ウェブサイトの通り。 なお、関係ない単語にもそのまま適用してみたが動くは動くようだ。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Engineering","slug":"Knowledge-Management/Data-Engineering","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Engineering/"},{"name":"Data Transformation","slug":"Knowledge-Management/Data-Engineering/Data-Transformation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Transformation/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Data Transformation","slug":"Data-Transformation","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Transformation/"}]},{"title":"Stopword with Python","slug":"Stopword-with-Python","date":"2019-04-06T12:47:58.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/04/06/Stopword-with-Python/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/04/06/Stopword-with-Python/","excerpt":"","text":"参考 メモ 参考 Kaggleのデータからbag of wordsを作ってみた nltkでstopwordsの設定 メモ 今回はConda環境だったので、 conda コマンドでインストールした。 1$ conda install nltk ストップワードをダウンロードする。 12import nltknltk.download(&quot;stopwords&quot;) ここでは、仮に対象となる単語のリストall_wordsがあったとする。 そのとき、以下のようにリストからストップワードを取り除くと良い。 12symbol = [&quot;&apos;&quot;, &apos;&quot;&apos;, &apos;:&apos;, &apos;;&apos;, &apos;.&apos;, &apos;,&apos;, &apos;-&apos;, &apos;!&apos;, &apos;?&apos;, &quot;&apos;s&quot;]words_wo_stopwords = [w.lower() for w in all_words if not w in stop_words + symbol] ストップワードの中には記号が含まれていないので、ここでは、symbolを定義して一緒に取り除いた。 次に頻度の高い単語を30件抽出してみる。 1clean_frequency = nltk.FreqDist(words_wo_stopwords) これを可視化する。 12plt.figure(figsize=(10, 7))clean_frequency.plot(30,cumulative=True)","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Engineering","slug":"Knowledge-Management/Data-Engineering","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Engineering/"},{"name":"Data Transformation","slug":"Knowledge-Management/Data-Engineering/Data-Transformation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Transformation/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Data Transformation","slug":"Data-Transformation","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Transformation/"},{"name":"nltk","slug":"nltk","permalink":"https://dobachi.github.io/memo-blog/tags/nltk/"}]},{"title":"Hydrosphere.io","slug":"Hydrosphere-io","date":"2019-03-22T10:43:38.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/03/22/Hydrosphere-io/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/22/Hydrosphere-io/","excerpt":"","text":"Hydrosphere.ioについて 参考 メモ Hydrosphere Serving 参考情報 開発状況 hydrosphere.ioのプロダクト 概要 セットアップ方法 利用方法 コンセプト 動作確認 Hydro sonar 参考 メモ Hydro Mist 参考 メモ 動作確認 Dockerでの起動 バイナリを配備しての起動 Mistの動作確認 アーキテクチャと動作 コンテキストの管理 mist-cli Scala API エントリポイント 引数定義 Sparkのコンテキスト 結果のハンドリング 余談：implicitクラスの使用 考察：フレームワークの良し悪し HTTP API Reactie API EMRとの連係 Hydrosphere.ioについて 参考 Hydroshpereブログの最も古い記事 メモ Hydroshpereブログの最も古い記事 が投稿されたのは、2016/6/14である。 上記ブログで強調されていたのは、Big DataプロジェクトのDevOps対応。 DockerやAnsibleといった道具を使いながら。 Hydrosphere Serving 参考情報 Hydro Servingの公式GitHub Envoy proxyについての日本語記事 Hydro Servingの公式ウェブサイト Hydro ServingでTensorFlowモデルをサーブ 開発状況 2019/03/21現在でも、わりと活発に開発されているようだ。 https://github.com/Hydrospheredata/hydro-serving/graphs/commit-activity 開発元は、hydrosphere.io。パロアルトにある企業らしい。 hydrosphere.ioのプロダクト ここで取り上げているServingを含む、以下のプロダクトがある。 Serving: モデルのサーブ、アドホック分析への対応 Sonar: モデルやパイプラインの品質管理 Mist: Sparkの計算リソースをREST API経由で提供する仕組み（マルチテナンシーの実現） 概要 GitHubのREADMEに特徴が書かれていが、その中でも個人的にポイントと思ったのは以下の通り。 Envoyプロキシを用いてサービスメッシュ型のサービングを実現 複数の機械学習モデルに対応し、パイプライン化も可能 UIがある Hydro Servingの公式ウェブサイト に掲載されていた動画を見る限り、 コマンドライン経由でモデルを登録することもでき、モデルを登録したあとは、 処理フロー（ただし、シリアルなフローに見える）を定義可能。 例えば、機械学習モデルの推論器に渡す前処理・後処理も組み込めるようだ。 セットアップ方法 Docker Composeを使う方法とk8sを使う方法があるようだ。 利用方法 モデルを学習させ、出力する。（例では、h5形式で出力していた） モデルや必要なライブラリを示したテキストなどを、ペイロードとして登楼する。 必要なライブラリの指定は以下のようにする。 requirements.txt 123Keras==2.2.0tensorflow==1.8.0numpy==1.13.3 また、それらの規約事項は、contract（定義ファイル）として保存する。 フォルダ内の構成は以下のようになる。 公式ドキュメントから引用 1234567linear_regression├── model.h5├── model.py├── requirements.txt├── serving.yaml└── src └── func_main.py 上記のようなファイル群を作り、 hs uploadコマンドを使ってアップロードする。 アップロードしたあとは、ウェブフロントエンドから処理フローを定義する。 クエリはREST APIで以下のように投げる。 公式ウェブサイトから引用 12$ curl -X POST --header &apos;Content-Type: application/json&apos; --header &apos;Accept: application/json&apos; -d &apos;&#123;&quot;x&quot;: [[1, 1],[1, 1]]&#125;&apos; &apos;http://localhost/gateway/applications/linear_regression/infer&apos; また上記のREST APIの他にも、gRPCを用いて推論結果を取得することも可能。 また、データ構造としてはTensorProtoを使える。 なお、TensorFlowモデルのサーブについては、 公式ウェブサイトの Hydro ServingでTensorFlowモデルをサーブ がわかりやすい。 また、単純にPython関数を渡すこともできる。（必ずしもTensorFlowにロックインではない） コンセプト モデル Hydro Servingに渡されたモデルはバージョン管理される。 フレーワークは多数対応 ただしフレームワークによって、出力されるメタデータの情報量に差がある。 アプリケーション 単一で動かす方法とパイプラインを構成する方法がある ランタイム 予め実行環境を整えたDockerイメージが提供されている Python、TensorFlow、Spark 動作確認 Hydro Servingの公式ドキュメント に従って動作確認する。 12345$ mkdir HydroServing$ cd HydroServing/$ git clone https://github.com/Hydrospheredata/hydro-serving$ hydro-serving$ sudo docker-compose up -d 起動したコンテナを確認する。 12345678$ sudo docker-compose ps Name Command State Ports-----------------------------------------------------------------------------------------------------gateway /hydro-serving/app/start.sh Up 0.0.0.0:29090-&gt;9090/tcp, 0.0.0.0:29091-&gt;9091/tcpmanager /hydro-serving/app/start.sh Up 0.0.0.0:19091-&gt;9091/tcpmanagerui /bin/sh -c envsubst &apos;$&#123;MAN ... Up 80/tcp, 9091/tcppostgres docker-entrypoint.sh postgres Upsidecar /hydro-serving/start.sh Up 0.0.0.0:80-&gt;8080/tcp, 0.0.0.0:8082-&gt;8082/tcp その他CLIを導入する。 1234$ conda create -n hydro-serving python=3.6 python$ source activate hydro-serving$ conda install keras scikit-learn$ pip install hs エラーが生じた。 12345678910111213141516171819202122232425262728293031323334353637$ hs cluster add --name local --server http://localhost--- Logging error ---Traceback (most recent call last): File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/logging/__init__.py&quot;, line 992, in emit msg = self.format(record) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/logging/__init__.py&quot;, line 838, in format return fmt.format(record) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/logging/__init__.py&quot;, line 575, in format record.message = record.getMessage() File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/logging/__init__.py&quot;, line 338, in getMessage msg = msg % self.argsTypeError: not all arguments converted during string formattingCall stack: File &quot;/home/centos/.conda/envs/hydro-serving/bin/hs&quot;, line 11, in &lt;module&gt; sys.exit(hs_cli()) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/click/core.py&quot;, line 722, in __call__ return self.main(*args, **kwargs) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/click/core.py&quot;, line 697, in main rv = self.invoke(ctx) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/click/core.py&quot;, line 1063, in invoke Command.invoke(self, ctx) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/click/core.py&quot;, line 895, in invoke return ctx.invoke(self.callback, **ctx.params) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/click/core.py&quot;, line 535, in invoke return callback(*args, **kwargs) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/click/decorators.py&quot;, line 17, in new_func return f(get_current_context(), *args, **kwargs) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/hydroserving/cli/hs.py&quot;, line 18, in hs_cli ctx.obj.services = ContextServices.with_config_path(HOME_PATH_EXPANDED) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/hydroserving/models/context_object.py&quot;, line 44, in with_config_path config_service = ConfigService(path) File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/hydroserving/services/config.py&quot;, line 17, in __init__ logging.error(&quot;&#123;&#125; is not an existing directory&quot;, home_path)Message: &apos;&#123;&#125; is not an existing directory&apos;Arguments: (&apos;/home/centos/.hs-home&apos;,)WARNING:root:Using local as current clusterCluster &apos;local&apos; @ http://localhost added successfully エラーが出ているのに登録されたように見える。 念の為、クラスタ情報を確認する。 12$ hs clusterCurrent cluster: &#123;&apos;cluster&apos;: &#123;&apos;server&apos;: &apos;http://localhost&apos;&#125;, &apos;name&apos;: &apos;local&apos;&#125; いったんこのまま進める。 まずはアプリを作成。 12345678910111213141516171819202122232425262728293031$ mkdir -p ~/Sources/linear_regression$ cd ~/Sources/linear_regression$ cat &lt;&lt; EOF &gt; model.py from keras.models import Sequential from keras.layers import Dense from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler # initialize data n_samples = 1000 X, y = make_regression(n_samples=n_samples, n_features=2, noise=0.5, random_state=112) scallar_x, scallar_y = MinMaxScaler(), MinMaxScaler() scallar_x.fit(X) scallar_y.fit(y.reshape(n_samples, 1)) X = scallar_x.transform(X) y = scallar_y.transform(y.reshape(n_samples, 1)) # create a model model = Sequential() model.add(Dense(4, input_dim=2, activation=&apos;relu&apos;)) model.add(Dense(4, activation=&apos;relu&apos;)) model.add(Dense(1, activation=&apos;linear&apos;)) model.compile(loss=&apos;mse&apos;, optimizer=&apos;adam&apos;) model.fit(X, y, epochs=100) # save model model.save(&apos;model.h5&apos;) EOF 学習し、モデルを出力。 1$ python model.py サーブ対象となる関数のアプリを作成する。 12345678910111213141516171819202122232425262728$ mkdir src$ cd src$ cat &lt;&lt; EOF &gt; func_main.py import numpy as np import hydro_serving_grpc as hs from keras.models import load_model # 0. Load model once model = load_model(&apos;/model/files/model.h5&apos;) def infer(x): # 1. Retrieve tensor&apos;s content and put it to numpy array data = np.array(x.double_val) data = data.reshape([dim.size for dim in x.tensor_shape.dim]) # 2. Make a prediction result = model.predict(data) # 3. Pack the answer y_shape = hs.TensorShapeProto(dim=[hs.TensorShapeProto.Dim(size=-1)]) y_tensor = hs.TensorProto( dtype=hs.DT_DOUBLE, double_val=result.flatten(), tensor_shape=y_shape) # 4. Return the result return hs.PredictResponse(outputs=&#123;&quot;y&quot;: y_tensor&#125;) EOF 1234567891011121314151617181920212223$ cd ..$ cat &lt;&lt; EOF &gt; serving.yaml kind: Model name: linear_regression model-type: python:3.6 payload: - &quot;src/&quot; - &quot;requirements.txt&quot; - &quot;model.h5&quot; contract: infer: # Signature function inputs: x: # Input field shape: [-1, 2] type: double profile: numerical outputs: y: # Output field shape: [-1] type: double profile: numerical EOF つづいて必要ライブラリを定義する。 12345$ cat &lt;&lt; EOF &gt; requirements.txt Keras==2.2.0 tensorflow==1.8.0 numpy==1.13.3 EOF 最終的に以下のような構成になった。 12345678910$ tree.├── model.h5├── model.py├── requirements.txt├── serving.yaml└── src └── func_main.py1 directory, 5 files つづいてモデルなどをアップロード。 1$ hs upload ここで以下のようなエラーが生じた。 12345678910111213141516Using &apos;local&apos; cluster[&apos;/home/centos/Sources/linear_regression/src&apos;, &apos;/home/centos/Sources/linear_regression/requirements.txt&apos;, &apos;/home/centos/Sources/linear_regression/model.h5&apos;]Packing the model [####################################] 100%Assembling the model [####################################] 100%Uploading to http://localhostUploading model assembly [####################################] 100%Traceback (most recent call last): File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/hydroserving/httpclient/remote_connection.py&quot;, line 82, in postprocess_response response.raise_for_status() File &quot;/home/centos/.conda/envs/hydro-serving/lib/python3.6/site-packages/requests/models.py&quot;, line 940, in raise_for_status raise HTTPError(http_error_msg, response=self)requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://localhost/api/v1/model/uploadDuring handling of the above exception, another exception occurred:(snip) 軽く調べると ISSUE255 が 関係していそうである。 ここでは動作確認のため、pip install hs==2.0.0rc2としてインストールして試した。 12$ pip uninstall hs$ pip install hs==2.0.0rc2 また、 serving.yaml に以下のエントリを追加した。 1runtime: &quot;hydrosphere/serving-runtime-python:3.6-latest&quot; 再び hs upload したところ完了のように見えた。 ウェブUIを確認したところ以下の通り。 ウェブUIに登録されたモデルが表示される それではテストを実行してみる。 1$ curl -X POST --header &apos;Content-Type: application/json&apos; --header &apos;Accept: application/json&apos; -d &apos;&#123; &quot;x&quot;: [ [ 1, 1 ] ] &#125;&apos; &apos;http://10.0.0.209/gateway/application/linear_regression&apos; 以下のようなエラーが生じた。 1&#123;&quot;error&quot;:&quot;InternalUncaught&quot;,&quot;information&quot;:&quot;UNKNOWN: Exception calling application: No module named &apos;numpy&apos;&quot;&#125; ランタイム上にnumpyがインストールされなかったのだろうか・・・ WIP Hydro sonar 参考 Hydro Sonarの公式ウェブサイト [公式のGUI画面] メモ 公式のSonarのGUI画面 を見ると、モデルの性能を観測するための仕組みに見える。 モデル選択、メンテナンス、リアイアメントのために用いられる。 例えば、入力データ変化を起因したモデル精度の劣化など。 Hydro Mist 参考 Hydro Mistの公式ウェブサイト 公式のMistのアーキテクチャイメージ Hydro Mistの公式ドキュメント Hydro Mistの公式クイックスタート Hydro Mistがどうやってジョブを起動するか Hydro Mistのジョブのステート Hydro Mistのジョブローンチの流れ Hydro Mistのコンテキスト管理 Hydro MIstのmist-cliのGitHub Hydro Mistのコンフィグの例 Hydro MistのScala API メモ 公式のMistのアーキテクチャイメージ を見ると、 Sparkのマルチテナンシーを実現するものに見える。 特徴として挙げられていたものの中で興味深いのは以下。 Spark Function as a Service ユーザのAPIをSparkのコンフィグレーションから分離する HTTP、Kafka、MQTTによるやりとり Hydro Mistの公式クイックスタートを見ると、Scala、Java、Pythonのサンプルアプリが掲載されている。 Mistはライブラリとしてインポートし、フレームワークに則ってアプリを作成すると、 アプリ内で定義された関数を実行するジョブをローンチできるようになる。 また実際にローンチするときには、REST API等で起動することになるが、 そのときに引数を渡すこともできる。 動作確認 Hydro Mistの公式クイックスタート に従い動作を確認する。 Dockerで実行するか、バイナリで実行するかすれば良い。 Dockerでの起動 Dockerの場合は以下の通り。 123$ sudo docker run -p 2004:2004 \\ -v /var/run/docker.sock:/var/run/docker.sock \\ hydrosphere/mist:1.1.1-2.3.0 バイナリを配備しての起動 バイナリをダウンロードして実行する場合は以下の通り。 まずSparkをダウンロードする。（なお、JDKは予めインストールされていることを前提とする） 12345$ mkdir ~/Spark$ cd ~/Spark$ wget http://ftp.riken.jp/net/apache/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz$ tar xvzf spark-2.3.3-bin-hadoop2.7.tgz$ ln -s spark-2.3.3-bin-hadoop2.7 default これで、~/Spark/default以下にSparkが配備された。 つづいて、Hydro Mistのバイナリをダウンロードし、配備する。 123456$ mkdir ~/HydroMist$ cd ~/HydroMist$ wget http://repo.hydrosphere.io/hydrosphere/static/mist-1.1.1.tar.gz$ tar xvfz mist-1.1.1.tar.gz$ ln -s mist-1.1.1 default$ cd default 以上でHydro Mistが配備された。 それではHydro Mistのマスタを起動する。 1$ SPARK_HOME=$&#123;HOME&#125;/Spark/default ./bin/mist-master start --debug true 以上で、バイナリを配備したHydro Mistが起動する。 Mistの動作確認 以降、サンプルアプリを実行している。 まずmistのCLIを導入する。 123$ conda create -n mist python=3.6 python$ source activate mist$ pip install mist-cli 続いて、サンプルプロジェクトをcloneする。 123$ mkdir -p ~/Sources$ cd ~/Sources$ git clone https://github.com/Hydrospheredata/hello_mist.git 試しにScala版を動かす。 （なお、SBTがインストールされていることを前提とする） 123$ cd hello_mist/scala$ sbt package$ mist-cli apply -f conf -u &apos;&apos; 上記コマンドの結果、以下のような出力が得られる。 123456789101112131415161718192021222324252627282930313233343536373839Process 5 file entriesupdating Artifact hello-mist-scala_0.0.1.jarSuccess: Artifact hello-mist-scala_0.0.1.jarupdating Context emr_ctxSuccess: Context emr_ctxupdating Context emr_autoscale_ctxSuccess: Context emr_autoscale_ctxupdating Context standaloneSuccess: Context standaloneupdating Function hello-mist-scalaSuccess: Function hello-mist-scalaGet context info--------------------------------------------------------------------------------curl -H &apos;Content-Type: application/json&apos; -X POST http://localhost:2004/v2/api/contexts/emr_ctxGet context info--------------------------------------------------------------------------------curl -H &apos;Content-Type: application/json&apos; -X POST http://localhost:2004/v2/api/contexts/emr_autoscale_ctxGet context info--------------------------------------------------------------------------------curl -H &apos;Content-Type: application/json&apos; -X POST http://localhost:2004/v2/api/contexts/standaloneGet info of function resource--------------------------------------------------------------------------------curl -H &apos;Content-Type: application/json&apos; -X GET http://localhost:2004/v2/api/functions/hello-mist-scalaStart job via mist-cli--------------------------------------------------------------------------------mist-cli --host localhost --port 2004 start job hello-mist-scala &apos;&#123;&quot;samples&quot;: 7&#125;&apos;Start job via curl--------------------------------------------------------------------------------curl --data &apos;&#123;&quot;samples&quot;: 7&#125;&apos; -H &apos;Content-Type: application/json&apos; -X POST http://localhost:2004/v2/api/functions/hello-mist-scala/jobs?force=true 試しにstandaloneのコンテキストを確認してみる。 （なお、上記メッセージではPOSTを使うよう書かれているが、GETでないとエラーになった） 123456789101112131415161718$ curl -H &apos;Content-Type: application/json&apos; -X GET http://localhost:2004/v2/api/contexts/standalone | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 252 100 252 0 0 26537 0 --:--:-- --:--:-- --:--:-- 28000&#123; &quot;name&quot;: &quot;standalone&quot;, &quot;maxJobs&quot;: 1, &quot;maxConnFailures&quot;: 1, &quot;workerMode&quot;: &quot;exclusive&quot;, &quot;precreated&quot;: false, &quot;sparkConf&quot;: &#123; &quot;spark.submit.deployMode&quot;: &quot;cluster&quot;, &quot;spark.master&quot;: &quot;spark://holy-excalibur:7077&quot; &#125;, &quot;runOptions&quot;: &quot;&quot;, &quot;downtime&quot;: &quot;1200s&quot;, &quot;streamingDuration&quot;: &quot;1s&quot;&#125; なお、サンプルアプリは以下の通り。 Piの値を簡易的に計算するものである。 12345678910111213141516171819202122232425262728import mist.api._import mist.api.dsl._import mist.api.encoding.defaults._import org.apache.spark.SparkContextobject HelloMist extends MistFn with Logging &#123; override def handle = &#123; withArgs( arg[Int](&quot;samples&quot;, 10000) ) .withMistExtras .onSparkContext((n: Int, extras: MistExtras, sc: SparkContext) =&gt; &#123; import extras._ logger.info(s&quot;Hello Mist started with samples: $n&quot;) val count = sc.parallelize(1 to n).filter(_ =&gt; &#123; val x = math.random val y = math.random x * x + y * y &lt; 1 &#125;).count() val pi = (4.0 * count) / n pi &#125;).asHandle &#125;&#125; 結果として、以下のようにジョブが登録される。 ウェブUIに登録されたアプリ 登録されたジョブを実行する。 ジョブを走らせる また上記で表示されていたcurl ...をコマンドで実行することでも、ジョブを走らせることができる。 ジョブの一覧は、ウェブUIから以下のように確認できる。 走らせたジョブ一覧 ジョブ一覧からジョブを選ぶと、そのメタデータ、渡されたパラメータ、結果などを確認できる。 さらにジョブ実行時のログもウェブUIから確認できる。 ジョブ実行時のログ Sparkのドライバログと思われるものも含まれている。 アーキテクチャと動作 Hydro Mistがどうやってジョブを起動するか によると以下の通り。 Mistで実行する関数は、mist-workerにラップされており、当該ワーカがSparkContextを保持しているようだ。 またワーカを通じてジョブを実行する。 （参考：Hydro Mistのジョブのステート ） ユーザがジョブを実行させようとしたとき、Mistのマスタはそのリクエストをキューに入れ、ワーカが空くのを待つ。 ワーカのモードには2種類がある。 exclusive ジョブごとにワーカを立ち上げる shared ひとつのジョブが完了してもワーカを立ち上げたままにし再利用する。 これにより、ジョブの並列度、ジョブの耐障害性の面で有利になる。 また、 Hydro Mistのジョブローンチの流れ からジョブ登録から実行までの大まかな流れがわかる。 コンテキストの管理 Sparkのコンテキスト管理やワーカのモード設定は、 mist-cliを通じてできるようだ。 Hydro Mistのコンテキスト管理 参照。 mist-cli Hydro MIstのmist-cliのGitHub 参照。 mist-cli は、コンフィグファイル（やコンフィグファイルが配備されたディレクトリ）を指定しながら実行する。 ディレクトリにコンフィグを置く場合は、ファイル名の先頭に2桁の数字を入れることで、 プライオリティを指定することができる。 コンフィグには、Artifact、Context、Functionの設定を記載する。 参考：Hydro Mistのコンフィグの例 Hydro Mistの公式クイックスタート を実行したあとの状態で確認してみる。 123456789101112$ mist-cli list contexts ID WORKER MODEdefault exclusiveemr_ctx exclusiveemr_autoscale_ctx exclusivestandalone exclusive$ mist-cli list functions FUNCTION DEFAULT CONTEXT PATH CLASS NAMEhello-mist-scala default hello-mist-scala_0.0.1.jar HelloMist$$ mist-cli statusMist version: 1.1.1Spark version: 2.4.0Java version: 1.8.0_201-b09 いくつかのコンテキストと、関数が登録されていることがわかる。 なお、GitHub上のバイナリを用いたので使用するSparkのバージョンが2.4.0になっていた。 Scala API Hydro MistのScala API とサンプルアプリ（mist\\examples\\examples\\src\\main\\scala\\PiExample.scala）を確認してみる。 エントリポイント MistFnがエントリポイントのようだ。 PiExample.scala:6 123456object PiExample extends MistFn &#123; override def handle: Handle = &#123; val samples = arg[Int](&quot;samples&quot;).validated(_ &gt; 0, &quot;Samples should be positive&quot;)(snip) 引数定義 また関数の引数設定はmist.api.ArgsInstances#arg[A]などで行う。 下記にサンプルに記載の例を示す。 PiExample.scala:9 1val samples = arg[Int](&quot;samples&quot;).validated(_ &gt; 0, &quot;Samples should be positive&quot;) [Int]により引数として渡される値の型を指定する。argメソッドの戻り値はNamedUserArgクラスだが、 当該クラスはUserArg[A]トレートを拡張している。 UserArg#validatedメソッドを用いることで、値の検証を行う。 複数の引数を渡すには、withArgsを使うか、combineや&amp;を使う。 例： 123val three = withArgs(arg[Int](&quot;n&quot;), arg[String](&quot;str&quot;), arg[Boolean](&quot;flag&quot;))val three = arg[Int](&quot;n&quot;) &amp; arg[String](&quot;str&quot;) &amp; arg[Boolean](&quot;flag&quot;) またドキュメントには、case classと各種Extractorを用いることで、JSON形式の入力データを 扱う方法が説明されていた。 Sparkのコンテキスト 引数を定義したのちは、Mistのコンテキスト管理のAPIを利用し、 SparkSessionなどを取得する。 onSparkContextやonSparkSessionなどを利用可能。 mist/api/MistFnSyntax.scala:91 12345def onSparkSession[F, Cmb, Out](f: F)( implicit cmb: ArgCombiner.Aux[A, SparkSession, Cmb], fnT: FnForTuple.Aux[Cmb, F, Out]): RawHandle[Out] = args.combine(sparkSessionArg).apply(f) サンプルでは以下のような使い方を示している。 PiExample.scala:10 1234 withArgs(samples).onSparkContext((n: Int, sc: SparkContext) =&gt; &#123; val count = sc.parallelize(1 to n).filter(_ =&gt; &#123;(snip) もしSparkSessionを使うならば以下の通りか。 12345678import org.apache.spark.sql.SparkSession(snip) withArgs(samples).onSparkSession((n: Int, spark: SparkSession) =&gt; &#123; val count = spark.sparkContext.parallelize(1 to n).filter(_ =&gt; &#123;(snip) なお、onStreamingContextというのもあり、Spark Streamingも実行可能のようだ。 結果のハンドリング 上記onSparkContextメソッドなどの戻り値の型はRawHandle[Out]である。 mist/api/MistFnSyntax.scala:58 12345def onSparkContext[F, Cmb, Out](f: F)( implicit cmb: ArgCombiner.Aux[A, SparkContext, Cmb], fnT: FnForTuple.Aux[Cmb, F, Out]): RawHandle[Out] = args.combine(sparkContextArg).apply(f) 最終的に結果をJSON形式で返すために、asHandleメソッドを利用する。 PiExample.scala:10 12345withArgs(samples).onSparkContext((n: Int, sc: SparkContext) =&gt; &#123;(snip)&#125;).asHandle asHandleメソッドは以下の通り。 mist/api/MistFnSyntax.scala:48 123implicit class AsHandleOps[A](raw: RawHandle[A]) &#123; def asHandle(implicit enc: JsEncoder[A]): Handle = raw.toHandle(enc)&#125; 余談：implicitクラスの使用 mist.api以下で複数のクラスがimplicit定義されて利用されており、一見して解析しづらい印象を覚えた。 例えばonSparkContextやonSparkSessionがimplicitクラスContextsOpsクラスに定義されている。 考察：フレームワークの良し悪し Sparkの単純なプロキシではなく、フレームワーク（ライブラリ）化されていることで、 ユーザはSparkのコンテキストの管理から開放される、という利点がある。 一方で、Hydro Mistのフレームワークに従って実装する必要があり、多少なり Sparkに加えて、Hydro Mistの学習コストがある Mistのフレームワークでは実装しづらいケースが存在するかもしれない トラブルシュートの際に、Mistの実装まで含めて確認が必要になるかもしれない という懸念点・欠点が挙げられる。 HTTP API Hydro MistのHTTP API に一覧が載っている。 例えば関数一覧を取得する。 123456789101112131415161718192021222324$ curl -X GET &apos;http://10.0.0.209:2004/v2/api/functions&apos; | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 208 100 208 0 0 13256 0 --:--:-- --:--:-- --:--:-- 13866[ &#123; &quot;name&quot;: &quot;hello-mist-scala&quot;, &quot;execute&quot;: &#123; &quot;samples&quot;: &#123; &quot;type&quot;: &quot;MOption&quot;, &quot;args&quot;: [ &#123; &quot;type&quot;: &quot;MInt&quot; &#125; ] &#125; &#125;, &quot;path&quot;: &quot;hello-mist-scala_0.0.1.jar&quot;, &quot;tags&quot;: [], &quot;className&quot;: &quot;HelloMist$&quot;, &quot;defaultContext&quot;: &quot;default&quot;, &quot;lang&quot;: &quot;scala&quot; &#125;] 続いて、当該関数についてジョブ一覧を取得する。 123456789101112131415161718192021222324252627$ curl -X GET &apos;http://10.0.0.209:2004/v2/api/functions/hello-mist-scala/jobs&apos; | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 1682 100 1682 0 0 29484 0 --:--:-- --:--:-- --:--:-- 30035[ &#123; &quot;source&quot;: &quot;Http&quot;, &quot;startTime&quot;: 1553358512167, &quot;createTime&quot;: 1553358507406, &quot;context&quot;: &quot;default&quot;, &quot;params&quot;: &#123; &quot;filePath&quot;: &quot;hello-mist-scala_0.0.1.jar&quot;, &quot;className&quot;: &quot;HelloMist$&quot;, &quot;arguments&quot;: &#123; &quot;samples&quot;: 9 &#125;, &quot;action&quot;: &quot;execute&quot; &#125;, &quot;endTime&quot;: 1553358513162, &quot;jobResult&quot;: 2.2222222222222223, &quot;status&quot;: &quot;finished&quot;, &quot;function&quot;: &quot;hello-mist-scala&quot;, &quot;jobId&quot;: &quot;b044d08f-9554-4be9-8e22-1d687e58c52e&quot;, &quot;workerId&quot;: &quot;default_1cb3b66d-99c3-400b-ac3a-f11d72ab8124_4&quot; &#125;,(snip) 上記のように、ジョブのメタデータと結果が取得される。 ジョブの情報であれば、直接jobs APIを用いても取得可能。 1$ curl -X GET &apos;http://10.0.0.209:2004/v2/api/jobs&apos; | jq またジョブのログを出力できる。 1234567891011121314151617181920212223$ curl -X GET &apos;http://10.0.0.209:2004/v2/api/jobs/a87197c8-1692-48bc-b151-978ea89b058a/logs&apos; | head -n 20 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0INFO 2019-03-23T16:22:44.178 [a87197c8-1692-48bc-b151-978ea89b058a] Waiting worker connectionINFO 2019-03-23T16:22:44.183 [a87197c8-1692-48bc-b151-978ea89b058a] InitializedEvent(externalId=None)INFO 2019-03-23T16:22:44.183 [a87197c8-1692-48bc-b151-978ea89b058a] QueuedEventINFO 2019-03-23T16:22:47.991 [a87197c8-1692-48bc-b151-978ea89b058a] WorkerAssigned(workerId=default_1cb3b66d-99c3-400b-ac3a-f11d72ab8124_2)INFO 2019-03-23T16:22:48.027 [a87197c8-1692-48bc-b151-978ea89b058a] JobFileDownloadingEventINFO 2019-03-23T16:22:48.885 [a87197c8-1692-48bc-b151-978ea89b058a] StartedEventINFO 2019-03-23T16:22:48.882 [a87197c8-1692-48bc-b151-978ea89b058a] Added JAR /home/centos/HydroMist/default/worker-default_1cb3b66d-99c3-400b-ac3a-f11d72ab8124_2/hello-mist-scala_0.0.1.jar at spark://dev:46260/jars/hello-mist-scala_0.0.1.jar with timestamp 1553358168882INFO 2019-03-23T16:22:48.965 [a87197c8-1692-48bc-b151-978ea89b058a] Hello Mist started with samples: 8INFO 2019-03-23T16:22:49.204 [a87197c8-1692-48bc-b151-978ea89b058a] Starting job: count at HelloMist.scala:22INFO 2019-03-23T16:22:49.218 [a87197c8-1692-48bc-b151-978ea89b058a] Got job 0 (count at HelloMist.scala:22) with 16 output partitionsINFO 2019-03-23T16:22:49.219 [a87197c8-1692-48bc-b151-978ea89b058a] Final stage: ResultStage 0 (count at HelloMist.scala:22)INFO 2019-03-23T16:22:49.22 [a87197c8-1692-48bc-b151-978ea89b058a] Parents of final stage: List()INFO 2019-03-23T16:22:49.221 [a87197c8-1692-48bc-b151-978ea89b058a] Missing parents: List()INFO 2019-03-23T16:22:49.229 [a87197c8-1692-48bc-b151-978ea89b058a] Submitting ResultStage 0 (MapPartitionsRDD[1] at filter at HelloMist.scala:18), which hasno missing parentsINFO 2019-03-23T16:22:49.438 [a87197c8-1692-48bc-b151-978ea89b058a] Block broadcast_0 stored as values in memory (estimated size 1808.0 B, free 366.3 MB)INFO 2019-03-23T16:22:49.471 [a87197c8-1692-48bc-b151-978ea89b058a] Block broadcast_0_piece0 stored as bytes in memory (estimated size 1232.0 B, free 366.3 MB)INFO 2019-03-23T16:22:49.473 [a87197c8-1692-48bc-b151-978ea89b058a] Added broadcast_0_piece0 in memory on dev:39976 (size: 1232.0 B, free: 366.3 MB)INFO 2019-03-23T16:22:49.476 [a87197c8-1692-48bc-b151-978ea89b058a] Created broadcast 0 from broadcast at DAGScheduler.scala:103910INFO 2019-03-23T16:22:49.494 [a87197c8-1692-48bc-b151-978ea89b058a] Submitting 16 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at filter at HelloMist.scala:18) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))0INFO 2019-03-23T16:22:49.495 [a87197c8-1692-48bc-b151-978ea89b058a] Adding task set 0.0 with 16 tasks(snip) Reactie API Hydro MistのReactive API を見ると、MQTTやKafkaと連携して動くAPIがあるようだが、 まだドキュメントが成熟していない。 デフォルトでは無効になっている。 EMRとの連係 Hydro MistのEMR連係 を眺めるとhello_mistプロジェクト等でEMRとの連係の仕方を示してくれているようだが、まだ情報が足りない。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Model Management","slug":"Model-Management","permalink":"https://dobachi.github.io/memo-blog/tags/Model-Management/"}]},{"title":"Corne Chocolate","slug":"Corne-Chocolate","date":"2019-03-20T14:29:23.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/03/20/Corne-Chocolate/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/20/Corne-Chocolate/","excerpt":"","text":"参考 メモ ビルドについて キーマップ OLED表示 参考 Corne Chocolateのビルドガイド メモ ビルドについて 基本的には、 Corne Chocolateのビルドガイド の通りで問題なかった。 キーマップ もともと使用していたLet's SplitやHelixとは、デフォルトのキー配置が異なる。 イメージとしては、 Raiseレイヤに記号系が集まっている Lowerレイヤに数字やファンクションキーが集まっている 矢印キーがない？ Ctrl、ESC、Tabあたりは好みでカスタマイズ必要そう。 親指エンターも好みが分かれそう という感じ。 個人的な感想として、記号をRaiseレイヤに集めるのは良いと思った。 一方、矢印キー、Ctrlなど、エンターあたりを中心にいじった。 dobachiのキーマップ を参照されたし。 OLED表示 押されたキー1個までならまだしも、入力のログがしばらく表示されるのは気になったので、 keymap.cを以下のように修正して表示しないようにした。 12345678910111213diff --git a/keyboards/crkbd/keymaps/dobachi/keymap.c b/keyboards/crkbd/keymaps/dobachi/keymap.cindex f80eff9..44a9137 100644--- a/keyboards/crkbd/keymaps/dobachi/keymap.c+++ b/keyboards/crkbd/keymaps/dobachi/keymap.c@@ -158,7 +158,7 @@ void matrix_render_user(struct CharacterMatrix *matrix) &#123; // If you want to change the display of OLED, you need to change here matrix_write_ln(matrix, read_layer_state()); matrix_write_ln(matrix, read_keylog());- matrix_write_ln(matrix, read_keylogs());+ //matrix_write_ln(matrix, read_keylogs()); //matrix_write_ln(matrix, read_mode_icon(keymap_config.swap_lalt_lgui)); //matrix_write_ln(matrix, read_host_led_state()); //matrix_write_ln(matrix, read_timelog());","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Keyboard","slug":"Knowledge-Management/Keyboard","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Keyboard/"},{"name":"Corne Chocolate","slug":"Knowledge-Management/Keyboard/Corne-Chocolate","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Keyboard/Corne-Chocolate/"}],"tags":[{"name":"Corne Chocolate","slug":"Corne-Chocolate","permalink":"https://dobachi.github.io/memo-blog/tags/Corne-Chocolate/"},{"name":"Keyboard","slug":"Keyboard","permalink":"https://dobachi.github.io/memo-blog/tags/Keyboard/"}]},{"title":"Kafka Summit SF 2019","slug":"Kafka-Summit-SF-2019","date":"2019-03-16T12:51:33.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/03/16/Kafka-Summit-SF-2019/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/16/Kafka-Summit-SF-2019/","excerpt":"","text":"参考 メモ 参考 スピーカ募集ページ メモ 123Call for Papers opens: February 19, 2019Call for Papers closes: April 15, 2019","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Kafka","slug":"Clipping/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"Kafka Summit","slug":"Kafka-Summit","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka-Summit/"}]},{"title":"Kafkaのグレースフルシャットダウン","slug":"Graceful-Shutdown-of-Kafka","date":"2019-03-13T13:29:42.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/03/13/Graceful-Shutdown-of-Kafka/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/13/Graceful-Shutdown-of-Kafka/","excerpt":"","text":"参考 メモ 公式ドキュメント 実装確認 参考 Graceful Shutdownに関する公式ドキュメントの記述 メモ 公式ドキュメント Graceful Shutdownに関する公式ドキュメントの記述によると、グレースフルシャットダウンの強みは以下の通り。 ログリカバリ処理をスキップする シャットダウンするブローカから予めリーダを移動する 実装確認 以下のあたりから、グレースフルシャットダウンの実装。 kafka/server/KafkaServer.scala:422 12345 private def controlledShutdown() &#123; def node(broker: Broker): Node = broker.node(config.interBrokerListenerName)(snip) コントローラを取得して接続し、コントローラに対してグレースフルシャットダウンの依頼を投げる。 なお、本メソッドはKafkaServer#shutdownメソッド内で呼び出される。 なお、当該メソッドは（KafkaServerStartableクラスでラップされているが）シャットダウンフックで適用される。 kafka/Kafka.scala:70 1234// attach shutdown handler to catch terminating signals as well as normal terminationRuntime.getRuntime().addShutdownHook(new Thread(&quot;kafka-shutdown-hook&quot;) &#123; override def run(): Unit = kafkaServerStartable.shutdown()&#125;)","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"Graceful Shutdown","slug":"Graceful-Shutdown","permalink":"https://dobachi.github.io/memo-blog/tags/Graceful-Shutdown/"}]},{"title":"KafkaConnectでTwitterデータを取り込む","slug":"KafkaConnectTwitter","date":"2019-03-08T13:37:11.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/03/08/KafkaConnectTwitter/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/08/KafkaConnectTwitter/","excerpt":"","text":"参考 メモ 参考 jcustenborder/kafka-connect-twitter メモ ここでは、 confluent-hub コマンドでインストールする。 1$ confluent-hub install jcustenborder/kafka-connect-twitter 以下の設定ファイルを作る。 /etc/kafka/connect-twitter-source.properties 12345678910111213name=connector1tasks.max=1connector.class=com.github.jcustenborder.kafka.connect.twitter.TwitterSourceConnector# Set these required valuestwitter.oauth.accessTokenSecret=hogeprocess.deletes=falsefilter.keywords=kafkakafka.status.topic=twitter-statuskafka.delete.topic=twitter-deletetwitter.oauth.consumerSecret=hogetwitter.oauth.accessToken=hogetwitter.oauth.consumerKey=hoge キーのところは、適宜TwitterのDeveloper向けページで生成して記載すること。 スタンドアローンモードで実行する。 1$ connect-standalone /etc/kafka/connect-standalone.properties /etc/kafka/connect-twitter-source.properties なお、もし分散モードだったら、以下のようにする。 1234567891011121314151617curl -H &quot;Content-Type: application/json&quot; -X POST http://localhost:8083/connectors -d &apos;&#123; &quot;name&quot;: &quot;twitter&quot;, &quot;config&quot;: &#123; &quot;tasks.max&quot;:1, &quot;connector.class&quot;:&quot;com.github.jcustenborder.kafka.connect.twitter.TwitterSourceConnector&quot;, &quot;twitter.oauth.accessTokenSecret&quot;:&quot;hoge&quot;, &quot;process.deletes&quot;:&quot;false&quot;, &quot;filter.keywords&quot;:&quot;kafka&quot;, &quot;kafka.status.topic&quot;:&quot;twitter-status&quot;, &quot;kafka.delete.topic&quot;:&quot;twitter-delete&quot;, &quot;twitter.oauth.consumerSecret&quot;:&quot;hoge&quot;, &quot;twitter.oauth.accessToken&quot;:&quot;hoge&quot;, &quot;twitter.oauth.consumerKey&quot;:&quot;hoge&quot;, &#125;&#125;&apos; 最後に、入力されるメッセージを確認する。 1$ kafka-console-consumer --bootstrap-server broker:9092 --topic twitter-status | jq . 結果は以下のような形式である。 1234567891011121314151617181920212223&#123; &quot;schema&quot;: &#123; &quot;type&quot;: &quot;struct&quot;, &quot;fields&quot;: [ &#123; &quot;type&quot;: &quot;int64&quot;, &quot;optional&quot;: true, &quot;name&quot;: &quot;org.apache.kafka.connect.data.Timestamp&quot;, &quot;version&quot;: 1, &quot;doc&quot;: &quot;Return the created_at&quot;, &quot;field&quot;: &quot;CreatedAt&quot; &#125;,(snip) &quot;payload&quot;: &#123; &quot;CreatedAt&quot;: XXXXXXXXXXXXX, &quot;Id&quot;: XXXXXXXXXXXXXXXXXXX, &quot;Text&quot;: &quot;hoge&quot;, &quot;Source&quot;: &quot;&lt;a href=\\&quot;http://twitter.com/download/android\\&quot; rel=\\&quot;nofollow\\&quot;&gt;Twitter for Android&lt;/a&gt;&quot;, &quot;Truncated&quot;: false,(snip)","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://dobachi.github.io/memo-blog/tags/ZooKeeper/"}]},{"title":"Kafkaのログ周りの調査メモ","slug":"Log-of-Kafka","date":"2019-03-08T04:32:23.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/03/08/Log-of-Kafka/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/08/Log-of-Kafka/","excerpt":"","text":"参考 メモ 参考 Apache Kafka Core Internals: A Deep Dive How Kafka’s Storage Internals Work Confluent Platform Quick Start (Docker) kafka-connect-twitter メモ How Kafka’s Storage Internals Work にログファイルの中身の読み方が載っているので試してみる。 予め、 kafka-connect-twitter のKafka Connectコネクタを用い、Twitterデータを投入しておいた。 トピック（パーティション）の確認 12$ ls /var/lib/kafka/data/twitter-status-000000000000000000000.index 00000000000000000000.log 00000000000000000000.timeindex leader-epoch-checkpoint インデックスの確認。 123456789$ kafka-run-class kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /var/lib/kafka/data/twitter-status-0/00000000000000000000.index | headDumping /var/lib/kafka/data/twitter-status-0/00000000000000000000.indexoffset: 1 position: 16674offset: 2 position: 33398offset: 3 position: 50562offset: 4 position: 67801(snip) 上記の通り、オフセットと位置が記載されている。 続いて、ログ本体の確認。 1234567891011$ kafka-run-class kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /var/lib/kafka/data/twitter-status-0/00000000000000000000.log | head -n 4Dumping /var/lib/kafka/data/twitter-status-0/00000000000000000000.logStarting offset: 0offset: 0 position: 0 CreateTime: 1552055018454 isvalid: true keysize: 239 valuesize: 16362 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: &#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;Id&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;com.github.jcustenborder.kafka.connect.twitter.StatusKey&quot;,&quot;doc&quot;:&quot;Key for a twitter status.&quot;(snip)offset: 1 position: 16674 CreateTime: 1552055018465 isvalid: true keysize: 239 valuesize: 16412 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: &#123;&quot;schema&quot;:&#123;&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[&#123;&quot;type&quot;:&quot;int64&quot;,&quot;optional&quot;:true,&quot;field&quot;:&quot;Id&quot;&#125;],&quot;optional&quot;:false,&quot;name&quot;:&quot;com.github.jcustenborder.kafka.connect.twitter.StatusKey&quot;,&quot;doc&quot;:&quot;Key for a twitter status.&quot;&#125;,&quot;payload&quot;:&#123;&quot;Id&quot;:1104024860143968257&#125;(snip) 12345$ kafka-run-class kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /var/lib/kafka/data/twitter-status-0/00000000000000000000.timeindex | head -n 10Dumping /var/lib/kafka/data/twitter-status-0/00000000000000000000.timeindextimestamp: 1552055018465 offset: 1timestamp: 1552055529166 offset: 2timestamp: 1552055536284 offset: 3timestamp: 1552055626862 offset: 4timestamp: 1552055652086 offset: 5timestamp: 1552055717443 offset: 6timestamp: 1552055788403 offset: 7timestamp: 1552055789505 offset: 8(snip)","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://dobachi.github.io/memo-blog/tags/ZooKeeper/"}]},{"title":"Apache KafkaにおけるZooKeeper","slug":"ZooKeeper-of-Kafka","date":"2019-03-05T13:03:17.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/03/05/ZooKeeper-of-Kafka/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/03/05/ZooKeeper-of-Kafka/","excerpt":"","text":"参考 メモ 前提 ZooKeeperには何が置かれるのか？ log_dir_event_notification isr_change_notification brokers controller updateLeaderAndIsrメソッド getLogConfigsメソッド setOrCreateEntityConfigsメソッド BrokerのEpochについて getAllLogDirEventNotificationsメソッド setOrCreatePartitionReassignmentメソッド 参考 kafka-docker メモ 前提 kafka-docker を使って環境を立てた docker-compose.yml内で環境変数で指定し、ZooKeeperじょうでは、 /kafka以下のパスを用いるようにした。 いったんtrunkで確認 ZooKeeperには何が置かれるのか？ 実機で確認してみる。 12bash-4.4# zookeeper-shell.sh zookeeper:2181Connecting to zookeeper:2181 12ls /kafka[log_dir_event_notification, isr_change_notification, admin, consumers, cluster, config, latest_producer_id_block, controller, brokers, controller_epoch] トピック準備 12345678910111213141516bash-4.4# kafka-topics.sh --create --topic topic --partitions 1 --zookeeper zookeeper:2181/kafka --replication-factor 1bash-4.4# kafka-topics.sh --topic topic --zookeeper zookeeper:2181/kafka --describeTopic:topic PartitionCount:1 ReplicationFactor:1 Configs: Topic: topic Partition: 0 Leader: 1001 Replicas: 1001 Isr: 1001bash-4.4# kafka-console-producer.sh --topic=topic --broker-list=kafka:9092&gt;hoge&gt;fuga&gt;hoge&gt;fugabash-4.4# kafka-console-consumer.sh --bootstrap-server kafka:9092 --from-beginning --topic topichogefugahogefuga log_dir_event_notification handleLogDirFailureメソッド内でオフラインとなったディレクトリを取り扱うために用いられる。 kafka/server/ReplicaManager.scala:203 12345678910private class LogDirFailureHandler(name: String, haltBrokerOnDirFailure: Boolean) extends ShutdownableThread(name) &#123; Override def doWork() &#123; val newOfflineLogDir = logDirFailureChannel.takeNextOfflineLogDir() if (haltBrokerOnDirFailure) &#123; fatal(s&quot;Halting broker because dir $newOfflineLogDir is offline&quot;) Exit.halt(1) &#125; handleLogDirFailure(newOfflineLogDir) &#125;&#125; isr_change_notification ISRに変化があったことを確認する。 kafka/server/ReplicaManager.scala:269 123456789101112def maybePropagateIsrChanges() &#123; val now = System.currentTimeMillis() isrChangeSet synchronized &#123; if (isrChangeSet.nonEmpty &amp;&amp; (lastIsrChangeMs.get() + ReplicaManager.IsrChangePropagationBlackOut &lt; now || lastIsrPropagationMs.get() + ReplicaManager.IsrChangePropagationInterval &lt; now)) &#123; zkClient.propagateIsrChanges(isrChangeSet) isrChangeSet.clear() lastIsrPropagationMs.set(now) &#125; &#125;&#125; brokers 以下のように、ブローカに関するいくつかの情報を保持する。 12ls /kafka/brokers[seqid, topics, ids] 例えば、ブローカ情報を記録するのは以下の通り。 kafka/zk/KafkaZkClient.scala:95 123456def registerBroker(brokerInfo: BrokerInfo): Long = &#123; val path = brokerInfo.path val stat = checkedEphemeralCreate(path, brokerInfo.toJsonBytes) info(s&quot;Registered broker $&#123;brokerInfo.broker.id&#125; at path $path with addresses: $&#123;brokerInfo.broker.endPoints&#125;, czxid (broker epoch): $&#123;stat.getCzxid&#125;&quot;) stat.getCzxid&#125; 例えば、トピック・パーティション情報は以下の通り。 12get /kafka/brokers/topics/topic/partitions/0/state&#123;&quot;controller_epoch&quot;:1,&quot;leader&quot;:1001,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[1001]&#125; controller 例えば、コントローラ情報は以下の通り。 12get /kafka/controller&#123;&quot;version&quot;:1,&quot;brokerid&quot;:1001,&quot;timestamp&quot;:&quot;1551794212551&quot;&#125; KafkaZKClient#registerControllerAndIncrementControllerEpochメソッドあたり。 updateLeaderAndIsrメソッド リーダとISRの情報を受けとり、ZooKeeper上の情報を更新する。 getLogConfigsメソッド ローカルのデフォルトの設定値と、ZooKeeper上のトピックレベルの設定値をマージする。 setOrCreateEntityConfigsメソッド トピックを作成する際に呼ばれるメソッドだが、これ自体は何かロックを取りながら、 トピックの情報を編集するわけではないようだ。★要確認 したがって、同じトピックを作成する処理が同時に呼ばれた場合、後勝ちになる。 ただしトピックが作成された後は、トピック作成時に当該トピックが存在するかどうかの確認が行われるので問題ない。 kafka/zk/AdminZkClient.scala:101 123def validateTopicCreate(topic: String, partitionReplicaAssignment: Map[Int, Seq[Int]], config: Properties): Unit = &#123; kafka/server/AdminManager.scala:109 123createTopicPolicy match &#123; case Some(policy) =&gt; adminZkClient.validateTopicCreate(topic.name(), assignments, configs) BrokerのEpochについて 以下の通り、BrokerのEpochとしては、ZooKeeperのznodeのcZxid（※）が用いられる。 ※znodeの作成に関するZooKeeper Transaction ID kafka/zk/KafkaZkClient.scala:417 1234567891011121314def getAllBrokerAndEpochsInCluster: Map[Broker, Long] = &#123; val brokerIds = getSortedBrokerList val getDataRequests = brokerIds.map(brokerId =&gt; GetDataRequest(BrokerIdZNode.path(brokerId), ctx = Some(brokerId))) val getDataResponses = retryRequestsUntilConnected(getDataRequests) getDataResponses.flatMap &#123; getDataResponse =&gt; val brokerId = getDataResponse.ctx.get.asInstanceOf[Int] getDataResponse.resultCode match &#123; case Code.OK =&gt; Some((BrokerIdZNode.decode(brokerId, getDataResponse.data).broker, getDataResponse.stat.getCzxid)) case Code.NONODE =&gt; None case _ =&gt; throw getDataResponse.resultException.get &#125; &#125;.toMap&#125; getAllLogDirEventNotificationsメソッド ログディレクトリの変化に関する情報を取得する。 コントローラのイベントハンドラ内で、呼び出されるLogDirEventNotification#processメソッドで用いられる。 何か変化のあったログ（ディレクトリ）を確認し、当該ログを保持するブローカのレプリカの情報を最新化する。★要確認 setOrCreatePartitionReassignmentメソッド パーティションリアサインメントの情報をZooKeeperに書き込む。 このメソッドは、パーティションリアサインメントの必要があるときに呼び出される。 例えばコントローラフェールオーバ時などにも呼び出される。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"},{"name":"Kafka","slug":"Knowledge-Management/Messaging-System/Kafka","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/Kafka/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://dobachi.github.io/memo-blog/tags/ZooKeeper/"}]},{"title":"Clipper","slug":"Clipper","date":"2019-02-24T14:29:27.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/02/24/Clipper/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/24/Clipper/","excerpt":"","text":"参考 メモ Clipper: A Low-Latency Online Prediction Serving System アーキ概要 モデル選択について TensorFlow Servingとの比較 ターゲットとなるアプリ チャレンジポイント アーキテクチャと大まかな処理フロー キャッシュ バッチ化 モデルコンテナ モデル選択層 信頼度とデフォルト動作 落伍者モデルの影響の軽減 ステートの管理 TensorFlow Servingとの比較 制約 類似技術 公式ドキュメント アーキテクチャ モデル管理 コンテナマネージャ APIドキュメント クライアントのサンプル コネクションの生成とエンドポイント定義 推論 XGBOOSTの例 実装確認 開発言語 コンテナの起動の流れを確認してみる create_endpointメソッドを確認 動作確認 クイックスタート 画像取扱のサンプル 参考 Strata NY 2018でのClipper RISELabのページ Clipperの論文 バンディットアルゴリズムについての説明 GitHubページ 公式ウェブサイト clipper APIドキュメント example_client.py xgboost_deployment Container Managers clipper APIドキュメント image_query image_query/example.ipynb メモ Strata NY 2018でのClipper が講演のようだが、スライドが公開されていない。 Clipper: A Low-Latency Online Prediction Serving System Clipperの論文によると、「A Low-Latency Online Prediction Serving System」と定義されている。 上記論文の図1を見るかぎり、モデルサービングを担い、複数の機械学習モデルをまとめ上げる「中層化」層の 役割も担うようだ。 また、モデル選択の機能、キャッシングなどの機能も含まれているようである。 これまでの研究では、学習フェーズに焦点が当たることが多かったが、 この研究では推論フェーズに焦点を当てている。 アーキ概要 モデル抽象化層 モデル選択層 論文Figure 1参照。 モデル選択について 複数の競合するモデルから得られる結果を扱い、動的にモデルを選択していく。 これにより、モデルの精度とロバストネスを高める。 またバンディットアルゴリズムを採用しており、複数のモデルの結果を組み合わせる。 スループットとレイテンシを保つため、キャッシングやバッチ処理を採用。 所定の（？）レイテンシを保持しながら、スループットを高めるためにバッチ処理化するようだ。 また、このあたりは、モデル抽象化層の上になりたっているから、 異なるフレームワーク発のモデルが含まれている場合でも動作するようである。 TensorFlow Servingとの比較 機能面で充実し、スループットやレイテンシの面でも有利。 ターゲットとなるアプリ 物体検知 音声認識 自動音声認識 チャレンジポイント 機械学習フレームワーク、ライブラリの乱立への対応 モデル抽象化レイヤを設けて差異を吸収 高アクセス数と低レイテンシへの対応 スループットのレイテンシの両面を考慮してバッチ化 A/Bテストの煩わしさと不確かさへの対応 機械的なモデル選択。アンサンブル化。 アーキテクチャと大まかな処理フロー 実際にはキューは、モデルの抽象化レイヤ側にあるようにも思う。 追って実装確認必要。 キャッシュ 頻度の高いクエリのキャッシュが有効なのはわかりやすいが、頻度の低いクエリの キャッシュも有効な面がある。 予測結果を使ったあとのフィードック（★要確認）はすぐに生じるからである。 キャッシュはrequestとfetchのAPIを持ち、ノンブロッキングである。 LRUポリシーを用いる。 バッチ化 レイテンシに関するSLOを指定すると、それを守る範囲内で、 バッチ化を試みる。これによりスループットの向上を狙う。 バッチ化で狙うのはRPCやメモリコピーの回数削減、 フレームワークのデータ並列の仕組みの活用。 なお、バックオフを設けながら、バッチサイズを変えることで、 最適なバッチサイズを探す。 AIMD=Additive-Increase-Multiplicative-decrease に基づいてサイズ変更。 また、キューに入っているバッチが少ないケースでは、 ある程度貯まるのを待ち、スループットを上げる工夫もする。 モデルコンテナ C++、Java、Pythonの言語バインディングが提供されている。 各言語バインディングでラッパーを実装すれば良い。 モデルコンテナのレプリカを作り、スケールアウトさせられる。 モデル選択層 アプリケーションのライフサイクル全体に渡り、 フィードバックを考慮しながら、複数のモデルを取り扱う。 これにより、一部のモデルが失敗しても問題ない。 また複数のモデルから得られた結果を統合することでaccuracyを向上させる。 モデルへの選択の基本的なAPIは、select、combine、observe。 selectとcombineはそのままの意味だが、observeは、アプリケーションからの フィードバックを受取り、ステートを更新するために用いるAPIである。 モデル選択については予めgeneralなアルゴリズムが実装されているが、 ユーザが実装することも可能。 ただし、計算量と精度はトレードオフになりがちなので注意。 バンディットアルゴリズム（多腕バンディット問題） ClipperではExp3アルゴリズムに対応 単一の結果を利用 アンサンブル Clipperでは線形アンサンブルに対応 重み計算にはExp4（バンディットアルゴリズム）を利用 なお、バンディットアルゴリズムについては バンディットアルゴリズムについての説明 などを参照されたし。 信頼度とデフォルト動作 モデルから得られた推測値の信頼度が定められた閾値よりも低い場合、 所定のデフォルト動作をさせられる。 複数のモデルを取り扱う場合、信頼度の指標としてそれらのモデルが 最終的な解を採用するかどうかとする方法が挙げられる。 要は、アンサンブルを利用する、ということである。 落伍者モデルの影響の軽減 アンサンブルの歳、モデル抽象化層でスケールアウトさせられる。 しかし弱点として、モデルの中に落伍者がいると、それに足を引っ張られレイテンシが悪化することである。（計算結果の取得が遅くなる） そこでClipperでは、モデル選択層で待ち時間（SLO）を設け、ベストエフォートで 結果をアグリゲートすることとした。 ステートの管理 ステート管理には現在の実装ではRedisを用いているようだ。 なお、DockerContainerMangerでは、外部のRedisサービスを利用することもできるし、 開発用に内部でRedisのDockerコンテナを起動させることもできる。 内部的には、コンストラクタ引数にredis_ipを渡すかどうかで管理されているようだ。 clipper_admin/clipper_admin/docker/docker_container_manager.py:77 12if redis_ip is None: self.external_redis = False TensorFlow Servingとの比較 TF Servingは、TFと密結合。 Clipper同様にスループット向上のための工夫が施されている。 バッチ化も行う。 ただし、基本的に1個のモデルで推論するようになっており、 アプリケーションからのフィードバックを受取り、 モデル選択するような仕組みは無い。 TF Servingと比べると、多少スループットが劣るものの健闘。 制約 Clipperはモデルをブラックボックスとして扱うので、 モデル内に踏み込んだ最適化は行わない。 類似技術 TF Serving LASER Velox 公式ドキュメント アーキテクチャ http://clipper.ai/images/clipper-structure.png に公式ドキュメント上の構成イメージが記載されている。 論文と比較して、モデル選択層の部分が「Clipper Query Processor」と表現されているように見える。 またClipperは、コンテナ群をオーケストレーションし、環境を構成する。 またClipperは、実質的に「ファンクション・サーバ」であると考えられる。 実のところ、ファンクションは機械学習モデルでなくてもよい。 （公式ドキュメントのクイックスタートでも、簡単な四則演算をデプロイする例が載っている） モデル管理 モデルは、リストを入力し、値かJSONを出力する。 入力がリストなのは、機械学習モデルによってはデータ並列で処理し、性能向上を狙うものがあるため。 またClipperはモデル管理のためのライブラリを提供する。 これにより、ある程度決まったモデルであれば、コンテナの作成やモデルの保存などを 自前で作り込む必要がない。 現在は以下の内容に対応する。 One to deploy arbitrary Python functions (within some constraints) One to deploy PySpark models along with pre- and post-processing logic One to deploy R models アプリケーションとモデルの間は、必ずしも1対1でなくてもよい。 参考） * http://clipper.ai/images/link_model.png * http://clipper.ai/images/update_model.png 以上の通り、モデルをデプロイしながら、複数のモデルをアプリケーションにリンクして切り替えることができる。 例えばデプロイした新しいモデルが想定通り動作しなかったとき、もとのモデルに切り戻す、など。 なお、モデルを登録するときには、いくつかの引数を渡す。 入力型 レイテンシのSLO（サービスレベルオブジェクト） デフォルトの出力 さらに、 ClipperConnection.set_num_replicas() を用いて、モデルのレプリカ数を決められる。 コンテナマネージャ Container Managers によると、Dockerコンテナを自前のライブラリか、k8sでオーケストレーションすることが可能。 自前のライブラリは開発向けのようである。 なお、ステートを保存するRedisもコンテナでローンチするようになっているが、 勝手にローンチしてくれるものはステートを永続化するようになっていない。 したがってコンテナが落ちるとステートが消える。 プロダクションのケースでは、きちんと可用性を考慮した構成で予めローンチしておくことが推奨されている。 注意点として、2019/2/27時点でのClipperでは、クエリフロントエンドのオートスケールには対応していない。 これは、クエリフロントエンドとモデル抽象化層のコンテナの間を長命なコネクション（TCPコネクション）で 結んでいるからである。クエリフロントエンドをスケールアウトする時に、これをリバランスする機能が まだ存在していない。 APIドキュメント クライアントのサンプル example_client.py コネクションの生成とエンドポイント定義 1234clipper_conn = ClipperConnection(DockerContainerManager())clipper_conn.start_clipper()python_deployer.create_endpoint(clipper_conn, &quot;simple-example&quot;, &quot;doubles&quot;, feature_sum) ここで渡しているファンクションfeature_sumは以下の通り。 12def feature_sum(xs): return [str(sum(x)) for x in xs] 推論 123456789while True: if batch_size &gt; 1: predict( clipper_conn.get_query_addr(), [list(np.random.random(200)) for i in range(batch_size)], batch=True) else: predict(clipper_conn.get_query_addr(), np.random.random(200)) time.sleep(0.2) predict関数の定義は以下の通り。 1234567891011121314def predict(addr, x, batch=False): url = &quot;http://%s/simple-example/predict&quot; % addr if batch: req_json = json.dumps(&#123;&apos;input_batch&apos;: x&#125;) else: req_json = json.dumps(&#123;&apos;input&apos;: list(x)&#125;) headers = &#123;&apos;Content-type&apos;: &apos;application/json&apos;&#125; start = datetime.now() r = requests.post(url, headers=headers, data=req_json) end = datetime.now() latency = (end - start).total_seconds() * 1000.0 print(&quot;&apos;%s&apos;, %f ms&quot; % (r.text, latency)) 与えられた入力をJSONに変換し、REST APIで渡している。 XGBOOSTの例 12(clipper) $ pip install xgboost(clipper) $ ipython Clipperを起動。 1234import logging, xgboost as xgb, numpy as npfrom clipper_admin import ClipperConnection, DockerContainerManagercl = ClipperConnection(DockerContainerManager())cl.start_clipper() 結果 1219-02-27:22:49:09 INFO [docker_container_manager.py:119] Starting managed Redis instance in Docker19-02-27:22:49:14 INFO [clipper_admin.py:126] Clipper is running アプリを登録。 1cl.register_application(&apos;xgboost-test&apos;, &apos;integers&apos;, &apos;default_pred&apos;, 100000) 結果 119-02-27:22:49:55 INFO [clipper_admin.py:201] Application xgboost-test was successfully registered ファンクション定義 123456789101112def get_test_point(): return [np.random.randint(255) for _ in range(784)]# Create a training matrix.dtrain = xgb.DMatrix(get_test_point(), label=[0])# We then create parameters, watchlist, and specify the number of rounds# This is code that we use to build our XGBoost Model, and your code may differ.param = &#123;&apos;max_depth&apos;: 2, &apos;eta&apos;: 1, &apos;silent&apos;: 1, &apos;objective&apos;: &apos;binary:logistic&apos;&#125;watchlist = [(dtrain, &apos;train&apos;)]num_round = 2bst = xgb.train(param, dtrain, num_round, watchlist) 結果 12[0] train-error:0[1] train-error:0 推論用の関数定義 12def predict(xs): return bst.predict(xgb.DMatrix(xs)) 推論用のコンテンをビルドし、ローンチ。 123456from clipper_admin.deployers import python as python_deployer# We specify which packages to install in the pkgs_to_install arg.# For example, if we wanted to install xgboost and psycopg2, we would use# pkgs_to_install = [&apos;xgboost&apos;, &apos;psycopg2&apos;]python_deployer.deploy_python_closure(cl, name=&apos;xgboost-model&apos;, version=1, input_type=&quot;integers&quot;, func=predict, pkgs_to_install=[&apos;xgboost&apos;]) なお、ここではコンテナをビルドするときに、xgboostをインストールするように指定している。 結果 12345678919-02-27:22:54:35 INFO [deployer_utils.py:44] Saving function to /tmp/clipper/tmpincj4sg219-02-27:22:54:35 INFO [deployer_utils.py:54] Serialized and supplied predict function19-02-27:22:54:35 INFO [python.py:192] Python closure saved(snip)19-02-27:22:54:53 INFO [docker_container_manager.py:257] Found 0 replicas for xgboost-model:1. Adding 119-02-27:22:55:00 INFO [clipper_admin.py:635] Successfully registered model xgboost-model:119-02-27:22:55:00 INFO [clipper_admin.py:553] Done deploying model xgboost-model:1. モデルをアプリにリンク。 1cl.link_model_to_app(&apos;xgboost-test&apos;, &apos;xgboost-model&apos;) 123456789101112131415161718import requests, json# Get Addressaddr = cl.get_query_addr()# Post Queryresponse = requests.post( &quot;http://%s/%s/predict&quot; % (addr, &apos;xgboost-test&apos;), headers=&#123;&quot;Content-type&quot;: &quot;application/json&quot;&#125;, data=json.dumps(&#123; &apos;input&apos;: get_test_point() &#125;))result = response.json()if response.status_code == requests.codes.ok and result[&quot;default&quot;]: print(&apos;A default prediction was returned.&apos;)elif response.status_code != requests.codes.ok: print(result) raise BenchmarkException(response.text)else: print(&apos;Prediction Returned:&apos;, result) 結果 1Prediction Returned: &#123;&apos;query_id&apos;: 2, &apos;output&apos;: 0.3266071, &apos;default&apos;: False&#125; 実装確認 開発言語 1C++ 56.0% Python 24.1% CMake 8.1% Scala 3.0% Shell 2.5% Java 2.2% Other 4.1% コンテナの起動の流れを確認してみる ClipperConnection#start_clipperメソッドを確認する。 ContainerManager#start_clipperメソッドが中で呼ばれる。 clipper_admin/clipper_admin/clipper_admin.py:123 123self.cm.start_clipper(query_frontend_image, mgmt_frontend_image, frontend_exporter_image, cache_size, num_frontend_replicas) clipper_admin/clipper_admin/container_manager.py:63 12345class ContainerManager(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def start_clipper(self, query_frontend_image, mgmt_frontend_image, ContainerManagerは親クラスであり、KubernetesContainerManagerやDockerContainerManagerのstart_clipperが実行される。 例えばDockerContainerManagerを見てみる。 Docker SDKを使い、docker networkを作る。 clipper_admin/clipper_admin/docker/docker_container_manager.py:128 12self.docker_client.networks.create( self.docker_network, check_duplicate=True) Redisを起動する。 clipper_admin/clipper_admin/docker/docker_container_manager.py:156 1234567891011121314151617if not self.external_redis: self.logger.info(&quot;Starting managed Redis instance in Docker&quot;) self.redis_port = find_unbound_port(self.redis_port) redis_labels = self.common_labels.copy() redis_labels[CLIPPER_DOCKER_PORT_LABELS[&apos;redis&apos;]] = str( self.redis_port) redis_container = self.docker_client.containers.run( &apos;redis:alpine&apos;, &quot;redis-server --port %s&quot; % CLIPPER_INTERNAL_REDIS_PORT, name=&quot;redis-&#123;&#125;&quot;.format(random.randint( 0, 100000)), # generate a random name ports=&#123; &apos;%s/tcp&apos; % CLIPPER_INTERNAL_REDIS_PORT: self.redis_port &#125;, labels=redis_labels, **self.extra_container_kwargs) self.redis_ip = redis_container.name マネジメントフロントエンドを起動。 clipper_admin/clipper_admin/docker/docker_container_manager.py:168 12345678910111213141516171819mgmt_cmd = &quot;--redis_ip=&#123;redis_ip&#125; --redis_port=&#123;redis_port&#125;&quot;.format( redis_ip=self.redis_ip, redis_port=CLIPPER_INTERNAL_REDIS_PORT)self.clipper_management_port = find_unbound_port( self.clipper_management_port)mgmt_labels = self.common_labels.copy()mgmt_labels[CLIPPER_MGMT_FRONTEND_CONTAINER_LABEL] = &quot;&quot;mgmt_labels[CLIPPER_DOCKER_PORT_LABELS[&apos;management&apos;]] = str( self.clipper_management_port)self.docker_client.containers.run( mgmt_frontend_image, mgmt_cmd, name=&quot;mgmt_frontend-&#123;&#125;&quot;.format(random.randint( 0, 100000)), # generate a random name ports=&#123; &apos;%s/tcp&apos; % CLIPPER_INTERNAL_MANAGEMENT_PORT: self.clipper_management_port &#125;, labels=mgmt_labels, **self.extra_container_kwargs) なお、コンテナ起動のコマンドに、上記で起動された（もしくは外部のサービスとして与えられた）Redisの IPアドレス、ポート等の情報が渡されていることがわかる。 クエリフロントエンドの起動。 1234567891011self.docker_client.containers.run( query_frontend_image, query_cmd, name=query_name, ports=&#123; &apos;%s/tcp&apos; % CLIPPER_INTERNAL_QUERY_PORT: self.clipper_query_port, &apos;%s/tcp&apos; % CLIPPER_INTERNAL_RPC_PORT: self.clipper_rpc_port &#125;, labels=query_labels, **self.extra_container_kwargs) その他、メトリクスのコンテナを起動する。 123run_metric_image(self.docker_client, metric_labels, self.prometheus_port, self.prom_config_path, self.extra_container_kwargs) コンテナを起動したあとは、ポートの情報を更新する。 create_endpointメソッドを確認 公式の例でも、create_endpointメソッドがよく用いられるので確認する。 メソッド引数は以下の通り。 clipper_admin/clipper_admin/deployers/python.py:16 12345678910111213def create_endpoint(clipper_conn, name, input_type, func, default_output=&quot;None&quot;, version=1, slo_micros=3000000, labels=None, registry=None, base_image=&quot;default&quot;, num_replicas=1, batch_size=-1, pkgs_to_install=None): 基本的には、ClipperConnectionインスタンス、関数名、入力データのタイプ、関数を指定しながら利用する。 メソッド内部の処理は以下のとおり。 clipper_admin/clipper_admin/deployers/python.py:87 1234567clipper_conn.register_application(name, input_type, default_output, slo_micros)deploy_python_closure(clipper_conn, name, version, input_type, func, base_image, labels, registry, num_replicas, batch_size, pkgs_to_install)clipper_conn.link_model_to_app(name, name) 最初にregister_applicationメソッドで、アプリケーションを Clipperに登録する。 なお、register_applicatoin メソッドは、Clipperの マネジメントフロントエンドに対してリクエストを送る。 マネジメントフロントエンドのURLの指定は以下の通り。 clipper_admin/clipper_admin/clipper_admin.py:201 12url = &quot;http://&#123;host&#125;/admin/add_app&quot;.format( host=self.cm.get_admin_addr()) マネジメントフロントエンド自体は、 \\src\\management\\src\\management_frontend.hpp が自体と思われる。 以下の通り、エンドポイント add_app が定義されている。 src/management/src/management_frontend.hpp:52 1const std::string ADD_APPLICATION = ADMIN_PATH + &quot;/add_app$&quot;; src/management/src/management_frontend.hpp:181 1234567server_.add_endpoint( ADD_APPLICATION, &quot;POST&quot;, [this](std::shared_ptr&lt;HttpServer::Response&gt; response, std::shared_ptr&lt;HttpServer::Request&gt; request) &#123; try &#123; clipper::log_info(LOGGING_TAG_MANAGEMENT_FRONTEND, &quot;Add application POST request&quot;); create_endopintの流れ確認に戻る。 アプリケーションの登録が完了したあとは、deploy_python_closure メソッドを使って、 引数は以下の通り。 clipper_admin/clipper_admin/deployers/python.py:96 1234567891011def deploy_python_closure(clipper_conn, name, version, input_type, func, base_image=&quot;default&quot;, labels=None, registry=None, num_replicas=1, batch_size=-1, pkgs_to_install=None): このメソッドでは、最初に save_python_function を使って 関数をシリアライズする。 clipper_admin/clipper_admin/deployers/python.py:189 1serialization_dir = save_python_function(name, func) つづいて、Pyhonのバージョンに従いながらベースとなるイメージを選択する。 以下にPython3.6の場合を載せる。 clipper_admin/clipper_admin/deployers/python.py:205 1234elif py_minor_version == (3, 6): logger.info(&quot;Using Python 3.6 base image&quot;) base_image = &quot;&#123;&#125;/python36-closure-container:&#123;&#125;&quot;.format( __registry__, __version__) 最後にClipperConnection#build_and_deploy_modelメソッドを使って 関数を含むコンテナイメージを作成する。 clipper_admin/clipper_admin/deployers/python.py:220 123clipper_conn.build_and_deploy_model( name, version, input_type, serialization_dir, base_image, labels, registry, num_replicas, batch_size, pkgs_to_install) build_and_deploy_model メソッドは以下の通り。 clipper_admin/clipper_admin/clipper_admin.py:352 123456if not self.connected: raise UnconnectedException()image = self.build_model(name, version, model_data_path, base_image, container_registry, pkgs_to_install)self.deploy_model(name, version, input_type, image, labels, num_replicas, batch_size) build_model メソッドでDockerイメージをビルドし、 レポジトリに登録する。 deploy_model メソッドで登録されたDockerイメージからコンテナを起動する。 このとき指定された個数だけコンテナを起動する。 なお、起動時には、抽象クラス ContainerManager の deploy_model メソッドが呼ばれる。 実際には具象クラスであるKubernetesContainerManagerやDockerContainerManagerクラスのメソッドが実行される。 ここに抽象化が施されていると考えられる。 動作確認 クイックスタート 公式ウェブサイト のクイックスタートの通り、実行してみる。 上記サイトでAnaconda上で環境構成するのを推奨する記述があったためそれに従う。 また、Pythonバージョンに指定があったので、指定された中で最も新しい3.6にした。 12345$ sudo yum install gcc$ conda create -n clipper python=3.6 python$ conda activate clipper$ conda install ipython$ pip install clipper_admin ipythonを起動する。 1(clipper) $ ipython Docker環境を立てる。 123from clipper_admin import ClipperConnection, DockerContainerManagerclipper_conn = ClipperConnection(DockerContainerManager())clipper_conn.start_clipper() 結果の例 1219-02-26:22:24:42 INFO [docker_container_manager.py:119] Starting managed Redis instance in Docker19-02-26:22:26:50 INFO [clipper_admin.py:126] Clipper is running 簡単な例を登録。これにより、エンドポイントが有効になるようだ。 実際の処理の登録は後ほど。 1clipper_conn.register_application(name=&quot;hello-world&quot;, input_type=&quot;doubles&quot;, default_output=&quot;-1.0&quot;, slo_micros=100000) 登録処理は以下の通り。 12def feature_sum(xs): return [str(sum(x)) for x in xs] デプロイ。 12from clipper_admin.deployers import python as python_deployerpython_deployer.deploy_python_closure(clipper_conn, name=&quot;sum-model&quot;, version=1, input_type=&quot;doubles&quot;, func=feature_sum) ここから、モデル抽象化層のDockerイメージが作られる。 123456789101119-02-26:22:30:59 INFO [deployer_utils.py:44] Saving function to /tmp/clipper/tmp67eliqhx19-02-26:22:30:59 INFO [deployer_utils.py:54] Serialized and supplied predict function19-02-26:22:30:59 INFO [python.py:192] Python closure saved19-02-26:22:30:59 INFO [python.py:206] Using Python 3.6 base image(snip)19-02-26:22:31:26 INFO [docker_container_manager.py:257] Found 0 replicas for sum-model:1. Adding 119-02-26:22:31:33 INFO [clipper_admin.py:635] Successfully registered model sum-model:119-02-26:22:31:33 INFO [clipper_admin.py:553] Done deploying model sum-model:1.19-02-26:22:30:59 INFO [clipper_admin.py:452] Building model Docker image with model data from /tmp/clipper/tmp67eliqhx モデルをアプリケーションにリンクさせる。 1clipper_conn.link_model_to_app(app_name=&quot;hello-world&quot;, model_name=&quot;sum-model&quot;) 以上で、エンドポイントhttp://localhost:1337/hello-world/predictを用いて、 推論結果（計算結果）を受け取れるようになる。 curlで結果の取得 1$ curl -X POST --header &quot;Content-Type:application/json&quot; -d &apos;&#123;&quot;input&quot;: [1.1, 2.2, 3.3]&#125;&apos; 127.0.0.1:1337/hello-world/predict 結果の例 1&#123;&quot;query_id&quot;:0,&quot;output&quot;:6.6,&quot;default&quot;:false&#125; Pythonから取得するパターン。 123import requests, json, numpy as npheaders = &#123;&quot;Content-type&quot;: &quot;application/json&quot;&#125;requests.post(&quot;http://localhost:1337/hello-world/predict&quot;, headers=headers, data=json.dumps(&#123;&quot;input&quot;: list(np.random.random(10))&#125;)).json() 結果の例 1Out[12]: &#123;&apos;query_id&apos;: 1, &apos;output&apos;: 4.710181343957851, &apos;default&apos;: False&#125; 参考までに、この時点で実行されているDockerコンテナは以下の通り。 12345678$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb041228b2a4d sum-model:1 &quot;/container/containe…&quot; 25 minutes ago Up 25 minutes (healthy) sum-model_1-373826f93447357bd prom/prometheus:v2.1.0 &quot;/bin/prometheus --c…&quot; 30 minutes ago Up 30 minutes 0.0.0.0:9090-&gt;9090/tcp metric_frontend-37801b3034a7352b8 clipper/frontend-exporter:0.3.0 &quot;python /usr/src/app…&quot; 30 minutes ago Up 30 minutes query_frontend_exporter-334436b775a49e1ff clipper/query_frontend:0.3.0 &quot;/clipper/release/sr…&quot; 30 minutes ago Up 30 minutes 0.0.0.0:1337-&gt;1337/tcp, 0.0.0.0:7000-&gt;7000/tcp query_frontend-334432c75406fda84 clipper/management_frontend:0.3.0 &quot;/clipper/release/sr…&quot; 30 minutes ago Up 30 minutes 0.0.0.0:1338-&gt;1338/tcp mgmt_frontend-39690ff14d91f313e redis:alpine &quot;docker-entrypoint.s…&quot; 32 minutes ago Up 32 minutes 0.0.0.0:6379-&gt;6379/tcp redis-28775 最後にコンテナを停止しておく。 1clipper_conn.stop_all() 画像取扱のサンプル image_query の通りに実行してみる。 また、 上記のノートブックが image_query/example.ipynb にある。 Clipperでは、REST APIでクエリが渡される。データはJSONにラップされて渡される。 そのため、ノートブック image_query/example.ipynb を見ると、 画像ファイルがバイト列で渡される場合と、BASE64でエンコードされて渡される場合の2種類の例が載っていた。 この例では、画像のサイズを返すような関数を定義して使っているが、 渡された画像に何らかの判定処理を加えて戻り値を返すような関数を定義すればよいだろう。 また、例では、python_deployer#create_endpointメソッドが用いられている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model Management","slug":"Knowledge-Management/Machine-Learning/Model-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/"},{"name":"Clipper","slug":"Knowledge-Management/Machine-Learning/Model-Management/Clipper","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/Clipper/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Model Management","slug":"Model-Management","permalink":"https://dobachi.github.io/memo-blog/tags/Model-Management/"},{"name":"Clipper","slug":"Clipper","permalink":"https://dobachi.github.io/memo-blog/tags/Clipper/"}]},{"title":"Tellus","slug":"Tellus","date":"2019-02-22T08:52:22.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/02/22/Tellus/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/22/Tellus/","excerpt":"","text":"参考 メモ 参考 https://www.tellusxdp.com/ja/ メモ 12TellusとはTellus（テルース）」は、政府衛星データを利用した新たなビジネスマーケットプレイスを創出することを目的とした、日本初のオープン＆フリーな衛星データプラットフォームです。複数のデータをかけ合わせ、新たなビジネス創出を促進するためのあらゆるファンクションを提供します。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Open Data","slug":"Knowledge-Management/Open-Data","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Open-Data/"},{"name":"Tellus","slug":"Knowledge-Management/Open-Data/Tellus","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Open-Data/Tellus/"}],"tags":[{"name":"Open Data","slug":"Open-Data","permalink":"https://dobachi.github.io/memo-blog/tags/Open-Data/"},{"name":"Tellus","slug":"Tellus","permalink":"https://dobachi.github.io/memo-blog/tags/Tellus/"},{"name":"Goverment","slug":"Goverment","permalink":"https://dobachi.github.io/memo-blog/tags/Goverment/"},{"name":"GIS","slug":"GIS","permalink":"https://dobachi.github.io/memo-blog/tags/GIS/"},{"name":"Map","slug":"Map","permalink":"https://dobachi.github.io/memo-blog/tags/Map/"}]},{"title":"PostgreSQLにおけるfsyncに関するバグフィックス","slug":"BugFix-about-fsync-in-PostgreSQL","date":"2019-02-18T00:17:13.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/02/18/BugFix-about-fsync-in-PostgreSQL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/18/BugFix-about-fsync-in-PostgreSQL/","excerpt":"","text":"参考 メモ 参考 澤田さんによる説明 PostgreSQL vs. fsync How is it possible that PostgreSQL used fsync incorrectly for 20 years, and what we'll do about it. PostgreSQL 11.2, 10.7, 9.6.12, 9.5.16, and 9.4.21 Released! メモ 基本的に 澤田さんによる説明 がわかりやすいのでそれを参照されたし。 より原典を当たる意では、FOSDEM'19の講演 PostgreSQL vs. fsync How is it possible that PostgreSQL used fsync incorrectly for 20 years, and what we'll do about it. を参照されたし。 fsyncに期待することの認識誤りのため発生していたバグとのこと。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"PostgreSQL","slug":"Clipping/PostgreSQL","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://dobachi.github.io/memo-blog/tags/PostgreSQL/"},{"name":"fsync","slug":"fsync","permalink":"https://dobachi.github.io/memo-blog/tags/fsync/"},{"name":"bug","slug":"bug","permalink":"https://dobachi.github.io/memo-blog/tags/bug/"}]},{"title":"デブサミでの澤田さん講演メモ","slug":"Sawada-san-DevSumi","date":"2019-02-15T01:03:24.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/02/15/Sawada-san-DevSumi/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/15/Sawada-san-DevSumi/","excerpt":"","text":"カンファレンスメモ 日頃の過ごし方 やりがいについて カンファレンスメモ 日頃の過ごし方 NTTOSSセンタにて、業務としてOSS活動 問題解決を水から実施することで、プロダクトの改善と組織の技術力向上の両方に貢献 PostgreSQLについて説明 PostgreSQLへの貢献活動は、問題を見つけて解決する流れもあれば、研究開発の一環として、 大きめの機能を追加していくこともある。 透過的な暗号化を入れる、など やりがいについて 一番は、自分の書いたコードが本体にマージされると嬉しい、というのが 単純だけど大事 他にも、社外での登壇。製品の開発どうこうに詳しくなる。英語が少し上手になった。も。 注意していること 英語は辛い。評価してもらえる仕組みが必要。成果がでる前に時間がかかることがある。会社上司の理解が必要。 PostgreSQLは問題管理（？）、開発管理もメールベースで実施する。（実はGit等を使っていない） 秘蔵のパッチ、眠っていませんか？ 人生最初のパッチは、pgbenchに関する困りごとの解消 ベンチマークで流すSQLが最大1024文字だったのを改善。 PostgreSQLの大きい機能を入れて、それのメンテナとしてコミッターになることが多い。 早い人手も3年単位。10年くらい活動してからなった人もいる。 貢献の方法を公開しているので気になる人はぜひ見てください。","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Conference","slug":"Research/Conference","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Conference/"},{"name":"DevSumi","slug":"Research/Conference/DevSumi","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Conference/DevSumi/"}],"tags":[{"name":"DevSumi","slug":"DevSumi","permalink":"https://dobachi.github.io/memo-blog/tags/DevSumi/"}]},{"title":"Ansibleでコマンド実行結果のJSONを辞書型に変換して用いる","slug":"STDOUT-to-JSON-on-Ansible","date":"2019-02-14T13:06:57.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/02/14/STDOUT-to-JSON-on-Ansible/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/14/STDOUT-to-JSON-on-Ansible/","excerpt":"","text":"参考 メモ 参考 Ansible で task の実行結果の json を dict オブジェクトとして後続の処理で利用する メモ Ansible で task の実行結果の json を dict オブジェクトとして後続の処理で利用する に記載の内容で問題ない。 自分の場合は、AnsibleでWindows PowerShellの実行結果を受け取るときに、オブジェクトをJSONに変換し、 それを辞書型に変換した上で後々when構文で使いたかった。 例） 12345678910- name: check_state_of_wsl win_shell: Get-WindowsOptionalFeature -Online | ? FeatureName -Match &quot;Microsoft-Windows-Subsystem-Linux&quot; | ConvertTo-Json register: wsl_check_json- set_fact: wsl_check: &quot;&#123;&#123; wsl_check_json.stdout &#125;&#125;&quot; - name: enable_wsl win_shell: Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux -NoRestart when: wsl_check.State != 2","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Configuration Management","slug":"Knowledge-Management/Configuration-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Configuration-Management/"},{"name":"Ansible","slug":"Knowledge-Management/Configuration-Management/Ansible","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Configuration-Management/Ansible/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://dobachi.github.io/memo-blog/tags/Windows/"},{"name":"Ansible","slug":"Ansible","permalink":"https://dobachi.github.io/memo-blog/tags/Ansible/"},{"name":"JSON","slug":"JSON","permalink":"https://dobachi.github.io/memo-blog/tags/JSON/"},{"name":"PowerShell","slug":"PowerShell","permalink":"https://dobachi.github.io/memo-blog/tags/PowerShell/"}]},{"title":"Ansible for Windows","slug":"Ansible-for-Windows","date":"2019-02-11T12:31:26.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/02/11/Ansible-for-Windows/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/11/Ansible-for-Windows/","excerpt":"","text":"参考 メモ 最初の環境のはじめ方 Ansibleのインストール、実行環境 パッケージ管理 注意点 Ansible環境 Windows環境での実行ユーザ シェルの実行 WSL導入 参考 WindowsをAnsibleで管理する方法を説明するブログ Windows10 に ゼロから Ansible をインストールする(Ansible for Windows) AnsibleでWindowsのシェル実行 chocolatey WSL導入手順の説明のブログ メモ WindowsをAnsibleで管理する方法を説明するブログで基本的に問題ない。 最初の環境のはじめ方 Ansibleのインストール、実行環境 Windows10 に ゼロから Ansible をインストールする(Ansible for Windows) ではDocker for Windowsで環境構築をしているが、 WSLを使う方法はどうだろうか？ パッケージ管理 chocolateyを利用すればよいだろう。 注意点 Ansible環境 自身の環境では、UbuntuのレポジトリからインストールしたAnsibleを使用していたが、 pywinrmを有効にする手間がかかりそうだったので、condaで環境を作った。 123$ conda create -n ansible python$ conda activate ansible$ pip install ansible pywinrm とした。 Windows環境での実行ユーザ MSアカウントに対しての実行が手間取りそうだったので、 ここではAdministratorで実験した。 Ansibleのインベントリは以下のようなものを用意した。 123456789[win]&lt;対象となるWin環境&gt;[win:vars]ansible_ssh_user=Administratoransible_ssh_port=5986ansible_connection=winrmansible_winrm_transport=ntlmansible_winrm_server_cert_validation=ignore Pingコマンドは以下の通り。 （パスワードを聞かれるので予め設定したAdministratorのパスワードを入れる） 1$ ansible -i hosts.win -m win_ping win -k パッケージインストールの動作確認は以下の通り。 12$ ansible -i hosts.win -m win_chocolatey -a &quot;name=googlechrome state=present&quot; win -k$ ansible -i hosts.win -m win_chocolatey -a &quot;name=vim state=present&quot; win -k シェルの実行 AnsibleでWindowsのシェル実行 によると、win_shell、win_command、scriptモジュールを使うと良いようだ。 WSL導入 WSL導入手順の説明のブログ の通り、PowerShellをAnsibleのwin_shellモジュール使ってインストールする。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Windows","slug":"Knowledge-Management/Windows","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/"},{"name":"Ansible","slug":"Knowledge-Management/Windows/Ansible","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/Ansible/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://dobachi.github.io/memo-blog/tags/Windows/"},{"name":"Ansible","slug":"Ansible","permalink":"https://dobachi.github.io/memo-blog/tags/Ansible/"},{"name":"Configuration Management","slug":"Configuration-Management","permalink":"https://dobachi.github.io/memo-blog/tags/Configuration-Management/"}]},{"title":"Machine Learning Model Management Tools","slug":"Machine-Learning-Model-Management","date":"2019-02-09T03:54:33.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/02/09/Machine-Learning-Model-Management/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/09/Machine-Learning-Model-Management/","excerpt":"","text":"参考 Strata studio.ml ModelDB MLflow Clipper Azure Machine Learning Studio Hydrosphere Serving studio.ml 注意事項 インストールと動作確認 ModelDB 注意点 概要 Light APIの例 Native API利用の例 ウェブUIの例 MLflow 概要 動機 機能 Clipper Clipper: A Low-Latency Online Prediction Serving System アーキ概要 Azure Machine Learning Studio Hydrosphere Serving 参考 Strata Strata Model Lifecycle Management studio.ml studio.ml studio.ml Issue-330 studio.mlの公式GitHub ModelDB ModelDBの公式GitHub ModelDB紹介 at Spark Summit ModelDB Clientの例 MLflow MLflowのO'Reilly記事 Clipper Strata NY 2018でのClipper RISELabのページ Clipperの論文 バンディットアルゴリズムについての説明 Azure Machine Learning Studio Azure Machine Learning Studio Azure Machine Learning Studio公式ドキュメント Hydrosphere Serving Hydrosphere.io のページを参照。 studio.ml Pythonベースの機械学習モデル管理のフレームワーク。 対応しているのは、Keras、TensorFlow、scikit-learnあたり。 注意事項 studio.mlの公式GitHubのREADMEを読むと「Authentication」の項目があり、 studio.mlに何らかの手段で認証しないといけないようである。 ゲストモードがあるようだが極力アカウント情報を渡さずに使おうとしたが うまく動作しなかった。 インストールと動作確認 123$ /opt/Anaconda/default/bin/conda create -n studioml python$ conda activate stduioml$ pip install studioml 以下のようなエラーが生じた。 1awscli 1.16.101 has requirement rsa&lt;=3.5.0,&gt;=3.1.2, but you&apos;ll have rsa 4.0 which is incompatible. 1234$ cd ~/Sources$ git clone https://github.com/studioml/studio.git$ cd studio$ cd examples/keras/ ここでGitHub上のREADMEに記載のとおり、~/.studioml/config.yamlのdatabaseセクションに guest: trueを追加した。 いったんUIを起動してみる。 1$ studio ui 以下のようなエラーが出た。 123 File &quot;/home/******/.conda/envs/studioml/lib/python3.7/site-packages/flask/app.py&quot;, line 1813, in full_dispatch_request rv = self.dispatch_request() File &quot;/home/******/.conda/envs/studioml/lib/python3.7/site-packages/flask/app.py&quot;, line 1799, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File &quot;/home/******/.conda/envs/studioml/lib/python3.7/site-packages/studio/apiserver.py&quot;, line 37, in dashboard return _render(&apos;dashboard.html&apos;) File &quot;/home/******/.conda/envs/studioml/lib/python3.7/site-packages/studio/apiserver.py&quot;, line 518, in _render auth = get_auth(get_auth_config()) File &quot;/home/******/.conda/envs/studioml/lib/python3.7/site-packages/studio/apiserver.py&quot;, line 511, in get_auth_config return get_config()[&apos;server&apos;][&apos;authentication&apos;]KeyError: &apos;server&apos; 参考: studio.ml Issue-330 設定を以下のように変えてみた。 123456789101112--- /home/dobachi/.studioml/config.yaml.2019021001 2019-02-10 01:04:10.751282800 +0900+++ /home/dobachi/.studioml/config.yaml 2019-02-10 01:17:20.178761700 +0900@@ -2,6 +2,10 @@ type: http serverUrl: https://zoo.studio.ml authentication: github+ guest: true++server:+ authentication: None storage: type: gcloud ブラウザでhttp://localhost:5000にアクセスした所、開けたような気がしたが、 GitHubの404ページが表示された。 コンソール上のエラーは以下のとおり。 1requests.exceptions.ConnectionError: HTTPSConnectionPool(host=&apos;zoo.studio.ml&apos;, port=443): Max retries exceeded with url: /api/get_user_experiments (Caused by NewConnectionError(&apos;&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7ff5a64a3da0&gt;: Failed to establisha new connection: [Errno -2] Name or service not known&apos;)) ModelDB 注意点 ModelDBの公式GitHubを見ると、20190211時点で最近更新されていないようだ。（最新が6ヶ月前の更新） 概要 モデルの情報を構造的に管理するための仕組み。 「ModelDB's Light API」を通じて、モデルのメトリクスとメタデータを管理できるようになる。 spark.mlとscikit-learnはネイティブクライアントを利用する。 ★要確認 ModelDBの公式GitHubのREADMEにわかりやすいフロントエンドの紹介がある。 Light APIの例 ModelDB BasicWorkflow.pyがワークフローの基本が分かる例。 基本は、 Syncerを使ってモデルの情報を登録する。 この例では、 データセット モデル モデルのコンフィグ モデルのメトリクス をModelDBに登録する。 ModelDB BasicSyncAll.pyがYAMLで登録する例。 Native API利用の例 ModelDB Clientの例にmodeldb.sklearn_native.SyncableMetricsの利用例が記載されている。 ウェブUIの例 ModelDB ウェブUIの例に例が載っている。 MLflow 概要 MLflowのO'Reilly記事に動機と機能概要が書かれている。 また動画でウェブUIの使い方が示されている。 動機 無数のツールを組み合わせるのが難しい 実験をトレースするのが難しい 結果の再現性担保が難しい デプロイが面倒 機能 トラッキング パラメータ、コードバージョン、メトリクス、アウトプットファイルなどを管理する プロジェクト コードをパッケージングし、結果の再現性を担保する。またユーザ間でプロジェクトを流通できる conda形式で依存関係を記述可能であり、実行コマンドも定義可能 もしMLflowの機能でトラッキングしていれば、バージョンやパラメータをトラックする モデル モデルをパッケージングし、デプロイできるようにする Clipper Strata NY 2018でのClipper が講演のようだが、スライドが公開されていない。 Clipper: A Low-Latency Online Prediction Serving System Clipperの論文によると、「A Low-Latency Online Prediction Serving System」と定義されている。 上記論文の図1を見るかぎり、モデルサービングを担い、複数の機械学習モデルをまとめ上げる「中層化」層の 役割も担うようだ。 また、モデル選択の機能、キャッシングなどの機能も含まれているようである。 これまでの研究では、学習フェーズに焦点が当たることが多かったが、 この研究では推論フェーズに焦点を当てている。 アーキ概要 モデル抽象化層 モデル選択層 論文Figure 1参照。 Azure Machine Learning Studio Azure Machine Learning Studioには、「完全に管理されたクラウド サービスで、ユーザーは簡単に予測分析ソリューションを構築、デプロイ、共有できます。」と 記載されている。 Hydrosphere Serving Hydrosphere.io のページを参照。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model Management","slug":"Knowledge-Management/Machine-Learning/Model-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Model Management","slug":"Model-Management","permalink":"https://dobachi.github.io/memo-blog/tags/Model-Management/"}]},{"title":"Install_vim8_on_centos7","slug":"Install-vim8-on-centos7","date":"2019-02-08T06:33:45.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/02/08/Install-vim8-on-centos7/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/08/Install-vim8-on-centos7/","excerpt":"","text":"参考 メモ 参考 方法を紹介するブログ メモ copr.fedorainfracloud.orgというサイトを使っているということで、 あまりお作法のよくない方法ではあるが動作はする。 どうしてもCentOS7でDein使いたい、というケースにおいて有効。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"vim","slug":"Knowledge-Management/vim","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"},{"name":"CentOS7","slug":"CentOS7","permalink":"https://dobachi.github.io/memo-blog/tags/CentOS7/"}]},{"title":"Manifold of Uber","slug":"Manifold-of-Uber","date":"2019-02-08T03:52:25.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/02/08/Manifold-of-Uber/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/08/Manifold-of-Uber/","excerpt":"","text":"参考 O'Reillyの記事 プロダクト概要 機能 より直感的な皮革のための取り組み アーキテクチャ ユースケース モデルの工夫の効果の確認 False Negativeの低減のための分析 参考 O'Reillyの記事 Uber eatsでの機械学習による配達時間短縮事例 IEEE Transacionの論文 O'Reillyの記事 プロダクト概要 モデル管理をより視覚的に行いやすくするためのツール ManifoldをUberが開発した。 モデル性能劣化の潜在的な要因を探すのに役立つ。 また、あるデータセットに関して、各モデルがどういった性能を示すかを可視化し、 モデル化以前の工夫の効果を測ることが出来る。 既存のモデル可視化ツールは、特定のツール、アルゴリズムの内部状態を示す物が多かった。 Manifoldはモデル性能を改善／改悪するデータセグメントを見つける。 それらがアグリゲートされた評価も示す。これはアンサンブル手法にとって使いやすい。 現時点では、回帰と識別器に対応。 今のところオープンソース化はされていないようだ。 機能 Visualization and workflow designの箇所に図ありで説明がされている。 性能比較ビュー 横軸：メトリクス、縦軸：データセグメント、色：モデル、という形で視覚的に モデルの傾向を確認できる 特徴ごとの比較ビュー 横軸：特徴空間、縦軸：データポイント数、色：データセグメント、という形で 視覚的にモデルの傾向を確認できる グラフを表示ながら以下のような操作を加えて確認可能 クラスタ数を変えながら、特徴がどのように変化するかを確認 スライス（領域を区切ってズーム） 特徴の差の大きさ（？）にフィルタをかけて、気になる点を探す より直感的な皮革のための取り組み またコンセプトとしては、さらに 横軸の性能 縦軸に特徴 という形で視覚化する方法が挙げられているが、まだ課題も挙げられている。 （例：データ点数が多くなりがちなので抽象化の仕組みが必要、など） そこでManifoldではデータセグメントをクラスタ化して見やすくする、などの工夫が施されている。 アーキテクチャ PythonとJavaScriptのそれぞれを好きな方を使える。 これにより、Uber内の他のフレームワークと連携しやすくなっている。 ただしJavaScriptでクラスタリングなどを計算させると遅いことから、 TensorFlow.jsを用いている。 ユースケース モデルの工夫の効果の確認 Uber eatsでの機械学習による配達時間短縮事例 に記載の事例にて、 モデル比較のため使用。 モデル改善の工夫の評価をする際にManifoldを使用。 これにより、特定のデータセグメントに改善効果があることがわかった。 False Negativeの低減のための分析 Safetyチームによる利用。 False Negativeレートの改善のために利用。 あるモデルがNegativeとしたデータセグメントについてドリルダウン分析。 当該モデルは、ある特徴の値が小さいときにNegativeと判定する傾向があることがわかった。 しかし実際にはその中にはPositiveなものも含まれいる。 ここから、その特徴に着目し、より差をつけやすい特徴を用いたり、モデルを用いるような工夫を施した。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Machine Learning","slug":"Knowledge-Management/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/"},{"name":"Model Management","slug":"Knowledge-Management/Machine-Learning/Model-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Machine-Learning/Model-Management/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Model Management","slug":"Model-Management","permalink":"https://dobachi.github.io/memo-blog/tags/Model-Management/"},{"name":"Uber","slug":"Uber","permalink":"https://dobachi.github.io/memo-blog/tags/Uber/"}]},{"title":"Create BigTop environment on Docker","slug":"Create-BigTop-environment-on-Docker","date":"2019-02-07T14:43:04.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/02/07/Create-BigTop-environment-on-Docker/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/07/Create-BigTop-environment-on-Docker/","excerpt":"","text":"参考 メモ Sparkも試す 参考 公式の手順 Dockerでのクラスタ構築を紹介するブログ メモ Dockerでのクラスタ構築を紹介するブログ と 公式の手順 を参考に試した。 基本的には、前者のブログの記事通りで問題ない。 Sparkも試す 以下のようにコンポーネントに追加してプロビジョニングするとSparkも使えるようになる。 12345678910111213diff --git a/provisioner/docker/config_centos-7.yaml b/provisioner/docker/config_centos-7.yamlindex 49f86aee..0f611a10 100644--- a/provisioner/docker/config_centos-7.yaml+++ b/provisioner/docker/config_centos-7.yaml@@ -19,6 +19,6 @@ docker: repo: &quot;http://repos.bigtop.apache.org/releases/1.3.0/centos/7/$basearch&quot; distro: centos-components: [hdfs, yarn, mapreduce]+components: [hdfs, yarn, mapreduce, spark] enable_local_repo: false-smoke_test_components: [hdfs, yarn, mapreduce]+smoke_test_components: [hdfs, yarn, mapreduce, spark]","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hadoop","slug":"Knowledge-Management/Hadoop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/"},{"name":"BigTop","slug":"Knowledge-Management/Hadoop/BigTop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/BigTop/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://dobachi.github.io/memo-blog/tags/Hadoop/"},{"name":"BigTop","slug":"BigTop","permalink":"https://dobachi.github.io/memo-blog/tags/BigTop/"}]},{"title":"WhereHows by LinkedIn","slug":"WhereHows-by-LinkedIN","date":"2019-02-06T13:27:59.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/02/06/WhereHows-by-LinkedIN/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/06/WhereHows-by-LinkedIN/","excerpt":"","text":"参考 ユースケースについて 機能概要 アーキテクチャ バックエンドのETLについて 参考 ユースケース Wiki Architecture ユースケースについて ユースケース に背景などが記載されている。 複数のデータソース、複数のデータストア、複数のデータ処理エンジン、そしてワークフロー管理ツール。 LinkedInの場合は以下のとおり。 Analytics Storage system: HDFS, Teradata, Hive, Espresso, Kafka, Voldemort Execution: MapReduce, Pig, Hive, Teradata SQL. Workflow management: Azkaban, Appworx, Oozie ユースケースは以下の通り。 新任の人が立ち上がりやすいようにする データセットを見つけられるようにする ジョブやデータの変更が及ぼす影響を確認する 機能概要 Wikiに画面キャプチャ付きで説明がある。 GUIの他、バックエンドAPIもあり、自動化できるようになっている。 例：https://github.com/LinkedIn/Wherehows/wiki/Backend-API#dataset-get アーキテクチャ Architecture に記載されている。 Akkaベースでスケジューラが組まれており、スケジューラによりクローラが動く。 データセットやオペレーションデータがエンドポイトであり、リネージがそれを結ぶブリッジであると考えられる。 またこれらは極力汎用的に設計されているので、異なるプロダクトのデータを取り入れることができる。 バックエンドのETLについて Java、Jython、MySQLで構成。 分析ロジックの実装しやすさとほかシステムとの連携しやすさを両立。 Dataset ETL HDFS内をクロールする Operation data ETL フロー管理ツールのログなどをクロールし、所定の形式に変換する Lineage ETL 定期的にジョブ情報を取得し、入力データと出力データの定義から リネージを生成する","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Data Engineering","slug":"Knowledge-Management/Data-Engineering","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Engineering/"},{"name":"Data Lineage","slug":"Knowledge-Management/Data-Engineering/Data-Lineage","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Data-Engineering/Data-Lineage/"}],"tags":[{"name":"LinkedIn","slug":"LinkedIn","permalink":"https://dobachi.github.io/memo-blog/tags/LinkedIn/"},{"name":"WhereHows","slug":"WhereHows","permalink":"https://dobachi.github.io/memo-blog/tags/WhereHows/"},{"name":"Data Lineage","slug":"Data-Lineage","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Lineage/"}]},{"title":"LinkedIn WareHows","slug":"LinkedIn-WareHows","date":"2019-02-05T11:45:53.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/02/05/LinkedIn-WareHows/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/05/LinkedIn-WareHows/","excerpt":"","text":"参考 メモ 参考 メモ links data objects with people and processes enables crowdsourcing for data knowledge provides data governance and provenance based on ownership and lineage","categories":[],"tags":[]},{"title":"Mount host's filesystem on KVM","slug":"Mount-host-s-filesystem-on-KVM","date":"2019-02-03T13:31:25.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/02/03/Mount-host-s-filesystem-on-KVM/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/03/Mount-host-s-filesystem-on-KVM/","excerpt":"","text":"参考 メモ 補足 参考 マウントの仕方を示しているブログ メモ マウントの仕方を示しているブログ の通りで問題ない。 補足 「ハードウェアの追加」から「ファイルシステム」を追加する。 ゲストOSでマウントするときに、ディレクトリを作成し忘れないこと。 手動でマウント 1$ sudo mount -t 9p -o trans=virtio hostdata /mnt/data fstabには以下のように記載 1hostdata /mnt/data 9p trans=virtio 0 1","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"KVM","slug":"Home-server/Ubuntu/KVM","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/KVM/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"},{"name":"KVM","slug":"KVM","permalink":"https://dobachi.github.io/memo-blog/tags/KVM/"}]},{"title":"Buy Windows10 licenses","slug":"Buy-Windows10-licenses","date":"2019-02-03T05:52:23.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/02/03/Buy-Windows10-licenses/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/03/Buy-Windows10-licenses/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Docker for Windows with WSL","slug":"Docker-for-Windows-with-WSL","date":"2019-02-01T15:08:22.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/02/02/Docker-for-Windows-with-WSL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/02/Docker-for-Windows-with-WSL/","excerpt":"","text":"参考 メモ コマンド 参考 動かし方を紹介するブログ メモ 上記ブログ 動かし方を紹介するブログ の内容で基本的に問題ない。 ただし、一般ユーザの.bashrcに環境変数DOCKER_HOSTを指定したので、 一般ユーザでDockerを起動できるようにしておいた。 （当該一般ユーザをdockerグループに加えた） コマンド コンテナ確認 12$ docker ps$ docker ps -a -vを用いてボリュームをマウント。 ついでに--rmを用いて停止後に削除することにする。 1$ docker run -it --rm -v c:/Users/&lt;user name&gt;:/mnt/&lt;user name&gt; &lt;image id&gt; /bin/bash","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"Docker","slug":"Knowledge-Management/WSL/Docker","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/Docker/"}],"tags":[{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"}]},{"title":"Hyper-V機能のオン・オフを切り替える","slug":"Switch-Hyper-V-on-and-off","date":"2019-02-01T15:02:03.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/02/02/Switch-Hyper-V-on-and-off/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/02/02/Switch-Hyper-V-on-and-off/","excerpt":"","text":"参考 メモ 参考 切り替えようPowerShellスクリプトを掲載してくれているサイト メモ 基本的には、 切り替えようPowerShellスクリプトを掲載してくれているサイト のとおりで問題ない。 パス等は環境に合わせること。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Windows","slug":"Knowledge-Management/Windows","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/"},{"name":"Hyper-V","slug":"Knowledge-Management/Windows/Hyper-V","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Windows/Hyper-V/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://dobachi.github.io/memo-blog/tags/Windows/"},{"name":"Hyper-V","slug":"Hyper-V","permalink":"https://dobachi.github.io/memo-blog/tags/Hyper-V/"}]},{"title":"Try Apache Superset","slug":"Try-Apache-Superset","date":"2019-01-31T13:46:58.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/01/31/Try-Apache-Superset/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/31/Try-Apache-Superset/","excerpt":"","text":"参考 動作確認 必要パッケージのインストール セットアップ 参考 公式ウェブサイト 公式ウェブサイトのインストール手順 Issue 2205 Issue 6770 動作確認 公式ウェブサイトのインストール手順 の「Start with Docker」のとおり、進めてみたところ、 トップ画面がうまく表示されない事象が生じたので、ここでは改めてCentOS7環境に構築してみることにする。 必要パッケージのインストール 以下が指定されていたが、libsasl2-develだけ無いというエラーが出た。 1gcc gcc-c++ libffi-devel python-devel python-pip python-wheel openssl-devel libsasl2-devel openldap-devel Issue 2205 を見ると、cyrus-sasl-develであれば存在するようだ。 セットアップ /opt/virtualenv/superset 以下にvirtualenv環境を作り、supersetをインストールしたものとする。 その上で、公式ドキュメントを参考に以下のようなコマンドを実行する。（極力対話設定を減らしている） 123$ source /opt/virtualenv/superset/bin/activate$ fabmanager create-admin --app superset --username admin --firstname admin --lastname admin --password admin --email admin@fab.org$ superset db upgrade 以下のようなエラーが出た。 1Was unable to import superset Error: cannot import name &apos;_maybe_box_datetimelike&apos;(superset) [vagrant@superset-01 ~]$ superset db upgrade [Issue 6670] を見ると、Pandasの新しいバージョンに起因する問題らしく、バージョンを下げることで対応可能だそうだ。 12$ pip uninstall pandas$ pip install pandas==0.23.4 してから、以下の通り改めて実行。 1234$ fabmanager create-admin --app superset --username admin --firstname admin --lastname admin --password admin --email admin@fab.org$ superset db upgrade$ superset load_examples$ superset init 上記を実行し、サーバの8088ポートをブラウザで開けば、トップが画面が表示される。","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Visualization","slug":"Research/Visualization","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Visualization/"},{"name":"Superset","slug":"Research/Visualization/Superset","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Visualization/Superset/"}],"tags":[{"name":"Superset","slug":"Superset","permalink":"https://dobachi.github.io/memo-blog/tags/Superset/"},{"name":"Visualization","slug":"Visualization","permalink":"https://dobachi.github.io/memo-blog/tags/Visualization/"}]},{"title":"Hyper plugin","slug":"Hyper-plugin","date":"2019-01-20T13:52:29.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/01/20/Hyper-plugin/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/20/Hyper-plugin/","excerpt":"","text":"参考 参考 色テーマのまとめサイト プラグインおすすめブログ１ 公式プラグインまとめ","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hyper","slug":"Knowledge-Management/Hyper","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hyper/"},{"name":"Plugin","slug":"Knowledge-Management/Hyper/Plugin","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hyper/Plugin/"}],"tags":[{"name":"Hyper","slug":"Hyper","permalink":"https://dobachi.github.io/memo-blog/tags/Hyper/"}]},{"title":"Nature Remo API","slug":"Nature-Remo-API","date":"2019-01-18T13:27:20.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/01/18/Nature-Remo-API/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/18/Nature-Remo-API/","excerpt":"","text":"参考 ブログもとに試す センサーの値を取り出してみる Python化 ローカルAPIを試してみる 参考 Nature Remo APIガイド ローカルAPI グローバルAPI API解説のブログ Nature Remo で温度超過時にアラート通知をする NatureRemoのAPIを活用する準備 curl to python ブログもとに試す API解説のブログを参考に試してみる。 home.nature.global につなぎ、アクセストークンを発行する。 なお、アクセストークンは決して漏らしてはならない。 ここではローカルファイルシステムにna.txtとして保存して利用することにする。 1234567891011121314151617181920212223242526272829303132333435$ curl -X GET &quot;https://api.nature.global/1/devices&quot; -H &quot;accept: application/json&quot; -k --header &quot;Authorization: Bearer `cat na.txt`&quot; | jq[ &#123; &quot;name&quot;: &quot;リビング&quot;, &quot;id&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;, &quot;created_at&quot;: &quot;xxxxxxxxxxxxxxxxxxxx&quot;, &quot;updated_at&quot;: &quot;xxxxxxxxxxxxxxxxxxxx&quot;, &quot;mac_address&quot;: &quot;xxxxxxxxxxxxxxxxxx&quot;, &quot;serial_number&quot;: &quot;xxxxxxxxxxxxxxxxxxxxx&quot;, &quot;firmware_version&quot;: &quot;Remo/1.0.62-gabbf5bd&quot;, &quot;temperature_offset&quot;: 0, &quot;humidity_offset&quot;: 0, &quot;users&quot;: [ &#123; &quot;id&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;, &quot;nickname&quot;: &quot;xxxxxxxxxxxxxxxxxxx&quot;, &quot;superuser&quot;: xxxxxx &#125; ], &quot;newest_events&quot;: &#123; &quot;hu&quot;: &#123; &quot;val&quot;: 20, &quot;created_at&quot;: &quot;xxxxxxxxxxxxxxx&quot; &#125;, &quot;il&quot;: &#123; &quot;val&quot;: 104.4, &quot;created_at&quot;: &quot;xxxxxxxxxxxxxxxxxx&quot; &#125;, &quot;te&quot;: &#123; &quot;val&quot;: 22.2, &quot;created_at&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxx&quot; &#125; &#125; &#125;] 1$ curl -X GET &quot;https://api.nature.global/1/appliances&quot; -H &quot;accept: application/json&quot; -k --header &quot;Authorization: Bearer `cat na.txt`&quot; | jq センサーの値を取り出してみる Nature Remo で温度超過時にアラート通知をする を参考に試す。 先の例と同様に、/1/devicesを叩くと、その戻りの中にセンサーの値が含まれているようだ。 そこでjqコマンドを使って取り出す。 1$ curl -X GET &quot;https://api.nature.global/1/devices&quot; -H &quot;accept: application/json&quot; -k --header &quot;Authorization: Bearer `cat na.txt`&quot; | jq &apos;.[].newest_events&apos; 結果の例 1234567891011121314&#123; &quot;hu&quot;: &#123; &quot;val&quot;: 20, &quot;created_at&quot;: &quot;2019-01-18T15:01:43Z&quot; &#125;, &quot;il&quot;: &#123; &quot;val&quot;: 104.4, &quot;created_at&quot;: &quot;2019-01-18T13:26:58Z&quot; &#125;, &quot;te&quot;: &#123; &quot;val&quot;: 21, &quot;created_at&quot;: &quot;2019-01-18T14:24:23Z&quot; &#125;&#125; これを保存しておけばよさそう。 Python化 curl to python のサイトを利用し、上記curlをPythonスクリプトに変換した。 ローカルAPIを試してみる Remoを探すため、公式サイトでは以下のように示されていた。 1% dns-sd -B _remo._tcp 代わりに以下のコマンドで確認した。 12dobachi@home:~$ avahi-browse _remo._tcp+ xxxxx IPv4 Remo-xxxxxx _remo._tcp local","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Nature Remo","slug":"Home-server/Nature-Remo","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Nature-Remo/"}],"tags":[{"name":"Nature Remo","slug":"Nature-Remo","permalink":"https://dobachi.github.io/memo-blog/tags/Nature-Remo/"},{"name":"Smart Home","slug":"Smart-Home","permalink":"https://dobachi.github.io/memo-blog/tags/Smart-Home/"}]},{"title":"Open Messaging Benchmark","slug":"Open-Messaging-Benchmark","date":"2019-01-15T15:25:15.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2019/01/16/Open-Messaging-Benchmark/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/16/Open-Messaging-Benchmark/","excerpt":"","text":"参考 メモ 所感 パラメータ その他 GitHubを見てみる Kafkaのドライバ bin Benchmarkクラス コントリビュータ イシュー 参考 Open Messaging Benchmarkのウェブサイト openmessaging-benchmarkのGitHub Kafkaのドライバ Kafka構築のAnsible Playbook Kafka構築のTerraformコンフィグ メモ ウェブサイトを確認してみる。 所感 あくまでメッセージングシステムに負荷をかけ、その特性を見るためのベンチマークに見える。 したがって負荷をかけるクライアントはシンプルであり、ただしワークロードを調整するための パラメータは一通り揃っている、という様子。 パラメータ The number of topics The size of the messages being produced and consumed The number of subscriptions per topic The number of producers per topic The rate at which producers produce messages (per second). Note: a value of 0 means that messages are produced as quickly as possible, with no rate limiting. The size of the consumer’s backlog (in gigabytes) The total duration of the test (in minutes) 「BENCHMARKING WORKLOADS」の章に代表的な組み合わせパターンの記載がある。 その他 「warm-up」とあったが、何をしているのだろう。 構成はTerraformとAnsibleでやるようだ GitHubを見てみる Kafkaのドライバ openmessaging-benchmarkのGitHub を眺めてみる。 Kafkaのドライバ を除くとデプロイ用のTerraformコンフィグとAnsibleプレイブックがあった。 Kafka構築のTerraformコンフィグ の通り、一通りAWSインスタンスをデプロイ。 Kafka構築のAnsible Playbook の通り、ひととおりKafkaクラスタから負荷がけクライアントまで構成。 なお、ソースコードとしてベンチマークのドライバが含まれているが、 その親クラスが BenchmarkDriver だった。 bin bin以下には、ベンチマークを実行すると思われる、 benchmarkが含まれていた。 Benchmarkクラス 以下の通り、ワークロードごとに、指定されたドライバのコンフィグレーションに基づき、 負荷をかけるようになっているようだ。 io/openmessaging/benchmark/Benchmark.java:126 12workloads.forEach((workloadName, workload) -&gt; &#123; arguments.drivers.forEach(driverConfig -&gt; &#123; また以下の通り、WorkloadGeneratorを通じて、ワークロードがかけられるようだ。 io/openmessaging/benchmark/Benchmark.java:140 123WorkloadGenerator generator = new WorkloadGenerator(driverConfiguration.name, workload, worker);TestResult result = generator.run(); コントリビュータ yahoo、alibaba、streamlioあたりからコントリビュータが出ている。 ただし主要な開発者は数名。 最初にYahooの若干名が活発に開発し、それに加わった人がいるようだ。 イシュー エンハンスを中心に、いくつか挙げられていた（2019/1/18時点）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Messaging System","slug":"Knowledge-Management/Messaging-System","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Messaging-System/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://dobachi.github.io/memo-blog/tags/Kafka/"},{"name":"Open Messaging Benchmark","slug":"Open-Messaging-Benchmark","permalink":"https://dobachi.github.io/memo-blog/tags/Open-Messaging-Benchmark/"},{"name":"Pulsar","slug":"Pulsar","permalink":"https://dobachi.github.io/memo-blog/tags/Pulsar/"}]},{"title":"Docker on WSL","slug":"Docker-on-WSL","date":"2019-01-14T12:12:59.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/01/14/Docker-on-WSL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/14/Docker-on-WSL/","excerpt":"","text":"参考 メモ Docker docker-compose 参考 geerlingguy/docker WSLでDockerをインストールする手順を示したブログ docker-composeの公式ドキュメント docker-composeのインストール手順 メモ Docker WSLでDockerをインストールする手順を示したブログを参考に、geerlingguy/docker を用いてAnsibleでDockerをインストールしてみたが、 2019/01現在のWSLでは17.09.0以前のDockerを用いる必要がある（Dockerのバージョンについての言及 参照） 自身のUbuntu18系の環境に置ける geerlingguy/docker では18系のDockerしかインストールできない ということから、 Dockerのバージョンについての言及 を参考に17系のDockerを無理やりインストールしたところ一応動いた。 まず 管理者権限で WSLを立ち上げる。 つづいて、以下のとおりインストールする。 12$ curl -O https://download.docker.com/linux/debian/dists/stretch/pool/stable/amd64/docker-ce_17.09.0~ce-0~debian_amd64.deb$ sudo dpkg -i docker-ce_17.09.0\\~ce-0\\~debian_amd64.deb サービスを起動して動作確認。 12$ sudo service docker start$ sudo docker run hello-world なお、WSLでDockerをインストールする手順を示したブログ では「管理者権限でWSLを起動する」ことがポイントとあげられていたので、それは守ることにした。 が、インストール後は管理者権限でなくても動いたので、このあたり真偽の確認が必要そうだ。 docker-compose おおむね docker-composeのインストール手順 のとおりにインストールする。 12$ sudo su - -c &quot;curl -L https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose&quot;$ sudo chmod +x /usr/local/bin/docker-compose docker-composeの公式ドキュメント の通りためしてみるが、以下のようなエラーが生じた。 1234$ sudo docker-compose upCreating network &quot;composetest_default&quot; with the default driverERROR: Failed to Setup IP tables: Unable to enable NAT rule: (iptables failed: iptables --wait -t nat -I POSTROUTING -s 172.18.0.0/16 ! -o br-3a6517e292d4 -j MASQUERADE: iptables: Invalid argument. Run `dmesg&apos; for more information. (exit status 1)) WSLがLinuxカーネルではないことに起因しているのだろう可。（iptablesまわりは期待したとおりに動かない？）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"Docker","slug":"Knowledge-Management/WSL/Docker","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/Docker/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"},{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"}]},{"title":"terminal prompt disabled on vim","slug":"terminal-prompt-disabled-on-vim","date":"2019-01-13T14:48:42.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/01/13/terminal-prompt-disabled-on-vim/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/13/terminal-prompt-disabled-on-vim/","excerpt":"","text":"参考 メモ 参考 CODE Q&amp;Aの質疑応答 deol.vim メモ gvimで deol.vim を使ってターミナルを開き、git push origin master をしたら、 以下のようなエラーが生じた。 1fatal: could not read Password for &apos;https://xxxxxxxxx@github.com&apos;: terminal prompts disabled CODE Q&amp;Aの質疑応答 にも記載があるが 1$ GIT_TERMINAL_PROMPT=1 git push origin master のように、GIT_TERMINAL_PROMPT=1でOK。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"vim","slug":"Knowledge-Management/vim","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"},{"name":"git","slug":"git","permalink":"https://dobachi.github.io/memo-blog/tags/git/"}]},{"title":"Jupyter with Spark","slug":"Jupyter-with-Spark","date":"2019-01-10T15:18:29.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2019/01/11/Jupyter-with-Spark/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/11/Jupyter-with-Spark/","excerpt":"","text":"参考 メモ 参考 Sparkのドキュメント（Configuration） メモ よく忘れると思われる、Jupyterをクライアントとしての起動方法をメモ。 Sparkのドキュメント（Environment Variables） の記載の通り、環境変数PYSPARK_DRIVER_PYTHONを使い、 ドライバが用いるPythonを指定する。 GitHub上のpysparkの実装#27 の通り、環境変数PYSPARK_DRIVER_PYTHON_OPTSを使い、 オプションを指定する。 例 1PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=&apos;notebook&apos; ~/Spark/default/bin/pyspark ちなみに、ガイドにも一応記載されている。 https://github.com/apache/spark/blob/9ccae0c9e7d1a0a704e8cd7574ba508419e05e30/docs/rdd-programming-guide.md#using-the-shell","categories":[],"tags":[]},{"title":"気になるカンファレンスのメモ2019年版(WIP)","slug":"Conferences-in-2019","date":"2019-01-04T15:50:52.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2019/01/05/Conferences-in-2019/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/05/Conferences-in-2019/","excerpt":"","text":"まとめサイト HPC Japan List of Machine Learning / Deep Learning conferences in 2019 The 16 AI and ML conferences you should attend in 2019 IEEE関連 IEEE Congress 2019 The 6th IEEE International Conference on Big Data and Smart Computing IEEE 2019 Summit On Future Technology For Smart Cities The IEEE International Conference on Cloud Engineering (IC2E) The 5th IEEE International Conference on Cloud and Big Data Computing (CBDCom 2019) USENIX 2019 USENIX Conference on Operational Machine Learning 2019 USENIX Annual Technical Conference 2019 USENIX Conference on Operational Machine Learning O'Reilly Strata Strata San Francisco 2019 Strata London 2019 Strata New York 2019 AI Conference AI Conference New York 2019 AI Conference San Jose 2019 VLDB VLDB 2019 情報処理学会 電子情報通信学会 IBISML LOIS SC 機械学習工学研究会（MLSE） その他 SysML 2019 AI for Government Summit MLCONF NEW YORK AI World Conference &amp; Expo Global Artificial Intelligence Conference Applied Artificial Intelligence Conference AI Toronto/Big Data Toronto ちゃんとしたまとめは後で実施するつもり。 まとめサイト HPC Japan http://www.hpcwire.jp/events List of Machine Learning / Deep Learning conferences in 2019 https://tryolabs.com/blog/machine-learning-deep-learning-conferences/ The 16 AI and ML conferences you should attend in 2019 https://www.hpe.com/us/en/insights/articles/the-16-ai-and-ml-conferences-you-should-attend-in-2019-1811.html IEEE関連 IEEE Congress 2019 http://conferences.computer.org/bigdatacongress/2019/ JULY 8-13, 2019, MILAN, ITALY 2019 International Congress on Big Data (BigData Congress 2019) aims to provide an international forum that formally explores various business insights of all kinds of value-added “services.” Big Data is a key enabler of exploring business insights and economics of services. The 6th IEEE International Conference on Big Data and Smart Computing February 27th - March 2nd, 2019 Kyoto, JAPAN http://www.bigcomputing.org/ IEEE 2019 Summit On Future Technology For Smart Cities April 6, 2019 San Francisco East Bay, California, USA http://www.big-dataservice.net/html/summit.html The IEEE International Conference on Cloud Engineering (IC2E) June 24-27, 2019, Prague, Czech Republic http://conferences.computer.org/IC2E/2019/index.htm http://www.hpcwire.jp/event/international-conference-on-cloud-engineering-ic2e-2019 The 5th IEEE International Conference on Cloud and Big Data Computing (CBDCom 2019) August 5-8 2019, Fukuoka, Japan http://cyber-science.org/2019/cbdcom/ USENIX 2019 USENIX Conference on Operational Machine Learning MAY 20, 2019 SANTA CLARA, CA, USA https://www.usenix.org/conference/opml19 2019 USENIX Annual Technical Conference JULY 10–12, 2019 RENTON, WA, USA https://www.usenix.org/conference/atc19 2019 USENIX Conference on Operational Machine Learning MAY 20, 2019 SANTA CLARA, CA, USA Papers and talk proposals due Wednesday, February 13, 2019 https://www.usenix.org/conference/opml19 O'Reilly Strata Strata San Francisco 2019 https://conferences.oreilly.com/strata/strata-ca March 25-28, 2019 San Francisco, CA Strata London 2019 https://conferences.oreilly.com/strata/strata-eu 29 April–2 May 2019 London, UK Strata New York 2019 https://conferences.oreilly.com/strata/strata-ny Sep 23-26, 2019 New York, NY AI Conference https://conferences.oreilly.com/artificial-intelligence AI Conference New York 2019 April 15-18, 2019 New York, NY AI Conference San Jose 2019 Sep 9-12, 2019 San Jose, CA VLDB VLDB 2019 http://vldb.org/2019/ Los Angeles, California - August 26-30, 2019 情報処理学会 https://www.ipsj.or.jp/kenkyukai/sig-plan2018.html 電子情報通信学会 https://www.ieice.org/ken/program/index.php IBISML https://www.ieice.org/ken/form/index.php?tgs_regid=552bd5f74c86aa9e100ee3878422e7760e89f6c41f180eebb9271daee1030b58&amp;cmd=info&amp;lang= LOIS 2019年3月7日(木) - 3月8日(金) 宮古島市中央公民館 視聴覚室 ライフログ活用技術、オフィスインフォメーションシステム、ライフインテリジェンス、および一般 SC 開催日 2019-03-15 - 2019-03-15 会場 国立情報学研究所 発表申込締切日 2019-01-07 議題 「サービスと機械学習」および一般 機械学習工学研究会 ★重要 と共催 機械学習工学研究会（MLSE） https://mlxse.connpass.com/ その他 SysML 2019 March 31 - April 2, 2019 Stanford, CA (Registration opens: January 16, 2019) https://www.sysml.cc/ AI for Government Summit https://www.re-work.co/events/ai-for-government-summit-canada-2018 Toronto 25 - 26 OCTOBER 2018 MLCONF NEW YORK https://mlconf.com/events/mlconf-new-york-2019/ March 29, 2019 AI World Conference &amp; Expo https://aiworld.com/ October 23-25, 2019 Boston, MA Global Artificial Intelligence Conference http://www.globalbigdataconference.com/ January 6-11, 2019, Santa Clara, CA; Applied Artificial Intelligence Conference https://bootstraplabs.com/artificial-intelligence/applied-artificial-intelligence-conference-2019/ April 18, 2019 AI Toronto/Big Data Toronto https://ai-toronto.com/ June 12-13, 2019, Toronto","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Conference","slug":"Research/Conference","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Conference/"}],"tags":[{"name":"Conference","slug":"Conference","permalink":"https://dobachi.github.io/memo-blog/tags/Conference/"},{"name":"Big Data","slug":"Big-Data","permalink":"https://dobachi.github.io/memo-blog/tags/Big-Data/"},{"name":"Academia","slug":"Academia","permalink":"https://dobachi.github.io/memo-blog/tags/Academia/"}]},{"title":"WSL向けのターミナルツール","slug":"Terminal-tools-for-WSL","date":"2019-01-01T14:30:45.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2019/01/01/Terminal-tools-for-WSL/","link":"","permalink":"https://dobachi.github.io/memo-blog/2019/01/01/Terminal-tools-for-WSL/","excerpt":"","text":"参考 Hyper ConEmu cmder 参考 Hyper ConEmu cmder Hyper ウェブ界隈の技術を活用したターミナル。 Hyper からダウンロードできる。 インストールすると %USERPROFILE%\\.hyper.js に設定ファイルが作成される。 デフォルトはcmd.exeが起動されるようになっているので、WSLを使うように変更する。 123456789101112131415161718192021$ diff -u /mnt/c/Users/dobachi/.hyper.js&#123;.2018010101,&#125;--- /mnt/c/Users/dobachi/.hyper.js.2018010101 2019-01-01 23:26:47.691869000 +0900+++ /mnt/c/Users/dobachi/.hyper.js 2019-01-01 23:28:25.273185100 +0900@@ -9,7 +9,7 @@ updateChannel: &apos;stable&apos;, // default font size in pixels for all tabs- fontSize: 12,+ fontSize: 15, // font family with optional fallbacks fontFamily: &apos;Menlo, &quot;DejaVu Sans Mono&quot;, Consolas, &quot;Lucida Console&quot;, monospace&apos;,@@ -107,7 +107,7 @@ // for setting shell arguments (i.e. for using interactive shellArgs: `[&apos;-i&apos;]`) // by default `[&apos;--login&apos;]` will be used- shellArgs: [&apos;--login&apos;],+ shellArgs: [&apos;/C&apos;, &apos;wsl&apos;], // for environment variables env: &#123;&#125;, ConEmu ConEmu からダウンロードできる。 ただし、手元のGPD Pocket上のWSLで使用した時には、ctrl + lで画面クリアするときに 意図と異なる動作をした。 cmder cmder からダウンロードできる。 ただし、手元のGPD Pocket上のWSLで使用した時には、ctrl + lで画面クリアするときに 意図と異なる動作をした。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"Terminal tool","slug":"Knowledge-Management/WSL/Terminal-tool","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/Terminal-tool/"}],"tags":[{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"Hyper","slug":"Hyper","permalink":"https://dobachi.github.io/memo-blog/tags/Hyper/"}]},{"title":"Ubuntu上のgvimでフォントが重なる現象","slug":"Fonts-overlap-on-Vim-on-Ubuntu","date":"2018-12-30T15:01:08.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/12/31/Fonts-overlap-on-Vim-on-Ubuntu/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/31/Fonts-overlap-on-Vim-on-Ubuntu/","excerpt":"","text":"参考 メモ 参考 現象についての議論 メモ 現象についての議論 の通り、~/.vimrcに以下を追記して回避した。 1set ambiwidth=double","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"vim","slug":"Home-server/Ubuntu/vim","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"}]},{"title":"X Window on wsl for Ubuntu","slug":"X-window-on-wsl-for-Ubuntu","date":"2018-12-30T14:07:47.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/12/30/X-window-on-wsl-for-Ubuntu/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/30/X-window-on-wsl-for-Ubuntu/","excerpt":"","text":"参考 メモ Windows側 WSL側 最低限のインストール 日本語関連 例：GUI vimのインストール 参考 一通りの流れが説明されたブログ VcXsrvのダウンロード VcXsrvの自動起動設定 WSLを使ってWindowsでmikutterを動かす -&gt; こちらの方が良いかも お前らのWSLはそれじゃダメだ -&gt; こちらの方が良いかも Ubuntu 18.04 な WSL 上に日本語入力環境を構築する fcitxで作るWSL日本語開発環境 WSL で gnome-terminal を動かすお話 Xサーバーを自動起動しよう！ メモ Windows側 一通りの流れが説明されたブログ に記載の通り試した。 VcXsrvのダウンロードからダウンロードしたパッケージをインストールする。 なお、インストールすると「XLaunch」がスタートメニューに追加される。 これを都度起動することになるが、スタートアップメニューにショートカットを登録し、 自動起動するようにした。 （ Xサーバーを自動起動しよう！ を参考にする） WSL側 最低限のインストール 関連パッケージのインストール 1$ sudo apt install git build-essential libssl-dev libreadline-dev zlib1g-dev x11-apps x11-utils x11-xserver-utils libsqlite3-dev nodejs fonts-ipafont libxml2-dev libxslt1-dev ~/.bashrcに環境変数DISPLAYの値を追加 12345678--- /home/dobachi/.bashrc.2018123001 2018-12-30 23:21:48.626055900 +0900+++ /home/dobachi/.bashrc 2018-12-30 23:20:26.333894000 +0900@@ -115,3 +115,5 @@ . /etc/bash_completion fi fi++export DISPLAY=localhost:0.0 日本語関連 上記手順だけでは日本語入力環境が整わないので、 WSLを使ってWindowsでmikutterを動かす と お前らのWSLはそれじゃダメだ を参考に日本語環境を構築する。 まず、WSLを使ってWindowsでmikutterを動かす のとおり、uim-anthyをインストールし、 お前らのWSLはそれじゃダメだ のとおり、~/.uimを設定する。 結局は、お前らのWSLはそれじゃダメだのとおりで良さそうではある。 1234567891011$ sudo apt install language-pack-ja$ sudo update-locale LANG=ja_JP.UTF-8$ sudo apt install -y uim uim-xim uim-fep uim-anthy dbus-x11$ cat &lt;&lt; EOF &gt; ~/.uim (define default-im-name &apos;anthy) (define-key generic-on-key? &apos;(&quot;&lt;Control&gt; &quot;)) (define-key generic-off-key? &apos;(&quot;&lt;Control&gt; &quot;)) EOF$ cat &lt;&lt; EOF &gt;&gt; ~/.profile export GTK_IM_MODULE=uim EOF 例：GUI vimのインストール 使いたいツールをインストール。 $ sudo apt install vim-gui-common (snip) $ gvim","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"WSL","slug":"Knowledge-Management/WSL","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/"},{"name":"X Window","slug":"Knowledge-Management/WSL/X-Window","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/WSL/X-Window/"}],"tags":[{"name":"WSL","slug":"WSL","permalink":"https://dobachi.github.io/memo-blog/tags/WSL/"},{"name":"X Window","slug":"X-Window","permalink":"https://dobachi.github.io/memo-blog/tags/X-Window/"}]},{"title":"GPD PocketでBluetoothトラックボールを使うと途中で使用できなくなる現象","slug":"GPD-Pocket-unexpected-behavior-of-Bluetooth-mouse","date":"2018-12-30T13:49:29.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/12/30/GPD-Pocket-unexpected-behavior-of-Bluetooth-mouse/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/30/GPD-Pocket-unexpected-behavior-of-Bluetooth-mouse/","excerpt":"","text":"参考 当たった症状 対処 備考：ブログとの差分 参考 関連情報を含むブログ記事 当たった症状 Digio2のBluetoothトラックボールを使っていたところ、唐突に使用できなくなる現象にあたった。 そのとき、Windowsの設定画面「Bluetoothとその他のデバイス」を開いたところ、 当該トラックボールは「接続済み」と表示されていた。 デバイスの削除と再度追加、ドライバの更新（結局最新版だった）を試みるも現象は収まらなかった。 対処 関連情報を含むブログ記事 で記載の通り、デバイスマネージャから節電関係の機能を オフにすることで現象は解消したように見える。 おおむね上記ブログのとおりだが、一部デバイス名に差分があったので、 手元で試した方法を記載する。 Windowsのランチャーからデバイスマネージャを開く 「ヒューマン インターフェイス デバイス」-&gt; 「Bluetooth HID デバイス」のプロパティを開く。 「電源の管理」を開き、「電力の節電のために…（省略）」のチェックボックスを外す なお、手元の環境では当該デバイスは2件表示されたので両方とも対応した おなじくデバイスマネージャ上で「bluetooth」-&gt;「Bluetooth 無線」のプロパティを開く。 「電源の管理」を開き、「電力の節電のために…（省略）」のチェックボックスを外す 備考：ブログとの差分 なお、関連情報を含むブログ記事では、「Bluetooth HID デバイス」のところが 「Bluetooth 低エネルギーGTT 対応 HID デバイス」となっていたようだ。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"GPD Pocket","slug":"Knowledge-Management/GPD-Pocket","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/GPD-Pocket/"},{"name":"Device","slug":"Knowledge-Management/GPD-Pocket/Device","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/"},{"name":"Bluetooth","slug":"Knowledge-Management/GPD-Pocket/Device/Bluetooth","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/GPD-Pocket/Device/Bluetooth/"}],"tags":[{"name":"GPD Pocket","slug":"GPD-Pocket","permalink":"https://dobachi.github.io/memo-blog/tags/GPD-Pocket/"},{"name":"Bluetooth","slug":"Bluetooth","permalink":"https://dobachi.github.io/memo-blog/tags/Bluetooth/"},{"name":"Mouse","slug":"Mouse","permalink":"https://dobachi.github.io/memo-blog/tags/Mouse/"}]},{"title":"tmuxでctrl + lを押したときに改行の挙動がおかしくなる現象","slug":"Unexpected-behaviour-of-tmux-with-ctrl-l","date":"2018-12-30T13:26:21.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/12/30/Unexpected-behaviour-of-tmux-with-ctrl-l/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/30/Unexpected-behaviour-of-tmux-with-ctrl-l/","excerpt":"","text":"参考 メモ 参考 tmux-1419 メモ tmux-1419 に記載の通り、~/.tmux.confに以下のように記載すると、 強制的に改行されるようになるようだ。 1set -as terminal-overrides &apos;,*:indn@&apos;","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"tmux","slug":"Knowledge-Management/Tools/tmux","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/tmux/"}],"tags":[{"name":"tmux","slug":"tmux","permalink":"https://dobachi.github.io/memo-blog/tags/tmux/"}]},{"title":"Basic of SQLAlchemy","slug":"Basic-of-SQLAlchemy","date":"2018-12-24T13:54:17.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/12/24/Basic-of-SQLAlchemy/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/24/Basic-of-SQLAlchemy/","excerpt":"","text":"参考 簡単な動作確認 挿入のクエリ 接続の取得と実行 複数ステートメントの実行 SELECT オペレータの結合 テキストで実行 グループ 結合 変数バインド その他の機能 Update 参考 SQLAlchemy公式のチュートリアル 簡単な動作確認 エンジンの定義。（今回はSQLiteのインメモリモードを使用） 12from sqlalchemy import create_engineengine = create_engine(&apos;sqlite:///:memory:&apos;, echo=True) テーブルの作成。今回は2種類のテーブルを作成。 ユーザのIDをプライマリキーとして使用。 12345678910111213from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKeymetadata = MetaData()users = Table(&apos;users&apos;, metadata, Column(&apos;id&apos;, Integer, primary_key=True), Column(&apos;name&apos;, String), Column(&apos;fullname&apos;, String),)addresses = Table(&apos;addresses&apos;, metadata, Column(&apos;id&apos;, Integer, primary_key=True), Column(&apos;user_id&apos;, None, ForeignKey(&apos;users.id&apos;)), Column(&apos;email_address&apos;, String, nullable=False) ) 実際にSQLite内でテーブルを定義する。このとき、テーブルの存在を確認するので複数回実行してもよい。（べき等である） 1metadata.create_all(engine) 挿入のクエリ まずは空のクエリを定義。 1ins = users.insert() strを使うと、SQLを確認できる。 123str(ins)--&apos;INSERT INTO users (id, name, fullname) VALUES (:id, :name, :fullname)&apos; 今度は値を入れてみる。 1234ins = users.insert().values(name=&apos;jack&apos;, fullname=&apos;Jack Jones&apos;)str(ins)--&apos;INSERT INTO users (name, fullname) VALUES (:name, :fullname)&apos; 接続の取得と実行 接続の取得 1conn = engine.connect() 実行 12345result = conn.execute(ins)--2018-12-24 14:08:34,031 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name, fullname) VALUES (?, ?)2018-12-24 14:08:34,031 INFO sqlalchemy.engine.base.Engine (&apos;jack&apos;, &apos;Jack Jones&apos;)2018-12-24 14:08:34,032 INFO sqlalchemy.engine.base.Engine COMMIT 挿入されたレコードのプライマリキーの確認。 123&gt;&gt;&gt; result.inserted_primary_key--[1] 複数ステートメントの実行 先の例と同じ1個ずつ挿入する場合。 1234567ins = users.insert()conn.execute(ins, id=2, name=&apos;wendy&apos;, fullname=&apos;Wendy Williams&apos;)--2018-12-24 14:12:55,688 INFO sqlalchemy.engine.base.Engine INSERT INTO users (id, name, fullname) VALUES (?, ?, ?)2018-12-24 14:12:55,688 INFO sqlalchemy.engine.base.Engine (2, &apos;wendy&apos;, &apos;Wendy Williams&apos;)2018-12-24 14:12:55,688 INFO sqlalchemy.engine.base.Engine COMMIT&lt;sqlalchemy.engine.result.ResultProxy object at 0x7f63c8ca7940&gt; 複数ステートメントを一度に実行する場合。 1234567891011conn.execute(addresses.insert(), [ &#123;&apos;user_id&apos;: 1, &apos;email_address&apos; : &apos;jack@yahoo.com&apos;&#125;, &#123;&apos;user_id&apos;: 1, &apos;email_address&apos; : &apos;jack@msn.com&apos;&#125;, &#123;&apos;user_id&apos;: 2, &apos;email_address&apos; : &apos;www@www.org&apos;&#125;, &#123;&apos;user_id&apos;: 2, &apos;email_address&apos; : &apos;wendy@aol.com&apos;&#125;,])--2018-12-24 14:13:42,077 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (user_id, email_address) VALUES (?, ?)2018-12-24 14:13:42,078 INFO sqlalchemy.engine.base.Engine ((1, &apos;jack@yahoo.com&apos;), (1, &apos;jack@msn.com&apos;), (2, &apos;www@www.org&apos;), (2, &apos;wendy@aol.com&apos;))2018-12-24 14:13:42,078 INFO sqlalchemy.engine.base.Engine COMMIT&lt;sqlalchemy.engine.result.ResultProxy object at 0x7f63c8ca7c18&gt; SELECT まずはクエリを実行。 1234567from sqlalchemy.sql import selects = select([users])result = conn.execute(s)--2018-12-24 14:16:11,363 INFO sqlalchemy.engine.base.Engine SELECT users.id, users.name, users.fullname FROM users2018-12-24 14:16:11,363 INFO sqlalchemy.engine.base.Engine () 結果のパース。タプルライクに取得。 12345for row in result: print(row)--(1, &apos;jack&apos;, &apos;Jack Jones&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;) 辞書ライクに取得。 12345result = conn.execute(s)row = result.fetchone()print(&quot;name:&quot;, row[&apos;name&apos;], &quot;; fullname:&quot;, row[&apos;fullname&apos;])--name: jack ; fullname: Jack Jones conn.execute()の結果はイテレータっぽく扱えるので以下のような書き方ができる。 今回は2個のテーブルのカルテシアン積を求める例。 1234567891011121314for row in conn.execute(select([users, addresses])): print(row)--2018-12-24 14:20:14,164 INFO sqlalchemy.engine.base.Engine SELECT users.id, users.name, users.fullname, addresses.id, addresses.user_id, addresses.email_address FROM users, addresses2018-12-24 14:20:14,164 INFO sqlalchemy.engine.base.Engine ()(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 1, 1, &apos;jack@yahoo.com&apos;)(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 2, 1, &apos;jack@msn.com&apos;)(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 3, 2, &apos;www@www.org&apos;)(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 4, 2, &apos;wendy@aol.com&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;, 1, 1, &apos;jack@yahoo.com&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;, 2, 1, &apos;jack@msn.com&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;, 3, 2, &apos;www@www.org&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;, 4, 2, &apos;wendy@aol.com&apos;) どちらかというと、こちらの方がイメージに近いか。 1234567891011for row in conn.execute(select([users, addresses]).where(users.c.id == addresses.c.user_id)): print(row)--2018-12-24 14:22:46,769 INFO sqlalchemy.engine.base.Engine SELECT users.id, users.name, users.fullname, addresses.id, addresses.user_id, addresses.email_address FROM users, addresses WHERE users.id = addresses.user_id2018-12-24 14:22:46,769 INFO sqlalchemy.engine.base.Engine ()(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 1, 1, &apos;jack@yahoo.com&apos;)(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 2, 1, &apos;jack@msn.com&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;, 3, 2, &apos;www@www.org&apos;)(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;, 4, 2, &apos;wendy@aol.com&apos;) オペレータの結合 and_などを用いる例。 123456789101112131415from sqlalchemy.sql import and_, or_, not_s = select([(users.c.fullname + &quot;, &quot; + addresses.c.email_address). label(&apos;title&apos;)]).\\ where( and_( users.c.id == addresses.c.user_id, users.c.name.between(&apos;m&apos;, &apos;z&apos;), or_( addresses.c.email_address.like(&apos;%@aol.com&apos;), addresses.c.email_address.like(&apos;%@msn.com&apos;) ) ) )conn.execute(s).fetchall() Pythonの演算子を用いる例。 123456789101112s = select([(users.c.fullname + &quot;, &quot; + addresses.c.email_address). label(&apos;title&apos;)]).\\ where( (users.c.id == addresses.c.user_id) &amp; (users.c.name.between(&apos;m&apos;, &apos;z&apos;)) &amp; ( addresses.c.email_address.like(&apos;%@aol.com&apos;) | \\ addresses.c.email_address.like(&apos;%@msn.com&apos;) ) )conn.execute(s).fetchall() and_はwhere句を並べることでも実現できる。 123456789101112s = select([(users.c.fullname + &quot;, &quot; + addresses.c.email_address). label(&apos;title&apos;)]).\\ where(users.c.id == addresses.c.user_id).\\ where(users.c.name.between(&apos;m&apos;, &apos;z&apos;)).\\ where( or_( addresses.c.email_address.like(&apos;%@aol.com&apos;), addresses.c.email_address.like(&apos;%@msn.com&apos;) ) )conn.execute(s).fetchall() テキストで実行 基本的な使い方 123456789from sqlalchemy.sql import texts = text( &quot;SELECT users.fullname || &apos;, &apos; || addresses.email_address AS title &quot; &quot;FROM users, addresses &quot; &quot;WHERE users.id = addresses.user_id &quot; &quot;AND users.name BETWEEN :x AND :y &quot; &quot;AND (addresses.email_address LIKE :e1 &quot; &quot;OR addresses.email_address LIKE :e2)&quot;)conn.execute(s, x=&apos;m&apos;, y=&apos;z&apos;, e1=&apos;%@aol.com&apos;, e2=&apos;%@msn.com&apos;).fetchall() 予めバインドされた値を予め定義することもできる。 1234567stmt = text(&quot;SELECT * FROM users WHERE users.name BETWEEN :x AND :y&quot;)stmt = stmt.bindparams(x=&quot;m&quot;, y=&quot;z&quot;)# stmt = stmt.bindparams(bindparam(&quot;x&quot;, type_=String), bindparam(&quot;y&quot;, type_=String))result = conn.execute(stmt, &#123;&quot;x&quot;: &quot;m&quot;, &quot;y&quot;: &quot;z&quot;&#125;)result.fetchall()--[(2, &apos;wendy&apos;, &apos;Wendy Williams&apos;)] ステートメントの中でSQLテキストを用いることもできる。 12345678910111213s = select([ text(&quot;users.fullname || &apos;, &apos; || addresses.email_address AS title&quot;) ]).\\ where( and_( text(&quot;users.id = addresses.user_id&quot;), text(&quot;users.name BETWEEN &apos;m&apos; AND &apos;z&apos;&quot;), text( &quot;(addresses.email_address LIKE :x &quot; &quot;OR addresses.email_address LIKE :y)&quot;) ) ).select_from(text(&apos;users, addresses&apos;))conn.execute(s, x=&apos;%@aol.com&apos;, y=&apos;%@msn.com&apos;).fetchall() グループ 単純な例 1234567from sqlalchemy import funcstmt = select([ addresses.c.user_id, func.count(addresses.c.id).label(&apos;num_addresses&apos;)]).\\ group_by(&quot;user_id&quot;).order_by(&quot;user_id&quot;, &quot;num_addresses&quot;)conn.execute(stmt).fetchall() 結合 単純な例 1print(users.join(addresses)) いろいろな表現を用いることができる。 1234print(users.join(addresses, addresses.c.email_address.like(users.c.name + &apos;%&apos;) )) 実施に実行 12345678910s = select([users.c.fullname]).select_from( users.join(addresses, addresses.c.email_address.like(users.c.name + &apos;%&apos;)) )conn.execute(s).fetchall()--2018-12-25 12:45:29,919 INFO sqlalchemy.engine.base.Engine SELECT users.fullname FROM users JOIN addresses ON addresses.email_address LIKE users.name || ?2018-12-25 12:45:29,919 INFO sqlalchemy.engine.base.Engine (&apos;%&apos;,)[(&apos;Jack Jones&apos;,), (&apos;Jack Jones&apos;,), (&apos;Wendy Williams&apos;,)] outer join 1234print(select([users.c.fullname]).select_from(users.outerjoin(addresses)))--SELECT users.fullname FROM users LEFT OUTER JOIN addresses ON users.id = addresses.user_id 変数バインド 単純な例 12345678from sqlalchemy.sql import bindparams = users.select(users.c.name.like(bindparam(&apos;username&apos;, type_=String) + text(&quot;&apos;%&apos;&quot;)))conn.execute(s, username=&apos;wendy&apos;).fetchall()--2018-12-25 13:34:19,392 INFO sqlalchemy.engine.base.Engine SELECT users.fullname FROM users LEFT OUTER JOIN addresses ON users.id = addresses.user_id2018-12-25 13:34:19,392 INFO sqlalchemy.engine.base.Engine ()[(&apos;Jack Jones&apos;,), (&apos;Jack Jones&apos;,), (&apos;Wendy Williams&apos;,), (&apos;Wendy Williams&apos;,)] バインドした変数は複数回使用できる。 123456789101112131415161718s = select([users, addresses]).\\ where( or_( users.c.name.like( bindparam(&apos;name&apos;, type_=String) + text(&quot;&apos;%&apos;&quot;)), addresses.c.email_address.like( bindparam(&apos;name&apos;, type_=String) + text(&quot;&apos;@%&apos;&quot;)) ) ).\\ select_from(users.outerjoin(addresses)).\\ order_by(addresses.c.id)conn.execute(s, name=&apos;jack&apos;).fetchall()--2018-12-25 13:43:43,880 INFO sqlalchemy.engine.base.Engine SELECT users.id, users.name, users.fullname, addresses.id, addresses.user_id, addresses.email_address FROM users LEFT OUTER JOIN addresses ON users.id = addresses.user_id WHERE users.name LIKE ? || &apos;%&apos; OR addresses.email_address LIKE ? || &apos;@%&apos; ORDER BY addresses.id2018-12-25 13:43:43,880 INFO sqlalchemy.engine.base.Engine (&apos;jack&apos;, &apos;jack&apos;)[(1, &apos;jack&apos;, &apos;Jack Jones&apos;, 1, 1, &apos;jack@yahoo.com&apos;), (1, &apos;jack&apos;, &apos;Jack Jones&apos;, 2, 1, &apos;jack@msn.com&apos;)] その他の機能 ファンクション ウィンドウファンクション union、except_ Label Ordering, Limiting, ... Update 12345stmt = users.update().\\ where(users.c.name == &apos;jack&apos;).\\ values(name=&apos;ed&apos;)conn.execute(stmt) 1234567891011121314stmt = users.update().\\ where(users.c.name == bindparam(&apos;oldname&apos;)).\\ values(name=bindparam(&apos;newname&apos;))conn.execute(stmt, [ &#123;&apos;oldname&apos;:&apos;jack&apos;, &apos;newname&apos;:&apos;ed&apos;&#125;, &#123;&apos;oldname&apos;:&apos;wendy&apos;, &apos;newname&apos;:&apos;mary&apos;&#125;, &#123;&apos;oldname&apos;:&apos;jim&apos;, &apos;newname&apos;:&apos;jake&apos;&#125;, ])conn.execute(select([users])).fetchall()--2018-12-25 15:11:08,628 INFO sqlalchemy.engine.base.Engine SELECT users.id, users.name, users.fullname FROM users2018-12-25 15:11:08,628 INFO sqlalchemy.engine.base.Engine ()[(1, &apos;ed&apos;, &apos;Jack Jones&apos;), (2, &apos;mary&apos;, &apos;Wendy Williams&apos;)]","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"SQLAlchemy","slug":"Knowledge-Management/SQLAlchemy","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/SQLAlchemy/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"https://dobachi.github.io/memo-blog/tags/SQLAlchemy/"},{"name":"SQLite","slug":"SQLite","permalink":"https://dobachi.github.io/memo-blog/tags/SQLite/"}]},{"title":"Flask SQLAlchemy","slug":"Flask-SQLAlchemy","date":"2018-12-23T14:29:57.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/12/23/Flask-SQLAlchemy/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/23/Flask-SQLAlchemy/","excerpt":"","text":"参考 シェルで動作確認 Dockerコンテナのビルド アプリ作成と実行 アプリから動作確認 アプリ作成 実行 参考 flask-sqlalchemyのクイックスタート Flask on Dockerのブログ シェルで動作確認 Dockerコンテナのビルド Flask on Dockerのブログ と同様にDockerfileを作り、ビルドする。 ファイルを作成 12345678910$ mkdir -p ~/Sources/docker_flask/sqlalchemy$ cd ~/Sources/docker_flask/sqlalchemy$ cat &lt;&lt; EOF &gt; Dockerfile FROM ubuntu:latest RUN apt-get update RUN apt-get install python3 python3-pip -y RUN pip3 install flask flask_sqlalchemy EOF ビルド 1$ sudo docker build . -t dobachi/flask_sqlalchemy:1.0 アプリ作成と実行 1234567891011121314151617$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config[&apos;SQLALCHEMY_DATABASE_URI&apos;] = &apos;sqlite:////tmp/test.db&apos; db = SQLAlchemy(app) class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return &apos;&lt;User %r&gt;&apos; % self.username EOF シェルを起動する。 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask_sqlalchemy:1.0 /bin/bash シェル内で動作確認。テーブルを作ってレコードを生成。 1234567891011121314&gt;&gt;&gt; from app import db/usr/local/lib/python3.6/dist-packages/flask_sqlalchemy/__init__.py:794: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future. Set it to True or False to suppress this warning. &apos;SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and &apos;&gt;&gt;&gt; db.create_all()&gt;&gt;&gt; from app import User&gt;&gt;&gt; admin = User(username=&apos;admin&apos;, email=&apos;admin@example.com&apos;)&gt;&gt;&gt; guest = User(username=&apos;guest&apos;, email=&apos;guest@example.com&apos;)&gt;&gt;&gt; db.session.add(admin)&gt;&gt;&gt; db.session.add(guest)&gt;&gt;&gt; db.session.commit()&gt;&gt;&gt; User.query.all()[&lt;User &apos;admin&apos;&gt;, &lt;User &apos;guest&apos;&gt;]&gt;&gt;&gt; User.query.filter_by(username=&apos;admin&apos;).first()&lt;User &apos;admin&apos;&gt; sqlite3コマンドで接続して内容を確認。 123456789root@a5d5ee81df78:/usr/local/apps# apt install sqlite3root@a5d5ee81df78:/usr/local/apps# sqlite3 /tmp/test.dbsqlite&gt; .databasesmain: /tmp/test.dbsqlite&gt; .tablesusersqlite&gt; select * from user;1|admin|admin@example.com2|guest|guest@example.com アプリから動作確認 上記と同様の流れをアプリから動作確認する。 Dockerイメージは上記で作ったものをベースとする。 アプリ作成 ここでは/registにアクセスすると、登録画面が出て、 登録するとリスト（/list）が表示されるアプリを作成する。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879$ mkdir -p ~/Sources/docker_flask/sqlalchemy_app$ cd ~/Sources/docker_flask/sqlalchemy_app$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask, url_for, request, render_template, redirect from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config[&apos;SQLALCHEMY_DATABASE_URI&apos;] = &apos;sqlite:////tmp/test.db&apos; db = SQLAlchemy(app) class User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return &apos;&lt;User %r&gt;&apos; % self.username db.create_all() @app.route(&apos;/regist&apos;, methods=[&apos;POST&apos;, &apos;GET&apos;]) def regist(): if request.method == &apos;POST&apos;: user = User(username=request.form[&apos;username&apos;], email=request.form[&apos;email&apos;]) db.session.add(user) db.session.commit() return redirect(url_for(&apos;list&apos;)) # the code below is executed if the request method # was GET or the credentials were invalid return render_template(&apos;regist.html&apos;) @app.route(&apos;/list&apos;) def list(): users = User.query.order_by(User.username).all() return render_template(&apos;list.html&apos;, users=users) EOF$ mkdir -p apps/templates$ cat &lt;&lt; EOF &gt; apps/templates/regist.html &lt;!doctype html&gt; &lt;title&gt;Registration&lt;/title&gt; &lt;form action=&quot;/regist&quot; method=&quot;post&quot;&gt; &lt;div&gt;username: &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;/div&gt; &lt;div&gt;email: &lt;input type=&quot;text&quot; name=&quot;email&quot;&gt;&lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;send&quot;&gt; &lt;input type=&quot;reset&quot; value=&quot;reset&quot;&gt; &lt;/form&gt; &lt;a href=&quot;/list&quot;&gt;list&lt;/a&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/list.html &lt;!doctype html&gt; &lt;title&gt;List ordered by username&lt;/title&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;id&lt;/th&gt; &lt;th&gt;username&lt;/th&gt; &lt;th&gt;email&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for item in users %&#125; &lt;tr&gt; &lt;td&gt; &#123;&#123; item.id &#125;&#125; &lt;/td&gt; &lt;td&gt; &#123;&#123; item.username &#125;&#125; &lt;/td&gt; &lt;td&gt; &#123;&#123; item.email &#125;&#125; &lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt; &lt;/table&gt; &lt;a href=&quot;/regist&quot;&gt;registration&lt;/a&gt; EOF 実行 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask_sqlalchemy:1.0 flask run --host 0.0.0.0 なお、上記の実装はエラーハンドリング等を一切実施していないので 例えば同じユーザ名、メールアドレスで登録しようとすると例外が生じる。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Flask","slug":"Knowledge-Management/Flask","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Flask/"}],"tags":[{"name":"Flask","slug":"Flask","permalink":"https://dobachi.github.io/memo-blog/tags/Flask/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"https://dobachi.github.io/memo-blog/tags/SQLAlchemy/"}]},{"title":"tmux_rectangle_select","slug":"tmux-rectangle-select","date":"2018-12-23T13:58:52.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/12/23/tmux-rectangle-select/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/23/tmux-rectangle-select/","excerpt":"","text":"参考 方法 参考 矩形選択にも言及のあるブログ 方法 tmuxを使ってコピーするとき、矩形選択したいときがある。 矩形選択にも言及のあるブログ にも記載あるが、 &lt;prefix&gt; + [ でコピーモードに入った後、 該当箇所まで移動し、スペースキーを押して選択開始、その後 v を押すと矩形選択になる。 （初期状態では、行選択になっているはず）","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"tmux","slug":"Knowledge-Management/Tools/tmux","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/tmux/"}],"tags":[{"name":"tmux","slug":"tmux","permalink":"https://dobachi.github.io/memo-blog/tags/tmux/"}]},{"title":"Gitのgitignore","slug":"Git-ignore-file","date":"2018-12-23T13:10:41.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/12/23/Git-ignore-file/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/23/Git-ignore-file/","excerpt":"","text":"参考 手順 gitignoreファイルのダウンロード 参考 GitHubのgitignoreファイルの雛形 後からまとめてignoreする方法 手順 後からまとめてignoreする方法 の通りで問題ない。 gitignoreファイルのダウンロード GitHubのgitignoreファイルの雛形 からダウンロード。 Pythonの例 12$ cd &lt;レポジトリ&gt;$ wget https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore 必要に応じて既存の.gitignoreとマージしたり、mvして.gitignoreとしたり。 1$ mv Python.gitignore .gitignore コミット。 12$ git add .gitignore$ git commit -m &quot;Add ignore pattern&quot; すでにコミットしてしまったファイルがあればまとめて削除。 1$ git rm --cached `git ls-files --full-name -i --exclude-from=.gitignore`","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Tools","slug":"Knowledge-Management/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/"},{"name":"Git","slug":"Knowledge-Management/Tools/Git","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Tools/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://dobachi.github.io/memo-blog/tags/Git/"}]},{"title":"Flask extension","slug":"Flask-extension","date":"2018-12-23T12:56:25.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/12/23/Flask-extension/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/23/Flask-extension/","excerpt":"","text":"参考 flask_adminを試す Dockerfileを作成してビルド アプリ作成と実行 参考 Flask公式ウェブサイト flask-adminのドキュメント Flask on Dockerのブログ flask_adminを試す flask-adminのドキュメント を見ながら進める。 またFlask on Dockerのブログ で作成したDockerイメージを使って環境を作る。 Dockerfileを作成してビルド 12345678910$ mkdir -p ~/Sources/docker_flask/docker_admin$ cd ~/Sources/docker_flask/docker_admin$ cat &lt;&lt; EOF &gt; Dockerfile FROM ubuntu:latest RUN apt-get update RUN apt-get install python3 python3-pip -y RUN pip3 install flask flask_admin EOF 1$ sudo docker build . -t dobachi/flask_admin:1.0 アプリ作成と実行 1234567891011121314151617$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask from flask_admin import Admin app = Flask(__name__) # set optional bootswatch theme app.config[&apos;FLASK_ADMIN_SWATCH&apos;] = &apos;cerulean&apos; admin = Admin(app, name=&apos;microblog&apos;, template_mode=&apos;bootstrap3&apos;) @app.route(&apos;/&apos;) def index(): return &quot;Hello world!!&quot; EOF$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask_admin:1.0 flask run --host 0.0.0.0","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Flask","slug":"Knowledge-Management/Flask","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Flask/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"},{"name":"Flask","slug":"Flask","permalink":"https://dobachi.github.io/memo-blog/tags/Flask/"},{"name":"Dockerfile","slug":"Dockerfile","permalink":"https://dobachi.github.io/memo-blog/tags/Dockerfile/"}]},{"title":"Check global IP address","slug":"Check-global-IP-address","date":"2018-12-16T15:59:21.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/12/17/Check-global-IP-address/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/17/Check-global-IP-address/","excerpt":"","text":"参考 手順 参考 手順 1$ curl globalip.me","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Network","slug":"Home-server/Network","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Network/"}],"tags":[{"name":"Network","slug":"Network","permalink":"https://dobachi.github.io/memo-blog/tags/Network/"}]},{"title":"Flask on Docker","slug":"Flask-on-Docker","date":"2018-12-14T05:52:33.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/12/14/Flask-on-Docker/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/14/Flask-on-Docker/","excerpt":"","text":"参考 自前のイメージを作って動かす手順 Dockerfileとビルド アプリ作成と実行 公式手順に則ったシンプルな例 準備 実行 フォームを使う例 準備 実行 ファイルのアップロード 準備 実行 クッキー 準備 実行 ロギング 準備 実行 JSON形式でやり取りする例 Dockerfile アプリ 動作確認 nginxと組み合わせて動かす Dockerfile、アプリなど ビルド 実行 nginxと組あわせてSPA Dockerfileとアプリ ビルド 実行 構造的なプロジェクト アーカイブをダウンロード ビルド 実行 一部修正してみる flask + NginxでSSLを使用（ここがうまく動かない） アドホックな対応 gitHub: uwsgi-nginx-flask-dockerのアーキテクチャ Supervisord化について 参考 Flask公式 Flask込みのDockerイメージを作る方法のブログ DockerコンテナでFlaskを起動し, JSONデータのPOSTとGET GitHub: FlaskOnDockerExamples GitHub: uwsgi-nginx-flask-docker GitHub: uwsgi-nginx-flask-dockerのクイックスタート GitHub: uwsgi-nginx-flask-dockerのSPA GitHub: 構造的なプロジェクトの例 example-flask-package-python3.7.zip Flask + NginxでのSSL対応に関する記事 自前のイメージを作って動かす手順 基本的には Flask込みのDockerイメージを作る方法のブログ で記載された内容で問題ない。 自身の環境向けにアレンジは必要。 Dockerfileとビルド Dockerfileは Flask込みのDockerイメージを作る方法のブログ の通り。 12345678910$ mkdir -p ~/Sources/docker_flask/simple$ cd ~/Sources/docker_flask/simple$ cat &lt;&lt; EOF &gt; Dockerfile FROM ubuntu:latest RUN apt-get update RUN apt-get install python3 python3-pip -y RUN pip3 install flask EOF ビルドコマンドは自分の環境に合わせた。 1$ sudo docker build . -t dobachi/flask:1.0 インタラクティブにつなげる例も自分の環境に合わせた。 1$ docker run -it --rm dobachi/flask:1.0 /bin/bash アプリ作成と実行 自分の環境に合わせた。 Flask込みのDockerイメージを作る方法のブログ では、 /bin/bashを起動してから、python3 &lt;アプリ &gt;としていたが、 ここでは直接指定して実行することにした。 1234567891011$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask app = Flask(__name__) @app.route(&apos;/&apos;) def index(): return &quot;Hello world!!&quot; EOF$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask:1.0 flask run --host 0.0.0.0 なお、もともとの Flask込みのDockerイメージを作る方法のブログ では、以下のようにPythonコマンドから直接実行するように記載されていたが、 今回は Flask公式 に従い、FlaskのCLIを用いることとした。 1sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps dobachi/flask:1.0 python3 /usr/local/apps/app.py その際、手元の環境ではLANG等の事情により、いくつか環境変数を設定しないとエラーを吐いたので、 上記のようなコマンドになった。 公式手順に則ったシンプルな例 上記で作ったDockerイメージを流用する。 準備 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$ mkdir -p ~/Sources/docker_flask/getting_started$ cd ~/Sources/docker_flask/getting_started$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask, url_for, request, render_template app = Flask(__name__) @app.route(&apos;/&apos;) def index(): return &apos;index&apos; @app.route(&apos;/user/&lt;username&gt;&apos;) def profile(username): return &apos;&#123;&#125;\\&apos;s profile&apos;.format(username) @app.route(&apos;/post/&lt;int:post_id&gt;&apos;) def post(post_id): # show the post with the given id, the id is an integer return &apos;Post %d&apos; % post_id @app.route(&apos;/path/&lt;path:subpath&gt;&apos;) def subpath(subpath): # show the subpath after /path/ return &apos;Subpath %s&apos; % subpath @app.route(&apos;/projects/&apos;) def projects(): return &apos;The project page&apos; @app.route(&apos;/about&apos;) def about(): return &apos;The about page&apos; @app.route(&apos;/login&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;]) def login(): if request.method == &apos;POST&apos;: return do_the_login() else: return show_the_login_form() def do_the_login(): return &apos;Do log in&apos; def show_the_login_form(): return &apos;This is a login form&apos; @app.route(&apos;/hello/&apos;) @app.route(&apos;/hello/&lt;name&gt;&apos;) def hello(name=None): return render_template(&apos;hello.html&apos;, name=name) # Test for URLs with app.test_request_context(): print(url_for(&apos;index&apos;)) print(url_for(&apos;login&apos;)) print(url_for(&apos;login&apos;, next=&apos;/&apos;)) print(url_for(&apos;profile&apos;, username=&apos;John Doe&apos;)) EOF$ mkdir -p apps/templates$ cat &lt;&lt; EOF &gt; apps/templates/hello.html &lt;!doctype html&gt; &lt;title&gt;Hello from Flask&lt;/title&gt; &#123;% if name %&#125; &lt;h1&gt;Hello &#123;&#123; name &#125;&#125;!&lt;/h1&gt; &#123;% else %&#125; &lt;h1&gt;Hello, World!&lt;/h1&gt; &#123;% endif %&#125; EOF 実行 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask:1.0 flask run --host 0.0.0.0 フォームを使う例 上記で作ったDockerイメージを流用する。 準備 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657$ mkdir -p ~/Sources/docker_flask/form$ cd ~/Sources/docker_flask/form$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask, url_for, request, render_template, redirect, session app = Flask(__name__) # Set the secret key to some random bytes. Keep this really secret! app.secret_key = b&apos;_5#y2L&quot;F4Q8z\\n\\xec]/&apos; @app.route(&apos;/login&apos;, methods=[&apos;POST&apos;, &apos;GET&apos;]) def login(): error = None if request.method == &apos;POST&apos;: if valid_login(request.form[&apos;username&apos;], request.form[&apos;password&apos;]): session[&apos;username&apos;] = request.form[&apos;username&apos;] return redirect(url_for(&apos;welcome&apos;)) else: error = &apos;Invalid username/password&apos; # the code below is executed if the request method # was GET or the credentials were invalid return render_template(&apos;login.html&apos;, error=error) @app.route(&apos;/welcome&apos;) def welcome(): if &apos;username&apos; in session: return render_template(&apos;welcome.html&apos;, username=session[&apos;username&apos;]) else: return redirect(url_for(&apos;login&apos;)) def valid_login(username, password): if username == &apos;hoge&apos; and password == &apos;fuga&apos;: return True else: return False EOF$ mkdir -p apps/templates$ cat &lt;&lt; EOF &gt; apps/templates/welcome.html &lt;!doctype html&gt; &lt;title&gt;Hello from Flask&lt;/title&gt; &lt;h1&gt;Hello &#123;&#123; username &#125;&#125;!&lt;/h1&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/login.html &lt;!doctype html&gt; &lt;title&gt;Question from Flask&lt;/title&gt; &#123;% if error %&#125; &lt;h1&gt;Please use valid username and password&lt;/h1&gt; &#123;% endif %&#125; &lt;form action=&quot;/login&quot; method=&quot;post&quot;&gt; &lt;div&gt;username: &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;/div&gt; &lt;div&gt;password: &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Send&quot;&gt; &lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt; &lt;/form&gt; EOF 実行 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask:1.0 flask run --host 0.0.0.0 ファイルのアップロード 上記で作ったDockerイメージを流用する。 準備 12345678910111213141516171819202122232425262728293031323334353637383940414243$ mkdir -p ~/Sources/docker_flask/file_upload$ cd ~/Sources/docker_flask/file_upload$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask, request, render_template, redirect, session, url_for from werkzeug.utils import secure_filename app = Flask(__name__) # Set the secret key to some random bytes. Keep this really secret! app.secret_key = b&apos;_5#y2L&quot;F4Q8z\\n\\xec]/&apos; @app.route(&apos;/upload&apos;, methods=[&apos;GET&apos;, &apos;POST&apos;]) def upload_file(): if request.method == &apos;POST&apos;: f = request.files[&apos;the_file&apos;] session[&apos;filename&apos;] = secure_filename(f.filename) f.save(&apos;/tmp/&apos; + session[&apos;filename&apos;]) return redirect(url_for(&apos;done&apos;)) elif request.method == &apos;GET&apos;: return render_template(&apos;upload.html&apos;) @app.route(&apos;/done&apos;) def done(): return render_template(&apos;done.html&apos;) EOF$ mkdir -p apps/templates$ cat &lt;&lt; EOF &gt; apps/templates/upload.html &lt;!doctype html&gt; &lt;title&gt;Fileuploader by Flask&lt;/title&gt; &lt;form action=&quot;/upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;div&gt;file: &lt;input type=&quot;file&quot; name=&quot;the_file&quot;&gt;&lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Send&quot;&gt; &lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt; &lt;/form&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/done.html &lt;!doctype html&gt; &lt;title&gt;Fileuploader by Flask&lt;/title&gt; &lt;h1&gt;The file: &#123;&#123; filename &#125;&#125; was uploaded&lt;/h1&gt; &lt;a href=&quot;/upload&quot;&gt;Got to the first page&lt;/a&gt; EOF 実行 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask:1.0 flask run --host 0.0.0.0 クッキー 上記で作ったDockerイメージを流用する。 準備 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182$ mkdir -p ~/Sources/docker_flask/cookie$ cd ~/Sources/docker_flask/cookie$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask, url_for, request, render_template, redirect, make_response app = Flask(__name__) # Set the secret key to some random bytes. Keep this really secret! app.secret_key = b&apos;_5#y2L&quot;F4Q8z\\n\\xec]/&apos; @app.route(&apos;/login&apos;, methods=[&apos;POST&apos;, &apos;GET&apos;]) def login(): error = None if request.method == &apos;POST&apos;: if valid_login(request.form[&apos;username&apos;], request.form[&apos;password&apos;]): username = request.form[&apos;username&apos;] resp = make_response(render_template(&apos;success.html&apos;)) resp.set_cookie(&apos;username&apos;, username) return resp else: error = &apos;Invalid username/password&apos; elif request.method == &apos;GET&apos;: if &apos;username&apos; in request.cookies: return redirect(url_for(&apos;welcome&apos;)) else: return render_template(&apos;login.html&apos;, error=error) @app.route(&apos;/logout&apos;) def logout(): resp = make_response(render_template(&apos;logout.html&apos;)) resp.delete_cookie(&apos;username&apos;) return resp @app.route(&apos;/welcome&apos;) def welcome(): if &apos;username&apos; in request.cookies: return render_template(&apos;welcome.html&apos;, username=request.cookies.get(&apos;username&apos;)) else: return redirect(url_for(&apos;login&apos;)) def valid_login(username, password): if username == &apos;hoge&apos; and password == &apos;fuga&apos;: return True else: return False EOF$ mkdir -p apps/templates$ cat &lt;&lt; EOF &gt; apps/templates/welcome.html &lt;!doctype html&gt; &lt;title&gt;Hello from Flask&lt;/title&gt; &lt;h1&gt;Hello &#123;&#123; username &#125;&#125;!&lt;/h1&gt; &lt;a href=&quot;/logout&quot;&gt;logout&lt;/a&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/success.html &lt;!doctype html&gt; &lt;title&gt;Login Success&lt;/title&gt; &lt;h1&gt;Login!&lt;/h1&gt; &lt;META http-equiv=&quot;Refresh&quot; content=&quot;3;URL=/welcome&quot;&gt; &lt;p&gt;Jump in 3 seconds&lt;/p&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/logout.html &lt;!doctype html&gt; &lt;title&gt;Logout Success&lt;/title&gt; &lt;h1&gt;Logout!&lt;/h1&gt; &lt;META http-equiv=&quot;Refresh&quot; content=&quot;3;URL=/login&quot;&gt; &lt;p&gt;Jump in 3 seconds&lt;/p&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/login.html &lt;!doctype html&gt; &lt;title&gt;Question from Flask&lt;/title&gt; &#123;% if error %&#125; &lt;h1&gt;Please use valid username and password&lt;/h1&gt; &#123;% endif %&#125; &lt;form action=&quot;/login&quot; method=&quot;post&quot;&gt; &lt;div&gt;username: &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;/div&gt; &lt;div&gt;password: &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Send&quot;&gt; &lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt; &lt;/form&gt; EOF 実行 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask:1.0 flask run --host 0.0.0.0 ロギング 上記で作ったDockerイメージを流用する。 準備 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758$ mkdir -p ~/Sources/docker_flask/logging$ cd ~/Sources/docker_flask/logging$ mkdir apps$ cat &lt;&lt; EOF &gt; apps/app.py from flask import Flask, url_for, request, render_template, redirect, session app = Flask(__name__) # Set the secret key to some random bytes. Keep this really secret! app.secret_key = b&apos;_5#y2L&quot;F4Q8z\\n\\xec]/&apos; @app.route(&apos;/login&apos;, methods=[&apos;POST&apos;, &apos;GET&apos;]) def login(): error = None if request.method == &apos;POST&apos;: if valid_login(request.form[&apos;username&apos;], request.form[&apos;password&apos;]): session[&apos;username&apos;] = request.form[&apos;username&apos;] return redirect(url_for(&apos;welcome&apos;)) else: app.logger.warning(&apos;Failed to login as:&apos; + request.form[&apos;username&apos;]) error = &apos;Invalid username/password&apos; # the code below is executed if the request method # was GET or the credentials were invalid return render_template(&apos;login.html&apos;, error=error) @app.route(&apos;/welcome&apos;) def welcome(): if &apos;username&apos; in session: return render_template(&apos;welcome.html&apos;, username=session[&apos;username&apos;]) else: return redirect(url_for(&apos;login&apos;)) def valid_login(username, password): if username == &apos;hoge&apos; and password == &apos;fuga&apos;: return True else: return False EOF$ mkdir -p apps/templates$ cat &lt;&lt; EOF &gt; apps/templates/welcome.html &lt;!doctype html&gt; &lt;title&gt;Hello from Flask&lt;/title&gt; &lt;h1&gt;Hello &#123;&#123; username &#125;&#125;!&lt;/h1&gt; EOF$ cat &lt;&lt; EOF &gt; apps/templates/login.html &lt;!doctype html&gt; &lt;title&gt;Question from Flask&lt;/title&gt; &#123;% if error %&#125; &lt;h1&gt;Please use valid username and password&lt;/h1&gt; &#123;% endif %&#125; &lt;form action=&quot;/login&quot; method=&quot;post&quot;&gt; &lt;div&gt;username: &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;/div&gt; &lt;div&gt;password: &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;Send&quot;&gt; &lt;input type=&quot;reset&quot; value=&quot;Reset&quot;&gt; &lt;/form&gt; EOF 実行 1$ sudo docker run -it --rm -p 5000:5000 -v $(pwd)/apps:/usr/local/apps -e &quot;FLASK_APP=app.py&quot; -w /usr/local/apps -e &quot;LC_ALL=C.UTF-8&quot; -e &quot;LANG=UTF-8&quot; dobachi/flask:1.0 flask run --host 0.0.0.0 JSON形式でやり取りする例 基本的には、 DockerコンテナでFlaskを起動し, JSONデータのPOSTとGET の通り。 Dockerfile dockerfileを作る。 1234567891011121314151617$ mkdir -p ~/Sources/docker_flask/post_json$ cd ~/Sources/docker_flask/post_json$ cat &lt;&lt; EOF &gt; Dockerfile fROM python:3.6 aRG project_dir=/app/ # ADD requirements.txt \\$project_dir aDD reply.py \\$project_dir wORKDIR \\$project_dir rUN pip install flask # RUN pip install -r requirements.txt cMD [&quot;python&quot;, &quot;reply.py&quot;] eOF アプリ アプリを作る。 1234567891011121314151617181920212223$ cat &lt;&lt; EOF &gt; reply.py from flask import Flask, jsonify, request import json app = Flask(__name__) @app.route(&quot;/&quot;, methods=[&apos;GET&apos;]) def hello(): return &quot;Hello World!&quot; @app.route(&apos;/reply&apos;, methods=[&apos;POST&apos;]) def reply(): data = json.loads(request.data) answer = &quot;Yes, it is %s!\\n&quot; % data[&quot;keyword&quot;] result = &#123; &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Answer&quot;:&#123;&quot;Text&quot;: answer&#125; &#125; # return answer return jsonify(result) if __name__ == &quot;__main__&quot;: app.run(host=&apos;0.0.0.0&apos;,port=5000,debug=True) eOF ビルドする。 1$ sudo docker build . -t dobachi/flask_json:1.0 動作確認 ポート5000で起動する。 1$ sudo docker run --rm -p 5000:5000 -it dobachi/flaskjson:1.0 動作確認する。 1$ curl http://localhost:5000/reply -X POST -H &quot;Content-Type: application/json&quot; -d &apos;&#123;&quot;keyword&quot;: &quot;Keffia&quot;&#125;&apos; nginxと組み合わせて動かす gitHub: uwsgi-nginx-flask-docker を参考にする。 gitHub: uwsgi-nginx-flask-dockerのクイックスタート を見ながら試す。 Dockerfile、アプリなど 12$ mkdir -p ~/Sources/docker_flask/nginx$ cd ~/Sources/docker_flask/nginx 123456789101112131415161718$ cat &lt;&lt; EOF &gt; Dockerfile fROM tiangolo/uwsgi-nginx-flask:python3.7 cOPY ./app /app eOF$ mkdir app$ cat &lt;&lt; EOF &gt; app/main.py from flask import Flask app = Flask(__name__) @app.route(&quot;/&quot;) def hello(): return &quot;Hello World from Flask&quot; if __name__ == &quot;__main__&quot;: # Only for debugging while developing app.run(host=&apos;0.0.0.0&apos;, debug=True, port=80) eOF ビルド 1$ sudo docker build -t dobachi/nginx-flask:1.0 . 実行 1$ sudo docker run --rm -it -p 8080:80 dobachi/nginx-flask:1.0 nginxと組あわせてSPA gitHub: uwsgi-nginx-flask-dockerのSPA を参考に進める。 Dockerfileとアプリ 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ mkdir -p ~/Sources/docker_flask/nginx_spa$ cd ~/Sources/docker_flask/nginx_spa$ cat &lt;&lt; EOF &gt; Dockerfile fROM tiangolo/uwsgi-nginx-flask:python3.7 eNV STATIC_INDEX 1 cOPY ./app /app eOF$ mkdir app$ cat &lt;&lt; EOF &gt; app/main.py import os from flask import Flask, send_file app = Flask(__name__) @app.route(&quot;/hello&quot;) def hello(): return &quot;Hello World from Flask&quot; @app.route(&quot;/&quot;) def main(): index_path = os.path.join(app.static_folder, &apos;index.html&apos;) return send_file(index_path) # Everything not declared before (not a Flask route / API endpoint)... @app.route(&apos;/&lt;path:path&gt;&apos;) def route_frontend(path): # ...could be a static file needed by the front end that # doesn&apos;t use the `static` path (like in `&lt;script src=&quot;bundle.js&quot;&gt;`) file_path = os.path.join(app.static_folder, path) if os.path.isfile(file_path): return send_file(file_path) # ...or should be handled by the SPA&apos;s &quot;router&quot; in front end else: index_path = os.path.join(app.static_folder, &apos;index.html&apos;) return send_file(index_path) if __name__ == &quot;__main__&quot;: # Only for debugging while developing app.run(host=&apos;0.0.0.0&apos;, debug=True, port=80) eOF 12345678910111213$ mkdir app/static$ cat &lt;&lt; EOF &gt; app/static/index.html &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Index&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello World from HTML&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; eOF ビルド 1$ sudo docker build -t dobachi/nginx-flask-spa:1.0 . 実行 1$ sudo docker run --rm -it -p 8080:80 dobachi/nginx-flask-spa:1.0 構造的なプロジェクト gitHub: 構造的なプロジェクトの例 に記載の例を示す。 example-flask-package-python3.7.zip をダウンロードしてビルドしてみる。 アーカイブをダウンロード 1234$ mkdir -p ~/Sources/docker_flask/$ wget https://github.com/tiangolo/uwsgi-nginx-flask-docker/releases/download/v0.3.10/example-flask-package-python3.7.zip$ unzip example-flask-package-python3.7.zip$ cd example-flask-package-python3.7 ビルド 1$ sudo docker build -t dobachi/nginx-flask-package:1.0 . 実行 1$ sudo docker run --rm -it -p 8080:80 dobachi/nginx-flask-package:1.0 一部修正してみる apiを追加する。 ファイルを修正 123$ cat example-flask-package-python3.7/app/app/api/api.py from .endpoints import userfrom .endpoints import get_ip ファイルを追加 1234567891011$ cat example-flask-package-python3.7/app/app/api/endpoints/get_ip.py from flask import requestfrom flask import jsonifyfrom ..utils import senseless_printfrom ...main import app@app.route(&quot;/get_ip&quot;, methods=[&quot;GET&quot;])def get_ip(): return jsonify(&#123;&apos;ip&apos;: request.remote_addr&#125;), 200 ビルドし、実行して、/get_ipにアクセスすると、接続元のIPアドレスが表示される。 flask + NginxでSSLを使用（ここがうまく動かない） アドホックな対応 flask + NginxでのSSL対応に関する記事 に記載された方法で試す。 また上記で使ったexample-flask-package-python3.7の例を用いてみる。 まずDockerfileを以下のように変更した。 1234567$ cat Dockerfile froM tiangolo/uwsgi-nginx-flask:python3.7run apt-get updaterun pip install pyopensslcopY ./app /app main.pyを以下のように修正し、アドホックなSSL対応を試せるようにする。 123456789101112$ cat app/app/main.py from flask import Flaskapp = Flask(__name__)from .core import app_setupif __name__ == &quot;__main__&quot;: # Only for debugging while developing # app.run(host=&apos;0.0.0.0&apos;, debug=True, port=80) app.run(host=&apos;0.0.0.0&apos;, debug=True, ssl_context=&apos;adhoc&apos;) 以上の修正を加えた上で、Dockerイメージをビルドし実行した。 12$ sudo docker build -t dobachi/nginx-flask-package:1.0 .$ sudo docker run --rm -it -p 8080:80 dobachi/nginx-flask-package:1.0 /bin/bash gitHub: uwsgi-nginx-flask-dockerのアーキテクチャ gitHub: uwsgi-nginx-flask-dockerのテクニカル詳細 で書かれている通り。 以下の役割分担。 ウェブサーバ：Nginx wSGIのアプリサーバ：uWSGI またアプリはFlaskで書かれていることを前提とし、 プロセスはSupervisordで管理される。 アプリはコンテナ内の/app以下に配備されることを前提とし、その中にとりあえずは動くuwsgi.iniも含まれる。 そのため、Dockerfile内でuwsgi.iniを上書きさせるようにして用いることを想定しているようだ。 Supervisord化について 以下のように/bin/bashを起動して確認した。 1$ sudo docker run --rm -it -p 8080:80 dobachi/nginx-flask-package:1.0 /bin/bash /etc/supervisor/conf.d/supervisord.confを確認する。 12345678910111213141516171819# cat /etc/supervisor/conf.d/supervisord.conf [supervisord]nodaemon=true[program:uwsgi]command=/usr/local/bin/uwsgi --ini /etc/uwsgi/uwsgi.ini --die-on-term --need-appstdout_logfile=/dev/stdoutstdout_logfile_maxbytes=0stderr_logfile=/dev/stderrstderr_logfile_maxbytes=0[program:nginx]command=/usr/sbin/nginxstdout_logfile=/dev/stdoutstdout_logfile_maxbytes=0stderr_logfile=/dev/stderrstderr_logfile_maxbytes=0# graceful stop, see http://nginx.org/en/docs/control.htmlstopsignal=QUIT uwsgiとnginxを起動させていることがわかる。 続いて、nginxの設定において、uwsgiと連携しているのを設定している箇所を確認。 1234567891011121314# cat /etc/nginx/conf.d/nginx.conf server &#123; listen 80; location / &#123; try_files $uri @app; &#125; location @app &#123; include uwsgi_params; uwsgi_pass unix:///tmp/uwsgi.sock; &#125; location /static &#123; alias /app/static; &#125;&#125; uwsgiでは、/appディレクトリ以下のuwsgi.iniと/etc/uwsgi/uwsgi.iniの両方を確認するようだ。 1234567# cat /etc/uwsgi/uwsgi.ini [uwsgi]socket = /tmp/uwsgi.sockchown-socket = nginx:nginxchmod-socket = 664# graceful shutdown on SIGTERM, see https://github.com/unbit/uwsgi/issues/849#issuecomment-118869386hook-master-start = unix_signal:15 gracefully_kill_them_all 1234# cat /app/uwsgi.ini [uwsgi]module = app.maincallable = app","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Flask","slug":"Knowledge-Management/Flask","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Flask/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://dobachi.github.io/memo-blog/tags/Docker/"},{"name":"Flask","slug":"Flask","permalink":"https://dobachi.github.io/memo-blog/tags/Flask/"},{"name":"Dockerfile","slug":"Dockerfile","permalink":"https://dobachi.github.io/memo-blog/tags/Dockerfile/"}]},{"title":"PixieDustを試してみる","slug":"PixieDust","date":"2018-12-13T13:30:57.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2018/12/13/PixieDust/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/13/PixieDust/","excerpt":"","text":"参考 公式サイトの確認 Scalaでも使える？ Jupyterに独自のカーネルをインストールして使う 動作確認 PixieDust 1 - Easy Visualizationsで気になったこと 参考 公式GitHub Qiitaの紹介ブログ 公式サイトの確認 ここでは気になったことを記しておく。 Scalaでも使える？ 公式GitHubのREADMEには、 Use in python or scala という項目があり、 そこには以下のような記述がある。 1PixieDust lets you bring robust Python visualization options to your Scala notebooks. 1Scala Bridge. Use Scala directly in your Python notebook. Variables are automatically transfered from Python to Scala and vice-versa. Learn more. 1Spark progress monitor. Track the status of your Spark job. No more waiting in the dark. Notebook users can now see how a cell&apos;s code is running behind the scenes. Jupyterに独自のカーネルをインストールして使う カーネルを追加することで、ユーザが自身の環境に依存するライブラリ（Sparkなど）をインストールしなくても 簡単に使えるようになっているようだ？ ★要確認 動作確認 まずは、公式ウェブサイトのインストール手順 を読みつつ、 Qiitaの紹介ブログ を読みながら、軽く試してみることにした。 Anacondaを使っているものとし、Condaで環境を作る。 12$ conda create -n PixieDustExample python=3.5 jupyter$ conda activate PixieDustExample また 公式ウェブサイトのインストール手順 には、以下のような記述があり、 Pythonは2.7もしくは3.5が良いようだ。 1Note PixieDust supports both Python 2.7 and Python 3.5. pipでpixiedustを探すと、エコシステムが形成されているようだった。 12345678910$ pip search pixiedustpixiedust-node (0.2.5) - Pixiedust extension for Node.jspixiedust-flightpredict (0.12) - Flight delay predictor application with PixieDustpixiedust-wordcloud (0.2.2) - Word Cloud Visualization Plugin for PixieDustpixiedust-twitterdemo1 (0.1) - Pixiedust demo of the Twitter Sentiment Analysis tutorialspixiedust-twitterdemo (0.4) - Pixiedust demo of the Twitter Sentiment Analysis tutorialspixiedust (1.1.14) - Productivity library for Jupyter Notebookpixiedust-optimus (1.4.0) - Productivity library for Spark Python Notebookpixiegateway (0.8) - Server for sharing PixieDust chart and running PixieAppsstem-ladiespixiedust-twitterdemo (0.2.2) - Pixiedust demo of the Twitter Sentiment Analysis tutorialsfor the STEM activity wall そのまま入れることとする。依存関係でいくつかライブラリが入る。 1$ pip install pixiedust エラー。 1twisted 18.7.0 requires PyHamcrest&gt;=1.9.0, which is not installed. condaでインストールする。 1$ conda install PyHamcrest PixieDust向けのJupyterカーネルをインストールする。 なお、本手順の前に、JDKがインストールされ、JAVA_HOMEがインストールされていることを前提とする。 1$ jupyter pixiedust install エラー。 12345678910111213141516171819$ jupyter pixiedust installStep 1: PIXIEDUST_HOME: /home/dobachi/pixiedust Keep y/n [y]? yStep 2: SPARK_HOME: /home/dobachi/pixiedust/bin/spark Keep y/n [y]? yDirectory /home/dobachi/pixiedust/bin/spark does not contain a valid SPARK install Download Spark y/n [y]? yWhat version would you like to download? 1.6.3, 2.0.2, 2.1.0, 2.2.0, 2.3.0 [2.3.0]: SPARK_HOME will be set to /home/dobachi/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7Downloading Spark 2.3.0Traceback (most recent call last):(snip) File &quot;/home/dobachi/.conda/envs/PixieDustExample/lib/python3.7/site-packages/install/createKernel.py&quot;, line 408, in download_spark temp_file = self.download_file(spark_download_url) File &quot;/home/dobachi/.conda/envs/PixieDustExample/lib/python3.7/site-packages/install/createKernel.py&quot;, line 455, in download_file raise Exception(&quot;&#123;&#125;&quot;.format(response.status_code))Exception: 404 Sparkのダウンロードで失敗しているようだ。 軽くデバッグしたところ、URLとして渡されているのは、 http://apache.claz.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgzのようだった。 確かに、リンク切れ？のようだった。いったん、Spark公式サイトのダウンロードページで 指定されるURLからダウンロードするよう、createKernel.pyを修正して実行することにした。 なお、上記手順により、指定したディレクトリにSparkとScalaがインストールされる。 成功すると、以下のようなメッセージが見られる。 1234567(snip)##################################################################################################### Congratulations: Kernel Python-with-Pixiedust_Spark-2.3 was successfully created in /home/dobachi/.local/share/jupyter/kernels/pythonwithpixiedustspark23# You can start the Notebook server with the following command:# jupyter notebook /home/dobachi/pixiedust/notebooks#################################################################################################### なお、そのまま実行したら、以下のライブラリが足りないと言われたので予めインストールすることとする。 （bokeh、seabornはついでにインストール。インストールしないとRendererの選択肢が出てこない） 1$ conda install matplotlib pandas PyYaml bokeh seaborn さて、 Qiitaの紹介ブログ では、PixieDustのカーネルインストール時に一緒にインストールされるノートブックではなく、 日本語でわかりやすい紹介記事が書かれているが、ここではいったん、上記のインストール時のメッセージに従うこととする。 Jupyterを起動 1$ jupyter notebook /home/dobachi/pixiedust/notebooks あとは、適当にノートブックを眺めて実行する。 PixieDust 1 - Easy Visualizationsで気になったこと Loading External Data節でD3 Rendererを試したところ、以下のようなエラー。 1Object of type ndarray is not JSON serializable","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Data Analytics","slug":"Research/Data-Analytics","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Data-Analytics/"},{"name":"Tools","slug":"Research/Data-Analytics/Tools","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Data-Analytics/Tools/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dobachi.github.io/memo-blog/tags/Python/"},{"name":"Data Analytics","slug":"Data-Analytics","permalink":"https://dobachi.github.io/memo-blog/tags/Data-Analytics/"},{"name":"Tools","slug":"Tools","permalink":"https://dobachi.github.io/memo-blog/tags/Tools/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://dobachi.github.io/memo-blog/tags/Jupyter/"}]},{"title":"Adobe reader on Ubuntu18","slug":"Adobe-reader-on-Ubuntu18","date":"2018-12-11T13:34:56.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/12/11/Adobe-reader-on-Ubuntu18/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/11/Adobe-reader-on-Ubuntu18/","excerpt":"","text":"参考 手順 参考 Adobe Readerのインストール手順を記したブログ ファイラから開くときのデフォルトアプリの設定の参考にした情報 手順 Adobe Readerのインストール手順を記したブログ に記載の手順で基本的には問題ない。 1234$ sudo apt install libxml2:i386 gdebi-core $ cd ~/Downloads$ wget ftp://ftp.adobe.com/pub/adobe/reader/unix/9.x/9.5.5/enu/AdbeRdr9.5.5-1_i386linux_enu.deb$ sudo gdebi AdbeRdr9.5.5-1_i386linux_enu.deb ファイラから開くときのデフォルトアプリの設定の参考にした情報 を参考に設定する。 1$ sudo vim /etc/gnome/defaults.list 変更内容は以下の通り。 1234567891011--- /etc/gnome/defaults.list.2018121101 2018-12-11 22:42:03.566334756 +0900+++ /etc/gnome/defaults.list 2018-12-11 22:42:15.970075520 +0900@@ -5,7 +5,7 @@ application/msword=libreoffice-writer.desktop application/ogg=rhythmbox.desktop application/oxps=evince.desktop-application/pdf=evince.desktop+application/pdf=acroread.desktop application/postscript=evince.desktop application/rtf=libreoffice-writer.desktop application/tab-separated-values=libreoffice-calc.desktop","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"Adobe Reader","slug":"Home-server/Ubuntu/Adobe-Reader","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/Adobe-Reader/"}],"tags":[{"name":"Adobe Reader","slug":"Adobe-Reader","permalink":"https://dobachi.github.io/memo-blog/tags/Adobe-Reader/"},{"name":"PDF","slug":"PDF","permalink":"https://dobachi.github.io/memo-blog/tags/PDF/"}]},{"title":"onedrive for linux","slug":"onedrive-for-linux","date":"2018-12-11T12:54:02.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/12/11/onedrive-for-linux/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/11/onedrive-for-linux/","excerpt":"","text":"参考 手順 参考 Ubuntu18での例 公式GitHub 手順 インストール 123456789$ sudo apt install libcurl4-openssl-dev libsqlite3-dev$ curl -fsS https://dlang.org/install.sh | bash -s dmd$ source ~/dlang/dmd-2.083.1/activate$ cd ~/Sources$ git clone https://github.com/abraunegg/onedrive.git$ cd onedrive$ source ~/dlang/dmd-2.083.1/activate$ make$ sudo make install 一部同期の設定 12$ mkdir -p ~/.config/onedrive$ vim ~/.config/onedrive/sync_list # 中にディレクトリを並べる （再）同期 1$ onedrive --synchronize --resync その後、初回同期時はウェブ認証が走るので従う。 白い画面がブラウザで現れたら、そのURLをコピーしてコンソールにペーストし、実行する。 アンインストール 1$ sudo make uninstall","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"OneDrive","slug":"Home-server/Ubuntu/OneDrive","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/OneDrive/"}],"tags":[{"name":"OneDrive","slug":"OneDrive","permalink":"https://dobachi.github.io/memo-blog/tags/OneDrive/"}]},{"title":"BlazeItの論文","slug":"Paper-of-Blazelt","date":"2018-12-06T09:31:27.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2018/12/06/Paper-of-Blazelt/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/06/Paper-of-Blazelt/","excerpt":"","text":"参考 論文の概要 動画の活用への期待が高まっている？ 課題感 BlazeItの構成 ユースケース システム概要 設定 フィルタ 特化NN フィルタのブートストラップ 現時点での制約 FrameQL FrameQLのスキーマ FCOUNT いくつかの例 クエリ実行の概要 aggregationの最適化 サンプリングについて補足 Control Variateについて補足 Optimizing Scrubbing Queriesについての補足 コンテントベースのSelection 実装 その他調べておきたいこと 参考 論文 Control Variateについての解説記事 GitHub 論文の概要 動画の活用への期待が高まっている？ 動画のボリュームが増し、分析者が動画分析に興味を持っている。 交通meter設定の計画、都市計画。 車の行き交うのを観測し、台数を算出する 自動車会社 ペデストリアンの感想？ ★要確認 これらを人手で確認するのは費用対効果が悪い。 既存手法としては、物体検知手法が挙げられる。 課題感 深層学習を使ったビジュアルデータの処理において、 GPUを使ったとしても高いfpsでの実時間処理が難しい。 state-of-artな物体検知手法Mask R-CNNですら、3fpsである。 またそれらの処理を実装するときには低レベルのライブラリ（例：OpenCV、Caffe2、Detectron）を使って 複雑な実装を施さないといけない。 なお、ビデオ処理の最適化の既存の研究はあるが、 それらは単一の機能を最適化するものである。 BlazeItは、Declarativeなクエリ言語を使って処理を記述し、さらに 最適化された処理のパイプラインを生成する。 BlazeItの構成 FrameQL 最適化と実行エンジン Aggregation 物体検知を行うサンプリングレートを指定可能 特化したNNを使う control variateを使う ★要確認 Scrubbing 稀に登場する物体を効率的に検知するため、 特化したNNのconfidence scoreを使う。 サンプリングにバイアスをかける Selection クエリの内容に応じていくつかのフィルタリングを組み合わせる NoScopeラベルベース、コンテンツベース、スキップ（サブサンプリング）、 クロッピング ユースケース BlazeItは探索的なクエリに注力。 ★重要 aggregate: 車の数を数える、など 探す: 動画の中で鳥が沢山飛んでいる瞬間を探す、など 特に動画が多く、全部に対して物体検知を行うのは非現実的なケースが適切。 ただし一部のデータに関しては、学習データとしてラベル付けされていることを前提とする。 ★重要 具体例: 都市計画 交通状況の観測と計画 例えばバスX台、車Y台がいるタイミングを見つける、など 自動運転 黄色信号時の状況把握 例えば黄色信号で通行者がX人いるタイミングを見つける、など ストア計画 店舗を区分けし、どの商品が人気なのか人の動向から把握する 鳥類の研究者 餌やりの状況把握 システム概要 設定 設定可能なコンポーネントは以下の通り。 物体検知 フレーム跨りの物体検知 UDF 例えばフレーム跨りで物体を検知するときに、車のナンバープレートを使う、など 独自のロジックの差し込みが可能。 フィルタ BlazeItのフィルタは確率的動作。 特化されたNNは、すべてのフレームにおいて精度が高いわけではない。 そこで、オプティマイザはエラーレートを参考にしつつプランを立てる。 特化NN 大きなNNの代わりに用いることができる単純化されたNN。 高速な計算が特徴。 既存研究ではバイナリ検知が主だったが、BlazeItの研究では カウントや多クラス分類に発展させた。 フィルタのブートストラップ 少量のラベル付け済みデータを用いる。 現時点での制約 モデルドリフト：動画の傾向が途中で変わる場合、モデルの再トレーニングが必要 ラベル付け済みデータセット：最初に特化NNをトレーニングするためのデータが必要 物体検知が物体検知手法に依存：もし特別なことをさせたいならUDFを使用 FrameQL ビデオを関係とみなし、selection、projection、aggregation（など関係に対するオペレーション）を可能とする。 テーブルのようなスキーマを提供することで、背景となる深層学習等の技術の専門家である必要なく、SQLが分かれば処理を記述可能とする。 また物体検知やフレーム間でのトラッキングロジックは、ユーザが独自のものを指定可能。 BlazeItはそれに応じて、スキーマを選択する。 シンタックスシュガーがあるが、それはBlinkDBと同じようなもののようだ。 ※BlinkDB由来、ということでUCBの流れを改めて感じる… FrameQLのスキーマ timestamp: オブジェクトのタイムスタンプ class: オブジェクトタイプ（車、バス、など） mask: オブジェクトの場所を表すポリゴン trackid: 時間をまたいでオブジェクトを追跡するID content: ポリゴンの中のピクセル features: 物体検知手法により出力された特徴 FCOUNT フレーム内に現れるオブジェクトの出現割合から算出される出現率？ いくつかの例 論文上はいくつかのFrameQLの例が載っている。 クエリ実行の概要 BlazeItにはFrameQLに基づいて処理を実行する際に最適化が行われる。 例えば、物体検知は計算コストが高いので、できるだけ減らそうとする。 aggregation、scrubbing、content-based selectionに関する最適化。 aggregationの最適化 最適化の方向性は以下の3点 トラディショナルなAQP 特化NNを用いて書き換え 特化NNを用いたControl Variate BlazeItは最初に最適化の種類を特定する。 まず学習データが十分にあるかどうか。もしなければAQPを用いる。 またアダプティブなサンプリング手法も用いる。（例: オンラインアグリゲーション、BlinkDB） 十分にデータがあれば特化NNを用いる。 サンプリングについて補足 所定の（詳しくは論文参照）サンプル数から始め、 所定の基準に達するまでサンプル数を増やしていく。 Control Variateについて補足 Control Variateについては、 Control Variateについての解説記事 などを参照。 Optimizing Scrubbing Queriesについての補足 分析者はレアなシーンこそ検索したくなるものだが、 そうするとすべてのフレームに対し、物体検知をしないといけない。（そしてそれは遅い） 「レアイベントシミュレーション」と特化NNを組み合わせる。 複数の対象（例：バスと車）を検知する特化NNを それぞれ 用いる。 もし「バス1台、車3台」のような条件の時にはアンドをとって用いる。 コンテントベースのSelection フィルタを使って検知処理を動かす前にフィルタする。 ラベルベース、コンテントベース、テンポラル、スペーシャル。 BlazeItはクエリからどのフィルタを使うべきかを推測する。 またユーザの期待するフレーム数に応じて、サンプリングレートを変える。 実装 既存のプロダクトを活用して作られている箇所とスクラッチで実装した箇所があるようだ。 コントロールプレーンはPython NNではないフィルタはC++ PyToarchを学習や評価に利用 物体検知にはFGFAを利用 MXNet v1.2、Mask R-CNNを利用 ビデオはOpenCVで走査 その他調べておきたいこと NoScope 物体検知関係の研究（YOLOv2、MS-COCO、Mask R-CNN、…）","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Video processing","slug":"Research/Video-processing","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Video-processing/"},{"name":"BlazeIt","slug":"Research/Video-processing/BlazeIt","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Video-processing/BlazeIt/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Video Processing","slug":"Video-Processing","permalink":"https://dobachi.github.io/memo-blog/tags/Video-Processing/"},{"name":"BlazeIt","slug":"BlazeIt","permalink":"https://dobachi.github.io/memo-blog/tags/BlazeIt/"}]},{"title":"TFoSparkのTFManager","slug":"TFManager-of-TFoSpark","date":"2018-12-06T03:43:25.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/12/06/TFManager-of-TFoSpark/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/06/TFManager-of-TFoSpark/","excerpt":"","text":"概要 実装の確認 startメソッド connectメソッド 概要 以下の通り、マルチプロセッシング処理におけるプロセス間通信を担うマネージャ。 tensorflowonspark/TFManager.py:15 1&quot;&quot;&quot;Python multiprocessing.Manager for distributed, multi-process communication.&quot;&quot;&quot; キューやキーバリューの辞書を管理。 キューは、SparkのExecutorとTFのWorkerの間の通信に用いられるようだ。 実装の確認 また以下の通り、BaseManagerを継承している。 tensorflowonspark/TFManager.py:14 1class TFManager(BaseManager): startメソッド startメソッドではリモートの接続を受け付けるかどうかを確認しながら、 BaseManager#startメソッドを使ってマネージャ機能を開始する。 tensorflowonspark/TFManager.py:60 12345if mode == &apos;remote&apos;: mgr = TFManager(address=(&apos;&apos;, 0), authkey=authkey)else: mgr = TFManager(authkey=authkey)mgr.start() connectメソッド アドレスと鍵を指定しながらTFManagerに接続する。 tensorflowonspark/TFManager.py:81 123m = TFManager(address, authkey=authkey)m.connect()return m","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Machine Learning","slug":"Research/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/"},{"name":"TensorFlow","slug":"Research/Machine-Learning/TensorFlow","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/TensorFlow/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://dobachi.github.io/memo-blog/tags/TensorFlow/"},{"name":"PySpark","slug":"PySpark","permalink":"https://dobachi.github.io/memo-blog/tags/PySpark/"},{"name":"TensorFlowOnSpark","slug":"TensorFlowOnSpark","permalink":"https://dobachi.github.io/memo-blog/tags/TensorFlowOnSpark/"}]},{"title":"TensorFlowOnSparkの例を確認する","slug":"Read-mnist-examples-of-TFoSpark","date":"2018-12-02T07:44:02.000Z","updated":"2024-08-27T15:34:26.027Z","comments":true,"path":"2018/12/02/Read-mnist-examples-of-TFoSpark/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/12/02/Read-mnist-examples-of-TFoSpark/","excerpt":"","text":"estimator mnist_estimator.pyのポイント spark mnist_spark.pyのポイント mnist_dist.pyのポイント tf examples/mnist/tf/mnist_spark.py map_funのポイント examples/mnist/tf/mnist_inference.py Executor内で実行される処理 estimator オリジナルのcnn_mnist.pyをTensorFlowOnSparkで動くように修正したもののようだ。 サンプルは、mnist_estimator.pyのみ。 学習・評価を実行する。 保存されたMNISTのデータを各ワーカが読んで学習する。 mnist_estimator.pyのポイント データロードの箇所は以下の通り。 学習データをすべて読み込むようになっていることが分かる。 examples/mnist/estimator/mnist_estimator.py:120 123456# Load training and eval datamnist = tf.contrib.learn.datasets.mnist.read_data_sets(args.data_dir)train_data = mnist.train.images # Returns np.arraytrain_labels = np.asarray(mnist.train.labels, dtype=np.int32)eval_data = mnist.test.images # Returns np.arrayeval_labels = np.asarray(mnist.test.labels, dtype=np.int32) モデルの定義の箇所は以下の通り。 モデル定義は実態的にはcnn_model_fn関数が担っている。 当該関数の中でCNNの層定義が行われている。 examples/mnist/estimator/mnist_estimator.py:127 12mnist_classifier = tf.estimator.Estimator( model_fn=cnn_model_fn, model_dir=args.model) 学習と評価の箇所は以下の通り。 入力データ用の関数を引数に渡しつつ実行する。 （これは各ワーカで動くところか） examples/mnist/estimator/mnist_estimator.py:158 123train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=args.steps, hooks=[logging_hook])eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)tf.estimator.train_and_evaluate(mnist_classifier, train_spec, eval_spec) spark mnist_spark.pyのポイント PySparkのアプリケーションとして、TensorFlowOnSparkを実行するアプリケーションである。 入力データの形式は、org.tensorflow.hadoop.io.TFRecordFileInputFormat（以降、TFRと呼ぶ）とCSVの両方に対応している。 TFRの場合は、入力されたデータをレコード単位でNumpy Array形式に変換して用いる。 CSVの場合、配列に変換する。 Executorで実行するメイン関数は、 mnist_dist.map_fun である。 mnist_dist.pyのポイント map_fun関数の定義がある。 パラメータサーバ、ワーカそれぞれのときに実行する処理が定義されている。 パラメータサーバのときは、TensorFlow Serverのjoinメソッドを呼ぶ。 ワーカのときはデバイスの指定、TensorFlow用の入力データの定義、層の定義、損失計算等が行われる。 なお学習と予測のそれぞれの動作が定義されている。 予測の時は、DataFeed#batch_resultsメソッドを使って、予測結果が戻り値のRDD向けにフィードされるようになっている。 tf 他のサンプルとは独立したものかどうか要確認。 examples/mnist/tf/mnist_spark.py READMEに書かれた最初のサンプル。 SparkContextを定義し、TFCluster#runメソッドを呼び出す。 runに渡されるTensorFlowのメイン関数はmnist_dist.map_funである。 map_funのポイント _parse_csvメソッドと_parse_tfrメソッドはそれぞれCSV形式、TFR形式のデータを読み、 ノーマライズし、画像データとラベルのペアを返す。 build_modelメソッドでモデルを定義する。 パラメータサーバとして機能するExecutorではTensorFlow Serverのjoinメソッドを呼び出す。 ワーカとして機能するExecutorは以下の流れ。 入力データのロードと加工 モデルのビルド TensorFlowまわりのセットアップ（Saverのセットアップなど） 予測の時 出力ファイルオープン 予測結果などの出力 出力ファイルクローズ 学習の時 定期的に状況をプリントしながら学習実行 タスクインデックスが0の場合のみ以下を実行 プレイスホルダ設定 モデルビルド チェックポイントからの復元 エクスポート ワーカごとにdoneファイルを生成（完了確認のため） examples/mnist/tf/mnist_inference.py TFClusterを使わず、Executor内でシングルノードのTFを使って予測結果を作る例。 Executor内で実行される処理 シングルノードのTFのセットアップ モデルロード 入力のための各種手続き（TFRecordを読むための関数定義、入力データの定義など）","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Machine Learning","slug":"Research/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/"},{"name":"TensorFlow","slug":"Research/Machine-Learning/TensorFlow","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/TensorFlow/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://dobachi.github.io/memo-blog/tags/TensorFlow/"},{"name":"PySpark","slug":"PySpark","permalink":"https://dobachi.github.io/memo-blog/tags/PySpark/"},{"name":"TensorFlowOnSpark","slug":"TensorFlowOnSpark","permalink":"https://dobachi.github.io/memo-blog/tags/TensorFlowOnSpark/"}]},{"title":"SX-AuroraやFrovedis","slug":"Aurora-Frovedis","date":"2018-11-23T13:16:00.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/11/23/Aurora-Frovedis/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/23/Aurora-Frovedis/","excerpt":"","text":"参考 SX-Auroraの物理構成の解説 記事に記載された感想の抜粋 NECとホートンワークスの協業 記事でのポイント 参考 まとめ情報のブログ SX-Auroraの簡単な紹介 SX-Auroraの物理構成の解説 SX-Auroraの物理構成の解説 記事に記載された感想の抜粋 SX-Auroraの物理構成の解説 に記載されていた考察を転記する。 一方、DGEMMではSX-Aurora TSUBASAはXeonの1.2倍程度の性能であり、V100が3.5程度であるのに比べて性能が低い。これはV100の方がピークFlopsが高いことが効いているのではないかと思われる。一方、Perf/WではSX-Aurora TSUBASAやV100は高い効率を示している。これは演算性能の割に消費電力が低いためと思われる。 まとめであるが、SX-Aurora TSUBASAはAuroraアーキテクチャに基づく新しいスパコンの製品ラインである。従来のように独自のプロセサを作るのではなく、x86/Linuxの環境にベクタ処理を付加した形になっている。 チップサイズの比較はともかく、NVIDIAのV100を16個搭載するDGX-2は約40万ドルで、V100 1個当たり250万円程度である。これに対して、SX-Aurora TSUBASAは、NECの発表では1VEのA100が170万円～、64VEのA500が1億2000万円～となっており、VE 1個あたり200万円弱である。これだけを見ると、NECの方が2割程度安いことになる。 しかし、これはいわゆる定価であり、実際のビジネスでは、これからどれだけ値引きされるのかは分からない。 Post-K(ポスト「京」)のような国家プロジェクトの無いNECがスパコンを開発するのは大変であるが、SX-Aurora TSUBASAはx86サーバにベクタエンジンを付けるという面白い切り口で挑んでいる。成功を祈る。 NECとホートンワークスの協業 記事でのポイント 現在はYARN対応がポイントのようだ。 共同研究成果は、OSSとして公開するつもりのようだ。","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"SX-Aurora/Frovedis","slug":"Research/SX-Aurora-Frovedis","permalink":"https://dobachi.github.io/memo-blog/categories/Research/SX-Aurora-Frovedis/"}],"tags":[{"name":"SX-Aurora","slug":"SX-Aurora","permalink":"https://dobachi.github.io/memo-blog/tags/SX-Aurora/"},{"name":"Frovedis","slug":"Frovedis","permalink":"https://dobachi.github.io/memo-blog/tags/Frovedis/"},{"name":"Vector Engine","slug":"Vector-Engine","permalink":"https://dobachi.github.io/memo-blog/tags/Vector-Engine/"},{"name":"HPC","slug":"HPC","permalink":"https://dobachi.github.io/memo-blog/tags/HPC/"}]},{"title":"Python3をHDP2.6のPySparkで用いたときのエラー","slug":"Error-using-Python3-on-HDP2-6","date":"2018-11-18T12:31:02.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/11/18/Error-using-Python3-on-HDP2-6/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/18/Error-using-Python3-on-HDP2-6/","excerpt":"","text":"参考 メモ Sparkでのvirtualenv対応 参考 Hortonworksのコミュニティによる情報 2to3を用いる方法の記事 HortonworkによるSparkのvirtualenv対応 SPARK-13587: Support virtualenv in PySpark メモ 以下のようなコマンドでPython3のipythonをドライバのPythonとして指定しながら起動したところ、 エラーを生じた。 123456$ pyspark --master yarn --conf spark.pyspark.driver.python=ipython$ ipython --version7.1.1$ python --versionPython 3.6.7 :: Anaconda, Inc. エラー内容は以下の通り。 123456789101118/11/18 12:30:09 WARN ScriptBasedMapping: Exception running /etc/hadoop/conf/topology_script.py 192.168.33.83ExitCodeException exitCode=1: File &quot;/etc/hadoop/conf/topology_script.py&quot;, line 63 print rack ^SyntaxError: Missing parentheses in call to &apos;print&apos;. Did you mean print(rack)? at org.apache.hadoop.util.Shell.runCommand(Shell.java:954) at org.apache.hadoop.util.Shell.run(Shell.java:855) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1163) at org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping.runResolveComman(snip) Hortonworksのコミュニティによる情報によると、Python2系しか対応していないというが、本当かどうか確認が必要。 同様のことを指摘している記事として2to3を用いる方法の記事が挙げられるが、そこでは /usr/bin/hdp-select /etc/hadoop/conf/topology_script.py の2ファイルがPython2でしか動作しない内容で記載されているようだ。 ためしに/usr/bin/hdp-selectをのぞいてみたら、確かにPython2系に限定された実装の箇所がチラホラ見受けられた。 Sparkでのvirtualenv対応 SPARK-13587: Support virtualenv in PySparkでHortonworkの人中心に議論されており、 ターゲットバージョンは3.0.1とされている。 HortonworkによるSparkのvirtualenv対応に記載の方法は、Hortonworksの独自拡張なのか。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hadoop","slug":"Knowledge-Management/Hadoop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/"},{"name":"HDP","slug":"Knowledge-Management/Hadoop/HDP","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/HDP/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://dobachi.github.io/memo-blog/tags/Spark/"},{"name":"PySpark","slug":"PySpark","permalink":"https://dobachi.github.io/memo-blog/tags/PySpark/"},{"name":"HDP","slug":"HDP","permalink":"https://dobachi.github.io/memo-blog/tags/HDP/"},{"name":"Python3","slug":"Python3","permalink":"https://dobachi.github.io/memo-blog/tags/Python3/"}]},{"title":"AmbariのAgent登録時にSSL関連のエラー","slug":"Ambari-SSL-Error","date":"2018-11-17T14:34:53.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/11/17/Ambari-SSL-Error/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/17/Ambari-SSL-Error/","excerpt":"","text":"参考 メモ 参考 解決手段の記載された記事 メモ 基本的に、 解決手段の記載された記事 の手順で問題なかった。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hadoop","slug":"Knowledge-Management/Hadoop","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/"},{"name":"Ambari","slug":"Knowledge-Management/Hadoop/Ambari","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hadoop/Ambari/"}],"tags":[{"name":"Ambari","slug":"Ambari","permalink":"https://dobachi.github.io/memo-blog/tags/Ambari/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://dobachi.github.io/memo-blog/tags/Hadoop/"}]},{"title":"vagrant-libvirt on Ubuntu18","slug":"vagrant-libvirt-on-Ubuntu18","date":"2018-11-17T11:52:10.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/11/17/vagrant-libvirt-on-Ubuntu18/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/17/vagrant-libvirt-on-Ubuntu18/","excerpt":"","text":"参考 手順 必要パッケージのインストール Vagrantのプラグイン追加 Vagrantfileの作成 ストレージプールdefaultの不在に関連したエラー Vagrant BoxのダウンロードURLに関連したエラー 参考 vagrant-libvirt デフォルトプールに関するエラーの情報 libvirtでストレージプールを作成 VagrantのダウンロードURL関連エラーの情報 手順 必要パッケージのインストール 最初に手順通り、vagrantをインストールしておく。 vagrant-libvirt によると、upstreamを 用いることが推奨されているので、ここではあえて1.8系を用いることにする。 Vagrantのプラグイン追加 1234$ sudo apt install gdebi # 念のため依存関係を解決しながらdebをインストールするため$ cd ~/Downloads$ wget https://releases.hashicorp.com/vagrant/1.8.7/vagrant_1.8.7_x86_64.deb$ sudo gdebi vagrant_1.8.7_x86_64.deb 続いてソースリストでdeb-srcを有効化する。 diffは以下の通り。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061$ sudo diff -u /etc/apt/sources.list&#123;,.2018111701&#125;--- /etc/apt/sources.list 2018-11-17 21:06:52.975641015 +0900+++ /etc/apt/sources.list.2018111701 2018-11-17 21:06:29.396143241 +0900@@ -3,20 +3,20 @@ # See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to # newer versions of the distribution. deb http://jp.archive.ubuntu.com/ubuntu/ bionic main restricted-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic main restricted+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic main restricted ## Major bug fix updates produced after the final release of the ## distribution. deb http://jp.archive.ubuntu.com/ubuntu/ bionic-updates main restricted-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-updates main restricted+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-updates main restricted ## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu ## team. Also, please note that software in universe WILL NOT receive any ## review or updates from the Ubuntu security team. deb http://jp.archive.ubuntu.com/ubuntu/ bionic universe-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic universe+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic universe deb http://jp.archive.ubuntu.com/ubuntu/ bionic-updates universe-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-updates universe+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-updates universe ## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu ## team, and may not be under a free licence. Please satisfy yourself as to@@ -24,9 +24,9 @@ ## multiverse WILL NOT receive any review or updates from the Ubuntu ## security team. deb http://jp.archive.ubuntu.com/ubuntu/ bionic multiverse-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic multiverse+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic multiverse deb http://jp.archive.ubuntu.com/ubuntu/ bionic-updates multiverse-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-updates multiverse+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-updates multiverse ## N.B. software from this repository may not have been tested as ## extensively as that contained in the main release, although it includes@@ -34,7 +34,7 @@ ## Also, please note that software in backports WILL NOT receive any review ## or updates from the Ubuntu security team. deb http://jp.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse-deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse+# deb-src http://jp.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse ## Uncomment the following two lines to add software from Canonical&apos;s ## &apos;partner&apos; repository.@@ -44,8 +44,8 @@ # deb-src http://archive.canonical.com/ubuntu bionic partner deb http://security.ubuntu.com/ubuntu bionic-security main restricted-deb-src http://security.ubuntu.com/ubuntu bionic-security main restricted+# deb-src http://security.ubuntu.com/ubuntu bionic-security main restricted deb http://security.ubuntu.com/ubuntu bionic-security universe-deb-src http://security.ubuntu.com/ubuntu bionic-security universe+# deb-src http://security.ubuntu.com/ubuntu bionic-security universe deb http://security.ubuntu.com/ubuntu bionic-security multiverse-deb-src http://security.ubuntu.com/ubuntu bionic-security multiverse+# deb-src http://security.ubuntu.com/ubuntu bionic-security multiverse 1234$ sudo apt update$ sudo apt-get build-dep vagrant ruby-libvirt$ sudo apt-get install qemu libvirt-bin ebtables dnsmasq$ sudo apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev Vagrantのプラグインを追加する。 1$ vagrant plugin install vagrant-libvirt Vagrantfileの作成 続いて、以下のようなVagrantfileを用意し、vagrant upする。 ※この時点のVagrantfileでは正常に動かないので注意。ここでは時系列のままに記載している。 123456789101112131415161718192021222324252627282930313233343536373839# -*- mode: ruby -*-# vi: set ft=ruby :# Vagrantfile API/syntax version. Don&apos;t touch unless you know what you&apos;re doing!VAGRANTFILE_API_VERSION = &quot;2&quot;Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|# config.vm.provider :virtualbox do |vb|# vb.auto_nat_dns_proxy = false# vb.customize [&quot;modifyvm&quot;, :id, &quot;--natdnsproxy1&quot;, &quot;off&quot;]# vb.customize [&quot;modifyvm&quot;, :id, &quot;--natdnshostresolver1&quot;, &quot;off&quot;]# end config.vm.define &quot;hd-01&quot; do |config| config.vm.box = &quot;centos/7&quot; config.vm.network :private_network, ip: &quot;192.168.33.81&quot; config.vm.hostname = &quot;hd-01&quot; end config.vm.define &quot;hd-02&quot; do |config| config.vm.box = &quot;centos/7&quot; config.vm.network :private_network, ip: &quot;192.168.33.82&quot; config.vm.hostname = &quot;hd-02&quot; end config.vm.define &quot;hd-03&quot; do |config| config.vm.box = &quot;centos/7&quot; config.vm.network :private_network, ip: &quot;192.168.33.83&quot; config.vm.hostname = &quot;hd-03&quot; end config.vm.define &quot;hd-04&quot; do |config| config.vm.box = &quot;centos/7&quot; config.vm.network :private_network, ip: &quot;192.168.33.84&quot; config.vm.hostname = &quot;hd-04&quot; endend ストレージプールdefaultの不在に関連したエラー このとき、 1There was error while creating libvirt storage pool: Call to virStoragePoolDefineXML failed: operation failed: Storage source conflict with pool: &apos;images&apos; のようなエラーが生じたが、これは デフォルトプールに関するエラーの情報 に掲載の課題のようだ。 libvirtでストレージプールを作成 を参考に、ストレージプールを作成する。 1234$ sudo virsh pool-define-as default dir --target /var/lib/libvirt/default$ sudo virsh pool-build default$ sudo virsh pool-start default$ sudo virsh pool-autostart default Vagrant BoxのダウンロードURLに関連したエラー つづいて、vagrant up --provider=libvirtしようとしたら、以下のエラーが生じた。 12345678The box &apos;centos/7&apos; could not be found orcould not be accessed in the remote catalog. If this is a privatebox on HashiCorp&apos;s Atlas, please verify you&apos;re logged in via`vagrant login`. Also, please double-check the name. The expandedURL and error message are shown below:URL: [&quot;https://atlas.hashicorp.com/centos/7&quot;]Error: The requested URL returned error: 404 Not Found VagrantのダウンロードURL関連エラーの情報 に記載の通り、いったん 1Vagrant::DEFAULT_SERVER_URL.replace(&apos;https://vagrantcloud.com&apos;) の記載をVagrantfileの先頭に記載する、というワークアラウンドを取り入れることにした。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"KVM","slug":"Home-server/Ubuntu/KVM","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/KVM/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://dobachi.github.io/memo-blog/tags/KVM/"},{"name":"Vagrant","slug":"Vagrant","permalink":"https://dobachi.github.io/memo-blog/tags/Vagrant/"},{"name":"libvirt","slug":"libvirt","permalink":"https://dobachi.github.io/memo-blog/tags/libvirt/"}]},{"title":"KVM on Ubuntu18","slug":"KVM-on-Ubuntu18","date":"2018-11-16T14:32:19.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/11/16/KVM-on-Ubuntu18/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/16/KVM-on-Ubuntu18/","excerpt":"","text":"参考 手順 パッケージインストール イメージの準備 インストール ゲスト側のconsole設定 参考 公式ドキュメント ネットワークインストールの方法 VMのIPアドレス確認 ゲストOSにおけるコンソール設定 手順 パッケージインストール 1$ sudo apt install qemu-kvm libvirt-bin 1$ sudo adduser $USER libvirt なお、公式ドキュメント上ではlibvertdグループとされていたが、注釈にあった 1In more recent releases (&gt;= Yakkety) the group was renamed to libvirt. Upgraded systems get a new libvirt group with the same gid as the libvirtd group to match that. に従い、libvertグループを使用した。 1$ sudo reboot イメージの準備 今回は18.04LTSを用いることとする。 123$ cd ~/Downloads # ディレクトリがなかったら作成$ wget http://ftp.jaist.ac.jp/pub/Linux/ubuntu-releases/18.04/ubuntu-18.04.1-desktop-amd64.iso$ wget http://ftp.jaist.ac.jp/pub/Linux/ubuntu-releases/18.04/ubuntu-18.04.1-live-server-amd64.iso インストール 1$ sudo apt install virtinst ネットワークインストールの方法ネットワーク経由でインストールすることにした。 1234567891011$ sudo virt-install \\ --name ubuntu1804 \\ --ram 1024 \\ --disk path=/var/lib/libvirt/images/test.img,bus=virtio,size=15 \\ --vcpus 2 \\ --os-type linux \\ --network default \\ --graphics none \\ --console pty,target_type=serial \\ --location &apos;http://jp.archive.ubuntu.com/ubuntu/dists/bionic/main/installer-amd64/&apos; \\ --extra-args &apos;console=ttyS0,115200n8 serial&apos; 参考までにイメージからインストールするのは以下の通り。（未動作確認） 12345sudo virt-install -n test -r 512 \\--disk path=/var/lib/libvirt/images/test.img,bus=virtio,size=4 -c \\~/Downloads/ubuntu-18.04.1-live-server-amd64.iso --network network=default,model=virtio \\--graphics none \\ --extra-args &apos;console=ttyS0,115200n8 serial&apos; ゲスト側のconsole設定 virsh console でコンソールに接続できるよう、SSHで接続して設定する。 VMのIPアドレス確認 を参考に、IPアドレスを確認する。 1234~$ sudo virsh net-dhcp-leases default Expiry Time MAC address Protocol IP address Hostname Client ID or DUID------------------------------------------------------------------------------------------------------------------- 2018-11-17 03:08:52 52:54:00:b0:68:26 ipv4 192.168.122.193/24 ubuntu1804 ff:32:39:f9:b5:00:02:00:00:ab:11:97:79:8b:63:02:9c:d8:07 ゲストOSにおけるコンソール設定 に従って設定する。 まず /etc/default/grub を修正する。 1234567891011121314151617181920212223242526272829303132333435363738$ sudo echo &gt; /etc/default/grub &lt;&lt; EOL # If you change this file, run &apos;update-grub&apos; afterwards to update # /boot/grub/grub.cfg. # For full documentation of the options in this file, see: # info -f grub -n &apos;Simple configuration&apos; GRUB_DEFAULT=0 GRUB_TIMEOUT_STYLE=hidden GRUB_TIMEOUT=10 GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash&quot; GRUB_CMDLINE_LINUX=&quot;console=tty1 console=ttyS0,115200&quot; # Uncomment to enable BadRAM filtering, modify to suit your needs # This works with Linux (no patch required) and with any kernel that obtains # the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...) #GRUB_BADRAM=&quot;0x01234567,0xfefefefe,0x89abcdef,0xefefefef&quot; # Uncomment to disable graphical terminal (grub-pc only) #GRUB_TERMINAL=console # The resolution used on graphical terminal # note that you can use only modes which your graphic card supports via VBE # you can see them in real GRUB with the command `vbeinfo&apos; #GRUB_GFXMODE=640x480 # Uncomment if you don&apos;t want GRUB to pass &quot;root=UUID=xxx&quot; parameter to Linux #GRUB_DISABLE_LINUX_UUID=true # Uncomment to disable generation of recovery mode menu entries #GRUB_DISABLE_RECOVERY=&quot;true&quot; # Uncomment to get a beep at grub start #GRUB_INIT_TUNE=&quot;480 440 1&quot; #GRUB_TERMINAL=serial GRUB_TERMINAL=&quot;console serial&quot; GRUB_SERIAL_COMMAND=&quot;serial --unit=0 --speed=115200 --word=8 --parity=no --stop=1&quot; EOL grubの設定ファイルを再生成する。 1$ sudo grub-mkconfig -o /boot/grub/grub.cfg rebootする。 1$ sudo reboot reboot後にvirsh console &lt;VM名&gt;でアクセスすればコンソールに接続できる。 抜けるときは、Ctrl + ]もしくはCtrl + 5。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Ubuntu","slug":"Home-server/Ubuntu","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/"},{"name":"KVM","slug":"Home-server/Ubuntu/KVM","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Ubuntu/KVM/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"},{"name":"KVM","slug":"KVM","permalink":"https://dobachi.github.io/memo-blog/tags/KVM/"}]},{"title":"Hexoでソースファイルもデプロイする","slug":"hexo-deploy-sources","date":"2018-11-03T14:29:45.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/11/03/hexo-deploy-sources/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/03/hexo-deploy-sources/","excerpt":"","text":"参考 方法 参考 関連する記事 方法 関連する記事 に記載された通り、extend_dirsを指定すればよい。 以下のようにした。 12345deploy: - type: git repo: &lt;git repository url&gt; branch: gh-pages extend_dirs: source","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"}]},{"title":"IEEE BigData 2018の気になった論文を列挙する","slug":"IEEE-BigData-2018","date":"2018-11-02T14:22:40.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/11/02/IEEE-BigData-2018/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/02/IEEE-BigData-2018/","excerpt":"","text":"参考情報 全体の傾向 気になった論文リスト L1 Scientific Data Management L3 Semantic-based Data Mining L4 Big Data Applications: Machine Learning L5 Novel Theoretical Models for Big Data L6 Big Data Analytics Frameworks L9 Recommendation Systems and Stream Data Management L11 Big Data Applications: Industry and Business L12 Big Data Applications: Health &amp; Science Discovery L14 HPC Platforms for Big Data L17 Big Data Applications: Deep Learning L18 Mobile and IoT Data L22 Privacy and Security S1 Big Data Infrastructure (1) S5 Big Data Infrastructure (2) S6 Big Data Applications (2) S12 Recommendation Systems &amp; Stream Data Mining I&amp;G Regular3: Big Data Platforms &amp; Frameworks I&amp;G Short3: Big Data Algorithms &amp; Systems (2) 参考情報 IEEE BigData 2018の公式ウェブサイト 論文リストのPDF 全体の傾向 基盤というより、アルゴリズム、アプリケーション層 一部ユースケースを想像させるものもある 気になった論文リスト L1 Scientific Data Management BigD529 Cloud based Real-Time and Low Latency Scientific Event Analysis Chen Yang, Zhihui Du, and Xiaofeng Meng L3 Semantic-based Data Mining BigD483 A Data-Centric Approach for Image Scene Localization Abdullah Alfarrarjeh, Seon Ho Kim, Shivnesh Rajan, Akshay Deshmukh, and Cyrus Shahabi BigD753 Improved Dynamic Memory Network for Dialogue Act Classification with Adversarial Training Yao Wan, Wenqiang Yan, Jianwe L4 Big Data Applications: Machine Learning BigD526 A Minimax Approach for Classification with Big-data Krishnan Raghavan, Jagannathan Sarangapani, and VA Samaranayake BigD445 Transfer learning for time series classification Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller L5 Novel Theoretical Models for Big Data BigD357 Linear Models with Many Cores and CPUs: A Stochastic Atomic Update Scheme Edward Raff and Jared Sylvester BigD580 Time Series Classification Using a Neural Network Ensemble Soukaina Filali Boubrahimi and Rafal Angryk L6 Big Data Analytics Frameworks BigD336 Online Density Estimation over Streaming Data: A Local Adaptive Solution Zhong Chen, Zhide Fang, Jiabin zhao, Wei Fan, Andrea Edwards, and Kun Zhang BigD397 Learning-based Automatic Parameter Tuning for Big Data Analytics Frameworks Liang Bao, Xin Liu, and Weizhao Chen L9 Recommendation Systems and Stream Data Management BigD626 StreamGuard: A Bayesian Network Approach to Copyright Infringement Detection Problem in Large-scale Live Video Sharing Systems Daniel Zhang, Lixing Song, Qi Li, Yang Zhang, and Dong Wang BigD600 BigSR: real-time expressive RDF stream reasoning on modern Big Data platforms Xiangnan Ren, Olivier Cure, Hubert Naacke, and GuohuiXiao L11 Big Data Applications: Industry and Business BigD528 Mining Illegal Insider Trading of Stocks: A Proactive Approach Sheikh Rabiul Islam, Sheikh Khaled Ghafoor, and William Eberle BigD547 Profiling Driver Behavior for Personalized Insurance Pricing and Maximal Profit Bing He, Dian Zhang, Siyuan Liu, Hao Liu, Dawei Han, and Lionel M. Ni BigD685 An Unsupervised Learning Based Approach for Mining Attribute Based Access Control Policies Leila Karimi and James Joshi BigD285 Realtime Robustification of Interdependent Networks under Cascading Attacks Zhen Chen, Hanghang Tong, and Lei Ying BigD643 Situation-Based Interaction Learning for Personality Prediction on Facebook Lei Zhang, Liang Zhao, Xuchao Zhang, Wenmo Kong, Zitong Sheng, and Chang-Tien Lu L12 Big Data Applications: Health &amp; Science Discovery BigD653 Technology Enablers for Big Data, Multi-Stage Analysis in Medical Image Processing Shunxing Bao, Prasanna Parvathaneni, Yuankai Huo, Yogesh Barve, Andrew Plassard, Yuang Yao, Hongyang Sun, Ilwoo Lyu, David Zald, Bennett Landman, and Aniruddha Gokhale BigD456 A Structured Learning Approach with Neural Conditional Random Fields for Sleep Staging Karan Aggarwal, Swaraj Khadanga, Shafiq Joty, Louis Kazaglis, and Jaideep Srivastava BigD402 Dynamic Prediction of ICU Mortality Risk Using Domain Adaptation Tiago Alves, Alberto Laender, Adriano Veloso, and Nivio Ziviani BigD353 Large-Scale Validation of Hypothesis Generation Systems via Candidate Ranking Justin Sybrandt, Micheal Shtutman, and Ilya Safro BigD354 Are Abstracts Enough for Hypothesis Generation? Justin Sybrandt, Angelo Carrabba, Alexander Herzog, and Ilya Safro L14 HPC Platforms for Big Data BigD294 Column Cache: Buffer Cache for Columnar Storage on HDFS Takeshi Yoshimura, Tatsuhiro Chiba, and Hiroshi Horii BigD431 Scalable Manifold Learning for Big Data with Apache Spark Frank Schoeneman and Jaroslaw Zola BigD598 Mira: Sharing Resources for Distributed Analytics at Small Timescales Michael Kaufmann, Kornilios Kourtis, Adrian Schuepbach, and Martina Zitterbart L17 Big Data Applications: Deep Learning BigD406 Two Birds with One Network: Unifying Event Prediction and Time-to-failure Modeling Karan Aggarwal, Onur Atan, Ahmed Farahat, Chi Zhang, Kosta Ristovski, and Chetan Gupta BigD292 Market Abnormality Period Detection via Comovement Attention Model Yue Wang, Chenwei Zhang, Shen Wang, Philip S. Yu, Lu Bai, and Lixin Cui BigD298 Optimizing Taxi Carpool Policies via Reinforcement Learning and Spatio-Temporal Mining Ishan Jindal, Zhiwei (Tony) Qin, Xuewen Chen, Matthew Nokleby, and Jieping Ye L18 Mobile and IoT Data BigD371 Enabling of Predictive Maintenance in the Brownfield through Low-Cost Sensors, an IIoT-Architecture and Machine Learning Patrick Straus, Rene Wostmann, Markus Schmitz, and Jochen Deuse BigD240 Using Smart Card Data to Model Commuters' Response Upon Unexpected Train Delays Xiancai Tian and Baihua Zheng BigD542 Hot Spot Analysis for Big Trajectory Data Panagiotis Nikitopoulos, Aris-Iakovos Paraskevopoulos, Christos Doulkeridis, Nikos Pelekis, and Yannis Theodoridis BigD722 Fusion of Terrain Information and Mobile Phone Location Data for Flood Area Detection in Rural Areas Takahiro Yabe, Kota Tsubouchi, and Yoshihide Sekimoto BigD296 Benchmarking Anomaly Detection Algorithms in an Industrial Context: Dealing with Scarce Labels and Multiple Positive Types David Renaudie, Maria A. Zuluaga, and Rodrigo AcunaAgost L22 Privacy and Security BigD247 Distributed Machine Learning Meets Blockchain: A Decentralized, Secure, and Privacy-preserving Realization Xuhui Chen, Jinlong Ji, Changqing Luo, Weixian Liao, and Pan Li S1 Big Data Infrastructure (1) BigD291 Analyzing Alibaba's Co-located Datacenter Workloads Yue Cheng, Ali Anwar, and Xuejing Duan BigD735 Spark-uDAPL: Cost-Saving Big Data Analytics on Microsoft Azure Cloud with RDMA Networks Xiaoyi Lu, Dipti Shankar, Haiyang Shi, and Dhabaleswar K. (DK) Panda BigD381 Integrated Real-Time Data Stream Analysis and SketchBased Video Retrieval in Team Sports Lukas Probst, Fabian Rauschenbach, Heiko Schuldt, Philipp Seidenschwarz, and Martin Rumo S5 Big Data Infrastructure (2) BigD252 Serverless Big Data Processing using Matrix Multiplication as Example Sebastian Werner, Jorn Kuhlenkamp, Markus Klems, Johannes Muller, and Stefan Tai S6 Big Data Applications (2) BigD535 Short-term local weather forecast using dense weather station by deep neural network Kazuo Yonekura, Hitoshi Hattori, and Taiji Suzuki S12 Recommendation Systems &amp; Stream Data Mining BigD383 Learning Fast and Slow - A Unified Batch/Stream Framework Jacob Montiel, Albert Bifet, Viktor Losing, Jesse Read, and Talel Abdessalem I&amp;G Regular3: Big Data Platforms &amp; Frameworks N211 Learning to Simplify Distributed Systems Management Christopher Streiffer, Ramya Raghavendra, Theophilus Benson, and Mudhakar Srivatsa N259 Finding Data Should be Easier than Finding Oil Evgeny Kharlamov, Martin Skjaeveland, Theofilos Mailis, Ernesto Jimenez-Ruiz, Guohui Xiao, Ahmet Soylu, Hallstein Lie, and Arild Waaler I&amp;G Short3: Big Data Algorithms &amp; Systems (2) N222 Using Real-World Store Data for Foot Traffic Forecasting Soheila Abrishami and Piyush Kumar N218 Distributed NoSQL Data Stores: Performance Analysis and a Case Study Abdeltawab Hendawi, Jayant Gupta, Liu Jiayi, Ankur Teredesai, Ramakrishnan Naveen, Shah Mohak, and Mohamed Ali","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Machine Learning","slug":"Research/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"IEEE","slug":"IEEE","permalink":"https://dobachi.github.io/memo-blog/tags/IEEE/"}]},{"title":"dein on windows","slug":"dein-on-windows","date":"2018-11-02T12:06:45.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/11/02/dein-on-windows/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/11/02/dein-on-windows/","excerpt":"","text":"参考 Windows環境でのKaoriya版のvimへのインストール 基本的な手順 設定 参考 公式GitHub windows10にdein.vimをインストールする手順を示したブログ dein.vimインストール手順や_vimrcをまとめたレポジトリ Windows環境でのKaoriya版のvimへのインストール 基本的な手順 基本的には、 windows10にdein.vimをインストールする手順を示したブログ に記載の方法で問題なかった。 ただし、vim81系でないと正常動作しないことに注意。（一度、うっかり74系で使ってしまった） ただ_vimrcに追記する設定のうち、PATHはインストーラで出力されたものではなく、 「~」を使って相対PATH指定にした。 例: 1234&quot; Required:set runtimepath+=~/vimfiles/bundles/repos/github.com/Shougo/dein.vim(snip) 設定 今のところの設定は、 dein.vimインストール手順や_vimrcをまとめたレポジトリ に格納した。→ _vimrc END","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"vim","slug":"Knowledge-Management/vim","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://dobachi.github.io/memo-blog/tags/vim/"},{"name":"dein","slug":"dein","permalink":"https://dobachi.github.io/memo-blog/tags/dein/"},{"name":"windows","slug":"windows","permalink":"https://dobachi.github.io/memo-blog/tags/windows/"}]},{"title":"Autonomous Database of Oracle","slug":"Autonomous-Database-of-Oracle","date":"2018-10-24T12:05:29.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/10/24/Autonomous-Database-of-Oracle/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/24/Autonomous-Database-of-Oracle/","excerpt":"","text":"「AWSのDBより速くて安い」―ラリー・エリソン氏がAutonomous Databaseを全力で解説 Oracle Cloud Infrastructure第2世代の利点の説明。 Autonomous Databaseは人間によるチューニングよりも効果的だという話。","categories":[{"name":"Clipping","slug":"Clipping","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/"},{"name":"Cloud","slug":"Clipping/Cloud","permalink":"https://dobachi.github.io/memo-blog/categories/Clipping/Cloud/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://dobachi.github.io/memo-blog/tags/Oracle/"},{"name":"Autonomous Database","slug":"Autonomous-Database","permalink":"https://dobachi.github.io/memo-blog/tags/Autonomous-Database/"},{"name":"Cloud","slug":"Cloud","permalink":"https://dobachi.github.io/memo-blog/tags/Cloud/"},{"name":"DB","slug":"DB","permalink":"https://dobachi.github.io/memo-blog/tags/DB/"}]},{"title":"WordからMarkdownを生成する","slug":"Word-to-Markdown","date":"2018-10-20T13:14:16.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/20/Word-to-Markdown/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/20/Word-to-Markdown/","excerpt":"","text":"ドンピシャのQiitaの記事 があったので 手元の環境で真似したところ、悪くない結果が得られた。 コマンドだけ引用すると以下の通り。 1pandoc hoge.docx -t markdown-raw_html-native_divs-native_spans -o hoge.md","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Documentation","slug":"Knowledge-Management/Documentation","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Documentation/"}],"tags":[{"name":"pandoc","slug":"pandoc","permalink":"https://dobachi.github.io/memo-blog/tags/pandoc/"},{"name":"Word","slug":"Word","permalink":"https://dobachi.github.io/memo-blog/tags/Word/"},{"name":"Markdown","slug":"Markdown","permalink":"https://dobachi.github.io/memo-blog/tags/Markdown/"}]},{"title":"IEEE BigData 2017の気になったものメモ","slug":"IEEE-BigData-2017","date":"2018-10-20T12:46:25.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/10/20/IEEE-BigData-2017/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/20/IEEE-BigData-2017/","excerpt":"","text":"BigD635 Lei Huang, Weijia Xu, Si Liu, Venktesh Pandey, and Natalia Ruiz Juri, Enabling Versatile Analysis of Large Scale Traffic Video Data with Deep Learning and HiveQL https://ieeexplore.ieee.org/document/8258041 HiveとSparkのプラットフォーム上で動く分析フレームワーク（？） 論文では車の検知（シーン検知？）を試す例が載っているようだ。 後で論文読む （追々追記予定）","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"Machine Learning","slug":"Research/Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/categories/Research/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dobachi.github.io/memo-blog/tags/Machine-Learning/"},{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"IEEE","slug":"IEEE","permalink":"https://dobachi.github.io/memo-blog/tags/IEEE/"}]},{"title":"不揮発性メモリに関連する情報源調査","slug":"Infomation-about-the-persistent-memory","date":"2018-10-14T14:07:48.000Z","updated":"2024-08-27T15:34:26.023Z","comments":true,"path":"2018/10/14/Infomation-about-the-persistent-memory/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/14/Infomation-about-the-persistent-memory/","excerpt":"","text":"まとまった情報が得られる情報源 日本国内ニュースの例 着目したカンファレンス、ペーパー USENIX Storage Developer Conference Persistent Memory Summit 2019 In-Memory Computing Summit IMC Summit Europe 2018 IMC Summit San Francisco 2018 ユースケース 書籍 Storage Developer Conference In-Memory Computing Summit 2016 他者の考察 Oracle 個人？ ミドルウェア関連 OS関連 まとまった情報が得られる情報源 http://pmem.io/ 引用: This site is dedicated to persistent memory programming. https://docs.pmem.io/ 引用: Background information, getting started guide, etc https://www.snia.org/ 引用: The SNIA is a non-profit global organization dedicated to developing standards and education programs to advance storage and information technology. https://www.imcsummit.org/ 引用: The only industry-wide events, the In-Memory Computing Summits focus on IMC-related technologies and solutions. https://www.usenix.org/ 引用: Since 1975, USENIX, the Advanced Computing Systems Association, has brought together the community of engineers, system administrators, scientists, and technicians working on the cutting edge of the computing world. 日本国内ニュースの例 https://pc.watch.impress.co.jp/docs/news/1122322.html タイトル: Intel、DRAMを置き換える3D XPoint NVDIMMを年内投入 https://pc.watch.impress.co.jp/docs/news/1137634.html タイトル: Intel、DRAMを超える高コスパメモリ「Optane DC Persistent Memory」を出荷。Googleが採用 https://tech.nikkeibp.co.jp/atcl/nxt/column/18/00001/00492/ タイトル: Persistent Memoryの能力引き出す、MSやSAPがインテルに協力 https://pc.watch.impress.co.jp/docs/news/1151589.html タイトル: Micron、容量32GBのサーバー向けNVDIMM http://jbpress.ismedia.jp/articles/-/55099 タイトル: 東芝メモリに期待！ NAND高密度化のイノベーション https://pc.watch.impress.co.jp/docs/column/semicon/1160009.html タイトル: 最先端マイコン/SoC向けで復活する相変化メモリ http://techtarget.itmedia.co.jp/tt/news/1812/11/news05.html タイトル: 東芝メモリ、YMTC、SK Hynixが語る、NANDフラッシュメモリの最新テクノロジ https://pc.watch.impress.co.jp/docs/news/event/1157164.html タイトル: 3次元クロスポイント構造で128Gbitの大容量不揮発性メモリをSK Hynixが開発 着目したカンファレンス、ペーパー いくつかの項目については、ポイントとキーワードを記載する。 USENIX Google schalarによる検索 等を利用して確認したいくつかの記事からピックアップする Intel Andy Rudoffによるプログラミングモデルへの影響考察 本文書ではDIMMなどシステムメモリバスに接続する不揮発性メモリを対象とする なお、アクセスレイテンシがナノ秒オーダであることを前提とする p.2にメモリアクセスのモデル図 CPUキャッシュを永続化領域に含むこともバッテリを積めば可能だが、それはいったんベンダの対応に任せる （カーネルを経ずに）ユーザスペースでのフラッシュにより、永続化を行うことを「Optimized Flush」という。 ただしOptimized Flushは安全な時にのみ使われるべき LinuxはDevice-DAXを提供。ファイルシステムを経由せずにアプリケーションが直接 不揮発性メモリを開ける libpmemがいつOptimized Flushを使えるかのAPIを提供している ★要確認 いくつかの課題 アトミックな書き込み→読み出しについて（p.38） インテルの場合8バイトまでならアトミックに扱える 8バイトを超えるものはソフトウェアで担保必要 PM-Aware File Systemを用いる前提のため、スペースの管理が課題になる（p.38） さらにロケーションに対して非依存であることもチャレンジ要素の一つ。 解決方法の見込みはいくつか存在（p.38） 上記「いくつかの課題」の解決をもくろむライブラリ：NVML by Intel。 libpmem: Basic Persistence Support libpmemobj: General-Purpose Allocations and Transactions libpmemblk and libpmemlog: Support for Specific Use Cases p.39にはlibpmemobj使用に関する簡単なC++の実装例 NoveLSM LSM(Log-structured Merge Tree)-based KVSにおける不揮発性メモリの活用 以下のオーバヘッドの改善を試みた シリアライズ、デシリアライズ コンパクション ロギング 並列読み出しの欠如 まずはLevelDBを改造して取り組んだ （補足）ユースケースに関する議論はない Caching or Not: Rethinking Virtual File System for Non-Volatile Main Memory A persistent lock-free queue for non-volatile memory LAWN: Boosting the performance of NVMM File System through Reducing Write Amplification Persistent Octrees for Parallel Mesh Refinement Through Non-Volatile Byte-Addressable Memory Memory and Storage System Design with Novolatile Memory Technologies 概要：2015年にリリースされた招待論文であり、その当時の革新的なメモリとストレージのデザインを紹介するもの 古い論文ではあるが、複数種類の不揮発性メモリに関する調査を実施したものであり、参考にはなる。 ★本記述は削除推奨 本論文で取り扱う不揮発性メモリの種類：STT-MRAM、PCM、ReRAM 不揮発性メモリの注意点は下記の通り 耐久性、性能特性 期待される役割は下記の通り プロセッサキャッシュ、メインメモリ、ストレージ さらに不揮発性メモリならではの使い方（MPPのチェックポイント置き場、など）も考えられる SRAMとDRAMのスケーラビリティの限界（サイズアップ困難、消費電力（漏れ電力？）増大）に際したNVMのメリットを説明あり 各アプローチが以下の各課題をどう解こうとしているかの解説。キーワードは以下の通り スケーラビリティと効率 NVMの効果を最大化するためのリ・デザイン STT-MRAM、PCM、ReRAMの特徴の紹介 2.4節には各不揮発性メモリの比較 不揮発性メモリの利点（従来のメモリと比べて…）p.4 PCMはスケールする STT-MRAM、PCM、ReRAMは密度を高められやすい リフレッシュ動作不要のため漏れ電力の面で有利。待機電力も低い。 欠点 p.4 3種類ともレイテンシが大きい。また書き込みの消費電力が大きい。（旧来のメモリと比べて） PCM、ReRAMは耐久性が低い 3章には製品の傾向が記載されている。 いくつかのデザインパターン ★要相談：という表現でよいか？ on-chipとoff-chip on-chip: CMOSと不揮発性メモリを異なるダイに載せ、ダイをつなげられる off-chip: いわゆるDIMMとして インダストリではMicron、AMD、SK Hynixあたりが主に開発を手掛けていたことが言及されている プロセッサキャッシュとして不揮発性メモリを用いるときの解説 SRAMはプロセッサキャッシュとして取り扱われる STT-MRAMはキャッシュに適していると考えられる不揮発性メモリ。L2、L3キャッシュそれぞれに 用いてみる研究が行われ、一定の成果（性能、消費電力（漏れ電力？））が出ている、と考察 SRAMよりも消費電力（漏れ電力？）小さく、容量を大きくできる、というところがポイントである 不揮発性メモリの高いレイテンシを補うため、SRAMとのハイブリッド構成も研究されている。 書き込み側に低レイテンシなSRAM、読み出し側に低消費電力（漏れ電力）な不揮発性メモリ 研究においては10-16%程度の性能改善（対SRAMのみ構成）が見られたと言及 CPUのプロセッサコアが多い場合、プロセッサコアとオフチップメモリ間のバンド幅が ボトルネックになりがちである。これに対し、ハイブリッドアーキテクチャを用い、 バンド幅の使い方を最適化する研究がある 最大58%の性能改善があったと言及 PCMやReRAMは耐久性が低い。このためキャッシュに用いづらい。書き込み頻度を 抑える研究がある メインメモリとして不揮発性メモリを用いるときの解説 メインメモリとして不揮発性メモリを用いる場合、期待するのはサイズの大きさと漏れ電力の小ささ。 * ただ、PCMをそのまま使うと旧来のDRAMと比べて1.6倍遅く、2.2倍消費電力が大きい（書き込みの消費電力が大きな要因） そこでいくつかの工夫を施す研究が存在している 複数のバッファに分け、必要な部分だけ更新する手法 DRAMと組み合わせたPCM 3倍は早くなった、との言及 プロセッサとメインメモリを同じチップに3Dスタックする手法 この手法は旧来のDRAMには難しい。電力と温度の関係で。 旧来のDRAMと比べ、65%しか電力消費しないことを示した そのほか、PCM、ReRAMの耐久性の低さに対する対策もいくつか研究されている ストレージとして不揮発性メモリを用いる観点の議論 PCMの密度はディスクに匹敵するほど高くなる、と予測する研究もある。 NANDフラッシュ型のSSDの「ログリージョン」に不揮発性メモリを用いるというアイデアもある。 不揮発性メモリ特有のデザイン MPPのチェックポイント先としての利用 チェックポイントには25%程度のフットプリントになるのだが、これをPCMにすることにより、 4%に抑えることができた、というもの 同様にチェックポイントを置く場所として活用するが、仮想メモリとしてアプリケーションに 見せる、という方式を取った研究もある。NVMをRAMディスクとして使う方式と比べ、 チェックポイント時間を15%減らせた、という結果が得られた GPGPUにおいて消費電力の面で効率的なメモリヒエラルキーを実現できる ある研究によると、STT-MRAMを用いた場合、4倍の密度を実現しながら、 3.5倍の消費電力（？）の低減に寄与した Persistent Memoryとして不揮発性メモリを用いる プロセッサの揮発性キャッシュと組みあわせたとき、Persistent Memory Systemを 実現するのは実は簡単ではない ライトバックキャッシュやメモリコントローラによるリクエストのリオーダリングが生じるため。 例えばリンクドリストにノードを追加するとき、先にノードのポインタを不揮発性メモリに書き、 その直後にクラッシュした場合、キャッシュ内にのみ存在したノードの値が失われる。 このため復旧が困難になる ここではPersistent Memory Systemが満たすべき特性について記載。 Durability、Atomicity、Consistency 旧来からあるファイルシステムやDBMSを改造 WAL、Copy-on-write（COW） B-treeにCOWの仕組みを足した研究もある ただWALとCOWはオーバヘッドがある。 そこでプロセッサのキャッシュとメインメモリのヒエラルキーを使い、 Atomicityを実現した研究もある。 ★要確認 ポイントはプロセッサで生じたリクエスト通りの並びで処理すること ただメモリコントローラやキャッシュの仕組みが性能改善のために リオーダリングする可能性があること そこで多くのPersistent Memory Systemは、キャッシュをライトスルーすることで、 順序を保証しようとする。つまり、プロセッサキャッシュをバイパスする。 また合わせてフラッシュ、メモリフェンス、msyncオペレーションを用いる ただ、プロセッサキャッシュのバイパスは、データがメインメモリに到達するのを 待つしかなくなる またフラッシュやメモリフェンスはバーストを引き起こすし、キャッシュのフラッシュは 他のアプリケーションのワーキングメモリをはじき出す可能性がある BPFSというファイルシステムは、エポックバリアを設けた。ただしデータ損失の可能性がある p.8にいくつかの手法の比較表が載っている。 BPFS、Mnemosyne、… In-place、Logging、… Storage Developer Conference Preparing Applications for Persistent Memory Persistent Memory Summit 2019 https://www.snia.org/pm-summit 2019/1/24 @ Santa Clara, CA Keynote: Realizing the Next Generation of Exabyte-scale Persistent Memory Centric Architectures and Memory Fabrics Zvonimir Bandic, Senior Director, Research and Development Engineering, Western Digital Corporation Big Data and Fast Data。アーキテクチャのダイバーシティ。（フォームファクタの差） メモリファブリックのもたらす低レイテンシ（1.6 - 1.8 us） メモリセントリックアーキテクチャ。つまり巨大なメモリに対し、多数の計算デバイスを接続するモデル p. 11 Persistent memoryのスケールアウト さらなる低レイテンシ（500ns）のためにはプロトコルのイノベーションが必要 Persistent Memory Programming: The Current State and Future Direction Andy Rudoff, Member, SNIA NVM Programming Technical Work Group and Persistent Memory SW Architect, Intel Corporation NVDIMMドライバを活用したプログラミングモデル 標準ファイルシステムAPIを経由する場合、必ずファイルを開かないとならない。 DAXであればPMアウェアなファイルシステムAPIをスキップできる https://pmem.io/pmdk/ https://github.com/pmem/pcj PMDKのlibpmemobjを内部的に用いたJavaバインド Persistent Memory over Fabrics (PMoF) ユースケース HA用のレプリケーション リモートのPersistent Memoryを扱えるようになるとして、何を考えないといけないか Streamline the API ... メモリオペレーションのように見せないといけない 非同期的な管理 永続化を明示的に管理する方法が必要 既存のファブリックのプロトコルに対し、同期のタイミングを作りこむ必要がある FAST DATA Linux Persistent Memory Support - \"Ask Me Anything\" Hype to Reality, a Panel Discussion with Leaders of Real World Persistent Memory Applications Quick and Painless System Performance Acceleration Using NVDIMMs p.2に2018時点でのPersistent Memoryの対応状況が示されている まだまだ現実のワークロードで受け入れられる状態ではない どうやったら適用が拡大するか？ ブロックエミュレーション 複数層のアーキテクチャ p.7にブロックIOのエミュレーションのイメージ図 Block Translation層が入っている まとめ ブロックエミュレーションとTieringによるNVDIMM利用は、アプリケーション変更せずに 性能面での恩恵を受けられる より多くの適用につながるだろう、という考察 Future Media、Future Media2 カーボンナノチューブによるメモリ: NRAM p.3に特長が書かれている 読み書きはDRAM並み、スケーラブル、耐久性高い、漏れ電力も小さい（もしくはない） 3Dスタックも可能との考察 p.6にNVDIMM-Nとの比較 Analysts Weigh In on Persistent Memory いくつかのセッションで構成されていた。 How Persistent Memory Will Succeed 3D NANDからの教訓 p.8 結局DRAMと勝負になるくらい安くないと… 「永続性」を使うためにはソフトウェアサポートが必要 そこでSNIAの出番 ただ、コストと性能の観点から用いる、というのが一般的 PCM ... 3D XPoint（Intel、Micron） MRAM ... TSMC、サムスン、東京エレクトロン FRAM ReRAM ... TSMC、UMC p.14に各種の比較が載っている 補足：PCM（3D XPoint）の実際のレイテンシはもっと大きいのでは？と思ったが… ★要確認 Analyst Perspective - IT Client 容量単価だけを気にするのであれば、テープを使うべき。 Persistent Memory Dinamics 2008年あたりにStorage Class MemoryがHDDを置き換えると話題に上ったものの、 NAND型フラッシュSSDが流行り、SCMに対する市場の需要はいったん遅れた 現在、バイトアドレッサビリティがSCMに足された NVRAM 2020年にはモバイルデバイスのDRAM、NANDを置き換えていくのではないか トレンド コグニティブコンピューティング 大きなPersistent Memory AIアルゴリズムはその計算時間を減らせるだろう Processor In Memory In-Memory Computing Summit https://www.imcsummit.org/ IMC Summit Europe 2018 IN-MEMORY COMPUTING - UNLOCKING THE DIGITAL FUTURE まとめ ユースケースの具体例は載っていない IMC（インメモリコンピューティング）の技術が2019年 - 2022年にかけて段階的に世の中に 受け入れられていく流れを説明 p.10にハーバードビジネススクールによるDigital Laggards、Digital LeadersのGross Marginなどに関する分析結果。 p.13にいわゆるHTAPにおけるビジネス分析、ビジネストランザクション、自動化された意思決定の関係 p.14に旧来の方式（Hadoop + RDBMS + ETL + ML/DL Engine + App）とIMC方式（IMC Platform + App）の比較 とてもシンプルになる、という考察 p.15：2021年にはインメモリDBMS、インメモリグリッドゲイン、ストリーム（分析）処理プラットフォーム、 他のIMC技術はインメモリコンピューティングに収れんされていく。ガートナーによる意見。 2019年 - 2022年にかけてIMCに関する技術が世の中に受け入れられていく予想が記載されている。 Embracing the service consumption shift in banking まとめ：INGのIMC（インメモリコンピューティング）についての概要 p.10 - 12に16Mトランザクション/日をさばくアーキテクチャ（SEPA DD）の説明 一度Kafkaに入れて、IMC基盤に入れる p.16にGridGain as a Serviceの説明。GridGainおよびAppをコンテナに入れてデプロイする。 Memory-Centric Architecture - a New Approach to Distributed Systems IMCの進化の流れを説明 Distributed Cache In-Memory Data Grids In-Memory Databases Distributed Databases Memory-Centric Databases 最初のローカルキャッシング Distributed Cache シェアードナッシング。シンプルなクライアント。 Memcached、Redis、AWS ElastiCache p.7に欠点のリスト In-Memory Data Grids HazelCast、GigaSpaces、Apache Ignite p.11に欠点のリスト In-Memory Databases In-Memory Data Grids for SQLとも考えられるようだ VoltDB、SAP HANA p.15に欠点のリスト Distributed NoSQL Databases SQLではない。トランザクションに対応していない。IMCではない。 Distributed SQL Databases IMCではない。キーバリューAPIの欠如。イベントノーティフィケーションの欠如。 GridGain Memory-Centric Architecture Memory Centric Storageがそれより下の永続化の層を隠蔽しているように見える p.21に特長をまとめた図がある Ignite Native Persistenceの利用（そのほかの永続化の仕組みも利用可能なようだ） HTAP IMC Summit San Francisco 2018 https://www.imcsummit.org/2018/us/ ユースケース 書籍 Green Computing with Emerging Memory: Low-Power Computation for Social Innovation Storage Developer Conference The Impact of the NVM Programming Model p.20、21あたり In-Memory Computing Summit 2016 NVDIMM - CHANGES ARE HERE SO WHAT'S NEXT? ★ 2016年時点での概観 p.6に特性による種類が掲載されている p.8にエコシステムの考え方 p.15にユースケース 他者の考察 Oracle クラウド時代におけるデータベースの歩みと急速な技術進化の方向性 Transforming Data Management 個人？ Persistent memory (Benoit Hudzia, Head of Operations - Non Exec Director - Senior Architect) Usecaseの画像 ミドルウェア関連 OS関連","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"NVM","slug":"Research/NVM","permalink":"https://dobachi.github.io/memo-blog/categories/Research/NVM/"}],"tags":[{"name":"Paper","slug":"Paper","permalink":"https://dobachi.github.io/memo-blog/tags/Paper/"},{"name":"NVM","slug":"NVM","permalink":"https://dobachi.github.io/memo-blog/tags/NVM/"}]},{"title":"HexoのIcarusテーマを設定する","slug":"configure-icarus","date":"2018-10-14T13:27:07.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/14/configure-icarus/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/14/configure-icarus/","excerpt":"","text":"設定ファイル 設定内容 設定ファイル themes/icarus/_config.yml.example にサンプルがあるので、 コピーして用いる。 12$ cp themes/icarus/_config.yml&#123;.example,&#125;$ vim themes/icarus/_config.yml 設定内容 menuにTags、Categoriesを足した logo画像を差し替えた themes/icarus/source/css/images 以下にlogo.pngがあるので差し替えた。 プロフィール（例：アバター画像など）を差し替えた アバター画像は. themes/icarus/source/css/images 以下にavatar.pngがあるので差し替えた。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"},{"name":"Icarus","slug":"Icarus","permalink":"https://dobachi.github.io/memo-blog/tags/Icarus/"}]},{"title":"ロゴを作る","slug":"create-logos","date":"2018-10-14T13:02:27.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/14/create-logos/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/14/create-logos/","excerpt":"","text":"参考 備考 参考 https://www.freelogodesign.org/ ★ https://www.logocrisp.com/logomaker/ 備考 スモールビジネス向けのロゴメーカーSaaSはいくつか存在するようだ。 基本的にはフリーとしているが、高解像度なものを取得しようとすると有料、というモデルか。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"},{"name":"Web","slug":"Web","permalink":"https://dobachi.github.io/memo-blog/tags/Web/"}]},{"title":"Hexoでicarusテーマを使う","slug":"hexo-icarus","date":"2018-10-14T12:22:08.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/14/hexo-icarus/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/14/hexo-icarus/","excerpt":"","text":"参考 基本な手順 注意点 参考 HEXOテーマ「icarus」導入のあれこれ 基本な手順 基本的な手順は、 HEXOテーマ「icarus」導入のあれこれ に記載されている通りで問題ない。 注意点 そのままだと普通の固定ページになるので、デモのようなページにしたい場合、 themes/icarus/source/categoriesにできているindex.mdを、themesと同じ階層にあるsourceフォルダ内のcategoriesフォルダに themes/icarus/source/tagsにできているindex.mdを、themesと同じ階層にあるsourceフォルダ内のtagsフォルダにコピー（元のindex.mdと差し替え）します。 という記述があったが、手元の環境ではthemes/icarus/_source/categories、themes/icarus/_source/tagsのように、sourceの前にアンダーバーが必要だった。 つまり、 themes/icarus/_source/categories -&gt; source/categories themes/icarus/_source/tags -&gt; source/tags のように、ディレクトリごとコピーしたところ、それぞれCategories、Tagsページが表示されるようになった。 ただ、Categoryはどう使うべきかいまだ決めかねている。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"}]},{"title":"Hexoを複数の環境で使う","slug":"Use-Hexo-on-several-environments","date":"2018-10-13T15:33:12.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/14/Use-Hexo-on-several-environments/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/14/Use-Hexo-on-several-environments/","excerpt":"","text":"Hexoを複数の環境で使おうとすると、 .deploy_git 以下の.git/configをそれぞれ再現する必要がある。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"}]},{"title":"UbuntuでビデオDVDを作る","slug":"Creating-video-DVD-on-Ubuntu","date":"2018-10-13T14:42:59.000Z","updated":"2024-08-27T15:34:26.019Z","comments":true,"path":"2018/10/13/Creating-video-DVD-on-Ubuntu/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/13/Creating-video-DVD-on-Ubuntu/","excerpt":"","text":"試したもの BraseroとDevede NGについて DVDドライブのリージョンコード確認 DVDメディアのリージョンコード確認 DVDStylerインストール 動画の編集 試したもの Brasero Devede NG DVDStyler ... これで作ったら再生してくれた BraseroとDevede NGについて 試したものの、リージョンマスクの問題でDVDプレイヤで表示できなかった。 リージョンマスクがゼロ、つまりすべてのリージョンで使えるような設定のようだったが…。 DVDドライブのリージョンコード確認 regionsetコマンドを用いれば良い。 DVDメディアのリージョンコード確認 vlcをコマンドラインから起動し、DVDを開くとコンソールに表示される。 DVDStylerインストール ubuntuhandbookのインストールページ を参考にインストールした。 サードパーティのPPAを加えないとならない部分が気がかりではある。 動画の編集 動画の編集には、Shotcutが用いられそうだった。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Video processing","slug":"Home-server/Video-processing","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Video-processing/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"}]},{"title":"UbuntuでJISとUSの両キーボードを使う","slug":"Use-US-and-JIS-keyboard-on-Ubuntu","date":"2018-10-13T13:10:46.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/13/Use-US-and-JIS-keyboard-on-Ubuntu/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/13/Use-US-and-JIS-keyboard-on-Ubuntu/","excerpt":"","text":"参考 変換、無変換キーのIMEオン・オフ割当 USキーボードを使う 参考 「変換」「無変換」キーの設定 USキーボードの利用 変換、無変換キーのIMEオン・オフ割当 「変換」「無変換」キーの設定 にも記載がある。 通知バー内のMozcのアイコンを右クリックし、 Mozcツールの設定を開き、「キー設定」から「編集」を選ぶ。 以下のようなエントリを追加する。 直接入力：変換：IMEを有効化 入力なし：無変換：IMEを無効化 ポイントは、IMEを有効化する機能を発動するは「入力なし」ではなく、「直接入力」時であること。 USキーボードを使う USキーボードの利用 にも記載があるとおり、 通知バーのMozcアイコンを右クリックし、「設定」から「入力メソッドの設定」を開く。 エントリにMozcがあるので、設定ボタンを押し、キーボードレイアウトを「英語」にする。 逆にこれを日本語に戻すと、JISキーボードとして認識される。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Hardware","slug":"Home-server/Hardware","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Hardware/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://dobachi.github.io/memo-blog/tags/Ubuntu/"},{"name":"Keyboard","slug":"Keyboard","permalink":"https://dobachi.github.io/memo-blog/tags/Keyboard/"}]},{"title":"XRDPでMATE Desktopを標準で用いる","slug":"xrdp-mate","date":"2018-10-06T13:36:35.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/06/xrdp-mate/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/06/xrdp-mate/","excerpt":"","text":"参考 基本的な手順 全ユーザで有効 ユーザごと 参考 http://www.mikitechnica.com/39-xrdp-mate.html 基本的な手順 全ユーザで有効 1$ sudo echo &quot;PREFERRED=/usr/bin/mate-session&quot; &gt; /etc/sysconfig/desktop vimで編集しても良い。 ユーザごと 12$ echo &quot;/usr/bin/mate-session&quot; &gt; ~/.Xclients$ chmod +x ~/.Xclients","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"Remote desktop","slug":"Home-server/Remote-desktop","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/Remote-desktop/"}],"tags":[{"name":"CentOS7","slug":"CentOS7","permalink":"https://dobachi.github.io/memo-blog/tags/CentOS7/"},{"name":"MATE Desktop","slug":"MATE-Desktop","permalink":"https://dobachi.github.io/memo-blog/tags/MATE-Desktop/"}]},{"title":"CentOS7でテキトーにSambaで共有するディレクトリを作成する","slug":"samba-centos7-for-share-directories","date":"2018-10-01T12:02:34.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/10/01/samba-centos7-for-share-directories/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/10/01/samba-centos7-for-share-directories/","excerpt":"","text":"参考情報 基本的な流れ 参考情報 CentOS 7とsambaでWindows用ファイルサーバーを設定する。古いサーバーの有効利用に最適 Mac と Windows からパスワードなしで Samba サーバーにアクセスする 基本的な流れ CentOS 7とsambaでWindows用ファイルサーバーを設定する。古いサーバーの有効利用に最適 の流れで 基本的に問題なかったのですが、上記ブログでは匿名ユーザの利用を前提としていませんでした。 そこで、 Mac と Windows からパスワードなしで Samba サーバーにアクセスする を参考に匿名ユーザでのアクセスを許可しました。","categories":[{"name":"Home server","slug":"Home-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/"},{"name":"File server","slug":"Home-server/File-server","permalink":"https://dobachi.github.io/memo-blog/categories/Home-server/File-server/"}],"tags":[{"name":"CentOS7","slug":"CentOS7","permalink":"https://dobachi.github.io/memo-blog/tags/CentOS7/"},{"name":"Samba","slug":"Samba","permalink":"https://dobachi.github.io/memo-blog/tags/Samba/"}]},{"title":"AWS Kinesisのコンシューマのサンプルを探す","slug":"kinesis-consumer-samples","date":"2018-09-19T15:52:29.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/09/20/kinesis-consumer-samples/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/09/20/kinesis-consumer-samples/","excerpt":"","text":"公式ドキュメント Kinesis Clinet Library Javaでの実装例 C言語のバインディングはないのか？ 公式ドキュメント Amazon Kinesis Data Streams コンシューマーの開発 を見ると、 Kinesis Client Library 1.x を使う場合 Kinesis Data Streams API および AWS SDK for Javaを使う場合 の2種類があることが分かりました。 一旦、 Kinesis Client Library 1.x を使う場合 を見てみることにします。 Kinesis Clinet Library Kinesis Client Library 1.x を使う場合 によると、以下の言語に対応しているように見えます。 Java Node.js .NET Python Ruby Javaでの実装例 Java での Kinesis Client Library コンシューマーの開発 を見ると、Javaライブラリの使い方が載っていました。 Kinesisから読んでどこかに書き出すときに使えそうです。 C言語のバインディングはないのか？ 少々気になるのは、c言語のバインディングがないかもしれない、ことです。 Amazon KCL support for other languages を参照すると、MultiLangDaemonなるものを立てて使え、とありました。 他にも、 Python での Kinesis Client Library コンシューマーの開発 においても、 KCL は Java ライブラリです。Java 以外の言語のサポートは、MultiLangDaemon という多言語インターフェイスを使用して提供されます。 と記載されています。 いずれにせよ、Java以外の開発では一癖ありそうな予感がしました。","categories":[{"name":"Research","slug":"Research","permalink":"https://dobachi.github.io/memo-blog/categories/Research/"},{"name":"AWS","slug":"Research/AWS","permalink":"https://dobachi.github.io/memo-blog/categories/Research/AWS/"}],"tags":[{"name":"AWS","slug":"AWS","permalink":"https://dobachi.github.io/memo-blog/tags/AWS/"},{"name":"Kinesis","slug":"Kinesis","permalink":"https://dobachi.github.io/memo-blog/tags/Kinesis/"}]},{"title":"Hexoブログを作る","slug":"create-hexo-blog","date":"2018-09-19T15:30:33.000Z","updated":"2024-08-27T15:34:26.031Z","comments":true,"path":"2018/09/20/create-hexo-blog/","link":"","permalink":"https://dobachi.github.io/memo-blog/2018/09/20/create-hexo-blog/","excerpt":"","text":"参考にした情報 ブログ構築の基本的な流れ type branch ブログの原稿や元データの保存について あまり考えずに使えそうなブログ生成ツールということで、Hexoを選んでみました。 参考にした情報 Hexo https://liginc.co.jp/web/programming/server/104594 https://liginc.co.jp/web/programming/node-js/85318 http://paki.github.io/2015/05/26/hexo%E3%81%A7%E3%83%96%E3%83%AD%E3%82%B0%E3%81%A4%E3%81%8F%E3%81%A3%E3%81%9F/ https://hexo.io/ GitHubページの作り方 https://ja.nuxtjs.org/faq/github-pages/ ブログ構築の基本的な流れ https://liginc.co.jp/web/programming/server/104594 に記載の流れで問題ありませんでしたが、 以下の点だけ少々異なる手順で実行しました。 type _config.yml の中で上記のブログでは以下のように設定するようになっています。 1234567・・・(省略)・・・# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: github repo: git@github.com:n0bisuke/n0bisuke.github.io.git branch: master このとき、自分がインストールした 12hexo: 3.7.1hexo-cli: 1.1.0 のバージョンのHexoではgithubではなく、gitを用いる必要がありました。 これについては、 https://liginc.co.jp/web/programming/node-js/85318 にも記載がありました。 本ブログに記載の通り、typeをgitにした後、 1$ npm install hexo-deployer-git --save と、hexo-deployer-gitをインストールしました。 branch 私はGitHubアカウントのプロジェクトページにデプロイしたかったので、masterブランチではなく、 gh-pagesブランチを指定するようにしました。 ブログの原稿や元データの保存について 公開されるデータは、 .deploy_git に保存されているように見えます。 またこれは生成されたデータであり、gh-pagesにデプロイされるデータも、この生成済みデータだけのように見えます。 ブログ原稿をどこでも記載できるようにするため、 hexo init したプロジェクトそのものをGitHubのプライベートレポジトリにも保存するようにしました。","categories":[{"name":"Knowledge Management","slug":"Knowledge-Management","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/"},{"name":"Hexo","slug":"Knowledge-Management/Hexo","permalink":"https://dobachi.github.io/memo-blog/categories/Knowledge-Management/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dobachi.github.io/memo-blog/tags/Hexo/"}]}]}